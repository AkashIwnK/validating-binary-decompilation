\section{Related Work}\label{sec:RW}

Given the importance of establishing the faithfulness of the binary lifters,
      there exists a couple of efforts towards that direction, which we will be
      elaborating on next.

\section{Recent Advances in Validation of Decompilation}\label{sec:recent-advances}
All the previous approaches can be broadly categorized to be based on (1)
  Testing, or (2) Formal Methods.

\subsection{Testing based Approaches}
This approach is similar to black-box testing in software engineering. Most
notable work include Martignoni et
al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010,Martignoni:ASPLOS2012} and
Chen \etal~\cite{CLSS2015}.


Martignoni et al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010} proposes
hardware-cosimulation based testing on QEMU~\cite{QEMU:USENIX05} and
Bochs~\cite{Bochs1996}.  Specifically, they compared the state between actual
CPU and  IA-32 CPU emulator (under test) after executing randomly selected
test-inputs on randomly chosen instructions  to discover any semantic
deviations. Although, a simple and scalable approach, it's effectiveness is
limited because many semantics bugs in binary lifters are triggered upon a
specific input and exercising all such corner inputs, using randomly generated
test-cases, is impractical.

%%

In general, such testing based approaches are unsuitable for validating whole
program (or even basic block) translations because even a correctly translated
program may not always produce exactly the same output as the original program
due to differences in the modeling of the architectural states in the
translated program (or basic block). Although, it is possible  to learn the
idioms used by the translator to model such architectural states and identify
the corresponding states for comparison, but still these approaches might not
detect some intermediate mistranslated instructions which are course corrected
at the end. As an example, suppose  a register is assigned values twice in a
program and the first assignment is mistranslated but the second assignment
statement is translated correctly. Comparing the architecture states at the end
of the program (or basic block) may not discover the mis-translation.

%%

Chen \etal~\cite{CLSS2015} proposed validating the static binary translator
LLBT~\cite{LLBT2012} and the hybrid binary translator~\cite{LLVMDBT2012},
  re-targeting ARM programs to x86 programs. First, an ARM program is
  translated offline to x86 program. Next, the translated x86 binary is
  executed  directly on a x86 system while the original ARM binary runs on the
  QEMU emulator. During run time, both the ARM binary and the translated x86
  binary produce a sequence of  architecture states, which are compared at the
  granularity of single instruction. The validator is
  evaluated using the ARM code compiled from EEMBC 1.1 benchmark. Like previous 
  approach, the validation of single instruction's translation is 
based on testing and hence shares the same limitation of not being exhaustive, 
which can be alleviated with  a more rigorous formal equivalence checking.
%%

Martignoni \etal~\cite{Martignoni:ASPLOS2012} applied symbolic execution on a 
\emph{Hi-Fi} (``faithful and more complete in terms of IA-32 ISA'') 
emulator\cite{Bochs1996}'s binary implementation of an instruction to generate 
high-fidelity test-inputs to validate a ``buggier and less complete'' Lo-Fi 
emulator~\cite{QEMU:USENIX05},
by executing a binary instruction twice, once on a real hardware and next on
the Lo-Fi emulator, and the output states are matched.
%%

For each path explored during the symbolic execution of an instruction's 
implementation, the underlying decision procedure computes an assignment of 
bits to the symbolic inputs states that would cause the emulator to execute 
that path: such assignments together covers all behavior of that instruction.
However, if the goal is to validate the translation of a binary program and if 
we have 
such test-inputs for  all the constituent instructions of the
program, still it will not help generate a high coverage test-inputs for that 
program because  such test-inputs are generated in an isolated 
context and may not be even satisfiable in the whole program context.

%%

However, the work~\cite{Martignoni:ASPLOS2012} does not aim to validate the 
translation of \ISA programs, 
which is one of out primary contributions. 
%%
%Note that, even though Martignoni \etal~\cite{Martignoni:ASPLOS2012}
%symbolically explored the test-cases which is supposed to cover all the paths
%of a given instruction's implementation, but being a differential testing-based
%approach, the faithfulness depends directly on  the fidelity of the Hi-Fi
%emulator. A wrong implementation or omission of a particular switch case of
%instruction semantics in the Hi-Fi, will lead to test-cases insufficient to
%explore all the paths and hence find bugs in the Low-Fi emulator. Also, the 
%method can capture  deviations in the behavior of only those
%instructions which are implemented in both the emulators.\footnote{We
%  note that our proposed semantics-driven \tv approach shares similar
%    assumptions about the faithfulness of the semantics.}.
%%
%Moreover, the symbolic execution of an instruction's implementation in the
%Hi-Fi emulator is achieved using an X86 interpreter FuzzBALL. A bug in the
%interpreter will affect the generation of high-fidelity test cases for a
%particular instruction, leading to incomplete coverage of that instruction's
%implementation in Low-Fi emulator.
%%
%However, their approach does not consider the floating point instruction
%  because the employed symbolic execution engine (FuzzBALL) does not support
%    it.

%    Schwartz \etal~\cite{Schwartz:2013} proposed control flow structure recovery by
%    employing semantics preventing schema and tested their binary to C decompiler,
%    Phoenix, which is based on BAP~\cite{BAP:CAV11}, on a set of 107 real
%    world programs from GNU coreutils. Along similar lines,
%    %
%    Yakdan \etal~\cite{Yakdan2015NDSS} presented a decompiler, DREAM, to offer a
%    goto-free output. DREAM uses a novel pattern independent control-flow
%    structuring algorithm that can recover all control constructs in binary
%    programs and produce structured decompiled code without any goto statement. The
%    correctness of our algorithms is demonstrated using the GNU coreutils suite of
%    utilities as a benchmark.
% 
%    Andriesse \etal~\cite{nucleus2017EuroSP} proposes a function detection
%    algorithm, Nucleus, for binaries. The algorithm does not require function
%    signature information or any learning phase. They evaluated Nucleus on a
%    diverse set of $476$ C and C++ binaries, compiled with gcc, clang and Visual
%    Studio for x86 and x64, at optimization levels O0--O3.
% 
%    Martignoni et al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010} attempted
%    to leverage differential testing on QEMU~\cite{QEMU:USENIX05} and
%    Bochs~\cite{Bochs1996}. Particularly, they compared the state between a
%    physical and an emulated CPU after executing randomly chosen instructions on
%    both to discover any semantic deviations. Although their technique can be
%    applied to testing binary lifters, it is fundamentally limited because its
%    effectiveness largely depends on randomly generated test cases. Typically,
%    semantic bugs in binary lifters are triggered only with specific
%    operand values. Therefore, a random test case generation does not
%    help much in finding such bugs.

\subsection{Formal Methods based Approaches}
Followings are the effort to establish strong guarantees for binary 
translations using formal methods.

MeanDiff~\cite{ASE2017} proposed an N-version IR testing to validate three
binary lifters, BAP~\cite{BAP:CAV11}, BINSEC~\cite{BINSEC2011}, and
PyVEX~\cite{PYVEX} by comparing their translation of a single binary
instruction to BIL, DBA, and VEX IRs respectively. The individual IRs are then 
converted to common IR representations which are then symbolically executed to 
generate symbolic summaries for comparison using a SAT solver.  The above 
approach shares the same fundamental limitations of any differential testing 
techniques. For example, if all the binary lifters are in  sync
on the behavior of a particular instruction, we get more confidence that the 
instruction semantics is correctly encoded in all of them, but we cannot rule 
out the possibility of all being incorrect. Moreover, the work~\cite{ASE2017} 
is primarily focused on validating single 
instruction and the problem 
of handling multiple instruction is left as future work.  

%%

As candidly mentioned in the paper~\cite{ASE2017}, one of the motivations for 
relying on differential testing  is that there were no 
formal specification of \ISA ISA at the time of 
writing the paper. Whereas we do not have such limitation because of the formal 
\ISA ISA specification~\cite{DasguptaAdve:PLDI19} made public recently.
Empowered with that, we can build a symbolic formula that encodes all execution 
paths of an IR instance lifted from a single machine instruction and then  
check if the symbolic formula matches the formal specification of the 
instruction, which is exactly what we did in this work.

%%
 
reopt-vcg~\cite{Galois:SPISA19} is closest to ours in terms of its goal of
proving that an translated LLVM program is a refinement of an x86 program.  The
translation verifier, reopt-vcg, takes a binary executable, LLVM bitcode file,
            and annotations that relate LLVM functions to addresses in the
            executable, and generates proof obligations in the SMT-LIB to
            verify each basic block independently. The annotations are used to
            identify basic block and variable correspondence, define the
            appropriate pre-conditions for the block, and argument mapping to
            machine code registers. The verifier is tested on several small
            programs with manually generated annotations. First, the approach
            is aimed for a particular decompiler~\cite{reopt} and generalizing
            it to others needs substantial instrumentation to correctly emit the
            annotations. On the other hand, our approach is decompiler agnostic
            in a sense that it does not need any such instrumentation  and is
            able to find variable and basic block correspondence on the fly,
            without using any heavy-weight equivalence checking. 
            \todo{Scalability issue: awaiting John reply if they are doing any 
            symbolic execution} 

%%

%\subsection{Using Machine Learning} Another recent work by Schulte
%\etal~\cite{eschulte2018bed} proposed Byte-Equivalent Decompilation (BED) which
%leveraged a genetic optimization algorithm to infer C source code from a
%binary. Given a target binary and an initial population of C code as
%decompilation candidates, they  drive a genetic algorithm to improve the
%initial candidates, driving them closer (using compilation to binary) to
%byte-equivalence w.r.t the target binary. The byte equivalence  is simply the
%edit distance to the target binary. As hypothesized in the future work section
%of the paper~\cite{eschulte2018bed}, BED could be applied to LLVM IR instead of
%C to evolve lifting from machine code to LLVM IR and may work well due to the
%relative simplicity of LLVM IR as compared to the C. Being byte-equivalent, the
%generated LLVM IR will be the faithful evolution from the machine code.
%However, as shown in the paper, this approach worked moderately well for
%smallish binaries. For example, out of $22$ binaries under test, only $4$
%achieve full byte equivalence when the initial population is augmented with
%decompilation candidates from the HEX-RAYS~\cite{hexray} Decompiler. It is
%still an open problem to realized an end-end byte-equivalent binary to LLVM
%decompiler using purely genetic optimization algorithm.  \cmt{only $3$ achieve
%  full byte equivalence when the initial population does not include decompiler
%    seeds, and}


%\cmt{ 
%    Myreen et al.~\cite{Myreen:FMCAD:2008,Myreen:FMCAD:2012} proposed
%    ``decompilation into logic'' which, given some concrete machine code and a
%    model of an ISA, extracts logic functions or symbolic summaries which 
%    captures
%    the functional behavior of the machine code. The decompiler works on top of 
%    ISA
%    models for IA-32 \cite{Karl2003}, ARM~\cite{Fox2003} and
%    PowerPC~\cite{Leroy:2006}. Assuming that the ISA models are trusted, the
%    extracted functions can be used to prove properties of the original machine
%    code. However, the work has not been applied to validate the binary 
%    translation
%    to an IR.  A recent work by Roessle et al.~\cite{Roessle:CPP19} improves the
%    aforementioned idea  by including a subset of \ISA, derived mostly from
%    Strata~\cite{Heule2016a}, in the trust-base of ISA models.
%}
%
%
%%%
%
%%    

%\cmt{Myreen et al.~\cite{Myreen:FMCAD:2008,Myreen:FMCAD:2012} extracted
%    function-level symbolic summaries which indeed is a promising building block
%    towards establishing correctness of binary lifters.\cmt{, which, however,
%        has many additional challenges to deal with (Refer
%        Section~\ref{sec:challenges}). Moreover,} However, both Myreen et al.
%    and Roessle et al. have limited \ISA instruction set coverage, which
%    might restricts their application on many  real-world binaries.}
%
%\cmt{Being a differential testing method, only those instructions which are 
%    supported in all the translators can be validated with higher confidence 
%    than 
%    the ones which are not supported in one or more.
%    
%    Also, as MeanDiff is testing
%    multiple binary lifters together, hence it cannot be used to establish the
%    faithfulness in lifting the semantics of an instruction which is not
%    implemented in any one of them.
%    
%    In this approach of differential testing, whether a particular instruction 
%    is 
%    going to be validated depends on its availability in other translators, 
%    even if 
%    some translator as a much better instruction support. In our case, we are 
%    testing McSema against the most complete user level \ISA ISA
%}
%\cmt{MeanDiff neither
%    handle floating point operations, nor the instructions which does not 
%    manifest
%    their side-effects (like flag updates) explicitly.  Moreover, MeanDiff 
%    reports
%    a bug whenever a deviation is detected w.r.t the 
%    instruction-semantics-behavior
%    in at least two binary lifters. But even if all the binary lifters are in 
%    sync
%    on the behavior of a particular instruction, we cannot guarantee that all 
%    the
%    lifters are faithful in lifting that instruction, which is however, a 
%    general
%    limitation of differential testing based approach. Also, as MeanDiff is 
%    testing
%    multiple binary lifters together, hence it cannot be used to establish the
%    faithfulness in lifting the semantics of an instruction which is not
%    implemented in any one of them.
%}
