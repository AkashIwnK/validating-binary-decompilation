\section{Approach Overview}\label{sec:Approach}

In this section we provide a high-level overview of the two main components of
our approach, i.e., \siv and \plv. Before we begin, we first describe the
scope of our problem.

\textbf{Problem Scope.} Our techniques are generally applicable to verify
binary lifters from any ISA, e.g., x86, ARM, RISC-V, PowerPC, to an
intermediate representation, such as LLVM IR~\cite{LLVM:CGO04}, VEX
IR~\cite{Valgrind:ENTCS03} etc., as long as (a) formal semantics for both the
ISA
%
\footnote{Fortunately, this is no more a bottleneck as most
of the widely used ISAs are backed-up with formally specified semantics
(\ISA~\cite{DasguptaAdve:PLDI19,Goel:FMCAD14},
x86~\cite{TSL:TOPLAS13,Leroy:2009,cakeML19},
ARM~\cite{sail-popl2019,ARMSel4:09,ArmFox2003},
RISC-V~\cite{cakeML19,Leroy:2009}, CHERI-MIPS~\cite{sail-popl2019},
PowerPC~\cite{Leroy:2009,TSL:TOPLAS13}).}
%
and the target language is available, and (b) the target language can be
normalized to a canonical representation through a series of
semantic-preserving transformations.  Through the rest of this work, we fix
our discussion to lifting \ISA to \LLVM using the most mature, open-source
lifter McSema~\cite{McSema:Recon14} and our normalizer to be a subset of \LLVM
optimization passes.
%
LLVM, being an industry standard compiler IR,  many decompilation
projects~\cite{McSema:Recon14,Remill,FCD,reopt,llvm-mctoll} prefer to employ
LLVM as their lifted representation mainly because it enables various
``out-of-the-box'' analyses
and optimizations  which minimizes the effort in the post-lifting
decompilation tasks. Moreover, \LLVM is backed-up with its formal
semantic models~\cite{LLVMSEMA,Vellvm12} which assists formal
reasoning on program written in \LLVM.
%
Similarly, \ISA instruction set architecture (ISA) is
one of the most complex and widely used ISAs on servers and desktops. Hence, it
is imperative to  ensure the correctness of lifter targeting \ISA program
binary to \LLVM.
%
The generality of our approach may allow other notable lifters~\cite{FCD,DiFederico:CC2017} 
from \ISA to \LLVM to be directly supported in our framework through minimal
engineering effort. Additionally, we restrict our work to the common case of
compiler generated binaries, and we do not consider binaries that are
deliberately obfuscated to deter reverse engineering, which is in-line with
previous work on \tv.  Lastly, as our aim is to validate the lifted code, we
do not focus on finding bugs lower in the pipeline, e.g., in the loading and
disassembly of binaries.  This is orthogonal to our work and has been shown to
be relatively mature for the common case of compiler generated
binaries~\cite{197147}.

\input{scalable-validation/figs/overview_diag.tex}

%\sd{We do not need to talk about related work, or why not some other approach
%here, leave it for the related work section. Skipping.}
%Traditionally, \tv~\cite{Pnueli:1998} uses compiler instrumentation to help
%generate a simulation relation to prove the correctness of compiler
%optimizations~\cite{Rival:2004,Kanade:2006}. In our attempt to solve the
%problem of \tv of the lifting of \ISA program, we tried to borrow insights
%from such efforts. However, to be effective,  we believe our validator should
%not instrument the lifter mainly because most the available lifters, being in
%early development phase,  are updated and improved at a frantic pace.  Similar
%in principle with Necula's work~\cite{Necula:2000}, as the translator is not
%instrumented, such simulation relations need to be inferred by collecting
%constraints from the input and output programs.  However, in the context of
%\tv of binary lifting, such inference is not straight-forward  mainly because
%the two program (\ISA binary and lifted IR) are structurally very different
%with potentially different number of basic blocks
%%
%\footnote{Instructions like \instr{adcq} exhibit different semantic behaviour
%based on input values and hence generate additional basic blocks upon lifting,
%which are not explicit in the binary program}.
%%
%This is the same limitation that Necula~\cite{Necula:2000} pointed out in
%their paper. Consequently, we decided to move away from simulation-based
%validation approaches.

Our overall approach is a composition of two techniques as shown
in Figure~\ref{fig:overview} with the goal of validating the 
translation of an \ISA program \s{P} to a lifted \LLVM program \s{T} using a 
lifter \s{D}.

\textbf{Single Instruction Validation.} The \siv proceeds in the following
steps: (i) each \ISA instruction \s{I} is lifted to an \LLVM sequence \s{S}
using the lifter \s{D}, (ii) Next, we identify the input/output variable
correspondence between \s{I} and \s{S}, i.e., we determine a mapping of
registers/memory in \s{I} to IR entities in \s{S},  (iii) Using the formal
semantics of the IR and \ISA, we perform symbolic execution to generate
symbolic summaries, (iv) Lastly, we say \s{I} and \s{S} are semantically
equivalent when each corresponding symbolic summary pair are equivalent. We
employ the \Z~\cite{z3:2008} solver for the equivalence checks. If the two
summaries mismatch, meaning we find a bug, they are then reported.  Otherwise,
we add the pair \s{<I,S>} to a validated database.

\textbf{Program-Level Validation} The \plv starts with lifting the \ISA
program \s{P} using the lifter \s{D} to an IR program \s{T}.
%
We employ a \compd (section~\ref{sec:compd}) which composes the validated (or
yet to be validated) IR sequences, corresponding to the x86-64 instructions to
generate an \LLVM program, \s{T$^\prime$}.
%
Next, we seek to compare \s{T} and \s{T$^\prime$} for a syntactical match,
\emph{one function at a time}. Towards that goal, we use a small set of LLVM
optimization passes (of average length \avgPassLength) as a normalizer to close the syntactic gap between \s{T}
\& \s{T$^\prime$} to be compared for syntactic equivalence by a matcher
(Section~\ref{sec:matcher}). 
%%

\textbf{Composing the Techniques} As such, the two techniques are
independent and their results do not depend on the other, with one minor
caveat: the results from \plv, either a complete equivalence match, or a
potential mismatch, are not sound until the IR instruction sequences used to construct
\s{T$^\prime$} are validated by the \siv. However, the ordering between the two
techniques does not matter, i.e., \siv may be done ahead of time, when
composing instruction during \plv, or done in a batch after \plv.

%\textbf{Some Technical Aspects of ``Composing the Techniques''}
%\textbf{Interaction between Phase 1 \& 2:} In Figure~\ref{fig:overview} and
%also in previous discussions, we presented Phase 1 as the pre-step of Phase 2,
     %which is meant to depict a logical view of the approach.  However, exact 
     %implementation of such logical view has
     %problems. The implementation requires pre-populating the database with the 
     %validated <\s{I}, \s{S}> pairs corresponding to the concrete variants of 
     %all the instructions, which is impractical for immediate and memory
     %instruction.  A way out could be to let Phase 1 populate the database with 
     %a single variant of
     %each instruction and let \compd generalize the IR sequence \s{S} for other
     %variants. For example, the phase 1 can generate validated pair for a
     %single variant \instr{addq \%rax, \%rbx} of \instr{addq} instruction and
     %the \compd, while lifting a different variant say \instr{addq \%rax, 
     %-4(\%rbp)},
     %uses generalization of former variant to generate the corresponding IR 
     %sequence for the later. However, there exist two problems with that: One, 
     %this
     %generalization requires adhoc templatization of the validated pairs and 
     %hence extremely prone to errors. Second, the generalization build
     %on the hypothesis that instruction semantics can be borrowed across
     %instruction variants; which is already falsified in
     %~\cite{DasguptaAdve:PLDI19}.

Below we present two different modes of implementing the interaction 
between the two phases.
%which nullifies the above mentioned problems.
%%% Different tradeoff of phase I/II highligtingting their independence.
%%An important point to note is that even though, in Figure~\ref{fig:overview}, 
%%we are depicting Phase 1 as the pre-requisite of the other, but this is not a 
%%hard requirement. Different execution modes of the phases are possible, as 
%%mentioned below, along with their trade-offs.
%%\begin{itemize}
%%    \item Mode I (``Phase 1 as a pre-step of Phase 2''): In this execution 
%%    mode, the 
%%    \compd reuses the validated <\s{I}, \s{S}> pairs, already populated during 
%%    Phase 1, for composition. The major problem with this mode
%%    is that it assumes the concrete variants of all the instructions can be 
%%    generated beforehand, which is impractical for immediate and memory 
%%    instruction. A logical way out is to generate a single variant of each 
%%    instruction and let \compd generalize the IR sequence \s{S} for other 
%%    variants. For example, the phase 1 can populate the database with the 
%%    validated pair for a single variant \instr{addq \%rax, 
%%    \%rbx} of \instr{addq} instruction and the \compd, upon lifting an 
%%    instruction say \instr{addq \%rax, 
%%    -4(\%rbp)}, generalizes the corresponding IR sequence from the one 
%%    available 
%%    in database. These exist two problems with that: One, this generalization 
%%    requires adhoc templatization of the validated pairs in the database and 
%%    extremely prone to errors. Second, the generalization build on the 
%%    hypothesis that instruction semantics can be borrowed across instruction 
%%    variants; which is already falsified in ~\cite{DasguptaAdve:PLDI19}.
\begin{itemize}    

    \item Mode I (``Phase 1 invoked on demand by Phase 2''):
      For each constituent instructions \s{I} of \s{P}, the \compd can check if
      the database already has the validated
        pair <\s{I}, \s{S}>. If yes, then it uses it for composition.
        Otherwise, it invokes the phase I, to (1) generate the lifting \s{S},
        (2) validate the <\s{I}, \s{S}> on the fly, (3) populate the database
        with the pair if they are semantically equivalent, and (4) use \s{S}
        for composition. The total 
        number of pairs generated is bounded by the number of instructions in 
        the program. However, as Phase I is interleaved with Phase 2, the 
        complexity of the former gets added to the performance of the later.
        
        %\cmt{
          %Note that, for a particular instruction variant, 
          %different validated pair could have been generated for different 
          %immediate or memory values. However,  the total 
          %number of pairs generated is bounded by the number of instructions in 
          %the program. Moreover, as Phase I is interleaved with Phase 2, the 
          %performance of the later is affected.   
            
        %In this mode Phase 2 can invoke Phase  1 to generate 
        %validated pair for each variant with different immediate or memory 
        %value. 
        
        %Note that, even though this mode might generate 
        %validated pairs for each 
        %immediate or memory value of a variant, but the total 
        %number of pairs generated is bounded by the number of instructions in 
        %the program. Also, we use the database as a cache so that frequently 
        %occurring instruction do not have to be regenerated.
    %}        
        %\cmt{
        %One potential drawback with this approach is that it
        %might slow down Phase 2.  Fortunately, this does not pose as a
        %practical bottleneck because of the fact that we are using the database
        %as the cache and only a small subset of available instructions are used
        %in binary executable. }

    \item Mode II (``Phase 1 validation happens after Phase 2''): Like mode
        II, for each constituent instructions \s{I} of \s{P}, the \compd can
        check if the database already has the validated pair <\s{I}, \s{S}>.
        If yes, then it reuses it for composition, as before.  Otherwise, it
        (1) invokes the lifter \s{D} to  generate the lifting \s{S}, (2)
        populate the database with the pair, \emph{without} checking if they
        are semantically equivalent, and (4) use \s{S} for composition.  The
        benefit of this mode is that it isolates the two phases and takes-away
        the complexity of phase 1 from phase 2 and handles them offline. 

\end{itemize}
We empirically found that Mode I \& II have similar performance for all 
practical purposes, and we chose to proceed with II.
