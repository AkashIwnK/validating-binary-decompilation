\section{Related Work}\label{sec:RW}

Traditionally, \tv~\cite{Pnueli:1998} uses compiler instrumentation to help
generate a simulation relation to prove the correctness of compiler
optimizations~\cite{Rival:2004,Kanade:2006}. In our initial attempt to solve the
problem of \tv of the lifting of \ISA program, we tried to borrow insights
from such efforts. However, to be effective,  we believe our validator should
not instrument the lifter mainly because most the available lifters, being in
early development phase,  are updated and improved at a frantic pace. Without 
instrumentation, such simulation relations can be inferred by collecting
constraints from the input and output programs (as demonstrated in Necula's 
work~\cite{Necula:2000}). However, in the context of
\tv of binary lifting, such inference is not straight-forward  mainly because
the two program (\ISA binary and lifted IR) are structurally very different
with potentially different number of basic blocks  
% 
\footnote{Instructions like \instr{adcq} exhibit different semantic behavior
    based on input values and hence generate additional basic blocks upon 
    lifting,
    which are not explicit in the binary program}.
%
Unsurprisingly, a similar challenge poses a hard requirement of branch 
equivalence in Necula's approach.  
%
% is the same limitation that Necula~\cite{Necula:2000}
%poses a hard assumption of 
% pointed out in
%their paper. 
Consequently, we decided to move away from simulation-based
validation approaches.

 All the previous efforts, establishing the faithfulness of the binary 
 lifters,  can be broadly categorized to be based on (1)
  Testing, or (2) Formal Methods.

\subsection{Testing based Approaches}
This approach is similar to black-box testing in software engineering. Most
notable work include Martignoni et
al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010,Martignoni:ASPLOS2012} and
Chen \etal~\cite{CLSS2015}.


Martignoni et al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010} proposes
hardware-cosimulation based testing on QEMU~\cite{QEMU:USENIX05} and
Bochs~\cite{Bochs1996}.  Specifically, they compared the state between actual
CPU and  IA-32 CPU emulator (under test) after executing randomly selected
test-inputs on randomly chosen instructions  to discover any semantic
deviations. Although, a simple and scalable approach, it's effectiveness is
limited because many semantics bugs in binary lifters are triggered upon a
specific input and exercising all such corner inputs, using randomly generated
test-cases, is impractical.

%%architecture-state-comparison problems,

In general, such testing based approaches are unsuitable for validating whole
program (or even basic block) translations because even a correctly translated
program may not always produce exactly the same output as the original program
due to the differences in modeling of the architectural states in the
translated program (or basic block) vs the original program. Although, it is 
possible to engineer out such architecture-state-comparison problems, but still 
these approaches might
not
detect some intermediate mistranslated instructions which are course corrected
at the end. As an example, suppose  a register is assigned values twice in a
program and the first assignment is mistranslated but the second assignment
statement is translated correctly. Comparing the architecture states at the end
of the program (or basic block) may not discover the mis-translation.

%%

Chen \etal~\cite{CLSS2015} proposed validating the static binary translator
LLBT~\cite{LLBT2012} and the hybrid binary translator~\cite{LLVMDBT2012},
  re-targeting ARM programs to x86 programs. First, an ARM program is
  translated offline to x86 program (via an intermediate translation to LLVM 
  IR). Next, the translated x86 binary is
  executed  directly on a x86 system while the original ARM binary runs on the
  QEMU emulator. During run time, both the ARM binary and the translated x86
  binary produce a sequence of  architecture states, which are compared at the
  granularity of single instruction after solving the 
  architecture-state-comparison problem, as mentioned above. The validator is
  evaluated using the ARM code compiled from EEMBC 1.1 benchmark. Like previous
  approach, the validation of single instruction's translation is
based on testing and hence shares the same limitation of not being exhaustive.
%%

Martignoni \etal~\cite{Martignoni:ASPLOS2012} applied symbolic execution on a
Hi-Fi (``faithful and more complete in terms of IA-32 ISA'')
emulator\cite{Bochs1996}'s implementation of an instruction semantics to
generate
high-fidelity test-inputs to validate a ``buggier and less complete'' Lo-Fi
emulator~\cite{QEMU:USENIX05},
by executing the binary instruction twice, once on a real hardware and next on
the Lo-Fi emulator, and the output states are matched. However, the 
work~\cite{Martignoni:ASPLOS2012} does not aim to validate the
translation of \ISA programs,
which is one of out primary contributions.

Note that an approach as above cannot scale naturally to binary function  
validation; A set of high-coverage test-inputs for all the constituent 
instructions of a function cannot trivially derive  high-coverage test-inputs 
for the whole function. 
%This is mainly mainly because  such test-inputs are generated in an isolated 
%context of that particular instruction and may not even be satisfiable in the 
%whole program context. 

%For each path explored during the symbolic execution of an instruction's
%implementation, the underlying decision procedure computes an assignment of
%bits to the symbolic inputs states that would cause the emulator to execute
%that path: such assignments, serve as the test-inputs, together cover
%all
%behaviors of that instruction.
%However, if the goal is to validate whole-program translation and if
%we have
%such test-inputs for  all the constituent instructions of the
%program, still it will not help to generate high coverage test-inputs for the
%whole program mainly because  such test-inputs are generated in an isolated
%context and may not even be satisfiable in the whole program context.

%%


%%
%Note that, even though Martignoni \etal~\cite{Martignoni:ASPLOS2012}
%symbolically explored the test-cases which is supposed to cover all the paths
%of a given instruction's implementation, but being a differential testing-based
%approach, the faithfulness depends directly on  the fidelity of the Hi-Fi
%emulator. A wrong implementation or omission of a particular switch case of
%instruction semantics in the Hi-Fi, will lead to test-cases insufficient to
%explore all the paths and hence find bugs in the Low-Fi emulator. Also, the
%method can capture  deviations in the behavior of only those
%instructions which are implemented in both the emulators.\footnote{We
%  note that our proposed semantics-driven \tv approach shares similar
%    assumptions about the faithfulness of the semantics.}.
%%
%Moreover, the symbolic execution of an instruction's implementation in the
%Hi-Fi emulator is achieved using an X86 interpreter FuzzBALL. A bug in the
%interpreter will affect the generation of high-fidelity test cases for a
%particular instruction, leading to incomplete coverage of that instruction's
%implementation in Low-Fi emulator.
%%
%However, their approach does not consider the floating point instruction
%  because the employed symbolic execution engine (FuzzBALL) does not support
%    it.

%    Schwartz \etal~\cite{Schwartz:2013} proposed control flow structure recovery by
%    employing semantics preventing schema and tested their binary to C decompiler,
%    Phoenix, which is based on BAP~\cite{BAP:CAV11}, on a set of 107 real
%    world programs from GNU coreutils. Along similar lines,
%    %
%    Yakdan \etal~\cite{Yakdan2015NDSS} presented a decompiler, DREAM, to offer a
%    goto-free output. DREAM uses a novel pattern independent control-flow
%    structuring algorithm that can recover all control constructs in binary
%    programs and produce structured decompiled code without any goto statement. The
%    correctness of our algorithms is demonstrated using the GNU coreutils suite of
%    utilities as a benchmark.
%
%    Andriesse \etal~\cite{nucleus2017EuroSP} proposes a function detection
%    algorithm, Nucleus, for binaries. The algorithm does not require function
%    signature information or any learning phase. They evaluated Nucleus on a
%    diverse set of $476$ C and C++ binaries, compiled with gcc, clang and Visual
%    Studio for x86 and x64, at optimization levels O0--O3.
%
%    Martignoni et al.~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010} attempted
%    to leverage differential testing on QEMU~\cite{QEMU:USENIX05} and
%    Bochs~\cite{Bochs1996}. Particularly, they compared the state between a
%    physical and an emulated CPU after executing randomly chosen instructions on
%    both to discover any semantic deviations. Although their technique can be
%    applied to testing binary lifters, it is fundamentally limited because its
%    effectiveness largely depends on randomly generated test cases. Typically,
%    semantic bugs in binary lifters are triggered only with specific
%    operand values. Therefore, a random test case generation does not
%    help much in finding such bugs.

\subsection{Formal Methods based Approaches}
Followings are the effort to establish strong guarantees for binary
translations using formal methods.

MeanDiff~\cite{ASE2017} proposed an N-version IR testing to validate three
binary lifters, BAP~\cite{BAP:CAV11}, BINSEC~\cite{BINSEC2011}, and
PyVEX~\cite{PYVEX} by comparing their translation of a single binary
instruction to BIL, DBA, and VEX IRs respectively. The individual IRs are then
converted to common IR representations which are then symbolically executed to
generate symbolic summaries for comparison using a SAT solver.  The above
approach shares the same fundamental limitations of any differential testing
techniques. For example, if all the binary lifters are in  sync
on the behavior of a particular instruction, we get more confidence in correct
 implementation of that instruction's semantics in all of them, but we
cannot rule
out the possibility of all being incorrect. Also, even if there a disagreement
in the behavior of two or more translators, still it might just be a false
alarm
in case all the candidates are buggy. However, the work~\cite{ASE2017}
is primarily focused on validating single
instruction and the problem
of handling multiple instruction is left as future work.

%%

Moreover, as candidly mentioned in the paper~\cite{ASE2017}, one of the 
motivations for
relying on differential testing  is that there were no
formal specification of \ISA ISA at the time of
writing the paper. Whereas we do not have such limitation because of the formal
\ISA ISA specification~\cite{DasguptaAdve:PLDI19} made public recently.
Empowered with that, we can build a symbolic formula that encodes all execution
paths of an IR instance lifted from a single machine instruction and then
check if the symbolic formula matches the formal specification of the
instruction, which is exactly what we did in this work.

%%
The work closest to ours, in term of the goals, is the translation verifier,
Reopt-vcg~\cite{Galois:SPISA19}, which is developed to cater the verification
challenges specific to the translator Reopt~\cite{reopt}. The verifier, which
validates the translations at basic-block level, is assisted by various
manually written annotations, which are prone to errors. Such annotations
could have been generated by instrumenting the lifter. 
%
 Contrary to that, our approach  does not need any such 
instrumentations, thereby avoided
the overhead of maintaining instrumentation patches whenever the lifter
design/implementation is updated. 
Moreover, 
the validator uses the semantics
definitions of a small subset of \ISA, which in turn limits its 
applicability
to small programs. We avoided this
limitation by incorporating the most complete and heavily tested \ISA
semantics~\cite{DasguptaAdve:PLDI19} available to date.

%
%More importantly, we aim for whole program
%validation.  
%
%
%Reopt-vcg~\cite{Galois:SPISA19} is closest to ours in terms of its goal of
%proving that a translated LLVM program is a refinement of a \ISA program.  The
%translation verifier takes a binary executable, LLVM bitcode file,
%             manually provided annotations  along with several simplifying 
%            assumptions specific to the translator (Reopt~\cite{reopt}) to 
%            generates proof 
%            obligations in SMTLIB to
%            verify each basic block independently. The annotation generation 
%            could have been automated by instrumenting the lifter,  which adds 
%            both implementation and maintenance overheads. 
% %           First, the approach
% %           is aimed for a particular decompiler~\cite{reopt} and generalizing
% %           it to others needs substantial instrumentation to correctly emit 
% %the
%  %          annotations. 
%  Contrary to that, our approach  does not need any such 
%            instrumentations and provides a simpler approach for program  level 
%            validation. 
%
        
  
%\cmt{The annotations are 
%    used to
%    identify basic block correspondence, define the
%    appropriate pre-conditions for the block, distinguish between 
%    operations with and without side effects, and identify argument 
%    mapping to
%    machine code registers.}        
  
        %            \todo[inline,color=yellow]{Second, the 
        %            verifier use a small subset of manual written \ISA 
        %semantics which 
        %            limits its applicability to small program. On the 
        %contrary, our 
        %            \siv is based on the most complete and heavily tested 
        %formal 
        %            semantics available to date and hence applicable to 
        %real-world 
        %            programs.}
%%
% \cmt{The underlying comparison tool uses 
%    those 
%    annotations to step through both the machine code and LLVM in 
%    parallel and ensure that for which each LLVM operation with side 
%    effects, including memory reads and writes, has an 
%    equivalent write in the machine code.}

%\subsection{Using Machine Learning} Another recent work by Schulte
%\etal~\cite{eschulte2018bed} proposed Byte-Equivalent Decompilation (BED) which
%leveraged a genetic optimization algorithm to infer C source code from a
%binary. Given a target binary and an initial population of C code as
%decompilation candidates, they  drive a genetic algorithm to improve the
%initial candidates, driving them closer (using compilation to binary) to
%byte-equivalence w.r.t the target binary. The byte equivalence  is simply the
%edit distance to the target binary. As hypothesized in the future work section
%of the paper~\cite{eschulte2018bed}, BED could be applied to LLVM IR instead of
%C to evolve lifting from machine code to LLVM IR and may work well due to the
%relative simplicity of LLVM IR as compared to the C. Being byte-equivalent, the
%generated LLVM IR will be the faithful evolution from the machine code.
%However, as shown in the paper, this approach worked moderately well for
%smallish binaries. For example, out of $22$ binaries under test, only $4$
%achieve full byte equivalence when the initial population is augmented with
%decompilation candidates from the HEX-RAYS~\cite{hexray} Decompiler. It is
%still an open problem to realized an end-end byte-equivalent binary to LLVM
%decompiler using purely genetic optimization algorithm.  \cmt{only $3$ achieve
%  full byte equivalence when the initial population does not include decompiler
%    seeds, and}


%\cmt{
%    Myreen et al.~\cite{Myreen:FMCAD:2008,Myreen:FMCAD:2012} proposed
%    ``decompilation into logic'' which, given some concrete machine code and a
%    model of an ISA, extracts logic functions or symbolic summaries which
%    captures
%    the functional behavior of the machine code. The decompiler works on top of
%    ISA
%    models for IA-32 \cite{Karl2003}, ARM~\cite{Fox2003} and
%    PowerPC~\cite{Leroy:2006}. Assuming that the ISA models are trusted, the
%    extracted functions can be used to prove properties of the original machine
%    code. However, the work has not been applied to validate the binary
%    translation
%    to an IR.  A recent work by Roessle et al.~\cite{Roessle:CPP19} improves the
%    aforementioned idea  by including a subset of \ISA, derived mostly from
%    Strata~\cite{Heule2016a}, in the trust-base of ISA models.
%}
%
%
%%%
%
%%

%\cmt{Myreen et al.~\cite{Myreen:FMCAD:2008,Myreen:FMCAD:2012} extracted
%    function-level symbolic summaries which indeed is a promising building block
%    towards establishing correctness of binary lifters.\cmt{, which, however,
%        has many additional challenges to deal with (Refer
%        Section~\ref{sec:challenges}). Moreover,} However, both Myreen et al.
%    and Roessle et al. have limited \ISA instruction set coverage, which
%    might restricts their application on many  real-world binaries.}
%
%\cmt{Being a differential testing method, only those instructions which are
%    supported in all the translators can be validated with higher confidence
%    than
%    the ones which are not supported in one or more.
%
%    Also, as MeanDiff is testing
%    multiple binary lifters together, hence it cannot be used to establish the
%    faithfulness in lifting the semantics of an instruction which is not
%    implemented in any one of them.
%
%    In this approach of differential testing, whether a particular instruction
%    is
%    going to be validated depends on its availability in other translators,
%    even if
%    some translator as a much better instruction support. In our case, we are
%    testing McSema against the most complete user level \ISA ISA
%}
%\cmt{MeanDiff neither
%    handle floating point operations, nor the instructions which does not
%    manifest
%    their side-effects (like flag updates) explicitly.  Moreover, MeanDiff
%    reports
%    a bug whenever a deviation is detected w.r.t the
%    instruction-semantics-behavior
%    in at least two binary lifters. But even if all the binary lifters are in
%    sync
%    on the behavior of a particular instruction, we cannot guarantee that all
%    the
%    lifters are faithful in lifting that instruction, which is however, a
%    general
%    limitation of differential testing based approach. Also, as MeanDiff is
%    testing
%    multiple binary lifters together, hence it cannot be used to establish the
%    faithfulness in lifting the semantics of an instruction which is not
%    implemented in any one of them.
%}
