\section{Introduction}
\label{sec:Intro}

% what is Binary analysis and why important.

The ability to directly reason about binary machine code is desirable, not only
because it allows analyzing binaries even when the source code is not
available (e.g., legacy code, closed-source software, or malware), but also
because it avoids the need to trust the correctness of
compilers~\cite{Thompson,WYSINWYE}.  Analyzing binary code is important in
certain subfields of software engineering and security tools,
including binary
instrumentation~\cite{Bruening:CGO2003,PEBIL10,Pin:2005,Valgrind:ENTCS03,DynamoRIO:2004},
binary re-targeting~\cite{UQBT:2000,Retarget11}, software
hardening~\cite{Cha:2015,Ford:2008,Zhang,Zhang:2013}, software
testing~\cite{Chipounov:2011,Avgerinos:2014,godefroid_automated_2008}, CPU
emulation~\cite{QEMU:USENIX05,Magnusson:2002}, malware
detection~\cite{Christodorescu:2005,Kruegel:2004,BitBlaze:2008,BAP:CAV11,Egele:USENIX07,Yin:CCS07},
automated reverse
engineering~\cite{Cui:2008,Lin:2008,Schwartz:2013,Yakdan2015NDSS,McSema:Recon14,Angr,Radare2},
sand-boxing~\cite{Kiriansky:2002:SEV,Erlingsson:2006,Yee:2009},
profiling~\cite{Harris:2005,Srivastava:1994}, and automatic exploit
generation~\cite{Cha:2012}.

% Introduce Binary lifters. Why are intermediate representations needed? What they do?
% Clarify the nomenclature.

To reason about binary code, binary analysis frameworks,
e.g.,~\cite{McSema:Recon14,Remill,Angr1,BAP:CAV11,Radare2}, first convert
raw bytes from the binary into a stream of instructions through
\emph{disassembly}.
%
To enable greater retargetability of the frameworks to multiple instruction sets,
these tools often use a binary \emph{lifter} to lift, or translate, all supported
ISAs to a uniform intermediate representation (IR), and then apply 
architecture-independent passes on this IR.
%
After lifting, several analysis passes may operate on the IR to: 
(i) recover higher-level constructs, such as functions,
stack frames, variables, and types, (ii) re-target to a different ISA, or 
(iii) instrument and recompile the binary for various purposes.
%
Many binary analysis frameworks published in
academia~\cite{BAP:CAV11,BitBlaze:2008,Fokin:2011,eschulte2018bed}, or
as open-source code~\cite{McSema:Recon14,Remill,Angr1,Radare2,FCD}, use
such a lifter as a first step in their pipeline.

%Binary analysis is assisted by decompilation frameworks (e.g.
%~\cite{McSema:Recon14,Remill,Angr1,BAP:CAV11,Radare2}) with the primary goal of
%analyzing the compiled binary to infer a close approximation of the high-level
%source code that could have generated it. Decompilation typically involves (1)
%Lifting, which is translating\footnote{Throughout the presentation, we use the
%term ``translation'' literally meaning converting text from one language to the
%other, without any connection with binary-translation~\cite{BT1993}, which is
%about reproducing the behavior of a binary program written in one architecture
%instructions to another.} machine code to an intermediate representation
%(IR), and thereby exposing various source-level artifacts like function
%boundary, control flow, instruction semantics etc.  and (2)
%Performing one or more post lifting actions (like further analysis, e.g. stack,
%type or function prototype
%recovery, re-targeting/transformation/optimization/instrumentation/further
%enrichment etc.)  at the IR level decided entirely by the
%ultimate goal of the decompilation framework.

% Challenges which makes the lifting process error prone.
Developing a lifter, especially for complex modern ISAs, is
challenging and error-prone~\cite{Meng:2016}, mainly because manually encoding
the effects of a vast number of instructions (and their variants) is hard.
This is made even harder when the informal specifications provided by the hardware
manufacturers run into thousands of pages~\cite{IntelManual,ArmManual}, have
mistakes~\cite{DasguptaAdve:PLDI19}, 
or allow for implementation-dependent undefined behaviors. Once such
a lifter is developed, the developers then run into the problem of not having
a way to test their implementation completely as there are no formal,
machine-readable semantics available for automated testing. Lastly, to make it
worse, these lifters need to be updated and rechecked for correctness every
time new instructions are added to an ISA.

%Needless to say that engineering a precise binary lifter is extremely
%difficult~\cite{Meng:2016} mainly because of (1) the various challenges in
%recovering source-level artifacts from binary which are otherwise lost during
%the compilation process, and (2) the complexity and the sheer large number of
%instructions that are informally (at times ambiguously) specified in
%approximately thousands of pages of the ISA instruction
%manuals~\cite{IntelManual,ArmManual}. Even worse, the number and the
%complexity keeps on increasing as new CPU features are added. To add to the
%complexity, the standard also admits undefined behaviors that  are
%implementation-dependent. All such challenging issues makes the binary lifters
%highly susceptible to errors.

Despite the correctness challenges in binary lifting, such lifters are sometimes
used for tasks where correctness is especially important, e.g., when looking for security
vulnerabilities in binary code, binary emulation of one processor ISA on another,
or recompiling embedded software to run on new platforms.
%
Beyond these more critical tasks, gaining confidence in decompilers through more 
effective testing is also generally important for developers of decompilers,
especially for complex ISAs.
%

Making matters worse, there has been very limited work to date on validating correctness of binary
decompilers, and that work has focused on translation of single instructions. 
All of this existing work falls short in at least
one of the following criteria: (i) require random test-inputs which leads to
incomplete coverage~\cite{Martignoni:ISSTA2009,
Martignoni:ISSTA2010,CLSS2015}, (ii) rely on symbolic analysis
and therefore do not scale to full program
\tv~\cite{Martignoni:ASPLOS2012,ASE2017}, or (iii) require modification of the
lifting frameworks under test to emit additional information required to prove
correctness~\cite{Galois:SPISA19}.

%Formally establishing faithfulness of the lifting (i.e. translation from
%machine code to high level IR) is pivotal to gain trust in any binary
%analysis. Any bug in the lifting would invalidate the binary analysis results.
%For example, a malware analysis system might miss vulnerabilities or a binary
%instrumentation system, instrumenting a buggy IR, might lead to failure or
%even crash in interpreting the instrumented program. Therefore, automatic
%validation tools are needed urgently to uncover hidden problems in a binary
%lifters, which constitute the motivation and theme of our work.

%% Why lifting alone is imporatnt to be verified: Already covered before
%% Only instruction lifting is what we are focussing on
%% Focus: TV of the instruction seantics exposed in a program
%% Not other disassembler stuff like cfg, function boundary
%% Only x86-64 binaries
%% Just LLVM IR, but the appraoch   can be generalized to any compiler IR
%% Both intruction and Program lifting has their own importance.
%% present a motivating example

%Program decompilation, that is analyzing the compiled binary to infer a
%possible source program that could have generated it, is a widely used
%technique for reverse engineering. Recently this technique has been used to
%assist in verifying compiled artifacts and to recompile applications for
%increased performance or security protections. In these latter applications,
%one does not need a full decompiler back to program source, and hence binary
%raising into an intermediate representation such as assembly code or LLVM is
%sufficient.

The goal of the work in this paper is to develop formal and informal techniques
to achieve high confidence in the correctness of a lifter from a complex machine
ISA (e.g., X86-64) to a rich IR (e.g., LLVM IR).
%
We present four key contributions.
%
\begin{enumerate}
  %
  \item First, we show that formal translation validation of single machine 
  instructions can be used as a building 
  block for scalable
  full-program translation validation.  The underlying insight is that decompilers are
  usually designed to perform simple instruction-by-instruction lifting
  (using a fixed and canonical representation of architectural state at the IR
  level), followed by standard IR optimization passes to achieve simpler IR
  code.
  %
  \item Second, we develop the first single instruction \tv framework for \ISA.
  Our work applies the \emph{most precise formal
  semantics} for \ISA known to date, and has the \emph{widest coverage} in terms
  of the number of instructions tested when compared to earlier work.
  We experimentally verify that such single instruction validation
  is capable of finding bugs, and in fact \emph{all bugs we have uncovered were
  found using said single instruction validation}.  
  In particular, we find
  discrepancies in the lifting of \sivFail instructions in McSema~\cite{McSema:Recon14,Remill}, 
  a well-tested, mature~\cite{McSema:Compare}, and  open source lifter for \ISA to 
  LLVM IR, clearly showing the
  effectiveness of our technique.  All of these have been confirmed as bugs in
  the lifter by the McSema developers.
  %
  \item Third, given a lifter, $D$, we show that we can automatically construct a 
  simple, \emph{compositional} decompiler, \Dp,
  by essentially just concatenating the lifted IR sequences for individual instructions
  (which are individually proven correct by part 1, above).  Although we do not
  \emph{prove} full formal equivalence of the compositional decompiler to the original,
  the tool is exceedingly simple, and moreover, 
  \emph{produces code sequences (say, \Tp) that are syntactically very similar} to those 
  (\s{T}) produced by the decompiler we want to validate.
  This serves as a foundation for scaling translation validation to full programs.
%
%  Moreover, if an input program triggers a bug in D, 
%  D$^\prime$ has a high likelihood of generating non-equivalent code, which helps 
%  detect the error.  
  %
  \item Fourth, we propose a scalable approach for full-program translation validation
  that does not require heavyweight symbolic execution or theorem provers.  Our key 
  insight is that there exists a semantics-preserving transformation --- dubbed a  
  normalizer --- of \T and \TP, say \s{norm(\T)} and \s{norm(\TP)}, such 
  that
  \s{norm(T)} and \s{norm(T$^\prime$)} are \emph{syntactically} equivalent iff \s{T} 
  and \s{T$^\prime$} are \emph{semantically} equivalent.  If such a normalizer
  exists, then we can reduce the problem of program-level semantics checking to
  a much cheaper program-level syntactic check!  In this paper, we construct a 
  normalizer out of a very short sequence of 15 LLVM IR passes, of which 11 are
  transformation passes.  We have found this approach to be effective at proving
  syntactic equivalence between \s{T} and \Tp for \plvP out of \plvT functions
  taken from  ''single-source-benchmark'' of LLVM test-suite.
  %
\end{enumerate}

%Our validation framework is publicly available at ~\cite{Suppl}.
%
The rest of this paper proceeds as follows.
%
The next section gives a high-level overview of our approach.
%
Section~\ref{sec:prelim} gives some background on building blocks used in our work.
%
Section~\ref{sec:siv} describes our approach for formal single-instruction \TV.
%
Section~\ref{sec:plv} describes how we scale to full-program \TV.
%
Section~\ref{sec:eval} describes our experimental evaluation, including the bugs 
in McSema uncovered by our work, and the effectiveness of full-program \TV.
%
Section~\ref{sec:discussion} briefly discusses some of the key limitations of this work
to date, along with avenues for future work.
%
Section~\ref{sec:RW} places our work in context of prior work, and
Section~\ref{sec:conc} concludes.



%%%-----------------------------------------------------------------------------
%%% OLD DISABLED TEXT.  SAVING TEMPORARILY BUT CAN BE DELETED.
%%%-----------------------------------------------------------------------------

%%%
%%Most existing approaches are focused on validating translation of single
%%instruction using either co-simulation testing ~\cite{Martignoni:ISSTA2009,
%%Martignoni:ISSTA2010,CLSS2015,Martignoni:ASPLOS2012}, or $N$-Version IR
%%testing ~\cite{ASE2017}. Some of those co-simulation based
%%approaches~\cite{Martignoni:ISSTA2009, Martignoni:ISSTA2010,CLSS2015}, owing
%%to rely either  on random test-inputs or the ones provided by different
%%benchmarks, are less likely to detect corner case or hard to detect bugs.
%%~\cite{Martignoni:ASPLOS2012} uses symbolic execution to generate test-inputs,
%%however, the approach is not meant to scale for whole program \tv. The
%%paper~\cite{ASE2017} proposes $N$-Version IR testing to compare symbolic
%%summaries extracted from different binary lifters using solver checks and
%%concluded their presentation by acknowledging various challenges in porting
%%their strategy to handle program level \tv; which they left as future work.
%%The work closest to ours, in term of the goals, is the translation verifier,
%%Reopt-vcg~\cite{Galois:SPISA19}, which is developed to cater the verification
%%challenges specific to the translator Reopt~\cite{reopt}. The verifier, which
%%validates the translations at basic-block level, is assisted by various
%%manually written annotations, which are prone to errors. Such annotations
%%could have been generated by instrumenting the lifter. However, one of the key
%%goals of our approach is not to instrument the translator, thereby avoiding
%%the overhead of maintaining instrumentation patches whenever the lifter
%%design/implementation is updated. More importantly, we aim for whole program
%%validation.  
%%Refer to Section~\ref{sec:RW} for a more detailed comparison to existing
%%approaches.
%
%%% Overview of our approach
%\paragraph{This Paper.} The goal of this paper is to demonstrate that
%validating binary lifters can be practical.
%
%\textbf{Challenges.} Practical, sound binary lifting is challenging for a
%number of reasons.  For example, consider a seemingly-correct approach which
%uses existing techniques in single-instruction \tv to prove that a lifter
%achieves semantic equivalence for each instruction in isolation---and declares
%the lifter correct if all per-instruction semantic equivalence checks pass.  We
%make the following observation: an instruction that may be correct and
%validated in isolation, may \emph{still be incorrect} in a \emph{full program
%context}.  Consider a simple example: \texttt{mov \$constant, \%rdi}.  All
%single-instruction validation can say is that this instruction successfully
%moves the constant to register \texttt{\%rdi}.  This is insufficient
%information from the lifter.  For example, \texttt{\$constant} may refer to a true
%constant, or an address pointing into the data section.  It is crucial for the
%lifter to tell the IR which is the case, so that down-stream transformations do
%not violate semantics.
%
%To get around these problems, one might also consider whole-program semantic
%equivalence checking, relative to a golden model. But the current
%state-of-the-art equivalence checkers have a hard time scaling to larger
%applications.
%
%\textbf{Our Approach.} To build a practical validation for binary lifters, we
%construct and compose two novel techniques: \emph{single-instruction
%validation} and \emph{program-level normalization and syntactic equivalence
%checking}. These techniques are based on two key insights:
%
%\emph{\textbf{1. Instructions have a clear input/output abstraction that are explicit
%in terms of the operands or implicit in the instruction semantics.}} Therefore,
%individual instructions can be lifted and checked for semantic equivalence in
%isolation. In short, we derive symbolic summaries for lifted instruction
%sequence and compare it against the formal semantics for the instruction under
%test to look for deviations. Our work is the \emph{first to target an
%architecture as extensive as \ISA}, uses the \emph{most precise formal
%semantics} for \ISA known to date, and has the \emph{widest coverage} in terms
%of the number of instructions tested when compared to earlier work. We find
%\emph{discrepancies in the lifting of 30 instructions} in McSema, the most
%mature and well-tested lifter for \ISA to LLVM IR, clearly showing the
%effectiveness of our technique.  These have been confirmed as bugs in
%the lifter by the developers.
%
%As a next step, to scale to full program validation, we build a tool,
%\emph{\compd} \s{D$^\prime$}, to compose the sequences of validated IR lifted
%from the constituent \ISA instructions of \s{P}, forming IR \s{T$^\prime$}.
%This IR \s{T$^\prime$} forms the basis of our golden specification that we
%then use for full program \tv.  When available, we augment \s{T$^\prime$} with
%additional sources of information, e.g., relocation information in binaries,
%to make \s{T$^\prime$} as precise as possible.
%
%\emph{\textbf{2. Many binary to IR lifters work in the following fashion: first,
%translate the IR semantics of individual instructions using templates
%available to assist the translation; second, compose those translations on the
%fly to generate the whole lifted program.}} Based on this insight, we find that
%the resulting IR \s{T} and \s{T$^\prime$}(i.e., that IR produced by the
%lifter-under-test \s{D} and compositional lifter \s{D$^\prime$}) are
%syntactically similar.  With this in mind, our hypothesis is: there exists a
%semantics-preserving transformation---dubbed a normalizer---from T and T',
%call it \s{norm(T)} and \s{norm(T$^\prime$)}, such that \s{norm(T)} and
%\s{norm(T$^\prime$)} are \emph{syntactically} equivalent iff \s{T} and
%\s{T$^\prime$} are semantically equivalent.  In the forward direction, this is
%intuitive.  For example, if \s{T} and \s{T$^\prime$} differ only in
%surface-level ways such as where NOPs are inserted, then a normalizer should
%be able to remove those NOPs and reveal syntactic equivalence.  In the
%backward direction, if the normalization is sound, clearly \s{norm(T)} and
%\s{norm(T$^\prime$)} cannot syntactically match if \s{T} and \s{T$^\prime$}
%are different semantically.  Putting it all together, if such a normalizer
%exists, then we can reduce the problem of program-level semantics checking to
%a much cheaper program-level syntactic check!
%
%The question is how to implement such a normalizer.  In this paper, we
%construct a normalizer made out of LLVM 03 passes.  We have found this
%approach to be very effective.  For example, LLVM optimizers can trivially
%remove NOPs and other ineffectual operations.  This prototype has some
%drawbacks, e.g., LLVM 03 passes are not known to be sound.  Yet, importantly,
%our goal in this paper is to demonstrate feasibility of the approach.  In
%future work, we plan to investigate alternative normalizer implementations
%that we can prove are sound.
%
%%correctness of lifting a single instruction, to validating the lifting of
%%an input \ISA function \s{P} to an output IR program \s{T} as follows:
%%we use a \emph{\compd} \s{D$^\prime$}, a tool we built based on
%%\textbf{\emph{ObB.}}, to compose the validated IR sequences lifted
%%from the constituent \ISA instructions of \s{P} to propose a IR
%%program \s{T$^\prime$} to be compared against \s{T}. At this point,
%%the problem of \tv from \s{P} to \s{T} has been reduced to check if
%%\s{T} and \s{T$^\prime$} are semantically equivalent, which, in
%%theory, could have been proved by a formal equivalence checker.
%%However, the fact that both \s{T} and \s{T$^\prime$} are in the same
%%IR abstraction can be leveraged further, by normalizing (using IR
%%optimizations) each of them to canonical representations and thus
%%closing the gap between them.  \sd{TODO: Mention the number of
%%functions that we were able to prove 100\% syntactic equivalence (and
%%their LOCs) here with this technique, thereby eliminating the need for
%%manual analysis or reasoning in these cases.}
%
%%The intuition behind getting syntactic similarity after normalization is based
%%on the observation \textbf{Observ. B} in the following way: As both the lifted
%%outputs (\s{T} and \s{T$^\prime$}) are generated using similar approach of
%%composition, they are very likely to be near identical to begin with and the
%%later normalization is likely to generate syntactically-close-enough outputs.
%%The intuition got validated by the experimental results as detailed in
%%Section~\ref{sec:eval}.
%%%
%
%%Next, such syntactically-close-enough normalized outputs are then compared
%%using a carefully designed matcher which uses exact graph-isomorphism, on the
%%corresponding data dependence graphs, when 1-1 matching is possible;
%%otherwise, uses an iterative matching and pruning strategy which iteratively
%%prunes the matched subgraphs and look for more isomorphic matches after
%%normalizing the residual graph. In hindsight, the matcher is the key towards
%%reducing the problem of semantics equivalence to syntactic equality, avoiding
%%performance-heavy program equivalence checker, and thus allowing  our approach
%%to scale.
%
%Although we do not \emph{prove} full formal equivalence for the construction
%of \compd, its design is exceedingly simple (essentially just concatenating
%proven-correct code templates for individual instructions).  The normalizer
%and matcher are somewhat more complex, but no more than the checkers used in
%previous work Necula~\cite{Necula:2000}, LLVM-MD~\cite{Tristan:2011} \&
%Peggy~\cite{Stepp:2011}.
%
%
%%Our approach is not designed to validate the correctness of the underlying
%%disassembler. Given the recent maturity of
%%disassemblers~\cite{IDA,idasite,PropDisas19,SupersetDisas18,Ramblr17,UROBOROS15}
%%in dealing with non-obfuscated and compiler generated sane binaries, we
%%decided not to invest efforts in that direction.
%
%%% Only x86-64 --> LLVM
%%The current work focuses on validating the correctness of binary lifters
%%translating \ISA program to LLVM~\cite{LLVM:CGO04} IR.
%%%% Why relevant
%%LLVM, being an industry standard compiler IR,  many decompilation
%%projects~\cite{McSema:Recon14,Remill,FCD,reopt,llvm-mctoll} prefer to employ
%%LLVM as their lifted representation mainly because it enables various
%%``out-of-the-box'' analyses
%%and optimizations  which minimizes the effort in the post-lifting
%%decompilation tasks. Moreover, \LLVM is backed-up with its formal
%%semantic models~\cite{LLVMSEMA,Vellvm12} which assists formal
%%reasoning on program written in \LLVM.
%%%
%%Similarly, \ISA instruction set architecture (ISA) is
%%one of the most complex and widely used ISAs on servers and desktops. Hence, it
%%is imperative to  ensure the correctness of lifter targeting \ISA program
%%binary to \LLVM.
%%%% But we are not limited
%
%%However, we believe that our core techniques are applicable to any IR which
%%(1) supports out-of-the box optimizations (e.g.  VEX IR used in
%%Valgrind~\cite{Valgrind:ENTCS03} or any compiler IRs), and (2) whose semantics
%%is formally specified. The first prerequisite is mainly because of the
%%normalizer relying on IR optimizations and the second one allows, as part of
%%the first phase, generating symbolic summary from IR sequence .
%
%%Moreover, the proposed approach is not restricted only to \ISA binary and will
%%work on any ISA whose semantics is formally specified\footnote{Formal model of
%%the ISA is need for translation validation of single instruction's lifting,
%%i.e. in the first phase.}. Fortunately, this is no more a bottleneck with most
%%of the widely used ISAs backed-up with formally specified semantics
%%(\ISA~\cite{DasguptaAdve:PLDI19,Goel:FMCAD14},
%%x86~\cite{TSL:TOPLAS13,Leroy:2009,cakeML19},
%%ARM~\cite{sail-popl2019,ARMSel4:09,ArmFox2003},
%%RISC-V~\cite{cakeML19,Leroy:2009}, CHERI-MIPS~\cite{sail-popl2019},
%%PowerPC~\cite{Leroy:2009,TSL:TOPLAS13}).
%
%
%%%Only Mcsema
%
%%We evaluated our approach on \mcsema~\cite{McSema:Recon14,McSemaSite}, which
%%is considered most mature~\cite{McSema:Compare} in lifting \ISA program
%%binaries to LLVM bitcode. The \compd is the component which has to follow some
%%idioms specific to the lifter (under test), which are mostly  related to
%%generating code while IR composition. Following such idioms, detailed in
%%Section~\ref{sec:compd}, makes \compd tied to \mcsema. However, we believe
%%that the efforts required to customize the \compd to cater to the needs of
%%some other lifter (like ~\cite{FCD,DiFederico:CC2017}) is quite practical and
%%does not overshadow the benefit that we reap-out by abandoning a heavy-weigh
%%equivalence checker. 
%%%\todo{We leave it for future work??}
%
%%\paragraph{Our TCB (Trusted Computing Base)}
%%Followings are the components our approach relies on and are considered to be
%%trusted.
%%\begin{itemize}
%    %\item The public available \LLVM semantics~\cite{LLVMSEMA}.
%    %\item The publicly available \ISA semantics~\cite{DasguptaAdve:PLDI19}.
%    %\item LLVM optimization passes. The idea of using compiler optimization for
%    %normalizing expression is not
%    %new and many instance of it can be found in modern compilers as part of
%    %compilation-time optimizations or in binary-similarity
%    %research~\cite{Yaniv17}. However, to the best of our knowledge, our work is
%    %the first attempt to applying compiler optimizations to assist program
%    %equivalence. The idea capitalizes on the plethora of research invested in
%    %validating the compiler optimizations using program equivalence
%    %checking
%    %~\cite{DDEC:OOPSLA:2013,Churchill19,BBEquiv17,Lopes2016,Kundu:2009,Tate:2009,Necula:2000,Pnueli:1998,Stepp:2011,Tristan:2011,TVOC:CAV2005}.
%
%%\end{itemize}
%
%\sd{These contributions need to be strengthened: talk about single-instruction
%validation and the bugs we found. Talk about number for the matcher, how
%successful it was (\# of alerts, lines of code analyzed etc.)}
%
%\paragraph{Contributions.} In summary, we make the following contributions:
%
%\begin{enumerate}
%%% novel: scaling SIV to PLV, whereas other stop a SIV
%    \item \emph{Simple \& Novel approach} 
%    
%    We present a highly simple, scalable approach
%    for establishing semantic equivalence between the source binary program and
%    the target lifted IR program.  The novelty lies in reducing the semantic
%    equivalence checking to syntactic checking by taking advantage of the
%    results of \siv and apply it for \plv.
%
%
%    \cmt{The novelty lies in scaling the \tv for single-instruction lifting to
%      solve \plv, by reusing validated IR sequences of individual lifted
%        instructions to create an artifact (the output of \compd), which makes
%        the program equivalence problem easier than what is it perceived to be.
%    }
%%
%    \cmt{Whereas, most of the existing work address the problem of \siv using
%      solutions which are either fundamentally limited because its
%        effectiveness largely depends on randomly generated test-cases or
%        non-scalable to \plv. }
%
%    \item \emph{Practical \& Scalable program equivalence.}
%
%    We demonstrated that  \tv of lifters is feasible without any
%    instrumentation of the lifter or employing heavy-weight equivalence
%    checking in the validation pipeline.
%
%    \cmt{ The \compd is responsible for reducing the \tv problem between \ISA
%      binary program \& lifted \LLVM to an equivalence checking problem, which
%        is then reduced further to a syntactic equality check by the normalizer
%        + matcher combination, thus avoiding any performance-heavy equivalence
%        checker in the pipeline. }
%
%    %\item \emph{Compiler IR optimizations assisting program equivalence.}
%
%\end{enumerate}
%
%%%
%%For example, when the source code is not available binary analysis is the only
%%viable option.
%%%
%%There are other scenarios when it is not desirable to trust the compiler. For
%%example, either due to compiler bugs or due to aggressive optimization that
%%compiler does in the presence of unspecified behavior, there can be a mismatch
%%between what a programmer intends and what is actually generated by the
%%compiler and hence executed on processor.  Consequently, analyses that are
%%performed on source code can fail to detect certain bugs and vulnerabilities
%%and this phenomenon is well known as ''What You See Is Not What You eXecute''.
%%This is mainly because an executable reveals more accurate information about
%%the behaviors that might occur during execution; including the actual memory
%%layout, register usage, execution order, optimizations, and artifacts of
%%compiler bugs. A source-level analysis on the other hand, must either make a
%%cruder over-approximation or an unsound under-approximation.
%%%
%%To add, for many programming languages, certain behaviors are left unspecified
%%by the semantics. In such cases, a source-level analysis must account for all
%%possible behaviors, whereas an analysis of an executable generally only has to
%%deal with one possible behavior -- namely, the one for the code sequence
%%chosen by
%%the compiler.
%%%%
%%Moreover, programs typically make extensive use of libraries, including
%%dynamically linked libraries (DLLs), which may not be available in source-code
%%form.  Typically, analyses are performed using code stubs that model the
%%effects of library calls. Such model abstraction, mostly manually created, are
%%likely to contain errors, which may cause an analysis to return incorrect
%%results.  Operating on the binary avoids these issues altogether, since all
%%source languages are translated into a hardware specific, but single target
%%language with no distinction between the source code or library code.
%
%%Any bug in the translation would invalidate the binary analysis results. For
%%example, a malware analysis system might miss vulnerabilities or a binary
%%instrumentation system, instrumenting a buggy IR, might lead to failure or even
%%crash in interpreting the instrumented program. If a performance analysis tool
%%is provided an inaccurate correspondence between the binary and source code,
%%profiling data may be attributed to wrong locations in source code, causing
%%users to miss-identify performance bottleneck.  Therefore, automatic
%%validation tools are needed urgently to uncover hidden problems in a binary
%%translator.
