; ModuleID = 'mcsema/test.proposed.inline.ll'
source_filename = "llvm-link"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-pc-linux-gnu-elf"

%__bss_start_type = type <{ [8 x i8] }>
%G_0x618d80_type = type <{ [8 x i8] }>
%G_0xb231__rip__type = type <{ [8 x i8] }>
%G__0x415fb8_type = type <{ [8 x i8] }>
%G__0x416519_type = type <{ [8 x i8] }>
%G__0x416559_type = type <{ [8 x i8] }>
%G__0x416594_type = type <{ [8 x i8] }>
%G__0x4165bd_type = type <{ [8 x i8] }>
%G__0x4165c1_type = type <{ [8 x i8] }>
%G__0x4165db_type = type <{ [8 x i8] }>
%G__0x4165ea_type = type <{ [8 x i8] }>
%G__0x4165fc_type = type <{ [8 x i8] }>
%struct.State = type { %struct.ArchState, [32 x %union.VectorReg], %struct.ArithFlags, %union.anon, %struct.Segments, %struct.AddressSpace, %struct.GPR, %struct.X87Stack, %struct.MMX, %struct.FPUStatusFlags, %union.anon, %union.FPU, %struct.SegmentCaches }
%struct.ArchState = type { i32, i32, %union.anon }
%union.VectorReg = type { %union.vec512_t }
%union.vec512_t = type { %struct.uint64v8_t }
%struct.uint64v8_t = type { [8 x i64] }
%struct.ArithFlags = type { i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8 }
%struct.Segments = type { i16, %union.SegmentSelector, i16, %union.SegmentSelector, i16, %union.SegmentSelector, i16, %union.SegmentSelector, i16, %union.SegmentSelector, i16, %union.SegmentSelector }
%union.SegmentSelector = type { i16 }
%struct.AddressSpace = type { i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg }
%struct.Reg = type { %union.anon }
%struct.GPR = type { i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg, i64, %struct.Reg }
%struct.X87Stack = type { [8 x %struct.anon.3] }
%struct.anon.3 = type { i64, double }
%struct.MMX = type { [8 x %struct.anon.4] }
%struct.anon.4 = type { i64, %union.vec64_t }
%union.vec64_t = type { %struct.uint64v1_t }
%struct.uint64v1_t = type { [1 x i64] }
%struct.FPUStatusFlags = type { i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, i8, [4 x i8] }
%union.anon = type { i64 }
%union.FPU = type { %struct.anon.13 }
%struct.anon.13 = type { %struct.FpuFXSAVE, [96 x i8] }
%struct.FpuFXSAVE = type { %union.SegmentSelector, %union.SegmentSelector, %union.FPUAbridgedTagWord, i8, i16, i32, %union.SegmentSelector, i16, i32, %union.SegmentSelector, i16, %union.FPUControlStatus, %union.FPUControlStatus, [8 x %struct.FPUStackElem], [16 x %union.vec128_t] }
%union.FPUAbridgedTagWord = type { i8 }
%union.FPUControlStatus = type { i32 }
%struct.FPUStackElem = type { %union.anon.11, [6 x i8] }
%union.anon.11 = type { %struct.float80_t }
%struct.float80_t = type { [10 x i8] }
%union.vec128_t = type { %struct.uint128v1_t }
%struct.uint128v1_t = type { [1 x i128] }
%struct.SegmentCaches = type { %struct.SegmentShadow, %struct.SegmentShadow, %struct.SegmentShadow, %struct.SegmentShadow, %struct.SegmentShadow, %struct.SegmentShadow }
%struct.SegmentShadow = type { %union.anon, i32, i32 }
%struct.Memory = type opaque

@__bss_start = local_unnamed_addr global %__bss_start_type zeroinitializer
@G_0x618d80 = local_unnamed_addr global %G_0x618d80_type zeroinitializer
@G_0xb231__rip_ = global %G_0xb231__rip__type zeroinitializer
@G__0x415fb8 = global %G__0x415fb8_type zeroinitializer
@G__0x416519 = global %G__0x416519_type zeroinitializer
@G__0x416559 = global %G__0x416559_type zeroinitializer
@G__0x416594 = global %G__0x416594_type zeroinitializer
@G__0x4165bd = global %G__0x4165bd_type zeroinitializer
@G__0x4165c1 = global %G__0x4165c1_type zeroinitializer
@G__0x4165db = global %G__0x4165db_type zeroinitializer
@G__0x4165ea = global %G__0x4165ea_type zeroinitializer
@G__0x4165fc = global %G__0x4165fc_type zeroinitializer

declare %struct.Memory* @__remill_error(%struct.State* dereferenceable(3376), i64, %struct.Memory*) local_unnamed_addr

; Function Attrs: nounwind readnone
declare i32 @llvm.ctpop.i32(i32) #0

declare extern_weak x86_64_sysvcc i64 @fprintf(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)

declare %struct.Memory* @__remill_function_call(%struct.State* dereferenceable(3376), i64, %struct.Memory*) local_unnamed_addr

declare %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* noalias dereferenceable(3376), i64, %struct.Memory* noalias readnone returned) local_unnamed_addr

declare %struct.Memory* @sub_4137b0.BZ2_hbMakeCodeLengths(%struct.State* noalias dereferenceable(3376), i64, %struct.Memory* noalias readnone returned) local_unnamed_addr

declare %struct.Memory* @sub_413e60.BZ2_hbAssignCodes(%struct.State* noalias dereferenceable(3376), i64, %struct.Memory* noalias readnone returned) local_unnamed_addr

declare %struct.Memory* @sub_409ca0.bsW(%struct.State* noalias dereferenceable(3376), i64, %struct.Memory* noalias readnone returned) local_unnamed_addr

; Function Attrs: alwaysinline
define %struct.Memory* @sendMTFValues(%struct.State* noalias, i64, %struct.Memory* noalias) local_unnamed_addr #1 {
entry:
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP.i = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP.i, align 8
  %5 = add i64 %1, 1
  store i64 %5, i64* %3, align 8
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = add i64 %7, -8
  %9 = inttoptr i64 %8 to i64*
  store i64 %4, i64* %9, align 8
  %10 = load i64, i64* %3, align 8
  store i64 %8, i64* %RBP.i, align 8
  %11 = add i64 %7, -280
  store i64 %11, i64* %6, align 8
  %12 = icmp ult i64 %8, 272
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = trunc i64 %11 to i32
  %16 = and i32 %15, 255
  %17 = tail call i32 @llvm.ctpop.i32(i32 %16)
  %18 = trunc i32 %17 to i8
  %19 = and i8 %18, 1
  %20 = xor i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %20, i8* %21, align 1
  %22 = xor i64 %8, 16
  %23 = xor i64 %22, %11
  %24 = lshr i64 %23, 4
  %25 = trunc i64 %24 to i8
  %26 = and i8 %25, 1
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %26, i8* %27, align 1
  %28 = icmp eq i64 %11, 0
  %29 = zext i1 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %29, i8* %30, align 1
  %31 = lshr i64 %11, 63
  %32 = trunc i64 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %32, i8* %33, align 1
  %34 = lshr i64 %8, 63
  %35 = xor i64 %31, %34
  %36 = add nuw nsw i64 %35, %34
  %37 = icmp eq i64 %36, 2
  %38 = zext i1 %37 to i8
  %39 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %38, i8* %39, align 1
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0
  %RDI.i2910 = getelementptr inbounds %union.anon, %union.anon* %40, i64 0, i32 0
  %41 = add i64 %7, -16
  %42 = load i64, i64* %RDI.i2910, align 8
  %43 = add i64 %10, 14
  store i64 %43, i64* %3, align 8
  %44 = inttoptr i64 %41 to i64*
  store i64 %42, i64* %44, align 8
  %45 = load i64, i64* %RBP.i, align 8
  %46 = add i64 %45, -8
  %47 = load i64, i64* %3, align 8
  %48 = add i64 %47, 4
  store i64 %48, i64* %3, align 8
  %49 = inttoptr i64 %46 to i64*
  %50 = load i64, i64* %49, align 8
  store i64 %50, i64* %RDI.i2910, align 8
  %51 = add i64 %50, 72
  %52 = add i64 %47, 8
  store i64 %52, i64* %3, align 8
  %53 = inttoptr i64 %51 to i64*
  %54 = load i64, i64* %53, align 8
  store i64 %54, i64* %RDI.i2910, align 8
  %55 = add i64 %45, -120
  %56 = add i64 %47, 12
  store i64 %56, i64* %3, align 8
  %57 = inttoptr i64 %55 to i64*
  store i64 %54, i64* %57, align 8
  %58 = load i64, i64* %RBP.i, align 8
  %59 = add i64 %58, -8
  %60 = load i64, i64* %3, align 8
  %61 = add i64 %60, 4
  store i64 %61, i64* %3, align 8
  %62 = inttoptr i64 %59 to i64*
  %63 = load i64, i64* %62, align 8
  store i64 %63, i64* %RDI.i2910, align 8
  %64 = add i64 %63, 656
  %65 = add i64 %60, 11
  store i64 %65, i64* %3, align 8
  %66 = inttoptr i64 %64 to i32*
  %67 = load i32, i32* %66, align 4
  %68 = add i32 %67, -3
  %69 = icmp ult i32 %67, 3
  %70 = zext i1 %69 to i8
  store i8 %70, i8* %14, align 1
  %71 = and i32 %68, 255
  %72 = tail call i32 @llvm.ctpop.i32(i32 %71)
  %73 = trunc i32 %72 to i8
  %74 = and i8 %73, 1
  %75 = xor i8 %74, 1
  store i8 %75, i8* %21, align 1
  %76 = xor i32 %68, %67
  %77 = lshr i32 %76, 4
  %78 = trunc i32 %77 to i8
  %79 = and i8 %78, 1
  store i8 %79, i8* %27, align 1
  %80 = icmp eq i32 %68, 0
  %81 = zext i1 %80 to i8
  store i8 %81, i8* %30, align 1
  %82 = lshr i32 %68, 31
  %83 = trunc i32 %82 to i8
  store i8 %83, i8* %33, align 1
  %84 = lshr i32 %67, 31
  %85 = xor i32 %82, %84
  %86 = add nuw nsw i32 %85, %84
  %87 = icmp eq i32 %86, 2
  %88 = zext i1 %87 to i8
  store i8 %88, i8* %39, align 1
  %89 = icmp ne i8 %83, 0
  %90 = xor i1 %89, %87
  %.v296 = select i1 %90, i64 73, i64 17
  %91 = add i64 %60, %.v296
  store i64 %91, i64* %3, align 8
  br i1 %90, label %entry.block_.L_40a2b4_crit_edge, label %block_40a27c

entry.block_.L_40a2b4_crit_edge:                  ; preds = %entry
  %.pre280 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %.pre281 = getelementptr inbounds %union.anon, %union.anon* %.pre280, i64 0, i32 0
  %.pre282 = bitcast %union.anon* %.pre280 to i32*
  br label %block_.L_40a2b4

block_40a27c:                                     ; preds = %entry
  %RSI.i11330 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  store i64 ptrtoint (%G__0x416519_type* @G__0x416519 to i64), i64* %RSI.i11330, align 8
  %92 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %92, i64* %RDI.i2910, align 8
  %93 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %RAX.i11588 = getelementptr inbounds %union.anon, %union.anon* %93, i64 0, i32 0
  %94 = add i64 %91, 22
  store i64 %94, i64* %3, align 8
  %95 = load i64, i64* %62, align 8
  store i64 %95, i64* %RAX.i11588, align 8
  %RDX.i11607 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %96 = add i64 %95, 108
  %97 = add i64 %91, 25
  store i64 %97, i64* %3, align 8
  %98 = inttoptr i64 %96 to i32*
  %99 = load i32, i32* %98, align 4
  %100 = zext i32 %99 to i64
  store i64 %100, i64* %RDX.i11607, align 8
  %101 = add i64 %91, 29
  store i64 %101, i64* %3, align 8
  %102 = load i64, i64* %62, align 8
  store i64 %102, i64* %RAX.i11588, align 8
  %RCX.i11601 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %103 = add i64 %102, 668
  %104 = add i64 %91, 35
  store i64 %104, i64* %3, align 8
  %105 = inttoptr i64 %103 to i32*
  %106 = load i32, i32* %105, align 4
  %107 = zext i32 %106 to i64
  store i64 %107, i64* %RCX.i11601, align 8
  %108 = add i64 %91, 39
  store i64 %108, i64* %3, align 8
  %109 = load i64, i64* %62, align 8
  store i64 %109, i64* %RAX.i11588, align 8
  %110 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %111 = add i64 %109, 124
  %112 = add i64 %91, 43
  store i64 %112, i64* %3, align 8
  %113 = inttoptr i64 %111 to i32*
  %114 = load i32, i32* %113, align 4
  %115 = zext i32 %114 to i64
  store i64 %115, i64* %110, align 8
  %AL.i11592 = bitcast %union.anon* %93 to i8*
  store i8 0, i8* %AL.i11592, align 1
  %116 = add i64 %91, -39244
  %117 = add i64 %91, 50
  %118 = load i64, i64* %6, align 8
  %119 = add i64 %118, -8
  %120 = inttoptr i64 %119 to i64*
  store i64 %117, i64* %120, align 8
  store i64 %119, i64* %6, align 8
  store i64 %116, i64* %3, align 8
  %121 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %2)
  %EAX.i11585 = bitcast %union.anon* %93 to i32*
  %122 = load i64, i64* %RBP.i, align 8
  %123 = add i64 %122, -220
  %124 = load i32, i32* %EAX.i11585, align 4
  %125 = load i64, i64* %3, align 8
  %126 = add i64 %125, 6
  store i64 %126, i64* %3, align 8
  %127 = inttoptr i64 %123 to i32*
  store i32 %124, i32* %127, align 4
  %.pre = load i64, i64* %RBP.i, align 8
  %.pre188 = load i64, i64* %3, align 8
  br label %block_.L_40a2b4

block_.L_40a2b4:                                  ; preds = %entry.block_.L_40a2b4_crit_edge, %block_40a27c
  %EAX.i11561.pre-phi = phi i32* [ %.pre282, %entry.block_.L_40a2b4_crit_edge ], [ %EAX.i11585, %block_40a27c ]
  %RAX.i11582.pre-phi = phi i64* [ %.pre281, %entry.block_.L_40a2b4_crit_edge ], [ %RAX.i11588, %block_40a27c ]
  %.pre-phi = phi %union.anon* [ %.pre280, %entry.block_.L_40a2b4_crit_edge ], [ %93, %block_40a27c ]
  %128 = phi i64 [ %91, %entry.block_.L_40a2b4_crit_edge ], [ %.pre188, %block_40a27c ]
  %129 = phi i64 [ %58, %entry.block_.L_40a2b4_crit_edge ], [ %.pre, %block_40a27c ]
  %MEMORY.0 = phi %struct.Memory* [ %2, %entry.block_.L_40a2b4_crit_edge ], [ %121, %block_40a27c ]
  %130 = add i64 %129, -8
  %131 = add i64 %128, 4
  store i64 %131, i64* %3, align 8
  %132 = inttoptr i64 %130 to i64*
  %133 = load i64, i64* %132, align 8
  store i64 %133, i64* %RAX.i11582.pre-phi, align 8
  %134 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %RCX.i11580 = getelementptr inbounds %union.anon, %union.anon* %134, i64 0, i32 0
  %135 = add i64 %133, 124
  %136 = add i64 %128, 7
  store i64 %136, i64* %3, align 8
  %137 = inttoptr i64 %135 to i32*
  %138 = load i32, i32* %137, align 4
  %139 = add i32 %138, 2
  %140 = zext i32 %139 to i64
  store i64 %140, i64* %RCX.i11580, align 8
  %141 = icmp ugt i32 %138, -3
  %142 = zext i1 %141 to i8
  store i8 %142, i8* %14, align 1
  %143 = and i32 %139, 255
  %144 = tail call i32 @llvm.ctpop.i32(i32 %143)
  %145 = trunc i32 %144 to i8
  %146 = and i8 %145, 1
  %147 = xor i8 %146, 1
  store i8 %147, i8* %21, align 1
  %148 = xor i32 %139, %138
  %149 = lshr i32 %148, 4
  %150 = trunc i32 %149 to i8
  %151 = and i8 %150, 1
  store i8 %151, i8* %27, align 1
  %152 = icmp eq i32 %139, 0
  %153 = zext i1 %152 to i8
  store i8 %153, i8* %30, align 1
  %154 = lshr i32 %139, 31
  %155 = trunc i32 %154 to i8
  store i8 %155, i8* %33, align 1
  %156 = lshr i32 %138, 31
  %157 = xor i32 %154, %156
  %158 = add nuw nsw i32 %157, %154
  %159 = icmp eq i32 %158, 2
  %160 = zext i1 %159 to i8
  store i8 %160, i8* %39, align 1
  %ECX.i11574 = bitcast %union.anon* %134 to i32*
  %161 = add i64 %129, -56
  %162 = add i64 %128, 13
  store i64 %162, i64* %3, align 8
  %163 = inttoptr i64 %161 to i32*
  store i32 %139, i32* %163, align 4
  %164 = load i64, i64* %RBP.i, align 8
  %165 = add i64 %164, -16
  %166 = load i64, i64* %3, align 8
  %167 = add i64 %166, 7
  store i64 %167, i64* %3, align 8
  %168 = inttoptr i64 %165 to i32*
  store i32 0, i32* %168, align 4
  %.pre189 = load i64, i64* %3, align 8
  br label %block_.L_40a2c8

block_.L_40a2c8:                                  ; preds = %block_.L_40a313, %block_.L_40a2b4
  %169 = phi i64 [ %368, %block_.L_40a313 ], [ %.pre189, %block_.L_40a2b4 ]
  %170 = load i64, i64* %RBP.i, align 8
  %171 = add i64 %170, -16
  %172 = add i64 %169, 4
  store i64 %172, i64* %3, align 8
  %173 = inttoptr i64 %171 to i32*
  %174 = load i32, i32* %173, align 4
  %175 = add i32 %174, -6
  %176 = icmp ult i32 %174, 6
  %177 = zext i1 %176 to i8
  store i8 %177, i8* %14, align 1
  %178 = and i32 %175, 255
  %179 = tail call i32 @llvm.ctpop.i32(i32 %178)
  %180 = trunc i32 %179 to i8
  %181 = and i8 %180, 1
  %182 = xor i8 %181, 1
  store i8 %182, i8* %21, align 1
  %183 = xor i32 %175, %174
  %184 = lshr i32 %183, 4
  %185 = trunc i32 %184 to i8
  %186 = and i8 %185, 1
  store i8 %186, i8* %27, align 1
  %187 = icmp eq i32 %175, 0
  %188 = zext i1 %187 to i8
  store i8 %188, i8* %30, align 1
  %189 = lshr i32 %175, 31
  %190 = trunc i32 %189 to i8
  store i8 %190, i8* %33, align 1
  %191 = lshr i32 %174, 31
  %192 = xor i32 %189, %191
  %193 = add nuw nsw i32 %192, %191
  %194 = icmp eq i32 %193, 2
  %195 = zext i1 %194 to i8
  store i8 %195, i8* %39, align 1
  %196 = icmp ne i8 %190, 0
  %197 = xor i1 %196, %194
  %.v297 = select i1 %197, i64 10, i64 94
  %198 = add i64 %169, %.v297
  store i64 %198, i64* %3, align 8
  br i1 %197, label %block_40a2d2, label %block_.L_40a326

block_40a2d2:                                     ; preds = %block_.L_40a2c8
  %199 = add i64 %170, -12
  %200 = add i64 %198, 7
  store i64 %200, i64* %3, align 8
  %201 = inttoptr i64 %199 to i32*
  store i32 0, i32* %201, align 4
  %.pre279 = load i64, i64* %3, align 8
  br label %block_.L_40a2d9

block_.L_40a2d9:                                  ; preds = %block_40a2e5, %block_40a2d2
  %202 = phi i64 [ %338, %block_40a2e5 ], [ %.pre279, %block_40a2d2 ]
  %203 = load i64, i64* %RBP.i, align 8
  %204 = add i64 %203, -12
  %205 = add i64 %202, 3
  store i64 %205, i64* %3, align 8
  %206 = inttoptr i64 %204 to i32*
  %207 = load i32, i32* %206, align 4
  %208 = zext i32 %207 to i64
  store i64 %208, i64* %RAX.i11582.pre-phi, align 8
  %209 = add i64 %203, -56
  %210 = add i64 %202, 6
  store i64 %210, i64* %3, align 8
  %211 = inttoptr i64 %209 to i32*
  %212 = load i32, i32* %211, align 4
  %213 = sub i32 %207, %212
  %214 = icmp ult i32 %207, %212
  %215 = zext i1 %214 to i8
  store i8 %215, i8* %14, align 1
  %216 = and i32 %213, 255
  %217 = tail call i32 @llvm.ctpop.i32(i32 %216)
  %218 = trunc i32 %217 to i8
  %219 = and i8 %218, 1
  %220 = xor i8 %219, 1
  store i8 %220, i8* %21, align 1
  %221 = xor i32 %212, %207
  %222 = xor i32 %221, %213
  %223 = lshr i32 %222, 4
  %224 = trunc i32 %223 to i8
  %225 = and i8 %224, 1
  store i8 %225, i8* %27, align 1
  %226 = icmp eq i32 %213, 0
  %227 = zext i1 %226 to i8
  store i8 %227, i8* %30, align 1
  %228 = lshr i32 %213, 31
  %229 = trunc i32 %228 to i8
  store i8 %229, i8* %33, align 1
  %230 = lshr i32 %207, 31
  %231 = lshr i32 %212, 31
  %232 = xor i32 %231, %230
  %233 = xor i32 %228, %230
  %234 = add nuw nsw i32 %233, %232
  %235 = icmp eq i32 %234, 2
  %236 = zext i1 %235 to i8
  store i8 %236, i8* %39, align 1
  %237 = icmp ne i8 %229, 0
  %238 = xor i1 %237, %235
  %.v295 = select i1 %238, i64 12, i64 58
  %239 = add i64 %202, %.v295
  store i64 %239, i64* %3, align 8
  br i1 %238, label %block_40a2e5, label %block_.L_40a313

block_40a2e5:                                     ; preds = %block_.L_40a2d9
  %240 = add i64 %203, -8
  %241 = add i64 %239, 4
  store i64 %241, i64* %3, align 8
  %242 = inttoptr i64 %240 to i64*
  %243 = load i64, i64* %242, align 8
  %244 = add i64 %243, 37708
  store i64 %244, i64* %RAX.i11582.pre-phi, align 8
  %245 = icmp ugt i64 %243, -37709
  %246 = zext i1 %245 to i8
  store i8 %246, i8* %14, align 1
  %247 = trunc i64 %244 to i32
  %248 = and i32 %247, 255
  %249 = tail call i32 @llvm.ctpop.i32(i32 %248)
  %250 = trunc i32 %249 to i8
  %251 = and i8 %250, 1
  %252 = xor i8 %251, 1
  store i8 %252, i8* %21, align 1
  %253 = xor i64 %244, %243
  %254 = lshr i64 %253, 4
  %255 = trunc i64 %254 to i8
  %256 = and i8 %255, 1
  store i8 %256, i8* %27, align 1
  %257 = icmp eq i64 %244, 0
  %258 = zext i1 %257 to i8
  store i8 %258, i8* %30, align 1
  %259 = lshr i64 %244, 63
  %260 = trunc i64 %259 to i8
  store i8 %260, i8* %33, align 1
  %261 = lshr i64 %243, 63
  %262 = xor i64 %259, %261
  %263 = add nuw nsw i64 %262, %259
  %264 = icmp eq i64 %263, 2
  %265 = zext i1 %264 to i8
  store i8 %265, i8* %39, align 1
  %266 = add i64 %203, -16
  %267 = add i64 %239, 14
  store i64 %267, i64* %3, align 8
  %268 = inttoptr i64 %266 to i32*
  %269 = load i32, i32* %268, align 4
  %270 = sext i32 %269 to i64
  %271 = mul nsw i64 %270, 258
  store i64 %271, i64* %RCX.i11580, align 8
  %272 = lshr i64 %271, 63
  %273 = add i64 %271, %244
  store i64 %273, i64* %RAX.i11582.pre-phi, align 8
  %274 = icmp ult i64 %273, %244
  %275 = icmp ult i64 %273, %271
  %276 = or i1 %274, %275
  %277 = zext i1 %276 to i8
  store i8 %277, i8* %14, align 1
  %278 = trunc i64 %273 to i32
  %279 = and i32 %278, 255
  %280 = tail call i32 @llvm.ctpop.i32(i32 %279)
  %281 = trunc i32 %280 to i8
  %282 = and i8 %281, 1
  %283 = xor i8 %282, 1
  store i8 %283, i8* %21, align 1
  %284 = xor i64 %271, %244
  %285 = xor i64 %284, %273
  %286 = lshr i64 %285, 4
  %287 = trunc i64 %286 to i8
  %288 = and i8 %287, 1
  store i8 %288, i8* %27, align 1
  %289 = icmp eq i64 %273, 0
  %290 = zext i1 %289 to i8
  store i8 %290, i8* %30, align 1
  %291 = lshr i64 %273, 63
  %292 = trunc i64 %291 to i8
  store i8 %292, i8* %33, align 1
  %293 = xor i64 %291, %259
  %294 = xor i64 %291, %272
  %295 = add nuw nsw i64 %293, %294
  %296 = icmp eq i64 %295, 2
  %297 = zext i1 %296 to i8
  store i8 %297, i8* %39, align 1
  %298 = load i64, i64* %RBP.i, align 8
  %299 = add i64 %298, -12
  %300 = add i64 %239, 28
  store i64 %300, i64* %3, align 8
  %301 = inttoptr i64 %299 to i32*
  %302 = load i32, i32* %301, align 4
  %303 = sext i32 %302 to i64
  store i64 %303, i64* %RCX.i11580, align 8
  %304 = add i64 %273, %303
  %305 = add i64 %239, 32
  store i64 %305, i64* %3, align 8
  %306 = inttoptr i64 %304 to i8*
  store i8 15, i8* %306, align 1
  %307 = load i64, i64* %RBP.i, align 8
  %308 = add i64 %307, -12
  %309 = load i64, i64* %3, align 8
  %310 = add i64 %309, 3
  store i64 %310, i64* %3, align 8
  %311 = inttoptr i64 %308 to i32*
  %312 = load i32, i32* %311, align 4
  %313 = add i32 %312, 1
  %314 = zext i32 %313 to i64
  store i64 %314, i64* %RAX.i11582.pre-phi, align 8
  %315 = icmp eq i32 %312, -1
  %316 = icmp eq i32 %313, 0
  %317 = or i1 %315, %316
  %318 = zext i1 %317 to i8
  store i8 %318, i8* %14, align 1
  %319 = and i32 %313, 255
  %320 = tail call i32 @llvm.ctpop.i32(i32 %319)
  %321 = trunc i32 %320 to i8
  %322 = and i8 %321, 1
  %323 = xor i8 %322, 1
  store i8 %323, i8* %21, align 1
  %324 = xor i32 %313, %312
  %325 = lshr i32 %324, 4
  %326 = trunc i32 %325 to i8
  %327 = and i8 %326, 1
  store i8 %327, i8* %27, align 1
  %328 = zext i1 %316 to i8
  store i8 %328, i8* %30, align 1
  %329 = lshr i32 %313, 31
  %330 = trunc i32 %329 to i8
  store i8 %330, i8* %33, align 1
  %331 = lshr i32 %312, 31
  %332 = xor i32 %329, %331
  %333 = add nuw nsw i32 %332, %329
  %334 = icmp eq i32 %333, 2
  %335 = zext i1 %334 to i8
  store i8 %335, i8* %39, align 1
  %336 = add i64 %309, 9
  store i64 %336, i64* %3, align 8
  store i32 %313, i32* %311, align 4
  %337 = load i64, i64* %3, align 8
  %338 = add i64 %337, -53
  store i64 %338, i64* %3, align 8
  br label %block_.L_40a2d9

block_.L_40a313:                                  ; preds = %block_.L_40a2d9
  %339 = add i64 %203, -16
  %340 = add i64 %239, 8
  store i64 %340, i64* %3, align 8
  %341 = inttoptr i64 %339 to i32*
  %342 = load i32, i32* %341, align 4
  %343 = add i32 %342, 1
  %344 = zext i32 %343 to i64
  store i64 %344, i64* %RAX.i11582.pre-phi, align 8
  %345 = icmp eq i32 %342, -1
  %346 = icmp eq i32 %343, 0
  %347 = or i1 %345, %346
  %348 = zext i1 %347 to i8
  store i8 %348, i8* %14, align 1
  %349 = and i32 %343, 255
  %350 = tail call i32 @llvm.ctpop.i32(i32 %349)
  %351 = trunc i32 %350 to i8
  %352 = and i8 %351, 1
  %353 = xor i8 %352, 1
  store i8 %353, i8* %21, align 1
  %354 = xor i32 %343, %342
  %355 = lshr i32 %354, 4
  %356 = trunc i32 %355 to i8
  %357 = and i8 %356, 1
  store i8 %357, i8* %27, align 1
  %358 = zext i1 %346 to i8
  store i8 %358, i8* %30, align 1
  %359 = lshr i32 %343, 31
  %360 = trunc i32 %359 to i8
  store i8 %360, i8* %33, align 1
  %361 = lshr i32 %342, 31
  %362 = xor i32 %359, %361
  %363 = add nuw nsw i32 %362, %359
  %364 = icmp eq i32 %363, 2
  %365 = zext i1 %364 to i8
  store i8 %365, i8* %39, align 1
  %366 = add i64 %239, 14
  store i64 %366, i64* %3, align 8
  store i32 %343, i32* %341, align 4
  %367 = load i64, i64* %3, align 8
  %368 = add i64 %367, -89
  store i64 %368, i64* %3, align 8
  br label %block_.L_40a2c8

block_.L_40a326:                                  ; preds = %block_.L_40a2c8
  %369 = add i64 %170, -8
  %370 = add i64 %198, 4
  store i64 %370, i64* %3, align 8
  %371 = inttoptr i64 %369 to i64*
  %372 = load i64, i64* %371, align 8
  store i64 %372, i64* %RAX.i11582.pre-phi, align 8
  %373 = add i64 %372, 668
  %374 = add i64 %198, 11
  store i64 %374, i64* %3, align 8
  %375 = inttoptr i64 %373 to i32*
  %376 = load i32, i32* %375, align 4
  store i8 0, i8* %14, align 1
  %377 = and i32 %376, 255
  %378 = tail call i32 @llvm.ctpop.i32(i32 %377)
  %379 = trunc i32 %378 to i8
  %380 = and i8 %379, 1
  %381 = xor i8 %380, 1
  store i8 %381, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %382 = icmp eq i32 %376, 0
  %383 = zext i1 %382 to i8
  store i8 %383, i8* %30, align 1
  %384 = lshr i32 %376, 31
  %385 = trunc i32 %384 to i8
  store i8 %385, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %386 = xor i1 %382, true
  %387 = icmp eq i8 %385, 0
  %388 = and i1 %387, %386
  %.v298 = select i1 %388, i64 27, i64 17
  %389 = add i64 %198, %.v298
  store i64 %389, i64* %3, align 8
  br i1 %388, label %block_.L_40a341, label %block_40a337

block_40a337:                                     ; preds = %block_.L_40a326
  store i64 3001, i64* %RDI.i2910, align 8
  %390 = add i64 %389, -21527
  %391 = add i64 %389, 10
  %392 = load i64, i64* %6, align 8
  %393 = add i64 %392, -8
  %394 = inttoptr i64 %393 to i64*
  store i64 %391, i64* %394, align 8
  store i64 %393, i64* %6, align 8
  store i64 %390, i64* %3, align 8
  %call2_40a33c = tail call %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* nonnull %0, i64 %390, %struct.Memory* %MEMORY.0)
  %.pre190 = load i64, i64* %RBP.i, align 8
  %.pre191 = load i64, i64* %3, align 8
  br label %block_.L_40a341

block_.L_40a341:                                  ; preds = %block_40a337, %block_.L_40a326
  %395 = phi i64 [ %389, %block_.L_40a326 ], [ %.pre191, %block_40a337 ]
  %396 = phi i64 [ %170, %block_.L_40a326 ], [ %.pre190, %block_40a337 ]
  %MEMORY.3 = phi %struct.Memory* [ %MEMORY.0, %block_.L_40a326 ], [ %call2_40a33c, %block_40a337 ]
  %397 = add i64 %396, -8
  %398 = add i64 %395, 4
  store i64 %398, i64* %3, align 8
  %399 = inttoptr i64 %397 to i64*
  %400 = load i64, i64* %399, align 8
  store i64 %400, i64* %RAX.i11582.pre-phi, align 8
  %401 = add i64 %400, 668
  %402 = add i64 %395, 14
  store i64 %402, i64* %3, align 8
  %403 = inttoptr i64 %401 to i32*
  %404 = load i32, i32* %403, align 4
  %405 = add i32 %404, -200
  %406 = icmp ult i32 %404, 200
  %407 = zext i1 %406 to i8
  store i8 %407, i8* %14, align 1
  %408 = and i32 %405, 255
  %409 = tail call i32 @llvm.ctpop.i32(i32 %408)
  %410 = trunc i32 %409 to i8
  %411 = and i8 %410, 1
  %412 = xor i8 %411, 1
  store i8 %412, i8* %21, align 1
  %413 = xor i32 %405, %404
  %414 = lshr i32 %413, 4
  %415 = trunc i32 %414 to i8
  %416 = and i8 %415, 1
  store i8 %416, i8* %27, align 1
  %417 = icmp eq i32 %405, 0
  %418 = zext i1 %417 to i8
  store i8 %418, i8* %30, align 1
  %419 = lshr i32 %405, 31
  %420 = trunc i32 %419 to i8
  store i8 %420, i8* %33, align 1
  %421 = lshr i32 %404, 31
  %422 = xor i32 %419, %421
  %423 = add nuw nsw i32 %422, %421
  %424 = icmp eq i32 %423, 2
  %425 = zext i1 %424 to i8
  store i8 %425, i8* %39, align 1
  %426 = icmp ne i8 %420, 0
  %427 = xor i1 %426, %424
  %.v285 = select i1 %427, i64 20, i64 32
  %428 = add i64 %395, %.v285
  store i64 %428, i64* %3, align 8
  br i1 %427, label %block_40a355, label %block_.L_40a361

block_40a355:                                     ; preds = %block_.L_40a341
  %429 = add i64 %396, -72
  %430 = add i64 %428, 7
  store i64 %430, i64* %3, align 8
  %431 = inttoptr i64 %429 to i32*
  store i32 2, i32* %431, align 4
  %432 = load i64, i64* %3, align 8
  %433 = add i64 %432, 123
  br label %block_.L_40a3d7

block_.L_40a361:                                  ; preds = %block_.L_40a341
  %434 = add i64 %428, 4
  store i64 %434, i64* %3, align 8
  %435 = load i64, i64* %399, align 8
  store i64 %435, i64* %RAX.i11582.pre-phi, align 8
  %436 = add i64 %435, 668
  %437 = add i64 %428, 14
  store i64 %437, i64* %3, align 8
  %438 = inttoptr i64 %436 to i32*
  %439 = load i32, i32* %438, align 4
  %440 = add i32 %439, -600
  %441 = icmp ult i32 %439, 600
  %442 = zext i1 %441 to i8
  store i8 %442, i8* %14, align 1
  %443 = and i32 %440, 255
  %444 = tail call i32 @llvm.ctpop.i32(i32 %443)
  %445 = trunc i32 %444 to i8
  %446 = and i8 %445, 1
  %447 = xor i8 %446, 1
  store i8 %447, i8* %21, align 1
  %448 = xor i32 %439, 16
  %449 = xor i32 %448, %440
  %450 = lshr i32 %449, 4
  %451 = trunc i32 %450 to i8
  %452 = and i8 %451, 1
  store i8 %452, i8* %27, align 1
  %453 = icmp eq i32 %440, 0
  %454 = zext i1 %453 to i8
  store i8 %454, i8* %30, align 1
  %455 = lshr i32 %440, 31
  %456 = trunc i32 %455 to i8
  store i8 %456, i8* %33, align 1
  %457 = lshr i32 %439, 31
  %458 = xor i32 %455, %457
  %459 = add nuw nsw i32 %458, %457
  %460 = icmp eq i32 %459, 2
  %461 = zext i1 %460 to i8
  store i8 %461, i8* %39, align 1
  %462 = icmp ne i8 %456, 0
  %463 = xor i1 %462, %460
  %.v284 = select i1 %463, i64 20, i64 32
  %464 = add i64 %428, %.v284
  store i64 %464, i64* %3, align 8
  br i1 %463, label %block_40a375, label %block_.L_40a381

block_40a375:                                     ; preds = %block_.L_40a361
  %465 = add i64 %396, -72
  %466 = add i64 %464, 7
  store i64 %466, i64* %3, align 8
  %467 = inttoptr i64 %465 to i32*
  store i32 3, i32* %467, align 4
  %468 = load i64, i64* %3, align 8
  %469 = add i64 %468, 86
  br label %block_.L_40a3d2

block_.L_40a381:                                  ; preds = %block_.L_40a361
  %470 = add i64 %464, 4
  store i64 %470, i64* %3, align 8
  %471 = load i64, i64* %399, align 8
  store i64 %471, i64* %RAX.i11582.pre-phi, align 8
  %472 = add i64 %471, 668
  %473 = add i64 %464, 14
  store i64 %473, i64* %3, align 8
  %474 = inttoptr i64 %472 to i32*
  %475 = load i32, i32* %474, align 4
  %476 = add i32 %475, -1200
  %477 = icmp ult i32 %475, 1200
  %478 = zext i1 %477 to i8
  store i8 %478, i8* %14, align 1
  %479 = and i32 %476, 255
  %480 = tail call i32 @llvm.ctpop.i32(i32 %479)
  %481 = trunc i32 %480 to i8
  %482 = and i8 %481, 1
  %483 = xor i8 %482, 1
  store i8 %483, i8* %21, align 1
  %484 = xor i32 %475, 16
  %485 = xor i32 %484, %476
  %486 = lshr i32 %485, 4
  %487 = trunc i32 %486 to i8
  %488 = and i8 %487, 1
  store i8 %488, i8* %27, align 1
  %489 = icmp eq i32 %476, 0
  %490 = zext i1 %489 to i8
  store i8 %490, i8* %30, align 1
  %491 = lshr i32 %476, 31
  %492 = trunc i32 %491 to i8
  store i8 %492, i8* %33, align 1
  %493 = lshr i32 %475, 31
  %494 = xor i32 %491, %493
  %495 = add nuw nsw i32 %494, %493
  %496 = icmp eq i32 %495, 2
  %497 = zext i1 %496 to i8
  store i8 %497, i8* %39, align 1
  %498 = icmp ne i8 %492, 0
  %499 = xor i1 %498, %496
  %.v283 = select i1 %499, i64 20, i64 32
  %500 = add i64 %464, %.v283
  store i64 %500, i64* %3, align 8
  br i1 %499, label %block_40a395, label %block_.L_40a3a1

block_40a395:                                     ; preds = %block_.L_40a381
  %501 = add i64 %396, -72
  %502 = add i64 %500, 7
  store i64 %502, i64* %3, align 8
  %503 = inttoptr i64 %501 to i32*
  store i32 4, i32* %503, align 4
  %504 = load i64, i64* %3, align 8
  %505 = add i64 %504, 49
  br label %block_.L_40a3cd

block_.L_40a3a1:                                  ; preds = %block_.L_40a381
  %506 = add i64 %500, 4
  store i64 %506, i64* %3, align 8
  %507 = load i64, i64* %399, align 8
  store i64 %507, i64* %RAX.i11582.pre-phi, align 8
  %508 = add i64 %507, 668
  %509 = add i64 %500, 14
  store i64 %509, i64* %3, align 8
  %510 = inttoptr i64 %508 to i32*
  %511 = load i32, i32* %510, align 4
  %512 = add i32 %511, -2400
  %513 = icmp ult i32 %511, 2400
  %514 = zext i1 %513 to i8
  store i8 %514, i8* %14, align 1
  %515 = and i32 %512, 255
  %516 = tail call i32 @llvm.ctpop.i32(i32 %515)
  %517 = trunc i32 %516 to i8
  %518 = and i8 %517, 1
  %519 = xor i8 %518, 1
  store i8 %519, i8* %21, align 1
  %520 = xor i32 %512, %511
  %521 = lshr i32 %520, 4
  %522 = trunc i32 %521 to i8
  %523 = and i8 %522, 1
  store i8 %523, i8* %27, align 1
  %524 = icmp eq i32 %512, 0
  %525 = zext i1 %524 to i8
  store i8 %525, i8* %30, align 1
  %526 = lshr i32 %512, 31
  %527 = trunc i32 %526 to i8
  store i8 %527, i8* %33, align 1
  %528 = lshr i32 %511, 31
  %529 = xor i32 %526, %528
  %530 = add nuw nsw i32 %529, %528
  %531 = icmp eq i32 %530, 2
  %532 = zext i1 %531 to i8
  store i8 %532, i8* %39, align 1
  %533 = icmp ne i8 %527, 0
  %534 = xor i1 %533, %531
  %.v = select i1 %534, i64 20, i64 32
  %535 = add i64 %500, %.v
  %536 = add i64 %396, -72
  %537 = add i64 %535, 7
  store i64 %537, i64* %3, align 8
  %538 = inttoptr i64 %536 to i32*
  br i1 %534, label %block_40a3b5, label %block_.L_40a3c1

block_40a3b5:                                     ; preds = %block_.L_40a3a1
  store i32 5, i32* %538, align 4
  %539 = load i64, i64* %3, align 8
  %540 = add i64 %539, 12
  store i64 %540, i64* %3, align 8
  br label %block_.L_40a3c8

block_.L_40a3c1:                                  ; preds = %block_.L_40a3a1
  store i32 6, i32* %538, align 4
  %.pre192 = load i64, i64* %3, align 8
  br label %block_.L_40a3c8

block_.L_40a3c8:                                  ; preds = %block_.L_40a3c1, %block_40a3b5
  %541 = phi i64 [ %.pre192, %block_.L_40a3c1 ], [ %540, %block_40a3b5 ]
  %542 = add i64 %541, 5
  store i64 %542, i64* %3, align 8
  br label %block_.L_40a3cd

block_.L_40a3cd:                                  ; preds = %block_.L_40a3c8, %block_40a395
  %storemerge89 = phi i64 [ %505, %block_40a395 ], [ %542, %block_.L_40a3c8 ]
  %543 = add i64 %storemerge89, 5
  store i64 %543, i64* %3, align 8
  br label %block_.L_40a3d2

block_.L_40a3d2:                                  ; preds = %block_.L_40a3cd, %block_40a375
  %storemerge88 = phi i64 [ %469, %block_40a375 ], [ %543, %block_.L_40a3cd ]
  %544 = add i64 %storemerge88, 5
  store i64 %544, i64* %3, align 8
  br label %block_.L_40a3d7

block_.L_40a3d7:                                  ; preds = %block_.L_40a3d2, %block_40a355
  %storemerge = phi i64 [ %433, %block_40a355 ], [ %544, %block_.L_40a3d2 ]
  %545 = load i64, i64* %RBP.i, align 8
  %546 = add i64 %545, -72
  %547 = add i64 %storemerge, 3
  store i64 %547, i64* %3, align 8
  %548 = inttoptr i64 %546 to i32*
  %549 = load i32, i32* %548, align 4
  %550 = zext i32 %549 to i64
  store i64 %550, i64* %RAX.i11582.pre-phi, align 8
  %551 = add i64 %545, -124
  %552 = add i64 %storemerge, 6
  store i64 %552, i64* %3, align 8
  %553 = inttoptr i64 %551 to i32*
  store i32 %549, i32* %553, align 4
  %554 = load i64, i64* %RBP.i, align 8
  %555 = add i64 %554, -8
  %556 = load i64, i64* %3, align 8
  %557 = add i64 %556, 4
  store i64 %557, i64* %3, align 8
  %558 = inttoptr i64 %555 to i64*
  %559 = load i64, i64* %558, align 8
  store i64 %559, i64* %RCX.i11580, align 8
  %560 = add i64 %559, 668
  %561 = add i64 %556, 10
  store i64 %561, i64* %3, align 8
  %562 = inttoptr i64 %560 to i32*
  %563 = load i32, i32* %562, align 4
  %564 = zext i32 %563 to i64
  store i64 %564, i64* %RAX.i11582.pre-phi, align 8
  %565 = add i64 %554, -128
  %566 = add i64 %556, 13
  store i64 %566, i64* %3, align 8
  %567 = inttoptr i64 %565 to i32*
  store i32 %563, i32* %567, align 4
  %568 = load i64, i64* %RBP.i, align 8
  %569 = add i64 %568, -28
  %570 = load i64, i64* %3, align 8
  %571 = add i64 %570, 7
  store i64 %571, i64* %3, align 8
  %572 = inttoptr i64 %569 to i32*
  store i32 0, i32* %572, align 4
  %573 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %574 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %575 = bitcast %union.anon* %574 to i32*
  %576 = getelementptr inbounds %union.anon, %union.anon* %574, i64 0, i32 0
  %AL.i11425 = bitcast %union.anon* %.pre-phi to i8*
  %CL.i11426 = bitcast %union.anon* %134 to i8*
  %DL.i11402 = bitcast %union.anon* %574 to i8*
  %RSI.i11290 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %577 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1
  %578 = bitcast [32 x %union.VectorReg]* %577 to double*
  %579 = getelementptr inbounds [32 x %union.VectorReg], [32 x %union.VectorReg]* %577, i64 0, i64 0, i32 0, i32 0, i32 0, i64 0
  %580 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 0, i32 0, i32 0, i32 0, i64 1
  %581 = bitcast i64* %580 to double*
  %582 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %583 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 19, i32 0, i32 0
  %584 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1
  %585 = bitcast %union.VectorReg* %584 to i8*
  %586 = bitcast %union.VectorReg* %584 to <2 x i32>*
  %587 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1, i32 0, i32 0, i32 0, i64 1
  %588 = bitcast i64* %587 to <2 x i32>*
  %589 = bitcast %union.VectorReg* %584 to float*
  %590 = getelementptr inbounds i8, i8* %585, i64 4
  %591 = bitcast i8* %590 to i32*
  %592 = bitcast i64* %587 to i32*
  %593 = getelementptr inbounds i8, i8* %585, i64 12
  %594 = bitcast i8* %593 to i32*
  %595 = bitcast %union.VectorReg* %584 to <2 x float>*
  %596 = bitcast %union.VectorReg* %584 to double*
  %597 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %RSI.i11312 = getelementptr inbounds %union.anon, %union.anon* %597, i64 0, i32 0
  %.pre193 = load i64, i64* %3, align 8
  br label %block_.L_40a3f1

block_.L_40a3f1:                                  ; preds = %block_.L_40a5ee, %block_.L_40a3d7
  %598 = phi i64 [ %.pre193, %block_.L_40a3d7 ], [ %1604, %block_.L_40a5ee ]
  %MEMORY.8 = phi %struct.Memory* [ %MEMORY.3, %block_.L_40a3d7 ], [ %MEMORY.12, %block_.L_40a5ee ]
  %599 = load i64, i64* %RBP.i, align 8
  %600 = add i64 %599, -124
  %601 = add i64 %598, 4
  store i64 %601, i64* %3, align 8
  %602 = inttoptr i64 %600 to i32*
  %603 = load i32, i32* %602, align 4
  store i8 0, i8* %14, align 1
  %604 = and i32 %603, 255
  %605 = tail call i32 @llvm.ctpop.i32(i32 %604)
  %606 = trunc i32 %605 to i8
  %607 = and i8 %606, 1
  %608 = xor i8 %607, 1
  store i8 %608, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %609 = icmp eq i32 %603, 0
  %610 = zext i1 %609 to i8
  store i8 %610, i8* %30, align 1
  %611 = lshr i32 %603, 31
  %612 = trunc i32 %611 to i8
  store i8 %612, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %613 = icmp ne i8 %612, 0
  %614 = or i1 %609, %613
  %.v299 = select i1 %614, i64 546, i64 10
  %615 = add i64 %598, %.v299
  store i64 %615, i64* %3, align 8
  br i1 %614, label %block_.L_40a613, label %block_40a3fb

block_40a3fb:                                     ; preds = %block_.L_40a3f1
  %616 = add i64 %599, -128
  %617 = add i64 %615, 3
  store i64 %617, i64* %3, align 8
  %618 = inttoptr i64 %616 to i32*
  %619 = load i32, i32* %618, align 4
  %620 = zext i32 %619 to i64
  store i64 %620, i64* %RAX.i11582.pre-phi, align 8
  %621 = sext i32 %619 to i64
  %622 = lshr i64 %621, 32
  store i64 %622, i64* %573, align 8
  %623 = add i64 %615, 7
  store i64 %623, i64* %3, align 8
  %624 = load i32, i32* %602, align 4
  %625 = sext i32 %624 to i64
  %626 = shl nuw i64 %622, 32
  %627 = or i64 %626, %620
  %628 = sdiv i64 %627, %625
  %629 = shl i64 %628, 32
  %630 = ashr exact i64 %629, 32
  %631 = icmp eq i64 %628, %630
  br i1 %631, label %634, label %632

; <label>:632:                                    ; preds = %block_40a3fb
  %633 = tail call %struct.Memory* @__remill_error(%struct.State* nonnull dereferenceable(3376) %0, i64 %623, %struct.Memory* %MEMORY.8)
  %.pre194 = load i64, i64* %RBP.i, align 8
  %.pre195 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %.pre196 = load i64, i64* %3, align 8
  br label %routine_idivl_MINUS0x7c__rbp_.exit

; <label>:634:                                    ; preds = %block_40a3fb
  %635 = srem i64 %627, %625
  %636 = and i64 %628, 4294967295
  store i64 %636, i64* %RAX.i11582.pre-phi, align 8
  %637 = and i64 %635, 4294967295
  store i64 %637, i64* %576, align 8
  store i8 0, i8* %14, align 1
  store i8 0, i8* %21, align 1
  store i8 0, i8* %27, align 1
  store i8 0, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %638 = trunc i64 %628 to i32
  br label %routine_idivl_MINUS0x7c__rbp_.exit

routine_idivl_MINUS0x7c__rbp_.exit:               ; preds = %634, %632
  %639 = phi i64 [ %.pre196, %632 ], [ %623, %634 ]
  %640 = phi i32 [ %.pre195, %632 ], [ %638, %634 ]
  %641 = phi i64 [ %.pre194, %632 ], [ %599, %634 ]
  %642 = phi %struct.Memory* [ %633, %632 ], [ %MEMORY.8, %634 ]
  %643 = add i64 %641, -132
  %644 = add i64 %639, 6
  store i64 %644, i64* %3, align 8
  %645 = inttoptr i64 %643 to i32*
  store i32 %640, i32* %645, align 4
  %646 = load i64, i64* %RBP.i, align 8
  %647 = add i64 %646, -28
  %648 = load i64, i64* %3, align 8
  %649 = add i64 %648, 3
  store i64 %649, i64* %3, align 8
  %650 = inttoptr i64 %647 to i32*
  %651 = load i32, i32* %650, align 4
  %652 = add i32 %651, -1
  %653 = zext i32 %652 to i64
  store i64 %653, i64* %RAX.i11582.pre-phi, align 8
  %654 = icmp eq i32 %651, 0
  %655 = zext i1 %654 to i8
  store i8 %655, i8* %14, align 1
  %656 = and i32 %652, 255
  %657 = tail call i32 @llvm.ctpop.i32(i32 %656)
  %658 = trunc i32 %657 to i8
  %659 = and i8 %658, 1
  %660 = xor i8 %659, 1
  store i8 %660, i8* %21, align 1
  %661 = xor i32 %652, %651
  %662 = lshr i32 %661, 4
  %663 = trunc i32 %662 to i8
  %664 = and i8 %663, 1
  store i8 %664, i8* %27, align 1
  %665 = icmp eq i32 %652, 0
  %666 = zext i1 %665 to i8
  store i8 %666, i8* %30, align 1
  %667 = lshr i32 %652, 31
  %668 = trunc i32 %667 to i8
  store i8 %668, i8* %33, align 1
  %669 = lshr i32 %651, 31
  %670 = xor i32 %667, %669
  %671 = add nuw nsw i32 %670, %669
  %672 = icmp eq i32 %671, 2
  %673 = zext i1 %672 to i8
  store i8 %673, i8* %39, align 1
  %674 = add i64 %646, -32
  %675 = add i64 %648, 9
  store i64 %675, i64* %3, align 8
  %676 = inttoptr i64 %674 to i32*
  store i32 %652, i32* %676, align 4
  %677 = load i64, i64* %RBP.i, align 8
  %678 = add i64 %677, -136
  %679 = load i64, i64* %3, align 8
  %680 = add i64 %679, 10
  store i64 %680, i64* %3, align 8
  %681 = inttoptr i64 %678 to i32*
  store i32 0, i32* %681, align 4
  %.pre197 = load i64, i64* %3, align 8
  br label %block_.L_40a41b

block_.L_40a41b:                                  ; preds = %block_.L_40a45e, %routine_idivl_MINUS0x7c__rbp_.exit
  %682 = phi i64 [ %900, %block_.L_40a45e ], [ %.pre197, %routine_idivl_MINUS0x7c__rbp_.exit ]
  store i64 0, i64* %RAX.i11582.pre-phi, align 8
  store i8 0, i8* %14, align 1
  store i8 1, i8* %21, align 1
  store i8 1, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  store i8 0, i8* %CL.i11426, align 1
  %683 = load i64, i64* %RBP.i, align 8
  %684 = add i64 %683, -136
  %685 = add i64 %682, 10
  store i64 %685, i64* %3, align 8
  %686 = inttoptr i64 %684 to i32*
  %687 = load i32, i32* %686, align 4
  %688 = zext i32 %687 to i64
  store i64 %688, i64* %RAX.i11582.pre-phi, align 8
  %689 = add i64 %683, -132
  %690 = add i64 %682, 16
  store i64 %690, i64* %3, align 8
  %691 = inttoptr i64 %689 to i32*
  %692 = load i32, i32* %691, align 4
  %693 = sub i32 %687, %692
  %694 = icmp ult i32 %687, %692
  %695 = zext i1 %694 to i8
  store i8 %695, i8* %14, align 1
  %696 = and i32 %693, 255
  %697 = tail call i32 @llvm.ctpop.i32(i32 %696)
  %698 = trunc i32 %697 to i8
  %699 = and i8 %698, 1
  %700 = xor i8 %699, 1
  store i8 %700, i8* %21, align 1
  %701 = xor i32 %692, %687
  %702 = xor i32 %701, %693
  %703 = lshr i32 %702, 4
  %704 = trunc i32 %703 to i8
  %705 = and i8 %704, 1
  store i8 %705, i8* %27, align 1
  %706 = icmp eq i32 %693, 0
  %707 = zext i1 %706 to i8
  store i8 %707, i8* %30, align 1
  %708 = lshr i32 %693, 31
  %709 = trunc i32 %708 to i8
  store i8 %709, i8* %33, align 1
  %710 = lshr i32 %687, 31
  %711 = lshr i32 %692, 31
  %712 = xor i32 %711, %710
  %713 = xor i32 %708, %710
  %714 = add nuw nsw i32 %713, %712
  %715 = icmp eq i32 %714, 2
  %716 = zext i1 %715 to i8
  store i8 %716, i8* %39, align 1
  %717 = add i64 %683, -221
  %718 = add i64 %682, 22
  store i64 %718, i64* %3, align 8
  %719 = inttoptr i64 %717 to i8*
  store i8 0, i8* %719, align 1
  %720 = load i64, i64* %3, align 8
  %721 = add i64 %720, 26
  %722 = add i64 %720, 6
  %723 = load i8, i8* %33, align 1
  %724 = icmp ne i8 %723, 0
  %725 = load i8, i8* %39, align 1
  %726 = icmp ne i8 %725, 0
  %727 = xor i1 %724, %726
  %728 = select i1 %727, i64 %722, i64 %721
  store i64 %728, i64* %3, align 8
  br i1 %727, label %block_40a437, label %block_.L_40a44b

block_40a437:                                     ; preds = %block_.L_40a41b
  %729 = load i64, i64* %RBP.i, align 8
  %730 = add i64 %729, -32
  %731 = add i64 %728, 3
  store i64 %731, i64* %3, align 8
  %732 = inttoptr i64 %730 to i32*
  %733 = load i32, i32* %732, align 4
  %734 = zext i32 %733 to i64
  store i64 %734, i64* %RAX.i11582.pre-phi, align 8
  %735 = add i64 %729, -56
  %736 = add i64 %728, 6
  store i64 %736, i64* %3, align 8
  %737 = inttoptr i64 %735 to i32*
  %738 = load i32, i32* %737, align 4
  %739 = add i32 %738, -1
  %740 = zext i32 %739 to i64
  store i64 %740, i64* %RCX.i11580, align 8
  %741 = lshr i32 %739, 31
  %742 = sub i32 %733, %739
  %743 = icmp ult i32 %733, %739
  %744 = zext i1 %743 to i8
  store i8 %744, i8* %14, align 1
  %745 = and i32 %742, 255
  %746 = tail call i32 @llvm.ctpop.i32(i32 %745)
  %747 = trunc i32 %746 to i8
  %748 = and i8 %747, 1
  %749 = xor i8 %748, 1
  store i8 %749, i8* %21, align 1
  %750 = xor i32 %739, %733
  %751 = xor i32 %750, %742
  %752 = lshr i32 %751, 4
  %753 = trunc i32 %752 to i8
  %754 = and i8 %753, 1
  store i8 %754, i8* %27, align 1
  %755 = icmp eq i32 %742, 0
  %756 = zext i1 %755 to i8
  store i8 %756, i8* %30, align 1
  %757 = lshr i32 %742, 31
  %758 = trunc i32 %757 to i8
  store i8 %758, i8* %33, align 1
  %759 = lshr i32 %733, 31
  %760 = xor i32 %741, %759
  %761 = xor i32 %757, %759
  %762 = add nuw nsw i32 %761, %760
  %763 = icmp eq i32 %762, 2
  %764 = zext i1 %763 to i8
  store i8 %764, i8* %39, align 1
  %765 = icmp ne i8 %758, 0
  %766 = xor i1 %765, %763
  %767 = zext i1 %766 to i8
  store i8 %767, i8* %DL.i11402, align 1
  %768 = add i64 %729, -221
  %769 = add i64 %728, 20
  store i64 %769, i64* %3, align 8
  %770 = inttoptr i64 %768 to i8*
  store i8 %767, i8* %770, align 1
  %.pre198 = load i64, i64* %3, align 8
  br label %block_.L_40a44b

block_.L_40a44b:                                  ; preds = %block_.L_40a41b, %block_40a437
  %771 = phi i64 [ %721, %block_.L_40a41b ], [ %.pre198, %block_40a437 ]
  %772 = load i64, i64* %RBP.i, align 8
  %773 = add i64 %772, -221
  %774 = add i64 %771, 6
  store i64 %774, i64* %3, align 8
  %775 = inttoptr i64 %773 to i8*
  %776 = load i8, i8* %775, align 1
  store i8 %776, i8* %AL.i11425, align 1
  %777 = and i8 %776, 1
  store i8 0, i8* %14, align 1
  %778 = zext i8 %777 to i32
  %779 = tail call i32 @llvm.ctpop.i32(i32 %778)
  %780 = trunc i32 %779 to i8
  %781 = xor i8 %780, 1
  store i8 %781, i8* %21, align 1
  %782 = xor i8 %777, 1
  store i8 %782, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %783 = icmp eq i8 %782, 0
  %.v286 = select i1 %783, i64 19, i64 14
  %784 = add i64 %771, %.v286
  store i64 %784, i64* %3, align 8
  %785 = add i64 %772, -32
  br i1 %783, label %block_.L_40a45e, label %block_40a459

block_40a459:                                     ; preds = %block_.L_40a44b
  %786 = add i64 %784, 49
  store i64 %786, i64* %3, align 8
  %787 = inttoptr i64 %785 to i32*
  %788 = load i32, i32* %787, align 4
  %789 = zext i32 %788 to i64
  store i64 %789, i64* %RAX.i11582.pre-phi, align 8
  %790 = add i64 %772, -28
  %791 = add i64 %784, 52
  store i64 %791, i64* %3, align 8
  %792 = inttoptr i64 %790 to i32*
  %793 = load i32, i32* %792, align 4
  %794 = sub i32 %788, %793
  %795 = icmp ult i32 %788, %793
  %796 = zext i1 %795 to i8
  store i8 %796, i8* %14, align 1
  %797 = and i32 %794, 255
  %798 = tail call i32 @llvm.ctpop.i32(i32 %797)
  %799 = trunc i32 %798 to i8
  %800 = and i8 %799, 1
  %801 = xor i8 %800, 1
  store i8 %801, i8* %21, align 1
  %802 = xor i32 %793, %788
  %803 = xor i32 %802, %794
  %804 = lshr i32 %803, 4
  %805 = trunc i32 %804 to i8
  %806 = and i8 %805, 1
  store i8 %806, i8* %27, align 1
  %807 = icmp eq i32 %794, 0
  %808 = zext i1 %807 to i8
  store i8 %808, i8* %30, align 1
  %809 = lshr i32 %794, 31
  %810 = trunc i32 %809 to i8
  store i8 %810, i8* %33, align 1
  %811 = lshr i32 %788, 31
  %812 = lshr i32 %793, 31
  %813 = xor i32 %812, %811
  %814 = xor i32 %809, %811
  %815 = add nuw nsw i32 %814, %813
  %816 = icmp eq i32 %815, 2
  %817 = zext i1 %816 to i8
  store i8 %817, i8* %39, align 1
  %818 = icmp ne i8 %810, 0
  %819 = xor i1 %818, %816
  %820 = or i1 %807, %819
  %.v300 = select i1 %820, i64 155, i64 58
  %821 = add i64 %784, %.v300
  store i64 %821, i64* %3, align 8
  br i1 %820, label %block_.L_40a4f4, label %block_40a493

block_.L_40a45e:                                  ; preds = %block_.L_40a44b
  %822 = add i64 %784, 3
  store i64 %822, i64* %3, align 8
  %823 = inttoptr i64 %785 to i32*
  %824 = load i32, i32* %823, align 4
  %825 = add i32 %824, 1
  %826 = zext i32 %825 to i64
  store i64 %826, i64* %RAX.i11582.pre-phi, align 8
  %827 = icmp eq i32 %824, -1
  %828 = icmp eq i32 %825, 0
  %829 = or i1 %827, %828
  %830 = zext i1 %829 to i8
  store i8 %830, i8* %14, align 1
  %831 = and i32 %825, 255
  %832 = tail call i32 @llvm.ctpop.i32(i32 %831)
  %833 = trunc i32 %832 to i8
  %834 = and i8 %833, 1
  %835 = xor i8 %834, 1
  store i8 %835, i8* %21, align 1
  %836 = xor i32 %825, %824
  %837 = lshr i32 %836, 4
  %838 = trunc i32 %837 to i8
  %839 = and i8 %838, 1
  store i8 %839, i8* %27, align 1
  %840 = zext i1 %828 to i8
  store i8 %840, i8* %30, align 1
  %841 = lshr i32 %825, 31
  %842 = trunc i32 %841 to i8
  store i8 %842, i8* %33, align 1
  %843 = lshr i32 %824, 31
  %844 = xor i32 %841, %843
  %845 = add nuw nsw i32 %844, %841
  %846 = icmp eq i32 %845, 2
  %847 = zext i1 %846 to i8
  store i8 %847, i8* %39, align 1
  %848 = add i64 %784, 9
  store i64 %848, i64* %3, align 8
  store i32 %825, i32* %823, align 4
  %849 = load i64, i64* %RBP.i, align 8
  %850 = add i64 %849, -8
  %851 = load i64, i64* %3, align 8
  %852 = add i64 %851, 4
  store i64 %852, i64* %3, align 8
  %853 = inttoptr i64 %850 to i64*
  %854 = load i64, i64* %853, align 8
  store i64 %854, i64* %RCX.i11580, align 8
  %855 = add i64 %849, -32
  %856 = add i64 %851, 8
  store i64 %856, i64* %3, align 8
  %857 = inttoptr i64 %855 to i32*
  %858 = load i32, i32* %857, align 4
  %859 = sext i32 %858 to i64
  store i64 %859, i64* %573, align 8
  %860 = shl nsw i64 %859, 2
  %861 = add i64 %854, 672
  %862 = add i64 %861, %860
  %863 = add i64 %851, 15
  store i64 %863, i64* %3, align 8
  %864 = inttoptr i64 %862 to i32*
  %865 = load i32, i32* %864, align 4
  %866 = zext i32 %865 to i64
  store i64 %866, i64* %RAX.i11582.pre-phi, align 8
  %867 = add i64 %849, -136
  %868 = add i64 %851, 21
  store i64 %868, i64* %3, align 8
  %869 = inttoptr i64 %867 to i32*
  %870 = load i32, i32* %869, align 4
  %871 = add i32 %870, %865
  %872 = zext i32 %871 to i64
  store i64 %872, i64* %RAX.i11582.pre-phi, align 8
  %873 = icmp ult i32 %871, %865
  %874 = icmp ult i32 %871, %870
  %875 = or i1 %873, %874
  %876 = zext i1 %875 to i8
  store i8 %876, i8* %14, align 1
  %877 = and i32 %871, 255
  %878 = tail call i32 @llvm.ctpop.i32(i32 %877)
  %879 = trunc i32 %878 to i8
  %880 = and i8 %879, 1
  %881 = xor i8 %880, 1
  store i8 %881, i8* %21, align 1
  %882 = xor i32 %870, %865
  %883 = xor i32 %882, %871
  %884 = lshr i32 %883, 4
  %885 = trunc i32 %884 to i8
  %886 = and i8 %885, 1
  store i8 %886, i8* %27, align 1
  %887 = icmp eq i32 %871, 0
  %888 = zext i1 %887 to i8
  store i8 %888, i8* %30, align 1
  %889 = lshr i32 %871, 31
  %890 = trunc i32 %889 to i8
  store i8 %890, i8* %33, align 1
  %891 = lshr i32 %865, 31
  %892 = lshr i32 %870, 31
  %893 = xor i32 %889, %891
  %894 = xor i32 %889, %892
  %895 = add nuw nsw i32 %893, %894
  %896 = icmp eq i32 %895, 2
  %897 = zext i1 %896 to i8
  store i8 %897, i8* %39, align 1
  %898 = add i64 %851, 27
  store i64 %898, i64* %3, align 8
  store i32 %871, i32* %869, align 4
  %899 = load i64, i64* %3, align 8
  %900 = add i64 %899, -103
  store i64 %900, i64* %3, align 8
  br label %block_.L_40a41b

block_40a493:                                     ; preds = %block_40a459
  %901 = add i64 %772, -124
  %902 = add i64 %821, 3
  store i64 %902, i64* %3, align 8
  %903 = inttoptr i64 %901 to i32*
  %904 = load i32, i32* %903, align 4
  %905 = zext i32 %904 to i64
  store i64 %905, i64* %RAX.i11582.pre-phi, align 8
  %906 = add i64 %772, -72
  %907 = add i64 %821, 6
  store i64 %907, i64* %3, align 8
  %908 = inttoptr i64 %906 to i32*
  %909 = load i32, i32* %908, align 4
  %910 = sub i32 %904, %909
  %911 = icmp ult i32 %904, %909
  %912 = zext i1 %911 to i8
  store i8 %912, i8* %14, align 1
  %913 = and i32 %910, 255
  %914 = tail call i32 @llvm.ctpop.i32(i32 %913)
  %915 = trunc i32 %914 to i8
  %916 = and i8 %915, 1
  %917 = xor i8 %916, 1
  store i8 %917, i8* %21, align 1
  %918 = xor i32 %909, %904
  %919 = xor i32 %918, %910
  %920 = lshr i32 %919, 4
  %921 = trunc i32 %920 to i8
  %922 = and i8 %921, 1
  store i8 %922, i8* %27, align 1
  %923 = icmp eq i32 %910, 0
  %924 = zext i1 %923 to i8
  store i8 %924, i8* %30, align 1
  %925 = lshr i32 %910, 31
  %926 = trunc i32 %925 to i8
  store i8 %926, i8* %33, align 1
  %927 = lshr i32 %904, 31
  %928 = lshr i32 %909, 31
  %929 = xor i32 %928, %927
  %930 = xor i32 %925, %927
  %931 = add nuw nsw i32 %930, %929
  %932 = icmp eq i32 %931, 2
  %933 = zext i1 %932 to i8
  store i8 %933, i8* %39, align 1
  %.v301 = select i1 %923, i64 97, i64 12
  %934 = add i64 %821, %.v301
  store i64 %934, i64* %3, align 8
  br i1 %923, label %block_.L_40a4f4, label %block_40a49f

block_40a49f:                                     ; preds = %block_40a493
  %935 = add i64 %934, 4
  store i64 %935, i64* %3, align 8
  %936 = load i32, i32* %903, align 4
  %937 = add i32 %936, -1
  %938 = icmp eq i32 %936, 0
  %939 = zext i1 %938 to i8
  store i8 %939, i8* %14, align 1
  %940 = and i32 %937, 255
  %941 = tail call i32 @llvm.ctpop.i32(i32 %940)
  %942 = trunc i32 %941 to i8
  %943 = and i8 %942, 1
  %944 = xor i8 %943, 1
  store i8 %944, i8* %21, align 1
  %945 = xor i32 %937, %936
  %946 = lshr i32 %945, 4
  %947 = trunc i32 %946 to i8
  %948 = and i8 %947, 1
  store i8 %948, i8* %27, align 1
  %949 = icmp eq i32 %937, 0
  %950 = zext i1 %949 to i8
  store i8 %950, i8* %30, align 1
  %951 = lshr i32 %937, 31
  %952 = trunc i32 %951 to i8
  store i8 %952, i8* %33, align 1
  %953 = lshr i32 %936, 31
  %954 = xor i32 %951, %953
  %955 = add nuw nsw i32 %954, %953
  %956 = icmp eq i32 %955, 2
  %957 = zext i1 %956 to i8
  store i8 %957, i8* %39, align 1
  %.v302 = select i1 %949, i64 85, i64 10
  %958 = add i64 %934, %.v302
  store i64 %958, i64* %3, align 8
  br i1 %949, label %block_.L_40a4f4, label %block_40a4a9

block_40a4a9:                                     ; preds = %block_40a49f
  store i64 2, i64* %RAX.i11582.pre-phi, align 8
  %959 = add i64 %958, 8
  store i64 %959, i64* %3, align 8
  %960 = load i32, i32* %908, align 4
  %961 = zext i32 %960 to i64
  store i64 %961, i64* %RCX.i11580, align 8
  %962 = add i64 %958, 11
  store i64 %962, i64* %3, align 8
  %963 = load i32, i32* %903, align 4
  %964 = sub i32 %960, %963
  %965 = zext i32 %964 to i64
  store i64 %965, i64* %RCX.i11580, align 8
  %966 = icmp ult i32 %960, %963
  %967 = zext i1 %966 to i8
  store i8 %967, i8* %14, align 1
  %968 = and i32 %964, 255
  %969 = tail call i32 @llvm.ctpop.i32(i32 %968)
  %970 = trunc i32 %969 to i8
  %971 = and i8 %970, 1
  %972 = xor i8 %971, 1
  store i8 %972, i8* %21, align 1
  %973 = xor i32 %963, %960
  %974 = xor i32 %973, %964
  %975 = lshr i32 %974, 4
  %976 = trunc i32 %975 to i8
  %977 = and i8 %976, 1
  store i8 %977, i8* %27, align 1
  %978 = icmp eq i32 %964, 0
  %979 = zext i1 %978 to i8
  store i8 %979, i8* %30, align 1
  %980 = lshr i32 %964, 31
  %981 = trunc i32 %980 to i8
  store i8 %981, i8* %33, align 1
  %982 = lshr i32 %960, 31
  %983 = lshr i32 %963, 31
  %984 = xor i32 %983, %982
  %985 = xor i32 %980, %982
  %986 = add nuw nsw i32 %985, %984
  %987 = icmp eq i32 %986, 2
  %988 = zext i1 %987 to i8
  store i8 %988, i8* %39, align 1
  %989 = add i64 %772, -228
  %990 = add i64 %958, 17
  store i64 %990, i64* %3, align 8
  %991 = inttoptr i64 %989 to i32*
  store i32 2, i32* %991, align 4
  %992 = load i32, i32* %ECX.i11574, align 4
  %993 = zext i32 %992 to i64
  %994 = load i64, i64* %3, align 8
  store i64 %993, i64* %RAX.i11582.pre-phi, align 8
  %995 = sext i32 %992 to i64
  %996 = lshr i64 %995, 32
  store i64 %996, i64* %573, align 8
  %997 = load i64, i64* %RBP.i, align 8
  %998 = add i64 %997, -228
  %999 = add i64 %994, 9
  store i64 %999, i64* %3, align 8
  %1000 = inttoptr i64 %998 to i32*
  %1001 = load i32, i32* %1000, align 4
  %1002 = zext i32 %1001 to i64
  store i64 %1002, i64* %RCX.i11580, align 8
  %1003 = add i64 %994, 11
  store i64 %1003, i64* %3, align 8
  %1004 = sext i32 %1001 to i64
  %1005 = shl nuw i64 %996, 32
  %1006 = or i64 %1005, %993
  %1007 = sdiv i64 %1006, %1004
  %1008 = shl i64 %1007, 32
  %1009 = ashr exact i64 %1008, 32
  %1010 = icmp eq i64 %1007, %1009
  br i1 %1010, label %1013, label %1011

; <label>:1011:                                   ; preds = %block_40a4a9
  %1012 = tail call %struct.Memory* @__remill_error(%struct.State* nonnull dereferenceable(3376) %0, i64 %1003, %struct.Memory* %642)
  %.pre199 = load i32, i32* %575, align 4
  %.pre200 = load i64, i64* %3, align 8
  br label %routine_idivl__ecx.exit

; <label>:1013:                                   ; preds = %block_40a4a9
  %1014 = srem i64 %1006, %1004
  %1015 = and i64 %1007, 4294967295
  store i64 %1015, i64* %RAX.i11582.pre-phi, align 8
  %1016 = and i64 %1014, 4294967295
  store i64 %1016, i64* %576, align 8
  store i8 0, i8* %14, align 1
  store i8 0, i8* %21, align 1
  store i8 0, i8* %27, align 1
  store i8 0, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %1017 = trunc i64 %1014 to i32
  br label %routine_idivl__ecx.exit

routine_idivl__ecx.exit:                          ; preds = %1013, %1011
  %1018 = phi i64 [ %.pre200, %1011 ], [ %1003, %1013 ]
  %1019 = phi i32 [ %.pre199, %1011 ], [ %1017, %1013 ]
  %1020 = phi %struct.Memory* [ %1012, %1011 ], [ %642, %1013 ]
  %1021 = add i32 %1019, -1
  %1022 = icmp eq i32 %1019, 0
  %1023 = zext i1 %1022 to i8
  store i8 %1023, i8* %14, align 1
  %1024 = and i32 %1021, 255
  %1025 = tail call i32 @llvm.ctpop.i32(i32 %1024)
  %1026 = trunc i32 %1025 to i8
  %1027 = and i8 %1026, 1
  %1028 = xor i8 %1027, 1
  store i8 %1028, i8* %21, align 1
  %1029 = xor i32 %1021, %1019
  %1030 = lshr i32 %1029, 4
  %1031 = trunc i32 %1030 to i8
  %1032 = and i8 %1031, 1
  store i8 %1032, i8* %27, align 1
  %1033 = icmp eq i32 %1021, 0
  %1034 = zext i1 %1033 to i8
  store i8 %1034, i8* %30, align 1
  %1035 = lshr i32 %1021, 31
  %1036 = trunc i32 %1035 to i8
  store i8 %1036, i8* %33, align 1
  %1037 = lshr i32 %1019, 31
  %1038 = xor i32 %1035, %1037
  %1039 = add nuw nsw i32 %1038, %1037
  %1040 = icmp eq i32 %1039, 2
  %1041 = zext i1 %1040 to i8
  store i8 %1041, i8* %39, align 1
  %.v303 = select i1 %1033, i64 9, i64 47
  %1042 = add i64 %1018, %.v303
  store i64 %1042, i64* %3, align 8
  br i1 %1033, label %block_40a4ce, label %block_.L_40a4f4

block_40a4ce:                                     ; preds = %routine_idivl__ecx.exit
  %1043 = load i64, i64* %RBP.i, align 8
  %1044 = add i64 %1043, -8
  %1045 = add i64 %1042, 4
  store i64 %1045, i64* %3, align 8
  %1046 = inttoptr i64 %1044 to i64*
  %1047 = load i64, i64* %1046, align 8
  store i64 %1047, i64* %RAX.i11582.pre-phi, align 8
  %1048 = add i64 %1043, -32
  %1049 = add i64 %1042, 8
  store i64 %1049, i64* %3, align 8
  %1050 = inttoptr i64 %1048 to i32*
  %1051 = load i32, i32* %1050, align 4
  %1052 = sext i32 %1051 to i64
  store i64 %1052, i64* %RCX.i11580, align 8
  %1053 = shl nsw i64 %1052, 2
  %1054 = add i64 %1047, 672
  %1055 = add i64 %1054, %1053
  %1056 = add i64 %1042, 15
  store i64 %1056, i64* %3, align 8
  %1057 = inttoptr i64 %1055 to i32*
  %1058 = load i32, i32* %1057, align 4
  %1059 = zext i32 %1058 to i64
  store i64 %1059, i64* %576, align 8
  %1060 = add i64 %1043, -136
  %1061 = add i64 %1042, 21
  store i64 %1061, i64* %3, align 8
  %1062 = inttoptr i64 %1060 to i32*
  %1063 = load i32, i32* %1062, align 4
  %1064 = sub i32 %1063, %1058
  %1065 = zext i32 %1064 to i64
  store i64 %1065, i64* %RSI.i11312, align 8
  %1066 = icmp ult i32 %1063, %1058
  %1067 = zext i1 %1066 to i8
  store i8 %1067, i8* %14, align 1
  %1068 = and i32 %1064, 255
  %1069 = tail call i32 @llvm.ctpop.i32(i32 %1068)
  %1070 = trunc i32 %1069 to i8
  %1071 = and i8 %1070, 1
  %1072 = xor i8 %1071, 1
  store i8 %1072, i8* %21, align 1
  %1073 = xor i32 %1058, %1063
  %1074 = xor i32 %1073, %1064
  %1075 = lshr i32 %1074, 4
  %1076 = trunc i32 %1075 to i8
  %1077 = and i8 %1076, 1
  store i8 %1077, i8* %27, align 1
  %1078 = icmp eq i32 %1064, 0
  %1079 = zext i1 %1078 to i8
  store i8 %1079, i8* %30, align 1
  %1080 = lshr i32 %1064, 31
  %1081 = trunc i32 %1080 to i8
  store i8 %1081, i8* %33, align 1
  %1082 = lshr i32 %1063, 31
  %1083 = lshr i32 %1058, 31
  %1084 = xor i32 %1083, %1082
  %1085 = xor i32 %1080, %1082
  %1086 = add nuw nsw i32 %1085, %1084
  %1087 = icmp eq i32 %1086, 2
  %1088 = zext i1 %1087 to i8
  store i8 %1088, i8* %39, align 1
  %1089 = add i64 %1042, 29
  store i64 %1089, i64* %3, align 8
  store i32 %1064, i32* %1062, align 4
  %1090 = load i64, i64* %RBP.i, align 8
  %1091 = add i64 %1090, -32
  %1092 = load i64, i64* %3, align 8
  %1093 = add i64 %1092, 3
  store i64 %1093, i64* %3, align 8
  %1094 = inttoptr i64 %1091 to i32*
  %1095 = load i32, i32* %1094, align 4
  %1096 = add i32 %1095, -1
  %1097 = zext i32 %1096 to i64
  store i64 %1097, i64* %576, align 8
  %1098 = icmp ne i32 %1095, 0
  %1099 = zext i1 %1098 to i8
  store i8 %1099, i8* %14, align 1
  %1100 = and i32 %1096, 255
  %1101 = tail call i32 @llvm.ctpop.i32(i32 %1100)
  %1102 = trunc i32 %1101 to i8
  %1103 = and i8 %1102, 1
  %1104 = xor i8 %1103, 1
  store i8 %1104, i8* %21, align 1
  %1105 = xor i32 %1095, 16
  %1106 = xor i32 %1105, %1096
  %1107 = lshr i32 %1106, 4
  %1108 = trunc i32 %1107 to i8
  %1109 = and i8 %1108, 1
  store i8 %1109, i8* %27, align 1
  %1110 = icmp eq i32 %1096, 0
  %1111 = zext i1 %1110 to i8
  store i8 %1111, i8* %30, align 1
  %1112 = lshr i32 %1096, 31
  %1113 = trunc i32 %1112 to i8
  store i8 %1113, i8* %33, align 1
  %1114 = lshr i32 %1095, 31
  %1115 = xor i32 %1112, %1114
  %1116 = xor i32 %1112, 1
  %1117 = add nuw nsw i32 %1115, %1116
  %1118 = icmp eq i32 %1117, 2
  %1119 = zext i1 %1118 to i8
  store i8 %1119, i8* %39, align 1
  %1120 = add i64 %1092, 9
  store i64 %1120, i64* %3, align 8
  store i32 %1096, i32* %1094, align 4
  %.pre201 = load i64, i64* %3, align 8
  br label %block_.L_40a4f4

block_.L_40a4f4:                                  ; preds = %routine_idivl__ecx.exit, %block_40a4ce, %block_40a49f, %block_40a493, %block_40a459
  %1121 = phi i64 [ %821, %block_40a459 ], [ %934, %block_40a493 ], [ %958, %block_40a49f ], [ %1042, %routine_idivl__ecx.exit ], [ %.pre201, %block_40a4ce ]
  %MEMORY.11 = phi %struct.Memory* [ %642, %block_40a459 ], [ %642, %block_40a493 ], [ %642, %block_40a49f ], [ %1020, %routine_idivl__ecx.exit ], [ %1020, %block_40a4ce ]
  %1122 = load i64, i64* %RBP.i, align 8
  %1123 = add i64 %1122, -8
  %1124 = add i64 %1121, 4
  store i64 %1124, i64* %3, align 8
  %1125 = inttoptr i64 %1123 to i64*
  %1126 = load i64, i64* %1125, align 8
  store i64 %1126, i64* %RAX.i11582.pre-phi, align 8
  %1127 = add i64 %1126, 656
  %1128 = add i64 %1121, 11
  store i64 %1128, i64* %3, align 8
  %1129 = inttoptr i64 %1127 to i32*
  %1130 = load i32, i32* %1129, align 4
  %1131 = add i32 %1130, -3
  %1132 = icmp ult i32 %1130, 3
  %1133 = zext i1 %1132 to i8
  store i8 %1133, i8* %14, align 1
  %1134 = and i32 %1131, 255
  %1135 = tail call i32 @llvm.ctpop.i32(i32 %1134)
  %1136 = trunc i32 %1135 to i8
  %1137 = and i8 %1136, 1
  %1138 = xor i8 %1137, 1
  store i8 %1138, i8* %21, align 1
  %1139 = xor i32 %1131, %1130
  %1140 = lshr i32 %1139, 4
  %1141 = trunc i32 %1140 to i8
  %1142 = and i8 %1141, 1
  store i8 %1142, i8* %27, align 1
  %1143 = icmp eq i32 %1131, 0
  %1144 = zext i1 %1143 to i8
  store i8 %1144, i8* %30, align 1
  %1145 = lshr i32 %1131, 31
  %1146 = trunc i32 %1145 to i8
  store i8 %1146, i8* %33, align 1
  %1147 = lshr i32 %1130, 31
  %1148 = xor i32 %1145, %1147
  %1149 = add nuw nsw i32 %1148, %1147
  %1150 = icmp eq i32 %1149, 2
  %1151 = zext i1 %1150 to i8
  store i8 %1151, i8* %39, align 1
  %1152 = icmp ne i8 %1146, 0
  %1153 = xor i1 %1152, %1150
  %.v304 = select i1 %1153, i64 109, i64 17
  %1154 = add i64 %1121, %.v304
  store i64 %1154, i64* %3, align 8
  br i1 %1153, label %block_.L_40a561, label %block_40a505

block_40a505:                                     ; preds = %block_.L_40a4f4
  store i64 ptrtoint (%G__0x416559_type* @G__0x416559 to i64), i64* %RSI.i11290, align 8
  %1155 = add i64 %1154, add (i64 ptrtoint (%G_0xb231__rip__type* @G_0xb231__rip_ to i64), i64 10)
  %1156 = add i64 %1154, 18
  store i64 %1156, i64* %3, align 8
  %1157 = inttoptr i64 %1155 to i64*
  %1158 = load i64, i64* %1157, align 8
  store i64 %1158, i64* %579, align 1
  store double 0.000000e+00, double* %581, align 1
  %1159 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %1159, i64* %RDI.i2910, align 8
  %1160 = add i64 %1122, -124
  %1161 = add i64 %1154, 29
  store i64 %1161, i64* %3, align 8
  %1162 = inttoptr i64 %1160 to i32*
  %1163 = load i32, i32* %1162, align 4
  %1164 = zext i32 %1163 to i64
  store i64 %1164, i64* %573, align 8
  %1165 = add i64 %1122, -28
  %1166 = add i64 %1154, 32
  store i64 %1166, i64* %3, align 8
  %1167 = inttoptr i64 %1165 to i32*
  %1168 = load i32, i32* %1167, align 4
  %1169 = zext i32 %1168 to i64
  store i64 %1169, i64* %RCX.i11580, align 8
  %1170 = add i64 %1122, -32
  %1171 = add i64 %1154, 36
  store i64 %1171, i64* %3, align 8
  %1172 = inttoptr i64 %1170 to i32*
  %1173 = load i32, i32* %1172, align 4
  %1174 = zext i32 %1173 to i64
  store i64 %1174, i64* %582, align 8
  %1175 = add i64 %1122, -136
  %1176 = add i64 %1154, 43
  store i64 %1176, i64* %3, align 8
  %1177 = inttoptr i64 %1175 to i32*
  %1178 = load i32, i32* %1177, align 4
  %1179 = zext i32 %1178 to i64
  store i64 %1179, i64* %583, align 8
  %1180 = add i64 %1154, 51
  store i64 %1180, i64* %3, align 8
  %1181 = load <2 x i32>, <2 x i32>* %586, align 1
  %1182 = load <2 x i32>, <2 x i32>* %588, align 1
  %1183 = load i32, i32* %1177, align 4
  %1184 = sitofp i32 %1183 to float
  store float %1184, float* %589, align 1
  %1185 = extractelement <2 x i32> %1181, i32 1
  store i32 %1185, i32* %591, align 1
  %1186 = extractelement <2 x i32> %1182, i32 0
  store i32 %1186, i32* %592, align 1
  %1187 = extractelement <2 x i32> %1182, i32 1
  store i32 %1187, i32* %594, align 1
  %1188 = load <2 x float>, <2 x float>* %595, align 1
  %1189 = extractelement <2 x float> %1188, i32 0
  %1190 = fpext float %1189 to double
  store double %1190, double* %596, align 1
  %1191 = bitcast i64 %1158 to double
  %1192 = fmul double %1190, %1191
  store double %1192, double* %578, align 1
  store i64 0, i64* %580, align 1
  %1193 = add i64 %1154, 63
  store i64 %1193, i64* %3, align 8
  %1194 = load i64, i64* %1125, align 8
  store i64 %1194, i64* %RAX.i11582.pre-phi, align 8
  %1195 = add i64 %1194, 668
  %1196 = add i64 %1154, 71
  store i64 %1196, i64* %3, align 8
  %1197 = bitcast double %1190 to <2 x i32>
  %1198 = load <2 x i32>, <2 x i32>* %588, align 1
  %1199 = inttoptr i64 %1195 to i32*
  %1200 = load i32, i32* %1199, align 4
  %1201 = sitofp i32 %1200 to float
  store float %1201, float* %589, align 1
  %1202 = extractelement <2 x i32> %1197, i32 1
  store i32 %1202, i32* %591, align 1
  %1203 = extractelement <2 x i32> %1198, i32 0
  store i32 %1203, i32* %592, align 1
  %1204 = extractelement <2 x i32> %1198, i32 1
  store i32 %1204, i32* %594, align 1
  %1205 = load <2 x float>, <2 x float>* %595, align 1
  %1206 = extractelement <2 x float> %1205, i32 0
  %1207 = fpext float %1206 to double
  store double %1207, double* %596, align 1
  %1208 = fdiv double %1192, %1207
  store double %1208, double* %578, align 1
  store i64 0, i64* %580, align 1
  store i8 1, i8* %AL.i11425, align 1
  %1209 = add i64 %1154, -39893
  %1210 = add i64 %1154, 86
  %1211 = load i64, i64* %6, align 8
  %1212 = add i64 %1211, -8
  %1213 = inttoptr i64 %1212 to i64*
  store i64 %1210, i64* %1213, align 8
  store i64 %1212, i64* %6, align 8
  store i64 %1209, i64* %3, align 8
  %1214 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %MEMORY.11)
  %1215 = load i64, i64* %RBP.i, align 8
  %1216 = add i64 %1215, -232
  %1217 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %1218 = load i64, i64* %3, align 8
  %1219 = add i64 %1218, 6
  store i64 %1219, i64* %3, align 8
  %1220 = inttoptr i64 %1216 to i32*
  store i32 %1217, i32* %1220, align 4
  %.pre202 = load i64, i64* %RBP.i, align 8
  %.pre203 = load i64, i64* %3, align 8
  br label %block_.L_40a561

block_.L_40a561:                                  ; preds = %block_40a505, %block_.L_40a4f4
  %1221 = phi i64 [ %1154, %block_.L_40a4f4 ], [ %.pre203, %block_40a505 ]
  %1222 = phi i64 [ %1122, %block_.L_40a4f4 ], [ %.pre202, %block_40a505 ]
  %MEMORY.12 = phi %struct.Memory* [ %MEMORY.11, %block_.L_40a4f4 ], [ %1214, %block_40a505 ]
  %1223 = add i64 %1222, -12
  %1224 = add i64 %1221, 7
  store i64 %1224, i64* %3, align 8
  %1225 = inttoptr i64 %1223 to i32*
  store i32 0, i32* %1225, align 4
  %.pre204 = load i64, i64* %3, align 8
  br label %block_.L_40a568

block_.L_40a568:                                  ; preds = %block_.L_40a5db, %block_.L_40a561
  %1226 = phi i64 [ %1504, %block_.L_40a5db ], [ %.pre204, %block_.L_40a561 ]
  %1227 = load i64, i64* %RBP.i, align 8
  %1228 = add i64 %1227, -12
  %1229 = add i64 %1226, 3
  store i64 %1229, i64* %3, align 8
  %1230 = inttoptr i64 %1228 to i32*
  %1231 = load i32, i32* %1230, align 4
  %1232 = zext i32 %1231 to i64
  store i64 %1232, i64* %RAX.i11582.pre-phi, align 8
  %1233 = add i64 %1227, -56
  %1234 = add i64 %1226, 6
  store i64 %1234, i64* %3, align 8
  %1235 = inttoptr i64 %1233 to i32*
  %1236 = load i32, i32* %1235, align 4
  %1237 = sub i32 %1231, %1236
  %1238 = icmp ult i32 %1231, %1236
  %1239 = zext i1 %1238 to i8
  store i8 %1239, i8* %14, align 1
  %1240 = and i32 %1237, 255
  %1241 = tail call i32 @llvm.ctpop.i32(i32 %1240)
  %1242 = trunc i32 %1241 to i8
  %1243 = and i8 %1242, 1
  %1244 = xor i8 %1243, 1
  store i8 %1244, i8* %21, align 1
  %1245 = xor i32 %1236, %1231
  %1246 = xor i32 %1245, %1237
  %1247 = lshr i32 %1246, 4
  %1248 = trunc i32 %1247 to i8
  %1249 = and i8 %1248, 1
  store i8 %1249, i8* %27, align 1
  %1250 = icmp eq i32 %1237, 0
  %1251 = zext i1 %1250 to i8
  store i8 %1251, i8* %30, align 1
  %1252 = lshr i32 %1237, 31
  %1253 = trunc i32 %1252 to i8
  store i8 %1253, i8* %33, align 1
  %1254 = lshr i32 %1231, 31
  %1255 = lshr i32 %1236, 31
  %1256 = xor i32 %1255, %1254
  %1257 = xor i32 %1252, %1254
  %1258 = add nuw nsw i32 %1257, %1256
  %1259 = icmp eq i32 %1258, 2
  %1260 = zext i1 %1259 to i8
  store i8 %1260, i8* %39, align 1
  %1261 = icmp ne i8 %1253, 0
  %1262 = xor i1 %1261, %1259
  %.v305 = select i1 %1262, i64 12, i64 134
  %1263 = add i64 %1226, %.v305
  store i64 %1263, i64* %3, align 8
  br i1 %1262, label %block_40a574, label %block_.L_40a5ee

block_40a574:                                     ; preds = %block_.L_40a568
  %1264 = add i64 %1263, 3
  store i64 %1264, i64* %3, align 8
  %1265 = load i32, i32* %1230, align 4
  %1266 = zext i32 %1265 to i64
  store i64 %1266, i64* %RAX.i11582.pre-phi, align 8
  %1267 = add i64 %1227, -28
  %1268 = add i64 %1263, 6
  store i64 %1268, i64* %3, align 8
  %1269 = inttoptr i64 %1267 to i32*
  %1270 = load i32, i32* %1269, align 4
  %1271 = sub i32 %1265, %1270
  %1272 = icmp ult i32 %1265, %1270
  %1273 = zext i1 %1272 to i8
  store i8 %1273, i8* %14, align 1
  %1274 = and i32 %1271, 255
  %1275 = tail call i32 @llvm.ctpop.i32(i32 %1274)
  %1276 = trunc i32 %1275 to i8
  %1277 = and i8 %1276, 1
  %1278 = xor i8 %1277, 1
  store i8 %1278, i8* %21, align 1
  %1279 = xor i32 %1270, %1265
  %1280 = xor i32 %1279, %1271
  %1281 = lshr i32 %1280, 4
  %1282 = trunc i32 %1281 to i8
  %1283 = and i8 %1282, 1
  store i8 %1283, i8* %27, align 1
  %1284 = icmp eq i32 %1271, 0
  %1285 = zext i1 %1284 to i8
  store i8 %1285, i8* %30, align 1
  %1286 = lshr i32 %1271, 31
  %1287 = trunc i32 %1286 to i8
  store i8 %1287, i8* %33, align 1
  %1288 = lshr i32 %1265, 31
  %1289 = lshr i32 %1270, 31
  %1290 = xor i32 %1289, %1288
  %1291 = xor i32 %1286, %1288
  %1292 = add nuw nsw i32 %1291, %1290
  %1293 = icmp eq i32 %1292, 2
  %1294 = zext i1 %1293 to i8
  store i8 %1294, i8* %39, align 1
  %1295 = icmp ne i8 %1287, 0
  %1296 = xor i1 %1295, %1293
  %.v306 = select i1 %1296, i64 66, i64 12
  %1297 = add i64 %1263, %.v306
  store i64 %1297, i64* %3, align 8
  br i1 %1296, label %block_.L_40a5b6, label %block_40a580

block_40a580:                                     ; preds = %block_40a574
  %1298 = add i64 %1297, 3
  store i64 %1298, i64* %3, align 8
  %1299 = load i32, i32* %1230, align 4
  %1300 = zext i32 %1299 to i64
  store i64 %1300, i64* %RAX.i11582.pre-phi, align 8
  %1301 = add i64 %1227, -32
  %1302 = add i64 %1297, 6
  store i64 %1302, i64* %3, align 8
  %1303 = inttoptr i64 %1301 to i32*
  %1304 = load i32, i32* %1303, align 4
  %1305 = sub i32 %1299, %1304
  %1306 = icmp ult i32 %1299, %1304
  %1307 = zext i1 %1306 to i8
  store i8 %1307, i8* %14, align 1
  %1308 = and i32 %1305, 255
  %1309 = tail call i32 @llvm.ctpop.i32(i32 %1308)
  %1310 = trunc i32 %1309 to i8
  %1311 = and i8 %1310, 1
  %1312 = xor i8 %1311, 1
  store i8 %1312, i8* %21, align 1
  %1313 = xor i32 %1304, %1299
  %1314 = xor i32 %1313, %1305
  %1315 = lshr i32 %1314, 4
  %1316 = trunc i32 %1315 to i8
  %1317 = and i8 %1316, 1
  store i8 %1317, i8* %27, align 1
  %1318 = icmp eq i32 %1305, 0
  %1319 = zext i1 %1318 to i8
  store i8 %1319, i8* %30, align 1
  %1320 = lshr i32 %1305, 31
  %1321 = trunc i32 %1320 to i8
  store i8 %1321, i8* %33, align 1
  %1322 = lshr i32 %1299, 31
  %1323 = lshr i32 %1304, 31
  %1324 = xor i32 %1323, %1322
  %1325 = xor i32 %1320, %1322
  %1326 = add nuw nsw i32 %1325, %1324
  %1327 = icmp eq i32 %1326, 2
  %1328 = zext i1 %1327 to i8
  store i8 %1328, i8* %39, align 1
  %1329 = icmp ne i8 %1321, 0
  %1330 = xor i1 %1329, %1327
  %.demorgan = or i1 %1318, %1330
  %.v307 = select i1 %.demorgan, i64 12, i64 54
  %1331 = add i64 %1297, %.v307
  store i64 %1331, i64* %3, align 8
  br i1 %.demorgan, label %block_40a58c, label %block_.L_40a5b6

block_40a58c:                                     ; preds = %block_40a580
  %1332 = add i64 %1227, -8
  %1333 = add i64 %1331, 4
  store i64 %1333, i64* %3, align 8
  %1334 = inttoptr i64 %1332 to i64*
  %1335 = load i64, i64* %1334, align 8
  %1336 = add i64 %1335, 37708
  store i64 %1336, i64* %RAX.i11582.pre-phi, align 8
  %1337 = icmp ugt i64 %1335, -37709
  %1338 = zext i1 %1337 to i8
  store i8 %1338, i8* %14, align 1
  %1339 = trunc i64 %1336 to i32
  %1340 = and i32 %1339, 255
  %1341 = tail call i32 @llvm.ctpop.i32(i32 %1340)
  %1342 = trunc i32 %1341 to i8
  %1343 = and i8 %1342, 1
  %1344 = xor i8 %1343, 1
  store i8 %1344, i8* %21, align 1
  %1345 = xor i64 %1336, %1335
  %1346 = lshr i64 %1345, 4
  %1347 = trunc i64 %1346 to i8
  %1348 = and i8 %1347, 1
  store i8 %1348, i8* %27, align 1
  %1349 = icmp eq i64 %1336, 0
  %1350 = zext i1 %1349 to i8
  store i8 %1350, i8* %30, align 1
  %1351 = lshr i64 %1336, 63
  %1352 = trunc i64 %1351 to i8
  store i8 %1352, i8* %33, align 1
  %1353 = lshr i64 %1335, 63
  %1354 = xor i64 %1351, %1353
  %1355 = add nuw nsw i64 %1354, %1351
  %1356 = icmp eq i64 %1355, 2
  %1357 = zext i1 %1356 to i8
  store i8 %1357, i8* %39, align 1
  %1358 = add i64 %1227, -124
  %1359 = add i64 %1331, 13
  store i64 %1359, i64* %3, align 8
  %1360 = inttoptr i64 %1358 to i32*
  %1361 = load i32, i32* %1360, align 4
  %1362 = add i32 %1361, -1
  %1363 = zext i32 %1362 to i64
  store i64 %1363, i64* %RCX.i11580, align 8
  %1364 = sext i32 %1362 to i64
  %1365 = mul nsw i64 %1364, 258
  store i64 %1365, i64* %573, align 8
  %1366 = lshr i64 %1365, 63
  %1367 = add i64 %1365, %1336
  store i64 %1367, i64* %RAX.i11582.pre-phi, align 8
  %1368 = icmp ult i64 %1367, %1336
  %1369 = icmp ult i64 %1367, %1365
  %1370 = or i1 %1368, %1369
  %1371 = zext i1 %1370 to i8
  store i8 %1371, i8* %14, align 1
  %1372 = trunc i64 %1367 to i32
  %1373 = and i32 %1372, 255
  %1374 = tail call i32 @llvm.ctpop.i32(i32 %1373)
  %1375 = trunc i32 %1374 to i8
  %1376 = and i8 %1375, 1
  %1377 = xor i8 %1376, 1
  store i8 %1377, i8* %21, align 1
  %1378 = xor i64 %1365, %1336
  %1379 = xor i64 %1378, %1367
  %1380 = lshr i64 %1379, 4
  %1381 = trunc i64 %1380 to i8
  %1382 = and i8 %1381, 1
  store i8 %1382, i8* %27, align 1
  %1383 = icmp eq i64 %1367, 0
  %1384 = zext i1 %1383 to i8
  store i8 %1384, i8* %30, align 1
  %1385 = lshr i64 %1367, 63
  %1386 = trunc i64 %1385 to i8
  store i8 %1386, i8* %33, align 1
  %1387 = xor i64 %1385, %1351
  %1388 = xor i64 %1385, %1366
  %1389 = add nuw nsw i64 %1387, %1388
  %1390 = icmp eq i64 %1389, 2
  %1391 = zext i1 %1390 to i8
  store i8 %1391, i8* %39, align 1
  %1392 = load i64, i64* %RBP.i, align 8
  %1393 = add i64 %1392, -12
  %1394 = add i64 %1331, 33
  store i64 %1394, i64* %3, align 8
  %1395 = inttoptr i64 %1393 to i32*
  %1396 = load i32, i32* %1395, align 4
  %1397 = sext i32 %1396 to i64
  store i64 %1397, i64* %573, align 8
  %1398 = add i64 %1367, %1397
  %1399 = add i64 %1331, 37
  store i64 %1399, i64* %3, align 8
  %1400 = inttoptr i64 %1398 to i8*
  store i8 0, i8* %1400, align 1
  %1401 = load i64, i64* %3, align 8
  %1402 = add i64 %1401, 42
  store i64 %1402, i64* %3, align 8
  br label %block_.L_40a5db

block_.L_40a5b6:                                  ; preds = %block_40a580, %block_40a574
  %1403 = phi i64 [ %1331, %block_40a580 ], [ %1297, %block_40a574 ]
  %1404 = add i64 %1227, -8
  %1405 = add i64 %1403, 4
  store i64 %1405, i64* %3, align 8
  %1406 = inttoptr i64 %1404 to i64*
  %1407 = load i64, i64* %1406, align 8
  %1408 = add i64 %1407, 37708
  store i64 %1408, i64* %RAX.i11582.pre-phi, align 8
  %1409 = icmp ugt i64 %1407, -37709
  %1410 = zext i1 %1409 to i8
  store i8 %1410, i8* %14, align 1
  %1411 = trunc i64 %1408 to i32
  %1412 = and i32 %1411, 255
  %1413 = tail call i32 @llvm.ctpop.i32(i32 %1412)
  %1414 = trunc i32 %1413 to i8
  %1415 = and i8 %1414, 1
  %1416 = xor i8 %1415, 1
  store i8 %1416, i8* %21, align 1
  %1417 = xor i64 %1408, %1407
  %1418 = lshr i64 %1417, 4
  %1419 = trunc i64 %1418 to i8
  %1420 = and i8 %1419, 1
  store i8 %1420, i8* %27, align 1
  %1421 = icmp eq i64 %1408, 0
  %1422 = zext i1 %1421 to i8
  store i8 %1422, i8* %30, align 1
  %1423 = lshr i64 %1408, 63
  %1424 = trunc i64 %1423 to i8
  store i8 %1424, i8* %33, align 1
  %1425 = lshr i64 %1407, 63
  %1426 = xor i64 %1423, %1425
  %1427 = add nuw nsw i64 %1426, %1423
  %1428 = icmp eq i64 %1427, 2
  %1429 = zext i1 %1428 to i8
  store i8 %1429, i8* %39, align 1
  %1430 = add i64 %1227, -124
  %1431 = add i64 %1403, 13
  store i64 %1431, i64* %3, align 8
  %1432 = inttoptr i64 %1430 to i32*
  %1433 = load i32, i32* %1432, align 4
  %1434 = add i32 %1433, -1
  %1435 = zext i32 %1434 to i64
  store i64 %1435, i64* %RCX.i11580, align 8
  %1436 = sext i32 %1434 to i64
  %1437 = mul nsw i64 %1436, 258
  store i64 %1437, i64* %573, align 8
  %1438 = lshr i64 %1437, 63
  %1439 = add i64 %1437, %1408
  store i64 %1439, i64* %RAX.i11582.pre-phi, align 8
  %1440 = icmp ult i64 %1439, %1408
  %1441 = icmp ult i64 %1439, %1437
  %1442 = or i1 %1440, %1441
  %1443 = zext i1 %1442 to i8
  store i8 %1443, i8* %14, align 1
  %1444 = trunc i64 %1439 to i32
  %1445 = and i32 %1444, 255
  %1446 = tail call i32 @llvm.ctpop.i32(i32 %1445)
  %1447 = trunc i32 %1446 to i8
  %1448 = and i8 %1447, 1
  %1449 = xor i8 %1448, 1
  store i8 %1449, i8* %21, align 1
  %1450 = xor i64 %1437, %1408
  %1451 = xor i64 %1450, %1439
  %1452 = lshr i64 %1451, 4
  %1453 = trunc i64 %1452 to i8
  %1454 = and i8 %1453, 1
  store i8 %1454, i8* %27, align 1
  %1455 = icmp eq i64 %1439, 0
  %1456 = zext i1 %1455 to i8
  store i8 %1456, i8* %30, align 1
  %1457 = lshr i64 %1439, 63
  %1458 = trunc i64 %1457 to i8
  store i8 %1458, i8* %33, align 1
  %1459 = xor i64 %1457, %1423
  %1460 = xor i64 %1457, %1438
  %1461 = add nuw nsw i64 %1459, %1460
  %1462 = icmp eq i64 %1461, 2
  %1463 = zext i1 %1462 to i8
  store i8 %1463, i8* %39, align 1
  %1464 = load i64, i64* %RBP.i, align 8
  %1465 = add i64 %1464, -12
  %1466 = add i64 %1403, 33
  store i64 %1466, i64* %3, align 8
  %1467 = inttoptr i64 %1465 to i32*
  %1468 = load i32, i32* %1467, align 4
  %1469 = sext i32 %1468 to i64
  store i64 %1469, i64* %573, align 8
  %1470 = add i64 %1439, %1469
  %1471 = add i64 %1403, 37
  store i64 %1471, i64* %3, align 8
  %1472 = inttoptr i64 %1470 to i8*
  store i8 15, i8* %1472, align 1
  %.pre205 = load i64, i64* %3, align 8
  br label %block_.L_40a5db

block_.L_40a5db:                                  ; preds = %block_.L_40a5b6, %block_40a58c
  %1473 = phi i64 [ %.pre205, %block_.L_40a5b6 ], [ %1402, %block_40a58c ]
  %1474 = load i64, i64* %RBP.i, align 8
  %1475 = add i64 %1474, -12
  %1476 = add i64 %1473, 8
  store i64 %1476, i64* %3, align 8
  %1477 = inttoptr i64 %1475 to i32*
  %1478 = load i32, i32* %1477, align 4
  %1479 = add i32 %1478, 1
  %1480 = zext i32 %1479 to i64
  store i64 %1480, i64* %RAX.i11582.pre-phi, align 8
  %1481 = icmp eq i32 %1478, -1
  %1482 = icmp eq i32 %1479, 0
  %1483 = or i1 %1481, %1482
  %1484 = zext i1 %1483 to i8
  store i8 %1484, i8* %14, align 1
  %1485 = and i32 %1479, 255
  %1486 = tail call i32 @llvm.ctpop.i32(i32 %1485)
  %1487 = trunc i32 %1486 to i8
  %1488 = and i8 %1487, 1
  %1489 = xor i8 %1488, 1
  store i8 %1489, i8* %21, align 1
  %1490 = xor i32 %1479, %1478
  %1491 = lshr i32 %1490, 4
  %1492 = trunc i32 %1491 to i8
  %1493 = and i8 %1492, 1
  store i8 %1493, i8* %27, align 1
  %1494 = zext i1 %1482 to i8
  store i8 %1494, i8* %30, align 1
  %1495 = lshr i32 %1479, 31
  %1496 = trunc i32 %1495 to i8
  store i8 %1496, i8* %33, align 1
  %1497 = lshr i32 %1478, 31
  %1498 = xor i32 %1495, %1497
  %1499 = add nuw nsw i32 %1498, %1495
  %1500 = icmp eq i32 %1499, 2
  %1501 = zext i1 %1500 to i8
  store i8 %1501, i8* %39, align 1
  %1502 = add i64 %1473, 14
  store i64 %1502, i64* %3, align 8
  store i32 %1479, i32* %1477, align 4
  %1503 = load i64, i64* %3, align 8
  %1504 = add i64 %1503, -129
  store i64 %1504, i64* %3, align 8
  br label %block_.L_40a568

block_.L_40a5ee:                                  ; preds = %block_.L_40a568
  %1505 = add i64 %1227, -124
  %1506 = add i64 %1263, 3
  store i64 %1506, i64* %3, align 8
  %1507 = inttoptr i64 %1505 to i32*
  %1508 = load i32, i32* %1507, align 4
  %1509 = add i32 %1508, -1
  %1510 = zext i32 %1509 to i64
  store i64 %1510, i64* %RAX.i11582.pre-phi, align 8
  %1511 = icmp ne i32 %1508, 0
  %1512 = zext i1 %1511 to i8
  store i8 %1512, i8* %14, align 1
  %1513 = and i32 %1509, 255
  %1514 = tail call i32 @llvm.ctpop.i32(i32 %1513)
  %1515 = trunc i32 %1514 to i8
  %1516 = and i8 %1515, 1
  %1517 = xor i8 %1516, 1
  store i8 %1517, i8* %21, align 1
  %1518 = xor i32 %1508, 16
  %1519 = xor i32 %1518, %1509
  %1520 = lshr i32 %1519, 4
  %1521 = trunc i32 %1520 to i8
  %1522 = and i8 %1521, 1
  store i8 %1522, i8* %27, align 1
  %1523 = icmp eq i32 %1509, 0
  %1524 = zext i1 %1523 to i8
  store i8 %1524, i8* %30, align 1
  %1525 = lshr i32 %1509, 31
  %1526 = trunc i32 %1525 to i8
  store i8 %1526, i8* %33, align 1
  %1527 = lshr i32 %1508, 31
  %1528 = xor i32 %1525, %1527
  %1529 = xor i32 %1525, 1
  %1530 = add nuw nsw i32 %1528, %1529
  %1531 = icmp eq i32 %1530, 2
  %1532 = zext i1 %1531 to i8
  store i8 %1532, i8* %39, align 1
  %1533 = add i64 %1263, 9
  store i64 %1533, i64* %3, align 8
  store i32 %1509, i32* %1507, align 4
  %1534 = load i64, i64* %RBP.i, align 8
  %1535 = add i64 %1534, -32
  %1536 = load i64, i64* %3, align 8
  %1537 = add i64 %1536, 3
  store i64 %1537, i64* %3, align 8
  %1538 = inttoptr i64 %1535 to i32*
  %1539 = load i32, i32* %1538, align 4
  %1540 = add i32 %1539, 1
  %1541 = zext i32 %1540 to i64
  store i64 %1541, i64* %RAX.i11582.pre-phi, align 8
  %1542 = icmp eq i32 %1539, -1
  %1543 = icmp eq i32 %1540, 0
  %1544 = or i1 %1542, %1543
  %1545 = zext i1 %1544 to i8
  store i8 %1545, i8* %14, align 1
  %1546 = and i32 %1540, 255
  %1547 = tail call i32 @llvm.ctpop.i32(i32 %1546)
  %1548 = trunc i32 %1547 to i8
  %1549 = and i8 %1548, 1
  %1550 = xor i8 %1549, 1
  store i8 %1550, i8* %21, align 1
  %1551 = xor i32 %1540, %1539
  %1552 = lshr i32 %1551, 4
  %1553 = trunc i32 %1552 to i8
  %1554 = and i8 %1553, 1
  store i8 %1554, i8* %27, align 1
  %1555 = zext i1 %1543 to i8
  store i8 %1555, i8* %30, align 1
  %1556 = lshr i32 %1540, 31
  %1557 = trunc i32 %1556 to i8
  store i8 %1557, i8* %33, align 1
  %1558 = lshr i32 %1539, 31
  %1559 = xor i32 %1556, %1558
  %1560 = add nuw nsw i32 %1559, %1556
  %1561 = icmp eq i32 %1560, 2
  %1562 = zext i1 %1561 to i8
  store i8 %1562, i8* %39, align 1
  %1563 = add i64 %1534, -28
  %1564 = add i64 %1536, 9
  store i64 %1564, i64* %3, align 8
  %1565 = inttoptr i64 %1563 to i32*
  store i32 %1540, i32* %1565, align 4
  %1566 = load i64, i64* %RBP.i, align 8
  %1567 = add i64 %1566, -136
  %1568 = load i64, i64* %3, align 8
  %1569 = add i64 %1568, 6
  store i64 %1569, i64* %3, align 8
  %1570 = inttoptr i64 %1567 to i32*
  %1571 = load i32, i32* %1570, align 4
  %1572 = zext i32 %1571 to i64
  store i64 %1572, i64* %RAX.i11582.pre-phi, align 8
  %1573 = add i64 %1566, -128
  %1574 = add i64 %1568, 9
  store i64 %1574, i64* %3, align 8
  %1575 = inttoptr i64 %1573 to i32*
  %1576 = load i32, i32* %1575, align 4
  %1577 = sub i32 %1576, %1571
  %1578 = zext i32 %1577 to i64
  store i64 %1578, i64* %RCX.i11580, align 8
  %1579 = icmp ult i32 %1576, %1571
  %1580 = zext i1 %1579 to i8
  store i8 %1580, i8* %14, align 1
  %1581 = and i32 %1577, 255
  %1582 = tail call i32 @llvm.ctpop.i32(i32 %1581)
  %1583 = trunc i32 %1582 to i8
  %1584 = and i8 %1583, 1
  %1585 = xor i8 %1584, 1
  store i8 %1585, i8* %21, align 1
  %1586 = xor i32 %1571, %1576
  %1587 = xor i32 %1586, %1577
  %1588 = lshr i32 %1587, 4
  %1589 = trunc i32 %1588 to i8
  %1590 = and i8 %1589, 1
  store i8 %1590, i8* %27, align 1
  %1591 = icmp eq i32 %1577, 0
  %1592 = zext i1 %1591 to i8
  store i8 %1592, i8* %30, align 1
  %1593 = lshr i32 %1577, 31
  %1594 = trunc i32 %1593 to i8
  store i8 %1594, i8* %33, align 1
  %1595 = lshr i32 %1576, 31
  %1596 = lshr i32 %1571, 31
  %1597 = xor i32 %1596, %1595
  %1598 = xor i32 %1593, %1595
  %1599 = add nuw nsw i32 %1598, %1597
  %1600 = icmp eq i32 %1599, 2
  %1601 = zext i1 %1600 to i8
  store i8 %1601, i8* %39, align 1
  %1602 = add i64 %1568, 14
  store i64 %1602, i64* %3, align 8
  store i32 %1577, i32* %1575, align 4
  %1603 = load i64, i64* %3, align 8
  %1604 = add i64 %1603, -541
  store i64 %1604, i64* %3, align 8
  br label %block_.L_40a3f1

block_.L_40a613:                                  ; preds = %block_.L_40a3f1
  %1605 = add i64 %599, -48
  %1606 = add i64 %615, 7
  store i64 %1606, i64* %3, align 8
  %1607 = inttoptr i64 %1605 to i32*
  store i32 0, i32* %1607, align 4
  %DX.i6075 = bitcast %union.anon* %574 to i16*
  %SI.i = bitcast %union.anon* %597 to i16*
  %DI.i6029 = bitcast %union.anon* %40 to i16*
  %1608 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %.pre206 = load i64, i64* %3, align 8
  br label %block_.L_40a61a

block_.L_40a61a:                                  ; preds = %block_.L_40d258, %block_.L_40a613
  %1609 = phi i64 [ %.pre206, %block_.L_40a613 ], [ %27316, %block_.L_40d258 ]
  %MEMORY.16 = phi %struct.Memory* [ %MEMORY.8, %block_.L_40a613 ], [ %MEMORY.35, %block_.L_40d258 ]
  %1610 = load i64, i64* %RBP.i, align 8
  %1611 = add i64 %1610, -48
  %1612 = add i64 %1609, 4
  store i64 %1612, i64* %3, align 8
  %1613 = inttoptr i64 %1611 to i32*
  %1614 = load i32, i32* %1613, align 4
  %1615 = add i32 %1614, -4
  %1616 = icmp ult i32 %1614, 4
  %1617 = zext i1 %1616 to i8
  store i8 %1617, i8* %14, align 1
  %1618 = and i32 %1615, 255
  %1619 = tail call i32 @llvm.ctpop.i32(i32 %1618)
  %1620 = trunc i32 %1619 to i8
  %1621 = and i8 %1620, 1
  %1622 = xor i8 %1621, 1
  store i8 %1622, i8* %21, align 1
  %1623 = xor i32 %1615, %1614
  %1624 = lshr i32 %1623, 4
  %1625 = trunc i32 %1624 to i8
  %1626 = and i8 %1625, 1
  store i8 %1626, i8* %27, align 1
  %1627 = icmp eq i32 %1615, 0
  %1628 = zext i1 %1627 to i8
  store i8 %1628, i8* %30, align 1
  %1629 = lshr i32 %1615, 31
  %1630 = trunc i32 %1629 to i8
  store i8 %1630, i8* %33, align 1
  %1631 = lshr i32 %1614, 31
  %1632 = xor i32 %1629, %1631
  %1633 = add nuw nsw i32 %1632, %1631
  %1634 = icmp eq i32 %1633, 2
  %1635 = zext i1 %1634 to i8
  store i8 %1635, i8* %39, align 1
  %1636 = icmp ne i8 %1630, 0
  %1637 = xor i1 %1636, %1634
  %.v308 = select i1 %1637, i64 10, i64 11345
  %1638 = add i64 %1609, %.v308
  store i64 %1638, i64* %3, align 8
  br i1 %1637, label %block_40a624, label %block_.L_40d26b

block_40a624:                                     ; preds = %block_.L_40a61a
  %1639 = add i64 %1610, -16
  %1640 = add i64 %1638, 7
  store i64 %1640, i64* %3, align 8
  %1641 = inttoptr i64 %1639 to i32*
  store i32 0, i32* %1641, align 4
  %.pre255 = load i64, i64* %3, align 8
  br label %block_.L_40a62b

block_.L_40a62b:                                  ; preds = %block_40a637, %block_40a624
  %1642 = phi i64 [ %1719, %block_40a637 ], [ %.pre255, %block_40a624 ]
  %1643 = load i64, i64* %RBP.i, align 8
  %1644 = add i64 %1643, -16
  %1645 = add i64 %1642, 3
  store i64 %1645, i64* %3, align 8
  %1646 = inttoptr i64 %1644 to i32*
  %1647 = load i32, i32* %1646, align 4
  %1648 = zext i32 %1647 to i64
  store i64 %1648, i64* %RAX.i11582.pre-phi, align 8
  %1649 = add i64 %1643, -72
  %1650 = add i64 %1642, 6
  store i64 %1650, i64* %3, align 8
  %1651 = inttoptr i64 %1649 to i32*
  %1652 = load i32, i32* %1651, align 4
  %1653 = sub i32 %1647, %1652
  %1654 = icmp ult i32 %1647, %1652
  %1655 = zext i1 %1654 to i8
  store i8 %1655, i8* %14, align 1
  %1656 = and i32 %1653, 255
  %1657 = tail call i32 @llvm.ctpop.i32(i32 %1656)
  %1658 = trunc i32 %1657 to i8
  %1659 = and i8 %1658, 1
  %1660 = xor i8 %1659, 1
  store i8 %1660, i8* %21, align 1
  %1661 = xor i32 %1652, %1647
  %1662 = xor i32 %1661, %1653
  %1663 = lshr i32 %1662, 4
  %1664 = trunc i32 %1663 to i8
  %1665 = and i8 %1664, 1
  store i8 %1665, i8* %27, align 1
  %1666 = icmp eq i32 %1653, 0
  %1667 = zext i1 %1666 to i8
  store i8 %1667, i8* %30, align 1
  %1668 = lshr i32 %1653, 31
  %1669 = trunc i32 %1668 to i8
  store i8 %1669, i8* %33, align 1
  %1670 = lshr i32 %1647, 31
  %1671 = lshr i32 %1652, 31
  %1672 = xor i32 %1671, %1670
  %1673 = xor i32 %1668, %1670
  %1674 = add nuw nsw i32 %1673, %1672
  %1675 = icmp eq i32 %1674, 2
  %1676 = zext i1 %1675 to i8
  store i8 %1676, i8* %39, align 1
  %1677 = icmp ne i8 %1669, 0
  %1678 = xor i1 %1677, %1675
  %.v345 = select i1 %1678, i64 12, i64 38
  %1679 = add i64 %1642, %.v345
  store i64 %1679, i64* %3, align 8
  br i1 %1678, label %block_40a637, label %block_.L_40a651

block_40a637:                                     ; preds = %block_.L_40a62b
  %1680 = add i64 %1679, 4
  store i64 %1680, i64* %3, align 8
  %1681 = load i32, i32* %1646, align 4
  %1682 = sext i32 %1681 to i64
  store i64 %1682, i64* %RAX.i11582.pre-phi, align 8
  %1683 = shl nsw i64 %1682, 2
  %1684 = add i64 %1643, -112
  %1685 = add i64 %1684, %1683
  %1686 = add i64 %1679, 12
  store i64 %1686, i64* %3, align 8
  %1687 = inttoptr i64 %1685 to i32*
  store i32 0, i32* %1687, align 4
  %1688 = load i64, i64* %RBP.i, align 8
  %1689 = add i64 %1688, -16
  %1690 = load i64, i64* %3, align 8
  %1691 = add i64 %1690, 3
  store i64 %1691, i64* %3, align 8
  %1692 = inttoptr i64 %1689 to i32*
  %1693 = load i32, i32* %1692, align 4
  %1694 = add i32 %1693, 1
  %1695 = zext i32 %1694 to i64
  store i64 %1695, i64* %RAX.i11582.pre-phi, align 8
  %1696 = icmp eq i32 %1693, -1
  %1697 = icmp eq i32 %1694, 0
  %1698 = or i1 %1696, %1697
  %1699 = zext i1 %1698 to i8
  store i8 %1699, i8* %14, align 1
  %1700 = and i32 %1694, 255
  %1701 = tail call i32 @llvm.ctpop.i32(i32 %1700)
  %1702 = trunc i32 %1701 to i8
  %1703 = and i8 %1702, 1
  %1704 = xor i8 %1703, 1
  store i8 %1704, i8* %21, align 1
  %1705 = xor i32 %1694, %1693
  %1706 = lshr i32 %1705, 4
  %1707 = trunc i32 %1706 to i8
  %1708 = and i8 %1707, 1
  store i8 %1708, i8* %27, align 1
  %1709 = zext i1 %1697 to i8
  store i8 %1709, i8* %30, align 1
  %1710 = lshr i32 %1694, 31
  %1711 = trunc i32 %1710 to i8
  store i8 %1711, i8* %33, align 1
  %1712 = lshr i32 %1693, 31
  %1713 = xor i32 %1710, %1712
  %1714 = add nuw nsw i32 %1713, %1710
  %1715 = icmp eq i32 %1714, 2
  %1716 = zext i1 %1715 to i8
  store i8 %1716, i8* %39, align 1
  %1717 = add i64 %1690, 9
  store i64 %1717, i64* %3, align 8
  store i32 %1694, i32* %1692, align 4
  %1718 = load i64, i64* %3, align 8
  %1719 = add i64 %1718, -33
  store i64 %1719, i64* %3, align 8
  br label %block_.L_40a62b

block_.L_40a651:                                  ; preds = %block_.L_40a62b
  %1720 = add i64 %1679, 7
  store i64 %1720, i64* %3, align 8
  store i32 0, i32* %1646, align 4
  %.pre256 = load i64, i64* %3, align 8
  br label %block_.L_40a658

block_.L_40a658:                                  ; preds = %block_.L_40a6a8, %block_.L_40a651
  %1721 = phi i64 [ %1929, %block_.L_40a6a8 ], [ %.pre256, %block_.L_40a651 ]
  %1722 = load i64, i64* %RBP.i, align 8
  %1723 = add i64 %1722, -16
  %1724 = add i64 %1721, 3
  store i64 %1724, i64* %3, align 8
  %1725 = inttoptr i64 %1723 to i32*
  %1726 = load i32, i32* %1725, align 4
  %1727 = zext i32 %1726 to i64
  store i64 %1727, i64* %RAX.i11582.pre-phi, align 8
  %1728 = add i64 %1722, -72
  %1729 = add i64 %1721, 6
  store i64 %1729, i64* %3, align 8
  %1730 = inttoptr i64 %1728 to i32*
  %1731 = load i32, i32* %1730, align 4
  %1732 = sub i32 %1726, %1731
  %1733 = icmp ult i32 %1726, %1731
  %1734 = zext i1 %1733 to i8
  store i8 %1734, i8* %14, align 1
  %1735 = and i32 %1732, 255
  %1736 = tail call i32 @llvm.ctpop.i32(i32 %1735)
  %1737 = trunc i32 %1736 to i8
  %1738 = and i8 %1737, 1
  %1739 = xor i8 %1738, 1
  store i8 %1739, i8* %21, align 1
  %1740 = xor i32 %1731, %1726
  %1741 = xor i32 %1740, %1732
  %1742 = lshr i32 %1741, 4
  %1743 = trunc i32 %1742 to i8
  %1744 = and i8 %1743, 1
  store i8 %1744, i8* %27, align 1
  %1745 = icmp eq i32 %1732, 0
  %1746 = zext i1 %1745 to i8
  store i8 %1746, i8* %30, align 1
  %1747 = lshr i32 %1732, 31
  %1748 = trunc i32 %1747 to i8
  store i8 %1748, i8* %33, align 1
  %1749 = lshr i32 %1726, 31
  %1750 = lshr i32 %1731, 31
  %1751 = xor i32 %1750, %1749
  %1752 = xor i32 %1747, %1749
  %1753 = add nuw nsw i32 %1752, %1751
  %1754 = icmp eq i32 %1753, 2
  %1755 = zext i1 %1754 to i8
  store i8 %1755, i8* %39, align 1
  %1756 = icmp ne i8 %1748, 0
  %1757 = xor i1 %1756, %1754
  %.v346 = select i1 %1757, i64 12, i64 99
  %1758 = add i64 %1721, %.v346
  store i64 %1758, i64* %3, align 8
  br i1 %1757, label %block_40a664, label %block_.L_40a6bb

block_40a664:                                     ; preds = %block_.L_40a658
  %1759 = add i64 %1722, -12
  %1760 = add i64 %1758, 7
  store i64 %1760, i64* %3, align 8
  %1761 = inttoptr i64 %1759 to i32*
  store i32 0, i32* %1761, align 4
  %.pre278 = load i64, i64* %3, align 8
  br label %block_.L_40a66b

block_.L_40a66b:                                  ; preds = %block_40a677, %block_40a664
  %1762 = phi i64 [ %1899, %block_40a677 ], [ %.pre278, %block_40a664 ]
  %1763 = load i64, i64* %RBP.i, align 8
  %1764 = add i64 %1763, -12
  %1765 = add i64 %1762, 3
  store i64 %1765, i64* %3, align 8
  %1766 = inttoptr i64 %1764 to i32*
  %1767 = load i32, i32* %1766, align 4
  %1768 = zext i32 %1767 to i64
  store i64 %1768, i64* %RAX.i11582.pre-phi, align 8
  %1769 = add i64 %1763, -56
  %1770 = add i64 %1762, 6
  store i64 %1770, i64* %3, align 8
  %1771 = inttoptr i64 %1769 to i32*
  %1772 = load i32, i32* %1771, align 4
  %1773 = sub i32 %1767, %1772
  %1774 = icmp ult i32 %1767, %1772
  %1775 = zext i1 %1774 to i8
  store i8 %1775, i8* %14, align 1
  %1776 = and i32 %1773, 255
  %1777 = tail call i32 @llvm.ctpop.i32(i32 %1776)
  %1778 = trunc i32 %1777 to i8
  %1779 = and i8 %1778, 1
  %1780 = xor i8 %1779, 1
  store i8 %1780, i8* %21, align 1
  %1781 = xor i32 %1772, %1767
  %1782 = xor i32 %1781, %1773
  %1783 = lshr i32 %1782, 4
  %1784 = trunc i32 %1783 to i8
  %1785 = and i8 %1784, 1
  store i8 %1785, i8* %27, align 1
  %1786 = icmp eq i32 %1773, 0
  %1787 = zext i1 %1786 to i8
  store i8 %1787, i8* %30, align 1
  %1788 = lshr i32 %1773, 31
  %1789 = trunc i32 %1788 to i8
  store i8 %1789, i8* %33, align 1
  %1790 = lshr i32 %1767, 31
  %1791 = lshr i32 %1772, 31
  %1792 = xor i32 %1791, %1790
  %1793 = xor i32 %1788, %1790
  %1794 = add nuw nsw i32 %1793, %1792
  %1795 = icmp eq i32 %1794, 2
  %1796 = zext i1 %1795 to i8
  store i8 %1796, i8* %39, align 1
  %1797 = icmp ne i8 %1789, 0
  %1798 = xor i1 %1797, %1795
  %.v294 = select i1 %1798, i64 12, i64 61
  %1799 = add i64 %1762, %.v294
  store i64 %1799, i64* %3, align 8
  br i1 %1798, label %block_40a677, label %block_.L_40a6a8

block_40a677:                                     ; preds = %block_.L_40a66b
  %1800 = add i64 %1763, -8
  %1801 = add i64 %1799, 4
  store i64 %1801, i64* %3, align 8
  %1802 = inttoptr i64 %1800 to i64*
  %1803 = load i64, i64* %1802, align 8
  %1804 = add i64 %1803, 45448
  store i64 %1804, i64* %RAX.i11582.pre-phi, align 8
  %1805 = icmp ugt i64 %1803, -45449
  %1806 = zext i1 %1805 to i8
  store i8 %1806, i8* %14, align 1
  %1807 = trunc i64 %1804 to i32
  %1808 = and i32 %1807, 255
  %1809 = tail call i32 @llvm.ctpop.i32(i32 %1808)
  %1810 = trunc i32 %1809 to i8
  %1811 = and i8 %1810, 1
  %1812 = xor i8 %1811, 1
  store i8 %1812, i8* %21, align 1
  %1813 = xor i64 %1804, %1803
  %1814 = lshr i64 %1813, 4
  %1815 = trunc i64 %1814 to i8
  %1816 = and i8 %1815, 1
  store i8 %1816, i8* %27, align 1
  %1817 = icmp eq i64 %1804, 0
  %1818 = zext i1 %1817 to i8
  store i8 %1818, i8* %30, align 1
  %1819 = lshr i64 %1804, 63
  %1820 = trunc i64 %1819 to i8
  store i8 %1820, i8* %33, align 1
  %1821 = lshr i64 %1803, 63
  %1822 = xor i64 %1819, %1821
  %1823 = add nuw nsw i64 %1822, %1819
  %1824 = icmp eq i64 %1823, 2
  %1825 = zext i1 %1824 to i8
  store i8 %1825, i8* %39, align 1
  %1826 = add i64 %1763, -16
  %1827 = add i64 %1799, 14
  store i64 %1827, i64* %3, align 8
  %1828 = inttoptr i64 %1826 to i32*
  %1829 = load i32, i32* %1828, align 4
  %1830 = sext i32 %1829 to i64
  %1831 = mul nsw i64 %1830, 1032
  store i64 %1831, i64* %RCX.i11580, align 8
  %1832 = lshr i64 %1831, 63
  %1833 = add i64 %1831, %1804
  store i64 %1833, i64* %RAX.i11582.pre-phi, align 8
  %1834 = icmp ult i64 %1833, %1804
  %1835 = icmp ult i64 %1833, %1831
  %1836 = or i1 %1834, %1835
  %1837 = zext i1 %1836 to i8
  store i8 %1837, i8* %14, align 1
  %1838 = trunc i64 %1833 to i32
  %1839 = and i32 %1838, 255
  %1840 = tail call i32 @llvm.ctpop.i32(i32 %1839)
  %1841 = trunc i32 %1840 to i8
  %1842 = and i8 %1841, 1
  %1843 = xor i8 %1842, 1
  store i8 %1843, i8* %21, align 1
  %1844 = xor i64 %1831, %1804
  %1845 = xor i64 %1844, %1833
  %1846 = lshr i64 %1845, 4
  %1847 = trunc i64 %1846 to i8
  %1848 = and i8 %1847, 1
  store i8 %1848, i8* %27, align 1
  %1849 = icmp eq i64 %1833, 0
  %1850 = zext i1 %1849 to i8
  store i8 %1850, i8* %30, align 1
  %1851 = lshr i64 %1833, 63
  %1852 = trunc i64 %1851 to i8
  store i8 %1852, i8* %33, align 1
  %1853 = xor i64 %1851, %1819
  %1854 = xor i64 %1851, %1832
  %1855 = add nuw nsw i64 %1853, %1854
  %1856 = icmp eq i64 %1855, 2
  %1857 = zext i1 %1856 to i8
  store i8 %1857, i8* %39, align 1
  %1858 = load i64, i64* %RBP.i, align 8
  %1859 = add i64 %1858, -12
  %1860 = add i64 %1799, 28
  store i64 %1860, i64* %3, align 8
  %1861 = inttoptr i64 %1859 to i32*
  %1862 = load i32, i32* %1861, align 4
  %1863 = sext i32 %1862 to i64
  store i64 %1863, i64* %RCX.i11580, align 8
  %1864 = shl nsw i64 %1863, 2
  %1865 = add i64 %1864, %1833
  %1866 = add i64 %1799, 35
  store i64 %1866, i64* %3, align 8
  %1867 = inttoptr i64 %1865 to i32*
  store i32 0, i32* %1867, align 4
  %1868 = load i64, i64* %RBP.i, align 8
  %1869 = add i64 %1868, -12
  %1870 = load i64, i64* %3, align 8
  %1871 = add i64 %1870, 3
  store i64 %1871, i64* %3, align 8
  %1872 = inttoptr i64 %1869 to i32*
  %1873 = load i32, i32* %1872, align 4
  %1874 = add i32 %1873, 1
  %1875 = zext i32 %1874 to i64
  store i64 %1875, i64* %RAX.i11582.pre-phi, align 8
  %1876 = icmp eq i32 %1873, -1
  %1877 = icmp eq i32 %1874, 0
  %1878 = or i1 %1876, %1877
  %1879 = zext i1 %1878 to i8
  store i8 %1879, i8* %14, align 1
  %1880 = and i32 %1874, 255
  %1881 = tail call i32 @llvm.ctpop.i32(i32 %1880)
  %1882 = trunc i32 %1881 to i8
  %1883 = and i8 %1882, 1
  %1884 = xor i8 %1883, 1
  store i8 %1884, i8* %21, align 1
  %1885 = xor i32 %1874, %1873
  %1886 = lshr i32 %1885, 4
  %1887 = trunc i32 %1886 to i8
  %1888 = and i8 %1887, 1
  store i8 %1888, i8* %27, align 1
  %1889 = zext i1 %1877 to i8
  store i8 %1889, i8* %30, align 1
  %1890 = lshr i32 %1874, 31
  %1891 = trunc i32 %1890 to i8
  store i8 %1891, i8* %33, align 1
  %1892 = lshr i32 %1873, 31
  %1893 = xor i32 %1890, %1892
  %1894 = add nuw nsw i32 %1893, %1890
  %1895 = icmp eq i32 %1894, 2
  %1896 = zext i1 %1895 to i8
  store i8 %1896, i8* %39, align 1
  %1897 = add i64 %1870, 9
  store i64 %1897, i64* %3, align 8
  store i32 %1874, i32* %1872, align 4
  %1898 = load i64, i64* %3, align 8
  %1899 = add i64 %1898, -56
  store i64 %1899, i64* %3, align 8
  br label %block_.L_40a66b

block_.L_40a6a8:                                  ; preds = %block_.L_40a66b
  %1900 = add i64 %1763, -16
  %1901 = add i64 %1799, 8
  store i64 %1901, i64* %3, align 8
  %1902 = inttoptr i64 %1900 to i32*
  %1903 = load i32, i32* %1902, align 4
  %1904 = add i32 %1903, 1
  %1905 = zext i32 %1904 to i64
  store i64 %1905, i64* %RAX.i11582.pre-phi, align 8
  %1906 = icmp eq i32 %1903, -1
  %1907 = icmp eq i32 %1904, 0
  %1908 = or i1 %1906, %1907
  %1909 = zext i1 %1908 to i8
  store i8 %1909, i8* %14, align 1
  %1910 = and i32 %1904, 255
  %1911 = tail call i32 @llvm.ctpop.i32(i32 %1910)
  %1912 = trunc i32 %1911 to i8
  %1913 = and i8 %1912, 1
  %1914 = xor i8 %1913, 1
  store i8 %1914, i8* %21, align 1
  %1915 = xor i32 %1904, %1903
  %1916 = lshr i32 %1915, 4
  %1917 = trunc i32 %1916 to i8
  %1918 = and i8 %1917, 1
  store i8 %1918, i8* %27, align 1
  %1919 = zext i1 %1907 to i8
  store i8 %1919, i8* %30, align 1
  %1920 = lshr i32 %1904, 31
  %1921 = trunc i32 %1920 to i8
  store i8 %1921, i8* %33, align 1
  %1922 = lshr i32 %1903, 31
  %1923 = xor i32 %1920, %1922
  %1924 = add nuw nsw i32 %1923, %1920
  %1925 = icmp eq i32 %1924, 2
  %1926 = zext i1 %1925 to i8
  store i8 %1926, i8* %39, align 1
  %1927 = add i64 %1799, 14
  store i64 %1927, i64* %3, align 8
  store i32 %1904, i32* %1902, align 4
  %1928 = load i64, i64* %3, align 8
  %1929 = add i64 %1928, -94
  store i64 %1929, i64* %3, align 8
  br label %block_.L_40a658

block_.L_40a6bb:                                  ; preds = %block_.L_40a658
  %1930 = add i64 %1758, 4
  store i64 %1930, i64* %3, align 8
  %1931 = load i32, i32* %1730, align 4
  %1932 = add i32 %1931, -6
  %1933 = icmp ult i32 %1931, 6
  %1934 = zext i1 %1933 to i8
  store i8 %1934, i8* %14, align 1
  %1935 = and i32 %1932, 255
  %1936 = tail call i32 @llvm.ctpop.i32(i32 %1935)
  %1937 = trunc i32 %1936 to i8
  %1938 = and i8 %1937, 1
  %1939 = xor i8 %1938, 1
  store i8 %1939, i8* %21, align 1
  %1940 = xor i32 %1932, %1931
  %1941 = lshr i32 %1940, 4
  %1942 = trunc i32 %1941 to i8
  %1943 = and i8 %1942, 1
  store i8 %1943, i8* %27, align 1
  %1944 = icmp eq i32 %1932, 0
  %1945 = zext i1 %1944 to i8
  store i8 %1945, i8* %30, align 1
  %1946 = lshr i32 %1932, 31
  %1947 = trunc i32 %1946 to i8
  store i8 %1947, i8* %33, align 1
  %1948 = lshr i32 %1931, 31
  %1949 = xor i32 %1946, %1948
  %1950 = add nuw nsw i32 %1949, %1948
  %1951 = icmp eq i32 %1950, 2
  %1952 = zext i1 %1951 to i8
  store i8 %1952, i8* %39, align 1
  %.v347 = select i1 %1944, i64 10, i64 230
  %1953 = add i64 %1758, %.v347
  store i64 %1953, i64* %3, align 8
  br i1 %1944, label %block_40a6c5, label %block_.L_40a7a1

block_40a6c5:                                     ; preds = %block_.L_40a6bb
  %1954 = add i64 %1722, -12
  %1955 = add i64 %1953, 7
  store i64 %1955, i64* %3, align 8
  %1956 = inttoptr i64 %1954 to i32*
  store i32 0, i32* %1956, align 4
  %.pre257 = load i64, i64* %3, align 8
  br label %block_.L_40a6cc

block_.L_40a6cc:                                  ; preds = %block_40a6d8, %block_40a6c5
  %1957 = phi i64 [ %2333, %block_40a6d8 ], [ %.pre257, %block_40a6c5 ]
  %1958 = load i64, i64* %RBP.i, align 8
  %1959 = add i64 %1958, -12
  %1960 = add i64 %1957, 3
  store i64 %1960, i64* %3, align 8
  %1961 = inttoptr i64 %1959 to i32*
  %1962 = load i32, i32* %1961, align 4
  %1963 = zext i32 %1962 to i64
  store i64 %1963, i64* %RAX.i11582.pre-phi, align 8
  %1964 = add i64 %1958, -56
  %1965 = add i64 %1957, 6
  store i64 %1965, i64* %3, align 8
  %1966 = inttoptr i64 %1964 to i32*
  %1967 = load i32, i32* %1966, align 4
  %1968 = sub i32 %1962, %1967
  %1969 = icmp ult i32 %1962, %1967
  %1970 = zext i1 %1969 to i8
  store i8 %1970, i8* %14, align 1
  %1971 = and i32 %1968, 255
  %1972 = tail call i32 @llvm.ctpop.i32(i32 %1971)
  %1973 = trunc i32 %1972 to i8
  %1974 = and i8 %1973, 1
  %1975 = xor i8 %1974, 1
  store i8 %1975, i8* %21, align 1
  %1976 = xor i32 %1967, %1962
  %1977 = xor i32 %1976, %1968
  %1978 = lshr i32 %1977, 4
  %1979 = trunc i32 %1978 to i8
  %1980 = and i8 %1979, 1
  store i8 %1980, i8* %27, align 1
  %1981 = icmp eq i32 %1968, 0
  %1982 = zext i1 %1981 to i8
  store i8 %1982, i8* %30, align 1
  %1983 = lshr i32 %1968, 31
  %1984 = trunc i32 %1983 to i8
  store i8 %1984, i8* %33, align 1
  %1985 = lshr i32 %1962, 31
  %1986 = lshr i32 %1967, 31
  %1987 = xor i32 %1986, %1985
  %1988 = xor i32 %1983, %1985
  %1989 = add nuw nsw i32 %1988, %1987
  %1990 = icmp eq i32 %1989, 2
  %1991 = zext i1 %1990 to i8
  store i8 %1991, i8* %39, align 1
  %1992 = icmp ne i8 %1984, 0
  %1993 = xor i1 %1992, %1990
  %.v361 = select i1 %1993, i64 12, i64 208
  %1994 = add i64 %1957, %.v361
  store i64 %1994, i64* %3, align 8
  br i1 %1993, label %block_40a6d8, label %block_.L_40a79c

block_40a6d8:                                     ; preds = %block_.L_40a6cc
  %1995 = add i64 %1958, -8
  %1996 = add i64 %1994, 4
  store i64 %1996, i64* %3, align 8
  %1997 = inttoptr i64 %1995 to i64*
  %1998 = load i64, i64* %1997, align 8
  store i64 %1998, i64* %RAX.i11582.pre-phi, align 8
  %1999 = add i64 %1994, 8
  store i64 %1999, i64* %3, align 8
  %2000 = load i32, i32* %1961, align 4
  %2001 = sext i32 %2000 to i64
  store i64 %2001, i64* %RCX.i11580, align 8
  %2002 = add nsw i64 %2001, 37966
  %2003 = add i64 %2002, %1998
  %2004 = add i64 %1994, 16
  store i64 %2004, i64* %3, align 8
  %2005 = inttoptr i64 %2003 to i8*
  %2006 = load i8, i8* %2005, align 1
  %2007 = zext i8 %2006 to i64
  %2008 = shl nuw nsw i64 %2007, 16
  store i64 %2008, i64* %576, align 8
  store i8 0, i8* %14, align 1
  store i8 1, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %2009 = icmp eq i8 %2006, 0
  %2010 = zext i1 %2009 to i8
  store i8 %2010, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %2011 = add i64 %1994, 23
  store i64 %2011, i64* %3, align 8
  %2012 = load i64, i64* %1997, align 8
  store i64 %2012, i64* %RAX.i11582.pre-phi, align 8
  %2013 = add i64 %1994, 27
  store i64 %2013, i64* %3, align 8
  %2014 = load i32, i32* %1961, align 4
  %2015 = sext i32 %2014 to i64
  store i64 %2015, i64* %RCX.i11580, align 8
  %2016 = add nsw i64 %2015, 37708
  %2017 = add i64 %2016, %2012
  %2018 = add i64 %1994, 35
  store i64 %2018, i64* %3, align 8
  %2019 = inttoptr i64 %2017 to i8*
  %2020 = load i8, i8* %2019, align 1
  %2021 = zext i8 %2020 to i64
  store i64 %2021, i64* %RSI.i11312, align 8
  %2022 = zext i8 %2020 to i64
  %2023 = or i64 %2022, %2008
  %2024 = trunc i64 %2023 to i32
  store i64 %2023, i64* %576, align 8
  store i8 0, i8* %14, align 1
  %2025 = and i32 %2024, 255
  %2026 = tail call i32 @llvm.ctpop.i32(i32 %2025)
  %2027 = trunc i32 %2026 to i8
  %2028 = and i8 %2027, 1
  %2029 = xor i8 %2028, 1
  store i8 %2029, i8* %21, align 1
  %2030 = icmp eq i32 %2024, 0
  %2031 = zext i1 %2030 to i8
  store i8 %2031, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %2032 = add i64 %1994, 41
  store i64 %2032, i64* %3, align 8
  %2033 = load i64, i64* %1997, align 8
  %2034 = add i64 %2033, 51640
  store i64 %2034, i64* %RAX.i11582.pre-phi, align 8
  %2035 = icmp ugt i64 %2033, -51641
  %2036 = zext i1 %2035 to i8
  store i8 %2036, i8* %14, align 1
  %2037 = trunc i64 %2034 to i32
  %2038 = and i32 %2037, 255
  %2039 = tail call i32 @llvm.ctpop.i32(i32 %2038)
  %2040 = trunc i32 %2039 to i8
  %2041 = and i8 %2040, 1
  %2042 = xor i8 %2041, 1
  store i8 %2042, i8* %21, align 1
  %2043 = xor i64 %2033, 16
  %2044 = xor i64 %2043, %2034
  %2045 = lshr i64 %2044, 4
  %2046 = trunc i64 %2045 to i8
  %2047 = and i8 %2046, 1
  store i8 %2047, i8* %27, align 1
  %2048 = icmp eq i64 %2034, 0
  %2049 = zext i1 %2048 to i8
  store i8 %2049, i8* %30, align 1
  %2050 = lshr i64 %2034, 63
  %2051 = trunc i64 %2050 to i8
  store i8 %2051, i8* %33, align 1
  %2052 = lshr i64 %2033, 63
  %2053 = xor i64 %2050, %2052
  %2054 = add nuw nsw i64 %2053, %2050
  %2055 = icmp eq i64 %2054, 2
  %2056 = zext i1 %2055 to i8
  store i8 %2056, i8* %39, align 1
  %2057 = load i64, i64* %RBP.i, align 8
  %2058 = add i64 %2057, -12
  %2059 = add i64 %1994, 51
  store i64 %2059, i64* %3, align 8
  %2060 = inttoptr i64 %2058 to i32*
  %2061 = load i32, i32* %2060, align 4
  %2062 = sext i32 %2061 to i64
  %2063 = shl nsw i64 %2062, 4
  store i64 %2063, i64* %RCX.i11580, align 8
  %2064 = add i64 %2063, %2034
  store i64 %2064, i64* %RAX.i11582.pre-phi, align 8
  %2065 = icmp ult i64 %2064, %2034
  %2066 = icmp ult i64 %2064, %2063
  %2067 = or i1 %2065, %2066
  %2068 = zext i1 %2067 to i8
  store i8 %2068, i8* %14, align 1
  %2069 = trunc i64 %2064 to i32
  %2070 = and i32 %2069, 255
  %2071 = tail call i32 @llvm.ctpop.i32(i32 %2070)
  %2072 = trunc i32 %2071 to i8
  %2073 = and i8 %2072, 1
  %2074 = xor i8 %2073, 1
  store i8 %2074, i8* %21, align 1
  %2075 = xor i64 %2063, %2034
  %2076 = xor i64 %2075, %2064
  %2077 = lshr i64 %2076, 4
  %2078 = trunc i64 %2077 to i8
  %2079 = and i8 %2078, 1
  store i8 %2079, i8* %27, align 1
  %2080 = icmp eq i64 %2064, 0
  %2081 = zext i1 %2080 to i8
  store i8 %2081, i8* %30, align 1
  %2082 = lshr i64 %2064, 63
  %2083 = trunc i64 %2082 to i8
  store i8 %2083, i8* %33, align 1
  %2084 = lshr i64 %2062, 59
  %2085 = and i64 %2084, 1
  %2086 = xor i64 %2082, %2050
  %2087 = xor i64 %2082, %2085
  %2088 = add nuw nsw i64 %2086, %2087
  %2089 = icmp eq i64 %2088, 2
  %2090 = zext i1 %2089 to i8
  store i8 %2090, i8* %39, align 1
  %2091 = inttoptr i64 %2064 to i32*
  %2092 = load i32, i32* %575, align 4
  %2093 = add i64 %1994, 60
  store i64 %2093, i64* %3, align 8
  store i32 %2092, i32* %2091, align 4
  %2094 = load i64, i64* %RBP.i, align 8
  %2095 = add i64 %2094, -8
  %2096 = load i64, i64* %3, align 8
  %2097 = add i64 %2096, 4
  store i64 %2097, i64* %3, align 8
  %2098 = inttoptr i64 %2095 to i64*
  %2099 = load i64, i64* %2098, align 8
  store i64 %2099, i64* %RAX.i11582.pre-phi, align 8
  %2100 = add i64 %2094, -12
  %2101 = add i64 %2096, 8
  store i64 %2101, i64* %3, align 8
  %2102 = inttoptr i64 %2100 to i32*
  %2103 = load i32, i32* %2102, align 4
  %2104 = sext i32 %2103 to i64
  store i64 %2104, i64* %RCX.i11580, align 8
  %2105 = add nsw i64 %2104, 38482
  %2106 = add i64 %2105, %2099
  %2107 = add i64 %2096, 16
  store i64 %2107, i64* %3, align 8
  %2108 = inttoptr i64 %2106 to i8*
  %2109 = load i8, i8* %2108, align 1
  %2110 = zext i8 %2109 to i64
  %2111 = shl nuw nsw i64 %2110, 16
  store i64 %2111, i64* %576, align 8
  store i8 0, i8* %14, align 1
  store i8 1, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %2112 = icmp eq i8 %2109, 0
  %2113 = zext i1 %2112 to i8
  store i8 %2113, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %2114 = add i64 %2096, 23
  store i64 %2114, i64* %3, align 8
  %2115 = load i64, i64* %2098, align 8
  store i64 %2115, i64* %RAX.i11582.pre-phi, align 8
  %2116 = add i64 %2096, 27
  store i64 %2116, i64* %3, align 8
  %2117 = load i32, i32* %2102, align 4
  %2118 = sext i32 %2117 to i64
  store i64 %2118, i64* %RCX.i11580, align 8
  %2119 = add nsw i64 %2118, 38224
  %2120 = add i64 %2119, %2115
  %2121 = add i64 %2096, 35
  store i64 %2121, i64* %3, align 8
  %2122 = inttoptr i64 %2120 to i8*
  %2123 = load i8, i8* %2122, align 1
  %2124 = zext i8 %2123 to i64
  store i64 %2124, i64* %RSI.i11312, align 8
  %2125 = zext i8 %2123 to i64
  %2126 = or i64 %2125, %2111
  %2127 = trunc i64 %2126 to i32
  store i64 %2126, i64* %576, align 8
  store i8 0, i8* %14, align 1
  %2128 = and i32 %2127, 255
  %2129 = tail call i32 @llvm.ctpop.i32(i32 %2128)
  %2130 = trunc i32 %2129 to i8
  %2131 = and i8 %2130, 1
  %2132 = xor i8 %2131, 1
  store i8 %2132, i8* %21, align 1
  %2133 = icmp eq i32 %2127, 0
  %2134 = zext i1 %2133 to i8
  store i8 %2134, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %2135 = add i64 %2096, 41
  store i64 %2135, i64* %3, align 8
  %2136 = load i64, i64* %2098, align 8
  %2137 = add i64 %2136, 51640
  store i64 %2137, i64* %RAX.i11582.pre-phi, align 8
  %2138 = icmp ugt i64 %2136, -51641
  %2139 = zext i1 %2138 to i8
  store i8 %2139, i8* %14, align 1
  %2140 = trunc i64 %2137 to i32
  %2141 = and i32 %2140, 255
  %2142 = tail call i32 @llvm.ctpop.i32(i32 %2141)
  %2143 = trunc i32 %2142 to i8
  %2144 = and i8 %2143, 1
  %2145 = xor i8 %2144, 1
  store i8 %2145, i8* %21, align 1
  %2146 = xor i64 %2136, 16
  %2147 = xor i64 %2146, %2137
  %2148 = lshr i64 %2147, 4
  %2149 = trunc i64 %2148 to i8
  %2150 = and i8 %2149, 1
  store i8 %2150, i8* %27, align 1
  %2151 = icmp eq i64 %2137, 0
  %2152 = zext i1 %2151 to i8
  store i8 %2152, i8* %30, align 1
  %2153 = lshr i64 %2137, 63
  %2154 = trunc i64 %2153 to i8
  store i8 %2154, i8* %33, align 1
  %2155 = lshr i64 %2136, 63
  %2156 = xor i64 %2153, %2155
  %2157 = add nuw nsw i64 %2156, %2153
  %2158 = icmp eq i64 %2157, 2
  %2159 = zext i1 %2158 to i8
  store i8 %2159, i8* %39, align 1
  %2160 = load i64, i64* %RBP.i, align 8
  %2161 = add i64 %2160, -12
  %2162 = add i64 %2096, 51
  store i64 %2162, i64* %3, align 8
  %2163 = inttoptr i64 %2161 to i32*
  %2164 = load i32, i32* %2163, align 4
  %2165 = sext i32 %2164 to i64
  %2166 = shl nsw i64 %2165, 4
  store i64 %2166, i64* %RCX.i11580, align 8
  %2167 = add i64 %2166, %2137
  store i64 %2167, i64* %RAX.i11582.pre-phi, align 8
  %2168 = icmp ult i64 %2167, %2137
  %2169 = icmp ult i64 %2167, %2166
  %2170 = or i1 %2168, %2169
  %2171 = zext i1 %2170 to i8
  store i8 %2171, i8* %14, align 1
  %2172 = trunc i64 %2167 to i32
  %2173 = and i32 %2172, 255
  %2174 = tail call i32 @llvm.ctpop.i32(i32 %2173)
  %2175 = trunc i32 %2174 to i8
  %2176 = and i8 %2175, 1
  %2177 = xor i8 %2176, 1
  store i8 %2177, i8* %21, align 1
  %2178 = xor i64 %2166, %2137
  %2179 = xor i64 %2178, %2167
  %2180 = lshr i64 %2179, 4
  %2181 = trunc i64 %2180 to i8
  %2182 = and i8 %2181, 1
  store i8 %2182, i8* %27, align 1
  %2183 = icmp eq i64 %2167, 0
  %2184 = zext i1 %2183 to i8
  store i8 %2184, i8* %30, align 1
  %2185 = lshr i64 %2167, 63
  %2186 = trunc i64 %2185 to i8
  store i8 %2186, i8* %33, align 1
  %2187 = lshr i64 %2165, 59
  %2188 = and i64 %2187, 1
  %2189 = xor i64 %2185, %2153
  %2190 = xor i64 %2185, %2188
  %2191 = add nuw nsw i64 %2189, %2190
  %2192 = icmp eq i64 %2191, 2
  %2193 = zext i1 %2192 to i8
  store i8 %2193, i8* %39, align 1
  %2194 = add i64 %2167, 4
  %2195 = load i32, i32* %575, align 4
  %2196 = add i64 %2096, 61
  store i64 %2196, i64* %3, align 8
  %2197 = inttoptr i64 %2194 to i32*
  store i32 %2195, i32* %2197, align 4
  %2198 = load i64, i64* %RBP.i, align 8
  %2199 = add i64 %2198, -8
  %2200 = load i64, i64* %3, align 8
  %2201 = add i64 %2200, 4
  store i64 %2201, i64* %3, align 8
  %2202 = inttoptr i64 %2199 to i64*
  %2203 = load i64, i64* %2202, align 8
  store i64 %2203, i64* %RAX.i11582.pre-phi, align 8
  %2204 = add i64 %2198, -12
  %2205 = add i64 %2200, 8
  store i64 %2205, i64* %3, align 8
  %2206 = inttoptr i64 %2204 to i32*
  %2207 = load i32, i32* %2206, align 4
  %2208 = sext i32 %2207 to i64
  store i64 %2208, i64* %RCX.i11580, align 8
  %2209 = add nsw i64 %2208, 38998
  %2210 = add i64 %2209, %2203
  %2211 = add i64 %2200, 16
  store i64 %2211, i64* %3, align 8
  %2212 = inttoptr i64 %2210 to i8*
  %2213 = load i8, i8* %2212, align 1
  %2214 = zext i8 %2213 to i64
  %2215 = shl nuw nsw i64 %2214, 16
  store i64 %2215, i64* %576, align 8
  store i8 0, i8* %14, align 1
  store i8 1, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %2216 = icmp eq i8 %2213, 0
  %2217 = zext i1 %2216 to i8
  store i8 %2217, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %2218 = add i64 %2200, 23
  store i64 %2218, i64* %3, align 8
  %2219 = load i64, i64* %2202, align 8
  store i64 %2219, i64* %RAX.i11582.pre-phi, align 8
  %2220 = add i64 %2200, 27
  store i64 %2220, i64* %3, align 8
  %2221 = load i32, i32* %2206, align 4
  %2222 = sext i32 %2221 to i64
  store i64 %2222, i64* %RCX.i11580, align 8
  %2223 = add nsw i64 %2222, 38740
  %2224 = add i64 %2223, %2219
  %2225 = add i64 %2200, 35
  store i64 %2225, i64* %3, align 8
  %2226 = inttoptr i64 %2224 to i8*
  %2227 = load i8, i8* %2226, align 1
  %2228 = zext i8 %2227 to i64
  store i64 %2228, i64* %RSI.i11312, align 8
  %2229 = zext i8 %2227 to i64
  %2230 = or i64 %2229, %2215
  %2231 = trunc i64 %2230 to i32
  store i64 %2230, i64* %576, align 8
  store i8 0, i8* %14, align 1
  %2232 = and i32 %2231, 255
  %2233 = tail call i32 @llvm.ctpop.i32(i32 %2232)
  %2234 = trunc i32 %2233 to i8
  %2235 = and i8 %2234, 1
  %2236 = xor i8 %2235, 1
  store i8 %2236, i8* %21, align 1
  %2237 = icmp eq i32 %2231, 0
  %2238 = zext i1 %2237 to i8
  store i8 %2238, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %2239 = add i64 %2200, 41
  store i64 %2239, i64* %3, align 8
  %2240 = load i64, i64* %2202, align 8
  %2241 = add i64 %2240, 51640
  store i64 %2241, i64* %RAX.i11582.pre-phi, align 8
  %2242 = icmp ugt i64 %2240, -51641
  %2243 = zext i1 %2242 to i8
  store i8 %2243, i8* %14, align 1
  %2244 = trunc i64 %2241 to i32
  %2245 = and i32 %2244, 255
  %2246 = tail call i32 @llvm.ctpop.i32(i32 %2245)
  %2247 = trunc i32 %2246 to i8
  %2248 = and i8 %2247, 1
  %2249 = xor i8 %2248, 1
  store i8 %2249, i8* %21, align 1
  %2250 = xor i64 %2240, 16
  %2251 = xor i64 %2250, %2241
  %2252 = lshr i64 %2251, 4
  %2253 = trunc i64 %2252 to i8
  %2254 = and i8 %2253, 1
  store i8 %2254, i8* %27, align 1
  %2255 = icmp eq i64 %2241, 0
  %2256 = zext i1 %2255 to i8
  store i8 %2256, i8* %30, align 1
  %2257 = lshr i64 %2241, 63
  %2258 = trunc i64 %2257 to i8
  store i8 %2258, i8* %33, align 1
  %2259 = lshr i64 %2240, 63
  %2260 = xor i64 %2257, %2259
  %2261 = add nuw nsw i64 %2260, %2257
  %2262 = icmp eq i64 %2261, 2
  %2263 = zext i1 %2262 to i8
  store i8 %2263, i8* %39, align 1
  %2264 = load i64, i64* %RBP.i, align 8
  %2265 = add i64 %2264, -12
  %2266 = add i64 %2200, 51
  store i64 %2266, i64* %3, align 8
  %2267 = inttoptr i64 %2265 to i32*
  %2268 = load i32, i32* %2267, align 4
  %2269 = sext i32 %2268 to i64
  %2270 = shl nsw i64 %2269, 4
  store i64 %2270, i64* %RCX.i11580, align 8
  %2271 = add i64 %2270, %2241
  store i64 %2271, i64* %RAX.i11582.pre-phi, align 8
  %2272 = icmp ult i64 %2271, %2241
  %2273 = icmp ult i64 %2271, %2270
  %2274 = or i1 %2272, %2273
  %2275 = zext i1 %2274 to i8
  store i8 %2275, i8* %14, align 1
  %2276 = trunc i64 %2271 to i32
  %2277 = and i32 %2276, 255
  %2278 = tail call i32 @llvm.ctpop.i32(i32 %2277)
  %2279 = trunc i32 %2278 to i8
  %2280 = and i8 %2279, 1
  %2281 = xor i8 %2280, 1
  store i8 %2281, i8* %21, align 1
  %2282 = xor i64 %2270, %2241
  %2283 = xor i64 %2282, %2271
  %2284 = lshr i64 %2283, 4
  %2285 = trunc i64 %2284 to i8
  %2286 = and i8 %2285, 1
  store i8 %2286, i8* %27, align 1
  %2287 = icmp eq i64 %2271, 0
  %2288 = zext i1 %2287 to i8
  store i8 %2288, i8* %30, align 1
  %2289 = lshr i64 %2271, 63
  %2290 = trunc i64 %2289 to i8
  store i8 %2290, i8* %33, align 1
  %2291 = lshr i64 %2269, 59
  %2292 = and i64 %2291, 1
  %2293 = xor i64 %2289, %2257
  %2294 = xor i64 %2289, %2292
  %2295 = add nuw nsw i64 %2293, %2294
  %2296 = icmp eq i64 %2295, 2
  %2297 = zext i1 %2296 to i8
  store i8 %2297, i8* %39, align 1
  %2298 = add i64 %2271, 8
  %2299 = load i32, i32* %575, align 4
  %2300 = add i64 %2200, 61
  store i64 %2300, i64* %3, align 8
  %2301 = inttoptr i64 %2298 to i32*
  store i32 %2299, i32* %2301, align 4
  %2302 = load i64, i64* %RBP.i, align 8
  %2303 = add i64 %2302, -12
  %2304 = load i64, i64* %3, align 8
  %2305 = add i64 %2304, 3
  store i64 %2305, i64* %3, align 8
  %2306 = inttoptr i64 %2303 to i32*
  %2307 = load i32, i32* %2306, align 4
  %2308 = add i32 %2307, 1
  %2309 = zext i32 %2308 to i64
  store i64 %2309, i64* %RAX.i11582.pre-phi, align 8
  %2310 = icmp eq i32 %2307, -1
  %2311 = icmp eq i32 %2308, 0
  %2312 = or i1 %2310, %2311
  %2313 = zext i1 %2312 to i8
  store i8 %2313, i8* %14, align 1
  %2314 = and i32 %2308, 255
  %2315 = tail call i32 @llvm.ctpop.i32(i32 %2314)
  %2316 = trunc i32 %2315 to i8
  %2317 = and i8 %2316, 1
  %2318 = xor i8 %2317, 1
  store i8 %2318, i8* %21, align 1
  %2319 = xor i32 %2308, %2307
  %2320 = lshr i32 %2319, 4
  %2321 = trunc i32 %2320 to i8
  %2322 = and i8 %2321, 1
  store i8 %2322, i8* %27, align 1
  %2323 = zext i1 %2311 to i8
  store i8 %2323, i8* %30, align 1
  %2324 = lshr i32 %2308, 31
  %2325 = trunc i32 %2324 to i8
  store i8 %2325, i8* %33, align 1
  %2326 = lshr i32 %2307, 31
  %2327 = xor i32 %2324, %2326
  %2328 = add nuw nsw i32 %2327, %2324
  %2329 = icmp eq i32 %2328, 2
  %2330 = zext i1 %2329 to i8
  store i8 %2330, i8* %39, align 1
  %2331 = add i64 %2304, 9
  store i64 %2331, i64* %3, align 8
  store i32 %2308, i32* %2306, align 4
  %2332 = load i64, i64* %3, align 8
  %2333 = add i64 %2332, -203
  store i64 %2333, i64* %3, align 8
  br label %block_.L_40a6cc

block_.L_40a79c:                                  ; preds = %block_.L_40a6cc
  %2334 = add i64 %1994, 5
  store i64 %2334, i64* %3, align 8
  br label %block_.L_40a7a1

block_.L_40a7a1:                                  ; preds = %block_.L_40a6bb, %block_.L_40a79c
  %2335 = phi i64 [ %2334, %block_.L_40a79c ], [ %1953, %block_.L_40a6bb ]
  %2336 = phi i64 [ %1958, %block_.L_40a79c ], [ %1722, %block_.L_40a6bb ]
  %2337 = add i64 %2336, -52
  %2338 = add i64 %2335, 7
  store i64 %2338, i64* %3, align 8
  %2339 = inttoptr i64 %2337 to i32*
  store i32 0, i32* %2339, align 4
  %2340 = load i64, i64* %RBP.i, align 8
  %2341 = add i64 %2340, -36
  %2342 = load i64, i64* %3, align 8
  %2343 = add i64 %2342, 7
  store i64 %2343, i64* %3, align 8
  %2344 = inttoptr i64 %2341 to i32*
  store i32 0, i32* %2344, align 4
  %2345 = load i64, i64* %RBP.i, align 8
  %2346 = add i64 %2345, -28
  %2347 = load i64, i64* %3, align 8
  %2348 = add i64 %2347, 7
  store i64 %2348, i64* %3, align 8
  %2349 = inttoptr i64 %2346 to i32*
  store i32 0, i32* %2349, align 4
  %.pre258 = load i64, i64* %3, align 8
  br label %block_.L_40a7b6

block_.L_40a7b6:                                  ; preds = %block_.L_40d11d, %block_.L_40a7a1
  %2350 = phi i64 [ %26881, %block_.L_40d11d ], [ %.pre258, %block_.L_40a7a1 ]
  %2351 = load i64, i64* %RBP.i, align 8
  %2352 = add i64 %2351, -28
  %2353 = add i64 %2350, 3
  store i64 %2353, i64* %3, align 8
  %2354 = inttoptr i64 %2352 to i32*
  %2355 = load i32, i32* %2354, align 4
  %2356 = zext i32 %2355 to i64
  store i64 %2356, i64* %RAX.i11582.pre-phi, align 8
  %2357 = add i64 %2351, -8
  %2358 = add i64 %2350, 7
  store i64 %2358, i64* %3, align 8
  %2359 = inttoptr i64 %2357 to i64*
  %2360 = load i64, i64* %2359, align 8
  store i64 %2360, i64* %RCX.i11580, align 8
  %2361 = add i64 %2360, 668
  %2362 = add i64 %2350, 13
  store i64 %2362, i64* %3, align 8
  %2363 = inttoptr i64 %2361 to i32*
  %2364 = load i32, i32* %2363, align 4
  %2365 = sub i32 %2355, %2364
  %2366 = icmp ult i32 %2355, %2364
  %2367 = zext i1 %2366 to i8
  store i8 %2367, i8* %14, align 1
  %2368 = and i32 %2365, 255
  %2369 = tail call i32 @llvm.ctpop.i32(i32 %2368)
  %2370 = trunc i32 %2369 to i8
  %2371 = and i8 %2370, 1
  %2372 = xor i8 %2371, 1
  store i8 %2372, i8* %21, align 1
  %2373 = xor i32 %2364, %2355
  %2374 = xor i32 %2373, %2365
  %2375 = lshr i32 %2374, 4
  %2376 = trunc i32 %2375 to i8
  %2377 = and i8 %2376, 1
  store i8 %2377, i8* %27, align 1
  %2378 = icmp eq i32 %2365, 0
  %2379 = zext i1 %2378 to i8
  store i8 %2379, i8* %30, align 1
  %2380 = lshr i32 %2365, 31
  %2381 = trunc i32 %2380 to i8
  store i8 %2381, i8* %33, align 1
  %2382 = lshr i32 %2355, 31
  %2383 = lshr i32 %2364, 31
  %2384 = xor i32 %2383, %2382
  %2385 = xor i32 %2380, %2382
  %2386 = add nuw nsw i32 %2385, %2384
  %2387 = icmp eq i32 %2386, 2
  %2388 = zext i1 %2387 to i8
  store i8 %2388, i8* %39, align 1
  %2389 = icmp ne i8 %2381, 0
  %2390 = xor i1 %2389, %2387
  %.v290 = select i1 %2390, i64 24, i64 19
  %2391 = add i64 %2350, %.v290
  store i64 %2391, i64* %3, align 8
  br i1 %2390, label %block_.L_40a7ce, label %block_40a7c9

block_40a7c9:                                     ; preds = %block_.L_40a7b6
  %2392 = add i64 %2391, 10598
  store i64 %2392, i64* %3, align 8
  %2393 = load i64, i64* %2359, align 8
  store i64 %2393, i64* %RAX.i11582.pre-phi, align 8
  %2394 = add i64 %2393, 656
  %2395 = add i64 %2391, 10605
  store i64 %2395, i64* %3, align 8
  %2396 = inttoptr i64 %2394 to i32*
  %2397 = load i32, i32* %2396, align 4
  %2398 = add i32 %2397, -3
  %2399 = icmp ult i32 %2397, 3
  %2400 = zext i1 %2399 to i8
  store i8 %2400, i8* %14, align 1
  %2401 = and i32 %2398, 255
  %2402 = tail call i32 @llvm.ctpop.i32(i32 %2401)
  %2403 = trunc i32 %2402 to i8
  %2404 = and i8 %2403, 1
  %2405 = xor i8 %2404, 1
  store i8 %2405, i8* %21, align 1
  %2406 = xor i32 %2398, %2397
  %2407 = lshr i32 %2406, 4
  %2408 = trunc i32 %2407 to i8
  %2409 = and i8 %2408, 1
  store i8 %2409, i8* %27, align 1
  %2410 = icmp eq i32 %2398, 0
  %2411 = zext i1 %2410 to i8
  store i8 %2411, i8* %30, align 1
  %2412 = lshr i32 %2398, 31
  %2413 = trunc i32 %2412 to i8
  store i8 %2413, i8* %33, align 1
  %2414 = lshr i32 %2397, 31
  %2415 = xor i32 %2412, %2414
  %2416 = add nuw nsw i32 %2415, %2414
  %2417 = icmp eq i32 %2416, 2
  %2418 = zext i1 %2417 to i8
  store i8 %2418, i8* %39, align 1
  %2419 = icmp ne i8 %2413, 0
  %2420 = xor i1 %2419, %2417
  %.v348 = select i1 %2420, i64 10782, i64 10611
  %2421 = add i64 %2391, %.v348
  store i64 %2421, i64* %3, align 8
  br i1 %2420, label %block_.L_40d1e7, label %block_40d13c

block_.L_40a7ce:                                  ; preds = %block_.L_40a7b6
  %2422 = add i64 %2391, 3
  store i64 %2422, i64* %3, align 8
  %2423 = load i32, i32* %2354, align 4
  %2424 = add i32 %2423, 50
  %2425 = icmp eq i32 %2424, 0
  %2426 = zext i1 %2425 to i8
  %2427 = lshr i32 %2424, 31
  %2428 = add i32 %2423, 49
  %2429 = zext i32 %2428 to i64
  store i64 %2429, i64* %RAX.i11582.pre-phi, align 8
  store i8 %2426, i8* %14, align 1
  %2430 = and i32 %2428, 255
  %2431 = tail call i32 @llvm.ctpop.i32(i32 %2430)
  %2432 = trunc i32 %2431 to i8
  %2433 = and i8 %2432, 1
  %2434 = xor i8 %2433, 1
  store i8 %2434, i8* %21, align 1
  %2435 = xor i32 %2428, %2424
  %2436 = lshr i32 %2435, 4
  %2437 = trunc i32 %2436 to i8
  %2438 = and i8 %2437, 1
  store i8 %2438, i8* %27, align 1
  %2439 = icmp eq i32 %2428, 0
  %2440 = zext i1 %2439 to i8
  store i8 %2440, i8* %30, align 1
  %2441 = lshr i32 %2428, 31
  %2442 = trunc i32 %2441 to i8
  store i8 %2442, i8* %33, align 1
  %2443 = xor i32 %2441, %2427
  %2444 = add nuw nsw i32 %2443, %2427
  %2445 = icmp eq i32 %2444, 2
  %2446 = zext i1 %2445 to i8
  store i8 %2446, i8* %39, align 1
  %2447 = add i64 %2351, -32
  %2448 = add i64 %2391, 12
  store i64 %2448, i64* %3, align 8
  %2449 = inttoptr i64 %2447 to i32*
  store i32 %2428, i32* %2449, align 4
  %2450 = load i64, i64* %RBP.i, align 8
  %2451 = add i64 %2450, -32
  %2452 = load i64, i64* %3, align 8
  %2453 = add i64 %2452, 3
  store i64 %2453, i64* %3, align 8
  %2454 = inttoptr i64 %2451 to i32*
  %2455 = load i32, i32* %2454, align 4
  %2456 = zext i32 %2455 to i64
  store i64 %2456, i64* %RAX.i11582.pre-phi, align 8
  %2457 = add i64 %2450, -8
  %2458 = add i64 %2452, 7
  store i64 %2458, i64* %3, align 8
  %2459 = inttoptr i64 %2457 to i64*
  %2460 = load i64, i64* %2459, align 8
  store i64 %2460, i64* %RCX.i11580, align 8
  %2461 = add i64 %2460, 668
  %2462 = add i64 %2452, 13
  store i64 %2462, i64* %3, align 8
  %2463 = inttoptr i64 %2461 to i32*
  %2464 = load i32, i32* %2463, align 4
  %2465 = sub i32 %2455, %2464
  %2466 = icmp ult i32 %2455, %2464
  %2467 = zext i1 %2466 to i8
  store i8 %2467, i8* %14, align 1
  %2468 = and i32 %2465, 255
  %2469 = tail call i32 @llvm.ctpop.i32(i32 %2468)
  %2470 = trunc i32 %2469 to i8
  %2471 = and i8 %2470, 1
  %2472 = xor i8 %2471, 1
  store i8 %2472, i8* %21, align 1
  %2473 = xor i32 %2464, %2455
  %2474 = xor i32 %2473, %2465
  %2475 = lshr i32 %2474, 4
  %2476 = trunc i32 %2475 to i8
  %2477 = and i8 %2476, 1
  store i8 %2477, i8* %27, align 1
  %2478 = icmp eq i32 %2465, 0
  %2479 = zext i1 %2478 to i8
  store i8 %2479, i8* %30, align 1
  %2480 = lshr i32 %2465, 31
  %2481 = trunc i32 %2480 to i8
  store i8 %2481, i8* %33, align 1
  %2482 = lshr i32 %2455, 31
  %2483 = lshr i32 %2464, 31
  %2484 = xor i32 %2483, %2482
  %2485 = xor i32 %2480, %2482
  %2486 = add nuw nsw i32 %2485, %2484
  %2487 = icmp eq i32 %2486, 2
  %2488 = zext i1 %2487 to i8
  store i8 %2488, i8* %39, align 1
  %2489 = icmp ne i8 %2481, 0
  %2490 = xor i1 %2489, %2487
  %.v351 = select i1 %2490, i64 35, i64 19
  %2491 = add i64 %2452, %.v351
  store i64 %2491, i64* %3, align 8
  br i1 %2490, label %block_.L_40a7fd, label %block_40a7ed

block_40a7ed:                                     ; preds = %block_.L_40a7ce
  %2492 = add i64 %2491, 4
  store i64 %2492, i64* %3, align 8
  %2493 = load i64, i64* %2459, align 8
  store i64 %2493, i64* %RAX.i11582.pre-phi, align 8
  %2494 = add i64 %2493, 668
  %2495 = add i64 %2491, 10
  store i64 %2495, i64* %3, align 8
  %2496 = inttoptr i64 %2494 to i32*
  %2497 = load i32, i32* %2496, align 4
  %2498 = add i32 %2497, -1
  %2499 = zext i32 %2498 to i64
  store i64 %2499, i64* %RCX.i11580, align 8
  %2500 = icmp eq i32 %2497, 0
  %2501 = zext i1 %2500 to i8
  store i8 %2501, i8* %14, align 1
  %2502 = and i32 %2498, 255
  %2503 = tail call i32 @llvm.ctpop.i32(i32 %2502)
  %2504 = trunc i32 %2503 to i8
  %2505 = and i8 %2504, 1
  %2506 = xor i8 %2505, 1
  store i8 %2506, i8* %21, align 1
  %2507 = xor i32 %2498, %2497
  %2508 = lshr i32 %2507, 4
  %2509 = trunc i32 %2508 to i8
  %2510 = and i8 %2509, 1
  store i8 %2510, i8* %27, align 1
  %2511 = icmp eq i32 %2498, 0
  %2512 = zext i1 %2511 to i8
  store i8 %2512, i8* %30, align 1
  %2513 = lshr i32 %2498, 31
  %2514 = trunc i32 %2513 to i8
  store i8 %2514, i8* %33, align 1
  %2515 = lshr i32 %2497, 31
  %2516 = xor i32 %2513, %2515
  %2517 = add nuw nsw i32 %2516, %2515
  %2518 = icmp eq i32 %2517, 2
  %2519 = zext i1 %2518 to i8
  store i8 %2519, i8* %39, align 1
  %2520 = add i64 %2491, 16
  store i64 %2520, i64* %3, align 8
  store i32 %2498, i32* %2454, align 4
  %.pre265 = load i64, i64* %RBP.i, align 8
  %.pre266 = load i64, i64* %3, align 8
  br label %block_.L_40a7fd

block_.L_40a7fd:                                  ; preds = %block_40a7ed, %block_.L_40a7ce
  %2521 = phi i64 [ %.pre266, %block_40a7ed ], [ %2491, %block_.L_40a7ce ]
  %2522 = phi i64 [ %.pre265, %block_40a7ed ], [ %2450, %block_.L_40a7ce ]
  %2523 = add i64 %2522, -16
  %2524 = add i64 %2521, 7
  store i64 %2524, i64* %3, align 8
  %2525 = inttoptr i64 %2523 to i32*
  store i32 0, i32* %2525, align 4
  %.pre267 = load i64, i64* %3, align 8
  br label %block_.L_40a804

block_.L_40a804:                                  ; preds = %block_40a810, %block_.L_40a7fd
  %2526 = phi i64 [ %2603, %block_40a810 ], [ %.pre267, %block_.L_40a7fd ]
  %2527 = load i64, i64* %RBP.i, align 8
  %2528 = add i64 %2527, -16
  %2529 = add i64 %2526, 3
  store i64 %2529, i64* %3, align 8
  %2530 = inttoptr i64 %2528 to i32*
  %2531 = load i32, i32* %2530, align 4
  %2532 = zext i32 %2531 to i64
  store i64 %2532, i64* %RAX.i11582.pre-phi, align 8
  %2533 = add i64 %2527, -72
  %2534 = add i64 %2526, 6
  store i64 %2534, i64* %3, align 8
  %2535 = inttoptr i64 %2533 to i32*
  %2536 = load i32, i32* %2535, align 4
  %2537 = sub i32 %2531, %2536
  %2538 = icmp ult i32 %2531, %2536
  %2539 = zext i1 %2538 to i8
  store i8 %2539, i8* %14, align 1
  %2540 = and i32 %2537, 255
  %2541 = tail call i32 @llvm.ctpop.i32(i32 %2540)
  %2542 = trunc i32 %2541 to i8
  %2543 = and i8 %2542, 1
  %2544 = xor i8 %2543, 1
  store i8 %2544, i8* %21, align 1
  %2545 = xor i32 %2536, %2531
  %2546 = xor i32 %2545, %2537
  %2547 = lshr i32 %2546, 4
  %2548 = trunc i32 %2547 to i8
  %2549 = and i8 %2548, 1
  store i8 %2549, i8* %27, align 1
  %2550 = icmp eq i32 %2537, 0
  %2551 = zext i1 %2550 to i8
  store i8 %2551, i8* %30, align 1
  %2552 = lshr i32 %2537, 31
  %2553 = trunc i32 %2552 to i8
  store i8 %2553, i8* %33, align 1
  %2554 = lshr i32 %2531, 31
  %2555 = lshr i32 %2536, 31
  %2556 = xor i32 %2555, %2554
  %2557 = xor i32 %2552, %2554
  %2558 = add nuw nsw i32 %2557, %2556
  %2559 = icmp eq i32 %2558, 2
  %2560 = zext i1 %2559 to i8
  store i8 %2560, i8* %39, align 1
  %2561 = icmp ne i8 %2553, 0
  %2562 = xor i1 %2561, %2559
  %.v352 = select i1 %2562, i64 12, i64 37
  %2563 = add i64 %2526, %.v352
  %2564 = add i64 %2563, 4
  store i64 %2564, i64* %3, align 8
  br i1 %2562, label %block_40a810, label %block_.L_40a829

block_40a810:                                     ; preds = %block_.L_40a804
  %2565 = load i32, i32* %2530, align 4
  %2566 = sext i32 %2565 to i64
  store i64 %2566, i64* %RAX.i11582.pre-phi, align 8
  %2567 = shl nsw i64 %2566, 1
  %2568 = add i64 %2527, -88
  %2569 = add i64 %2568, %2567
  %2570 = add i64 %2563, 11
  store i64 %2570, i64* %3, align 8
  %2571 = inttoptr i64 %2569 to i16*
  store i16 0, i16* %2571, align 2
  %2572 = load i64, i64* %RBP.i, align 8
  %2573 = add i64 %2572, -16
  %2574 = load i64, i64* %3, align 8
  %2575 = add i64 %2574, 3
  store i64 %2575, i64* %3, align 8
  %2576 = inttoptr i64 %2573 to i32*
  %2577 = load i32, i32* %2576, align 4
  %2578 = add i32 %2577, 1
  %2579 = zext i32 %2578 to i64
  store i64 %2579, i64* %RAX.i11582.pre-phi, align 8
  %2580 = icmp eq i32 %2577, -1
  %2581 = icmp eq i32 %2578, 0
  %2582 = or i1 %2580, %2581
  %2583 = zext i1 %2582 to i8
  store i8 %2583, i8* %14, align 1
  %2584 = and i32 %2578, 255
  %2585 = tail call i32 @llvm.ctpop.i32(i32 %2584)
  %2586 = trunc i32 %2585 to i8
  %2587 = and i8 %2586, 1
  %2588 = xor i8 %2587, 1
  store i8 %2588, i8* %21, align 1
  %2589 = xor i32 %2578, %2577
  %2590 = lshr i32 %2589, 4
  %2591 = trunc i32 %2590 to i8
  %2592 = and i8 %2591, 1
  store i8 %2592, i8* %27, align 1
  %2593 = zext i1 %2581 to i8
  store i8 %2593, i8* %30, align 1
  %2594 = lshr i32 %2578, 31
  %2595 = trunc i32 %2594 to i8
  store i8 %2595, i8* %33, align 1
  %2596 = lshr i32 %2577, 31
  %2597 = xor i32 %2594, %2596
  %2598 = add nuw nsw i32 %2597, %2594
  %2599 = icmp eq i32 %2598, 2
  %2600 = zext i1 %2599 to i8
  store i8 %2600, i8* %39, align 1
  %2601 = add i64 %2574, 9
  store i64 %2601, i64* %3, align 8
  store i32 %2578, i32* %2576, align 4
  %2602 = load i64, i64* %3, align 8
  %2603 = add i64 %2602, -32
  store i64 %2603, i64* %3, align 8
  br label %block_.L_40a804

block_.L_40a829:                                  ; preds = %block_.L_40a804
  %2604 = load i32, i32* %2535, align 4
  %2605 = add i32 %2604, -6
  %2606 = icmp ult i32 %2604, 6
  %2607 = zext i1 %2606 to i8
  store i8 %2607, i8* %14, align 1
  %2608 = and i32 %2605, 255
  %2609 = tail call i32 @llvm.ctpop.i32(i32 %2608)
  %2610 = trunc i32 %2609 to i8
  %2611 = and i8 %2610, 1
  %2612 = xor i8 %2611, 1
  store i8 %2612, i8* %21, align 1
  %2613 = xor i32 %2605, %2604
  %2614 = lshr i32 %2613, 4
  %2615 = trunc i32 %2614 to i8
  %2616 = and i8 %2615, 1
  store i8 %2616, i8* %27, align 1
  %2617 = icmp eq i32 %2605, 0
  %2618 = zext i1 %2617 to i8
  store i8 %2618, i8* %30, align 1
  %2619 = lshr i32 %2605, 31
  %2620 = trunc i32 %2619 to i8
  store i8 %2620, i8* %33, align 1
  %2621 = lshr i32 %2604, 31
  %2622 = xor i32 %2619, %2621
  %2623 = add nuw nsw i32 %2622, %2621
  %2624 = icmp eq i32 %2623, 2
  %2625 = zext i1 %2624 to i8
  store i8 %2625, i8* %39, align 1
  %.v353 = select i1 %2617, i64 10, i64 7472
  %2626 = add i64 %2563, %.v353
  store i64 %2626, i64* %3, align 8
  br i1 %2617, label %block_40a833, label %block_.L_40c559

block_40a833:                                     ; preds = %block_.L_40a829
  store i64 50, i64* %RAX.i11582.pre-phi, align 8
  %2627 = add i64 %2527, -32
  %2628 = add i64 %2626, 8
  store i64 %2628, i64* %3, align 8
  %2629 = inttoptr i64 %2627 to i32*
  %2630 = load i32, i32* %2629, align 4
  %2631 = zext i32 %2630 to i64
  store i64 %2631, i64* %RCX.i11580, align 8
  %2632 = add i64 %2527, -28
  %2633 = add i64 %2626, 11
  store i64 %2633, i64* %3, align 8
  %2634 = inttoptr i64 %2632 to i32*
  %2635 = load i32, i32* %2634, align 4
  %2636 = sub i32 %2630, %2635
  %2637 = add i32 %2636, 1
  %2638 = zext i32 %2637 to i64
  store i64 %2638, i64* %RCX.i11580, align 8
  %2639 = lshr i32 %2637, 31
  %2640 = sub i32 49, %2636
  %2641 = icmp ugt i32 %2637, 50
  %2642 = zext i1 %2641 to i8
  store i8 %2642, i8* %14, align 1
  %2643 = and i32 %2640, 255
  %2644 = tail call i32 @llvm.ctpop.i32(i32 %2643)
  %2645 = trunc i32 %2644 to i8
  %2646 = and i8 %2645, 1
  %2647 = xor i8 %2646, 1
  store i8 %2647, i8* %21, align 1
  %2648 = xor i32 %2637, 16
  %2649 = xor i32 %2648, %2640
  %2650 = lshr i32 %2649, 4
  %2651 = trunc i32 %2650 to i8
  %2652 = and i8 %2651, 1
  store i8 %2652, i8* %27, align 1
  %2653 = icmp eq i32 %2640, 0
  %2654 = zext i1 %2653 to i8
  store i8 %2654, i8* %30, align 1
  %2655 = lshr i32 %2640, 31
  %2656 = trunc i32 %2655 to i8
  store i8 %2656, i8* %33, align 1
  %2657 = add nuw nsw i32 %2655, %2639
  %2658 = icmp eq i32 %2657, 2
  %2659 = zext i1 %2658 to i8
  store i8 %2659, i8* %39, align 1
  %.v360 = select i1 %2653, i64 22, i64 7462
  %2660 = add i64 %2626, %.v360
  store i64 %2660, i64* %3, align 8
  %2661 = load i64, i64* %RBP.i, align 8
  br i1 %2653, label %block_40a849, label %block_.L_40c559

block_40a849:                                     ; preds = %block_40a833
  %2662 = add i64 %2661, -148
  %2663 = add i64 %2660, 10
  store i64 %2663, i64* %3, align 8
  %2664 = inttoptr i64 %2662 to i32*
  store i32 0, i32* %2664, align 4
  %2665 = load i64, i64* %RBP.i, align 8
  %2666 = add i64 %2665, -144
  %2667 = load i64, i64* %3, align 8
  %2668 = add i64 %2667, 10
  store i64 %2668, i64* %3, align 8
  %2669 = inttoptr i64 %2666 to i32*
  store i32 0, i32* %2669, align 4
  %2670 = load i64, i64* %RBP.i, align 8
  %2671 = add i64 %2670, -140
  %2672 = load i64, i64* %3, align 8
  %2673 = add i64 %2672, 10
  store i64 %2673, i64* %3, align 8
  %2674 = inttoptr i64 %2671 to i32*
  store i32 0, i32* %2674, align 4
  %2675 = load i64, i64* %RBP.i, align 8
  %2676 = add i64 %2675, -120
  %2677 = load i64, i64* %3, align 8
  %2678 = add i64 %2677, 4
  store i64 %2678, i64* %3, align 8
  %2679 = inttoptr i64 %2676 to i64*
  %2680 = load i64, i64* %2679, align 8
  store i64 %2680, i64* %RAX.i11582.pre-phi, align 8
  %2681 = add i64 %2675, -28
  %2682 = add i64 %2677, 7
  store i64 %2682, i64* %3, align 8
  %2683 = inttoptr i64 %2681 to i32*
  %2684 = load i32, i32* %2683, align 4
  %2685 = zext i32 %2684 to i64
  store i64 %2685, i64* %RCX.i11580, align 8
  store i8 0, i8* %14, align 1
  %2686 = and i32 %2684, 255
  %2687 = tail call i32 @llvm.ctpop.i32(i32 %2686)
  %2688 = trunc i32 %2687 to i8
  %2689 = and i8 %2688, 1
  %2690 = xor i8 %2689, 1
  store i8 %2690, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %2691 = icmp eq i32 %2684, 0
  %2692 = zext i1 %2691 to i8
  store i8 %2692, i8* %30, align 1
  %2693 = lshr i32 %2684, 31
  %2694 = trunc i32 %2693 to i8
  store i8 %2694, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %2695 = sext i32 %2684 to i64
  store i64 %2695, i64* %573, align 8
  %2696 = shl nsw i64 %2695, 1
  %2697 = add i64 %2680, %2696
  %2698 = add i64 %2677, 17
  store i64 %2698, i64* %3, align 8
  %2699 = inttoptr i64 %2697 to i16*
  %2700 = load i16, i16* %2699, align 2
  store i16 %2700, i16* %SI.i, align 2
  %2701 = add i64 %2675, -150
  %2702 = add i64 %2677, 24
  store i64 %2702, i64* %3, align 8
  %2703 = inttoptr i64 %2701 to i16*
  store i16 %2700, i16* %2703, align 2
  %2704 = load i64, i64* %RBP.i, align 8
  %2705 = add i64 %2704, -8
  %2706 = load i64, i64* %3, align 8
  %2707 = add i64 %2706, 4
  store i64 %2707, i64* %3, align 8
  %2708 = inttoptr i64 %2705 to i64*
  %2709 = load i64, i64* %2708, align 8
  %2710 = add i64 %2709, 51640
  store i64 %2710, i64* %RAX.i11582.pre-phi, align 8
  %2711 = icmp ugt i64 %2709, -51641
  %2712 = zext i1 %2711 to i8
  store i8 %2712, i8* %14, align 1
  %2713 = trunc i64 %2710 to i32
  %2714 = and i32 %2713, 255
  %2715 = tail call i32 @llvm.ctpop.i32(i32 %2714)
  %2716 = trunc i32 %2715 to i8
  %2717 = and i8 %2716, 1
  %2718 = xor i8 %2717, 1
  store i8 %2718, i8* %21, align 1
  %2719 = xor i64 %2709, 16
  %2720 = xor i64 %2719, %2710
  %2721 = lshr i64 %2720, 4
  %2722 = trunc i64 %2721 to i8
  %2723 = and i8 %2722, 1
  store i8 %2723, i8* %27, align 1
  %2724 = icmp eq i64 %2710, 0
  %2725 = zext i1 %2724 to i8
  store i8 %2725, i8* %30, align 1
  %2726 = lshr i64 %2710, 63
  %2727 = trunc i64 %2726 to i8
  store i8 %2727, i8* %33, align 1
  %2728 = lshr i64 %2709, 63
  %2729 = xor i64 %2726, %2728
  %2730 = add nuw nsw i64 %2729, %2726
  %2731 = icmp eq i64 %2730, 2
  %2732 = zext i1 %2731 to i8
  store i8 %2732, i8* %39, align 1
  %2733 = add i64 %2704, -150
  %2734 = add i64 %2706, 17
  store i64 %2734, i64* %3, align 8
  %2735 = inttoptr i64 %2733 to i16*
  %2736 = load i16, i16* %2735, align 2
  %2737 = zext i16 %2736 to i64
  store i64 %2737, i64* %RCX.i11580, align 8
  %2738 = zext i16 %2736 to i64
  %2739 = shl nuw nsw i64 %2738, 4
  store i64 %2739, i64* %573, align 8
  %2740 = add i64 %2739, %2710
  store i64 %2740, i64* %RAX.i11582.pre-phi, align 8
  %2741 = icmp ult i64 %2740, %2710
  %2742 = icmp ult i64 %2740, %2739
  %2743 = or i1 %2741, %2742
  %2744 = zext i1 %2743 to i8
  store i8 %2744, i8* %14, align 1
  %2745 = trunc i64 %2740 to i32
  %2746 = and i32 %2745, 255
  %2747 = tail call i32 @llvm.ctpop.i32(i32 %2746)
  %2748 = trunc i32 %2747 to i8
  %2749 = and i8 %2748, 1
  %2750 = xor i8 %2749, 1
  store i8 %2750, i8* %21, align 1
  %2751 = xor i64 %2739, %2710
  %2752 = xor i64 %2751, %2740
  %2753 = lshr i64 %2752, 4
  %2754 = trunc i64 %2753 to i8
  %2755 = and i8 %2754, 1
  store i8 %2755, i8* %27, align 1
  %2756 = icmp eq i64 %2740, 0
  %2757 = zext i1 %2756 to i8
  store i8 %2757, i8* %30, align 1
  %2758 = lshr i64 %2740, 63
  %2759 = trunc i64 %2758 to i8
  store i8 %2759, i8* %33, align 1
  %2760 = xor i64 %2758, %2726
  %2761 = add nuw nsw i64 %2760, %2758
  %2762 = icmp eq i64 %2761, 2
  %2763 = zext i1 %2762 to i8
  store i8 %2763, i8* %39, align 1
  %2764 = inttoptr i64 %2740 to i32*
  %2765 = add i64 %2706, 28
  store i64 %2765, i64* %3, align 8
  %2766 = load i32, i32* %2764, align 4
  %2767 = zext i32 %2766 to i64
  store i64 %2767, i64* %RCX.i11580, align 8
  %2768 = load i64, i64* %RBP.i, align 8
  %2769 = add i64 %2768, -140
  %2770 = add i64 %2706, 34
  store i64 %2770, i64* %3, align 8
  %2771 = inttoptr i64 %2769 to i32*
  %2772 = load i32, i32* %2771, align 4
  %2773 = add i32 %2772, %2766
  %2774 = zext i32 %2773 to i64
  store i64 %2774, i64* %RCX.i11580, align 8
  %2775 = icmp ult i32 %2773, %2766
  %2776 = icmp ult i32 %2773, %2772
  %2777 = or i1 %2775, %2776
  %2778 = zext i1 %2777 to i8
  store i8 %2778, i8* %14, align 1
  %2779 = and i32 %2773, 255
  %2780 = tail call i32 @llvm.ctpop.i32(i32 %2779)
  %2781 = trunc i32 %2780 to i8
  %2782 = and i8 %2781, 1
  %2783 = xor i8 %2782, 1
  store i8 %2783, i8* %21, align 1
  %2784 = xor i32 %2772, %2766
  %2785 = xor i32 %2784, %2773
  %2786 = lshr i32 %2785, 4
  %2787 = trunc i32 %2786 to i8
  %2788 = and i8 %2787, 1
  store i8 %2788, i8* %27, align 1
  %2789 = icmp eq i32 %2773, 0
  %2790 = zext i1 %2789 to i8
  store i8 %2790, i8* %30, align 1
  %2791 = lshr i32 %2773, 31
  %2792 = trunc i32 %2791 to i8
  store i8 %2792, i8* %33, align 1
  %2793 = lshr i32 %2766, 31
  %2794 = lshr i32 %2772, 31
  %2795 = xor i32 %2791, %2793
  %2796 = xor i32 %2791, %2794
  %2797 = add nuw nsw i32 %2795, %2796
  %2798 = icmp eq i32 %2797, 2
  %2799 = zext i1 %2798 to i8
  store i8 %2799, i8* %39, align 1
  %2800 = add i64 %2706, 40
  store i64 %2800, i64* %3, align 8
  store i32 %2773, i32* %2771, align 4
  %2801 = load i64, i64* %RBP.i, align 8
  %2802 = add i64 %2801, -8
  %2803 = load i64, i64* %3, align 8
  %2804 = add i64 %2803, 4
  store i64 %2804, i64* %3, align 8
  %2805 = inttoptr i64 %2802 to i64*
  %2806 = load i64, i64* %2805, align 8
  %2807 = add i64 %2806, 51640
  store i64 %2807, i64* %RAX.i11582.pre-phi, align 8
  %2808 = icmp ugt i64 %2806, -51641
  %2809 = zext i1 %2808 to i8
  store i8 %2809, i8* %14, align 1
  %2810 = trunc i64 %2807 to i32
  %2811 = and i32 %2810, 255
  %2812 = tail call i32 @llvm.ctpop.i32(i32 %2811)
  %2813 = trunc i32 %2812 to i8
  %2814 = and i8 %2813, 1
  %2815 = xor i8 %2814, 1
  store i8 %2815, i8* %21, align 1
  %2816 = xor i64 %2806, 16
  %2817 = xor i64 %2816, %2807
  %2818 = lshr i64 %2817, 4
  %2819 = trunc i64 %2818 to i8
  %2820 = and i8 %2819, 1
  store i8 %2820, i8* %27, align 1
  %2821 = icmp eq i64 %2807, 0
  %2822 = zext i1 %2821 to i8
  store i8 %2822, i8* %30, align 1
  %2823 = lshr i64 %2807, 63
  %2824 = trunc i64 %2823 to i8
  store i8 %2824, i8* %33, align 1
  %2825 = lshr i64 %2806, 63
  %2826 = xor i64 %2823, %2825
  %2827 = add nuw nsw i64 %2826, %2823
  %2828 = icmp eq i64 %2827, 2
  %2829 = zext i1 %2828 to i8
  store i8 %2829, i8* %39, align 1
  %2830 = add i64 %2801, -150
  %2831 = add i64 %2803, 17
  store i64 %2831, i64* %3, align 8
  %2832 = inttoptr i64 %2830 to i16*
  %2833 = load i16, i16* %2832, align 2
  %2834 = zext i16 %2833 to i64
  store i64 %2834, i64* %RCX.i11580, align 8
  %2835 = zext i16 %2833 to i64
  %2836 = shl nuw nsw i64 %2835, 4
  store i64 %2836, i64* %573, align 8
  %2837 = add i64 %2836, %2807
  store i64 %2837, i64* %RAX.i11582.pre-phi, align 8
  %2838 = icmp ult i64 %2837, %2807
  %2839 = icmp ult i64 %2837, %2836
  %2840 = or i1 %2838, %2839
  %2841 = zext i1 %2840 to i8
  store i8 %2841, i8* %14, align 1
  %2842 = trunc i64 %2837 to i32
  %2843 = and i32 %2842, 255
  %2844 = tail call i32 @llvm.ctpop.i32(i32 %2843)
  %2845 = trunc i32 %2844 to i8
  %2846 = and i8 %2845, 1
  %2847 = xor i8 %2846, 1
  store i8 %2847, i8* %21, align 1
  %2848 = xor i64 %2836, %2807
  %2849 = xor i64 %2848, %2837
  %2850 = lshr i64 %2849, 4
  %2851 = trunc i64 %2850 to i8
  %2852 = and i8 %2851, 1
  store i8 %2852, i8* %27, align 1
  %2853 = icmp eq i64 %2837, 0
  %2854 = zext i1 %2853 to i8
  store i8 %2854, i8* %30, align 1
  %2855 = lshr i64 %2837, 63
  %2856 = trunc i64 %2855 to i8
  store i8 %2856, i8* %33, align 1
  %2857 = xor i64 %2855, %2823
  %2858 = add nuw nsw i64 %2857, %2855
  %2859 = icmp eq i64 %2858, 2
  %2860 = zext i1 %2859 to i8
  store i8 %2860, i8* %39, align 1
  %2861 = add i64 %2837, 4
  %2862 = add i64 %2803, 29
  store i64 %2862, i64* %3, align 8
  %2863 = inttoptr i64 %2861 to i32*
  %2864 = load i32, i32* %2863, align 4
  %2865 = zext i32 %2864 to i64
  store i64 %2865, i64* %RCX.i11580, align 8
  %2866 = load i64, i64* %RBP.i, align 8
  %2867 = add i64 %2866, -144
  %2868 = add i64 %2803, 35
  store i64 %2868, i64* %3, align 8
  %2869 = inttoptr i64 %2867 to i32*
  %2870 = load i32, i32* %2869, align 4
  %2871 = add i32 %2870, %2864
  %2872 = zext i32 %2871 to i64
  store i64 %2872, i64* %RCX.i11580, align 8
  %2873 = icmp ult i32 %2871, %2864
  %2874 = icmp ult i32 %2871, %2870
  %2875 = or i1 %2873, %2874
  %2876 = zext i1 %2875 to i8
  store i8 %2876, i8* %14, align 1
  %2877 = and i32 %2871, 255
  %2878 = tail call i32 @llvm.ctpop.i32(i32 %2877)
  %2879 = trunc i32 %2878 to i8
  %2880 = and i8 %2879, 1
  %2881 = xor i8 %2880, 1
  store i8 %2881, i8* %21, align 1
  %2882 = xor i32 %2870, %2864
  %2883 = xor i32 %2882, %2871
  %2884 = lshr i32 %2883, 4
  %2885 = trunc i32 %2884 to i8
  %2886 = and i8 %2885, 1
  store i8 %2886, i8* %27, align 1
  %2887 = icmp eq i32 %2871, 0
  %2888 = zext i1 %2887 to i8
  store i8 %2888, i8* %30, align 1
  %2889 = lshr i32 %2871, 31
  %2890 = trunc i32 %2889 to i8
  store i8 %2890, i8* %33, align 1
  %2891 = lshr i32 %2864, 31
  %2892 = lshr i32 %2870, 31
  %2893 = xor i32 %2889, %2891
  %2894 = xor i32 %2889, %2892
  %2895 = add nuw nsw i32 %2893, %2894
  %2896 = icmp eq i32 %2895, 2
  %2897 = zext i1 %2896 to i8
  store i8 %2897, i8* %39, align 1
  %2898 = add i64 %2803, 41
  store i64 %2898, i64* %3, align 8
  store i32 %2871, i32* %2869, align 4
  %2899 = load i64, i64* %RBP.i, align 8
  %2900 = add i64 %2899, -8
  %2901 = load i64, i64* %3, align 8
  %2902 = add i64 %2901, 4
  store i64 %2902, i64* %3, align 8
  %2903 = inttoptr i64 %2900 to i64*
  %2904 = load i64, i64* %2903, align 8
  %2905 = add i64 %2904, 51640
  store i64 %2905, i64* %RAX.i11582.pre-phi, align 8
  %2906 = icmp ugt i64 %2904, -51641
  %2907 = zext i1 %2906 to i8
  store i8 %2907, i8* %14, align 1
  %2908 = trunc i64 %2905 to i32
  %2909 = and i32 %2908, 255
  %2910 = tail call i32 @llvm.ctpop.i32(i32 %2909)
  %2911 = trunc i32 %2910 to i8
  %2912 = and i8 %2911, 1
  %2913 = xor i8 %2912, 1
  store i8 %2913, i8* %21, align 1
  %2914 = xor i64 %2904, 16
  %2915 = xor i64 %2914, %2905
  %2916 = lshr i64 %2915, 4
  %2917 = trunc i64 %2916 to i8
  %2918 = and i8 %2917, 1
  store i8 %2918, i8* %27, align 1
  %2919 = icmp eq i64 %2905, 0
  %2920 = zext i1 %2919 to i8
  store i8 %2920, i8* %30, align 1
  %2921 = lshr i64 %2905, 63
  %2922 = trunc i64 %2921 to i8
  store i8 %2922, i8* %33, align 1
  %2923 = lshr i64 %2904, 63
  %2924 = xor i64 %2921, %2923
  %2925 = add nuw nsw i64 %2924, %2921
  %2926 = icmp eq i64 %2925, 2
  %2927 = zext i1 %2926 to i8
  store i8 %2927, i8* %39, align 1
  %2928 = add i64 %2899, -150
  %2929 = add i64 %2901, 17
  store i64 %2929, i64* %3, align 8
  %2930 = inttoptr i64 %2928 to i16*
  %2931 = load i16, i16* %2930, align 2
  %2932 = zext i16 %2931 to i64
  store i64 %2932, i64* %RCX.i11580, align 8
  %2933 = zext i16 %2931 to i64
  %2934 = shl nuw nsw i64 %2933, 4
  store i64 %2934, i64* %573, align 8
  %2935 = add i64 %2934, %2905
  store i64 %2935, i64* %RAX.i11582.pre-phi, align 8
  %2936 = icmp ult i64 %2935, %2905
  %2937 = icmp ult i64 %2935, %2934
  %2938 = or i1 %2936, %2937
  %2939 = zext i1 %2938 to i8
  store i8 %2939, i8* %14, align 1
  %2940 = trunc i64 %2935 to i32
  %2941 = and i32 %2940, 255
  %2942 = tail call i32 @llvm.ctpop.i32(i32 %2941)
  %2943 = trunc i32 %2942 to i8
  %2944 = and i8 %2943, 1
  %2945 = xor i8 %2944, 1
  store i8 %2945, i8* %21, align 1
  %2946 = xor i64 %2934, %2905
  %2947 = xor i64 %2946, %2935
  %2948 = lshr i64 %2947, 4
  %2949 = trunc i64 %2948 to i8
  %2950 = and i8 %2949, 1
  store i8 %2950, i8* %27, align 1
  %2951 = icmp eq i64 %2935, 0
  %2952 = zext i1 %2951 to i8
  store i8 %2952, i8* %30, align 1
  %2953 = lshr i64 %2935, 63
  %2954 = trunc i64 %2953 to i8
  store i8 %2954, i8* %33, align 1
  %2955 = xor i64 %2953, %2921
  %2956 = add nuw nsw i64 %2955, %2953
  %2957 = icmp eq i64 %2956, 2
  %2958 = zext i1 %2957 to i8
  store i8 %2958, i8* %39, align 1
  %2959 = add i64 %2935, 8
  %2960 = add i64 %2901, 29
  store i64 %2960, i64* %3, align 8
  %2961 = inttoptr i64 %2959 to i32*
  %2962 = load i32, i32* %2961, align 4
  %2963 = zext i32 %2962 to i64
  store i64 %2963, i64* %RCX.i11580, align 8
  %2964 = load i64, i64* %RBP.i, align 8
  %2965 = add i64 %2964, -148
  %2966 = add i64 %2901, 35
  store i64 %2966, i64* %3, align 8
  %2967 = inttoptr i64 %2965 to i32*
  %2968 = load i32, i32* %2967, align 4
  %2969 = add i32 %2968, %2962
  %2970 = zext i32 %2969 to i64
  store i64 %2970, i64* %RCX.i11580, align 8
  %2971 = icmp ult i32 %2969, %2962
  %2972 = icmp ult i32 %2969, %2968
  %2973 = or i1 %2971, %2972
  %2974 = zext i1 %2973 to i8
  store i8 %2974, i8* %14, align 1
  %2975 = and i32 %2969, 255
  %2976 = tail call i32 @llvm.ctpop.i32(i32 %2975)
  %2977 = trunc i32 %2976 to i8
  %2978 = and i8 %2977, 1
  %2979 = xor i8 %2978, 1
  store i8 %2979, i8* %21, align 1
  %2980 = xor i32 %2968, %2962
  %2981 = xor i32 %2980, %2969
  %2982 = lshr i32 %2981, 4
  %2983 = trunc i32 %2982 to i8
  %2984 = and i8 %2983, 1
  store i8 %2984, i8* %27, align 1
  %2985 = icmp eq i32 %2969, 0
  %2986 = zext i1 %2985 to i8
  store i8 %2986, i8* %30, align 1
  %2987 = lshr i32 %2969, 31
  %2988 = trunc i32 %2987 to i8
  store i8 %2988, i8* %33, align 1
  %2989 = lshr i32 %2962, 31
  %2990 = lshr i32 %2968, 31
  %2991 = xor i32 %2987, %2989
  %2992 = xor i32 %2987, %2990
  %2993 = add nuw nsw i32 %2991, %2992
  %2994 = icmp eq i32 %2993, 2
  %2995 = zext i1 %2994 to i8
  store i8 %2995, i8* %39, align 1
  %2996 = add i64 %2901, 41
  store i64 %2996, i64* %3, align 8
  store i32 %2969, i32* %2967, align 4
  %2997 = load i64, i64* %RBP.i, align 8
  %2998 = add i64 %2997, -120
  %2999 = load i64, i64* %3, align 8
  %3000 = add i64 %2999, 4
  store i64 %3000, i64* %3, align 8
  %3001 = inttoptr i64 %2998 to i64*
  %3002 = load i64, i64* %3001, align 8
  store i64 %3002, i64* %RAX.i11582.pre-phi, align 8
  %3003 = add i64 %2997, -28
  %3004 = add i64 %2999, 7
  store i64 %3004, i64* %3, align 8
  %3005 = inttoptr i64 %3003 to i32*
  %3006 = load i32, i32* %3005, align 4
  %3007 = add i32 %3006, 1
  %3008 = zext i32 %3007 to i64
  store i64 %3008, i64* %RCX.i11580, align 8
  %3009 = icmp eq i32 %3006, -1
  %3010 = icmp eq i32 %3007, 0
  %3011 = or i1 %3009, %3010
  %3012 = zext i1 %3011 to i8
  store i8 %3012, i8* %14, align 1
  %3013 = and i32 %3007, 255
  %3014 = tail call i32 @llvm.ctpop.i32(i32 %3013)
  %3015 = trunc i32 %3014 to i8
  %3016 = and i8 %3015, 1
  %3017 = xor i8 %3016, 1
  store i8 %3017, i8* %21, align 1
  %3018 = xor i32 %3007, %3006
  %3019 = lshr i32 %3018, 4
  %3020 = trunc i32 %3019 to i8
  %3021 = and i8 %3020, 1
  store i8 %3021, i8* %27, align 1
  %3022 = zext i1 %3010 to i8
  store i8 %3022, i8* %30, align 1
  %3023 = lshr i32 %3007, 31
  %3024 = trunc i32 %3023 to i8
  store i8 %3024, i8* %33, align 1
  %3025 = lshr i32 %3006, 31
  %3026 = xor i32 %3023, %3025
  %3027 = add nuw nsw i32 %3026, %3023
  %3028 = icmp eq i32 %3027, 2
  %3029 = zext i1 %3028 to i8
  store i8 %3029, i8* %39, align 1
  %3030 = sext i32 %3007 to i64
  store i64 %3030, i64* %573, align 8
  %3031 = shl nsw i64 %3030, 1
  %3032 = add i64 %3002, %3031
  %3033 = add i64 %2999, 17
  store i64 %3033, i64* %3, align 8
  %3034 = inttoptr i64 %3032 to i16*
  %3035 = load i16, i16* %3034, align 2
  store i16 %3035, i16* %SI.i, align 2
  %3036 = add i64 %2997, -150
  %3037 = add i64 %2999, 24
  store i64 %3037, i64* %3, align 8
  %3038 = inttoptr i64 %3036 to i16*
  store i16 %3035, i16* %3038, align 2
  %3039 = load i64, i64* %RBP.i, align 8
  %3040 = add i64 %3039, -8
  %3041 = load i64, i64* %3, align 8
  %3042 = add i64 %3041, 4
  store i64 %3042, i64* %3, align 8
  %3043 = inttoptr i64 %3040 to i64*
  %3044 = load i64, i64* %3043, align 8
  %3045 = add i64 %3044, 51640
  store i64 %3045, i64* %RAX.i11582.pre-phi, align 8
  %3046 = icmp ugt i64 %3044, -51641
  %3047 = zext i1 %3046 to i8
  store i8 %3047, i8* %14, align 1
  %3048 = trunc i64 %3045 to i32
  %3049 = and i32 %3048, 255
  %3050 = tail call i32 @llvm.ctpop.i32(i32 %3049)
  %3051 = trunc i32 %3050 to i8
  %3052 = and i8 %3051, 1
  %3053 = xor i8 %3052, 1
  store i8 %3053, i8* %21, align 1
  %3054 = xor i64 %3044, 16
  %3055 = xor i64 %3054, %3045
  %3056 = lshr i64 %3055, 4
  %3057 = trunc i64 %3056 to i8
  %3058 = and i8 %3057, 1
  store i8 %3058, i8* %27, align 1
  %3059 = icmp eq i64 %3045, 0
  %3060 = zext i1 %3059 to i8
  store i8 %3060, i8* %30, align 1
  %3061 = lshr i64 %3045, 63
  %3062 = trunc i64 %3061 to i8
  store i8 %3062, i8* %33, align 1
  %3063 = lshr i64 %3044, 63
  %3064 = xor i64 %3061, %3063
  %3065 = add nuw nsw i64 %3064, %3061
  %3066 = icmp eq i64 %3065, 2
  %3067 = zext i1 %3066 to i8
  store i8 %3067, i8* %39, align 1
  %3068 = add i64 %3039, -150
  %3069 = add i64 %3041, 17
  store i64 %3069, i64* %3, align 8
  %3070 = inttoptr i64 %3068 to i16*
  %3071 = load i16, i16* %3070, align 2
  %3072 = zext i16 %3071 to i64
  store i64 %3072, i64* %RCX.i11580, align 8
  %3073 = zext i16 %3071 to i64
  %3074 = shl nuw nsw i64 %3073, 4
  store i64 %3074, i64* %573, align 8
  %3075 = add i64 %3074, %3045
  store i64 %3075, i64* %RAX.i11582.pre-phi, align 8
  %3076 = icmp ult i64 %3075, %3045
  %3077 = icmp ult i64 %3075, %3074
  %3078 = or i1 %3076, %3077
  %3079 = zext i1 %3078 to i8
  store i8 %3079, i8* %14, align 1
  %3080 = trunc i64 %3075 to i32
  %3081 = and i32 %3080, 255
  %3082 = tail call i32 @llvm.ctpop.i32(i32 %3081)
  %3083 = trunc i32 %3082 to i8
  %3084 = and i8 %3083, 1
  %3085 = xor i8 %3084, 1
  store i8 %3085, i8* %21, align 1
  %3086 = xor i64 %3074, %3045
  %3087 = xor i64 %3086, %3075
  %3088 = lshr i64 %3087, 4
  %3089 = trunc i64 %3088 to i8
  %3090 = and i8 %3089, 1
  store i8 %3090, i8* %27, align 1
  %3091 = icmp eq i64 %3075, 0
  %3092 = zext i1 %3091 to i8
  store i8 %3092, i8* %30, align 1
  %3093 = lshr i64 %3075, 63
  %3094 = trunc i64 %3093 to i8
  store i8 %3094, i8* %33, align 1
  %3095 = xor i64 %3093, %3061
  %3096 = add nuw nsw i64 %3095, %3093
  %3097 = icmp eq i64 %3096, 2
  %3098 = zext i1 %3097 to i8
  store i8 %3098, i8* %39, align 1
  %3099 = inttoptr i64 %3075 to i32*
  %3100 = add i64 %3041, 28
  store i64 %3100, i64* %3, align 8
  %3101 = load i32, i32* %3099, align 4
  %3102 = zext i32 %3101 to i64
  store i64 %3102, i64* %RCX.i11580, align 8
  %3103 = load i64, i64* %RBP.i, align 8
  %3104 = add i64 %3103, -140
  %3105 = add i64 %3041, 34
  store i64 %3105, i64* %3, align 8
  %3106 = inttoptr i64 %3104 to i32*
  %3107 = load i32, i32* %3106, align 4
  %3108 = add i32 %3107, %3101
  %3109 = zext i32 %3108 to i64
  store i64 %3109, i64* %RCX.i11580, align 8
  %3110 = icmp ult i32 %3108, %3101
  %3111 = icmp ult i32 %3108, %3107
  %3112 = or i1 %3110, %3111
  %3113 = zext i1 %3112 to i8
  store i8 %3113, i8* %14, align 1
  %3114 = and i32 %3108, 255
  %3115 = tail call i32 @llvm.ctpop.i32(i32 %3114)
  %3116 = trunc i32 %3115 to i8
  %3117 = and i8 %3116, 1
  %3118 = xor i8 %3117, 1
  store i8 %3118, i8* %21, align 1
  %3119 = xor i32 %3107, %3101
  %3120 = xor i32 %3119, %3108
  %3121 = lshr i32 %3120, 4
  %3122 = trunc i32 %3121 to i8
  %3123 = and i8 %3122, 1
  store i8 %3123, i8* %27, align 1
  %3124 = icmp eq i32 %3108, 0
  %3125 = zext i1 %3124 to i8
  store i8 %3125, i8* %30, align 1
  %3126 = lshr i32 %3108, 31
  %3127 = trunc i32 %3126 to i8
  store i8 %3127, i8* %33, align 1
  %3128 = lshr i32 %3101, 31
  %3129 = lshr i32 %3107, 31
  %3130 = xor i32 %3126, %3128
  %3131 = xor i32 %3126, %3129
  %3132 = add nuw nsw i32 %3130, %3131
  %3133 = icmp eq i32 %3132, 2
  %3134 = zext i1 %3133 to i8
  store i8 %3134, i8* %39, align 1
  %3135 = add i64 %3041, 40
  store i64 %3135, i64* %3, align 8
  store i32 %3108, i32* %3106, align 4
  %3136 = load i64, i64* %RBP.i, align 8
  %3137 = add i64 %3136, -8
  %3138 = load i64, i64* %3, align 8
  %3139 = add i64 %3138, 4
  store i64 %3139, i64* %3, align 8
  %3140 = inttoptr i64 %3137 to i64*
  %3141 = load i64, i64* %3140, align 8
  %3142 = add i64 %3141, 51640
  store i64 %3142, i64* %RAX.i11582.pre-phi, align 8
  %3143 = icmp ugt i64 %3141, -51641
  %3144 = zext i1 %3143 to i8
  store i8 %3144, i8* %14, align 1
  %3145 = trunc i64 %3142 to i32
  %3146 = and i32 %3145, 255
  %3147 = tail call i32 @llvm.ctpop.i32(i32 %3146)
  %3148 = trunc i32 %3147 to i8
  %3149 = and i8 %3148, 1
  %3150 = xor i8 %3149, 1
  store i8 %3150, i8* %21, align 1
  %3151 = xor i64 %3141, 16
  %3152 = xor i64 %3151, %3142
  %3153 = lshr i64 %3152, 4
  %3154 = trunc i64 %3153 to i8
  %3155 = and i8 %3154, 1
  store i8 %3155, i8* %27, align 1
  %3156 = icmp eq i64 %3142, 0
  %3157 = zext i1 %3156 to i8
  store i8 %3157, i8* %30, align 1
  %3158 = lshr i64 %3142, 63
  %3159 = trunc i64 %3158 to i8
  store i8 %3159, i8* %33, align 1
  %3160 = lshr i64 %3141, 63
  %3161 = xor i64 %3158, %3160
  %3162 = add nuw nsw i64 %3161, %3158
  %3163 = icmp eq i64 %3162, 2
  %3164 = zext i1 %3163 to i8
  store i8 %3164, i8* %39, align 1
  %3165 = add i64 %3136, -150
  %3166 = add i64 %3138, 17
  store i64 %3166, i64* %3, align 8
  %3167 = inttoptr i64 %3165 to i16*
  %3168 = load i16, i16* %3167, align 2
  %3169 = zext i16 %3168 to i64
  store i64 %3169, i64* %RCX.i11580, align 8
  %3170 = zext i16 %3168 to i64
  %3171 = shl nuw nsw i64 %3170, 4
  store i64 %3171, i64* %573, align 8
  %3172 = add i64 %3171, %3142
  store i64 %3172, i64* %RAX.i11582.pre-phi, align 8
  %3173 = icmp ult i64 %3172, %3142
  %3174 = icmp ult i64 %3172, %3171
  %3175 = or i1 %3173, %3174
  %3176 = zext i1 %3175 to i8
  store i8 %3176, i8* %14, align 1
  %3177 = trunc i64 %3172 to i32
  %3178 = and i32 %3177, 255
  %3179 = tail call i32 @llvm.ctpop.i32(i32 %3178)
  %3180 = trunc i32 %3179 to i8
  %3181 = and i8 %3180, 1
  %3182 = xor i8 %3181, 1
  store i8 %3182, i8* %21, align 1
  %3183 = xor i64 %3171, %3142
  %3184 = xor i64 %3183, %3172
  %3185 = lshr i64 %3184, 4
  %3186 = trunc i64 %3185 to i8
  %3187 = and i8 %3186, 1
  store i8 %3187, i8* %27, align 1
  %3188 = icmp eq i64 %3172, 0
  %3189 = zext i1 %3188 to i8
  store i8 %3189, i8* %30, align 1
  %3190 = lshr i64 %3172, 63
  %3191 = trunc i64 %3190 to i8
  store i8 %3191, i8* %33, align 1
  %3192 = xor i64 %3190, %3158
  %3193 = add nuw nsw i64 %3192, %3190
  %3194 = icmp eq i64 %3193, 2
  %3195 = zext i1 %3194 to i8
  store i8 %3195, i8* %39, align 1
  %3196 = add i64 %3172, 4
  %3197 = add i64 %3138, 29
  store i64 %3197, i64* %3, align 8
  %3198 = inttoptr i64 %3196 to i32*
  %3199 = load i32, i32* %3198, align 4
  %3200 = zext i32 %3199 to i64
  store i64 %3200, i64* %RCX.i11580, align 8
  %3201 = load i64, i64* %RBP.i, align 8
  %3202 = add i64 %3201, -144
  %3203 = add i64 %3138, 35
  store i64 %3203, i64* %3, align 8
  %3204 = inttoptr i64 %3202 to i32*
  %3205 = load i32, i32* %3204, align 4
  %3206 = add i32 %3205, %3199
  %3207 = zext i32 %3206 to i64
  store i64 %3207, i64* %RCX.i11580, align 8
  %3208 = icmp ult i32 %3206, %3199
  %3209 = icmp ult i32 %3206, %3205
  %3210 = or i1 %3208, %3209
  %3211 = zext i1 %3210 to i8
  store i8 %3211, i8* %14, align 1
  %3212 = and i32 %3206, 255
  %3213 = tail call i32 @llvm.ctpop.i32(i32 %3212)
  %3214 = trunc i32 %3213 to i8
  %3215 = and i8 %3214, 1
  %3216 = xor i8 %3215, 1
  store i8 %3216, i8* %21, align 1
  %3217 = xor i32 %3205, %3199
  %3218 = xor i32 %3217, %3206
  %3219 = lshr i32 %3218, 4
  %3220 = trunc i32 %3219 to i8
  %3221 = and i8 %3220, 1
  store i8 %3221, i8* %27, align 1
  %3222 = icmp eq i32 %3206, 0
  %3223 = zext i1 %3222 to i8
  store i8 %3223, i8* %30, align 1
  %3224 = lshr i32 %3206, 31
  %3225 = trunc i32 %3224 to i8
  store i8 %3225, i8* %33, align 1
  %3226 = lshr i32 %3199, 31
  %3227 = lshr i32 %3205, 31
  %3228 = xor i32 %3224, %3226
  %3229 = xor i32 %3224, %3227
  %3230 = add nuw nsw i32 %3228, %3229
  %3231 = icmp eq i32 %3230, 2
  %3232 = zext i1 %3231 to i8
  store i8 %3232, i8* %39, align 1
  %3233 = add i64 %3138, 41
  store i64 %3233, i64* %3, align 8
  store i32 %3206, i32* %3204, align 4
  %3234 = load i64, i64* %RBP.i, align 8
  %3235 = add i64 %3234, -8
  %3236 = load i64, i64* %3, align 8
  %3237 = add i64 %3236, 4
  store i64 %3237, i64* %3, align 8
  %3238 = inttoptr i64 %3235 to i64*
  %3239 = load i64, i64* %3238, align 8
  %3240 = add i64 %3239, 51640
  store i64 %3240, i64* %RAX.i11582.pre-phi, align 8
  %3241 = icmp ugt i64 %3239, -51641
  %3242 = zext i1 %3241 to i8
  store i8 %3242, i8* %14, align 1
  %3243 = trunc i64 %3240 to i32
  %3244 = and i32 %3243, 255
  %3245 = tail call i32 @llvm.ctpop.i32(i32 %3244)
  %3246 = trunc i32 %3245 to i8
  %3247 = and i8 %3246, 1
  %3248 = xor i8 %3247, 1
  store i8 %3248, i8* %21, align 1
  %3249 = xor i64 %3239, 16
  %3250 = xor i64 %3249, %3240
  %3251 = lshr i64 %3250, 4
  %3252 = trunc i64 %3251 to i8
  %3253 = and i8 %3252, 1
  store i8 %3253, i8* %27, align 1
  %3254 = icmp eq i64 %3240, 0
  %3255 = zext i1 %3254 to i8
  store i8 %3255, i8* %30, align 1
  %3256 = lshr i64 %3240, 63
  %3257 = trunc i64 %3256 to i8
  store i8 %3257, i8* %33, align 1
  %3258 = lshr i64 %3239, 63
  %3259 = xor i64 %3256, %3258
  %3260 = add nuw nsw i64 %3259, %3256
  %3261 = icmp eq i64 %3260, 2
  %3262 = zext i1 %3261 to i8
  store i8 %3262, i8* %39, align 1
  %3263 = add i64 %3234, -150
  %3264 = add i64 %3236, 17
  store i64 %3264, i64* %3, align 8
  %3265 = inttoptr i64 %3263 to i16*
  %3266 = load i16, i16* %3265, align 2
  %3267 = zext i16 %3266 to i64
  store i64 %3267, i64* %RCX.i11580, align 8
  %3268 = zext i16 %3266 to i64
  %3269 = shl nuw nsw i64 %3268, 4
  store i64 %3269, i64* %573, align 8
  %3270 = add i64 %3269, %3240
  store i64 %3270, i64* %RAX.i11582.pre-phi, align 8
  %3271 = icmp ult i64 %3270, %3240
  %3272 = icmp ult i64 %3270, %3269
  %3273 = or i1 %3271, %3272
  %3274 = zext i1 %3273 to i8
  store i8 %3274, i8* %14, align 1
  %3275 = trunc i64 %3270 to i32
  %3276 = and i32 %3275, 255
  %3277 = tail call i32 @llvm.ctpop.i32(i32 %3276)
  %3278 = trunc i32 %3277 to i8
  %3279 = and i8 %3278, 1
  %3280 = xor i8 %3279, 1
  store i8 %3280, i8* %21, align 1
  %3281 = xor i64 %3269, %3240
  %3282 = xor i64 %3281, %3270
  %3283 = lshr i64 %3282, 4
  %3284 = trunc i64 %3283 to i8
  %3285 = and i8 %3284, 1
  store i8 %3285, i8* %27, align 1
  %3286 = icmp eq i64 %3270, 0
  %3287 = zext i1 %3286 to i8
  store i8 %3287, i8* %30, align 1
  %3288 = lshr i64 %3270, 63
  %3289 = trunc i64 %3288 to i8
  store i8 %3289, i8* %33, align 1
  %3290 = xor i64 %3288, %3256
  %3291 = add nuw nsw i64 %3290, %3288
  %3292 = icmp eq i64 %3291, 2
  %3293 = zext i1 %3292 to i8
  store i8 %3293, i8* %39, align 1
  %3294 = add i64 %3270, 8
  %3295 = add i64 %3236, 29
  store i64 %3295, i64* %3, align 8
  %3296 = inttoptr i64 %3294 to i32*
  %3297 = load i32, i32* %3296, align 4
  %3298 = zext i32 %3297 to i64
  store i64 %3298, i64* %RCX.i11580, align 8
  %3299 = load i64, i64* %RBP.i, align 8
  %3300 = add i64 %3299, -148
  %3301 = add i64 %3236, 35
  store i64 %3301, i64* %3, align 8
  %3302 = inttoptr i64 %3300 to i32*
  %3303 = load i32, i32* %3302, align 4
  %3304 = add i32 %3303, %3297
  %3305 = zext i32 %3304 to i64
  store i64 %3305, i64* %RCX.i11580, align 8
  %3306 = icmp ult i32 %3304, %3297
  %3307 = icmp ult i32 %3304, %3303
  %3308 = or i1 %3306, %3307
  %3309 = zext i1 %3308 to i8
  store i8 %3309, i8* %14, align 1
  %3310 = and i32 %3304, 255
  %3311 = tail call i32 @llvm.ctpop.i32(i32 %3310)
  %3312 = trunc i32 %3311 to i8
  %3313 = and i8 %3312, 1
  %3314 = xor i8 %3313, 1
  store i8 %3314, i8* %21, align 1
  %3315 = xor i32 %3303, %3297
  %3316 = xor i32 %3315, %3304
  %3317 = lshr i32 %3316, 4
  %3318 = trunc i32 %3317 to i8
  %3319 = and i8 %3318, 1
  store i8 %3319, i8* %27, align 1
  %3320 = icmp eq i32 %3304, 0
  %3321 = zext i1 %3320 to i8
  store i8 %3321, i8* %30, align 1
  %3322 = lshr i32 %3304, 31
  %3323 = trunc i32 %3322 to i8
  store i8 %3323, i8* %33, align 1
  %3324 = lshr i32 %3297, 31
  %3325 = lshr i32 %3303, 31
  %3326 = xor i32 %3322, %3324
  %3327 = xor i32 %3322, %3325
  %3328 = add nuw nsw i32 %3326, %3327
  %3329 = icmp eq i32 %3328, 2
  %3330 = zext i1 %3329 to i8
  store i8 %3330, i8* %39, align 1
  %3331 = add i64 %3236, 41
  store i64 %3331, i64* %3, align 8
  store i32 %3304, i32* %3302, align 4
  %3332 = load i64, i64* %RBP.i, align 8
  %3333 = add i64 %3332, -120
  %3334 = load i64, i64* %3, align 8
  %3335 = add i64 %3334, 4
  store i64 %3335, i64* %3, align 8
  %3336 = inttoptr i64 %3333 to i64*
  %3337 = load i64, i64* %3336, align 8
  store i64 %3337, i64* %RAX.i11582.pre-phi, align 8
  %3338 = add i64 %3332, -28
  %3339 = add i64 %3334, 7
  store i64 %3339, i64* %3, align 8
  %3340 = inttoptr i64 %3338 to i32*
  %3341 = load i32, i32* %3340, align 4
  %3342 = add i32 %3341, 2
  %3343 = zext i32 %3342 to i64
  store i64 %3343, i64* %RCX.i11580, align 8
  %3344 = icmp ugt i32 %3341, -3
  %3345 = zext i1 %3344 to i8
  store i8 %3345, i8* %14, align 1
  %3346 = and i32 %3342, 255
  %3347 = tail call i32 @llvm.ctpop.i32(i32 %3346)
  %3348 = trunc i32 %3347 to i8
  %3349 = and i8 %3348, 1
  %3350 = xor i8 %3349, 1
  store i8 %3350, i8* %21, align 1
  %3351 = xor i32 %3342, %3341
  %3352 = lshr i32 %3351, 4
  %3353 = trunc i32 %3352 to i8
  %3354 = and i8 %3353, 1
  store i8 %3354, i8* %27, align 1
  %3355 = icmp eq i32 %3342, 0
  %3356 = zext i1 %3355 to i8
  store i8 %3356, i8* %30, align 1
  %3357 = lshr i32 %3342, 31
  %3358 = trunc i32 %3357 to i8
  store i8 %3358, i8* %33, align 1
  %3359 = lshr i32 %3341, 31
  %3360 = xor i32 %3357, %3359
  %3361 = add nuw nsw i32 %3360, %3357
  %3362 = icmp eq i32 %3361, 2
  %3363 = zext i1 %3362 to i8
  store i8 %3363, i8* %39, align 1
  %3364 = sext i32 %3342 to i64
  store i64 %3364, i64* %573, align 8
  %3365 = shl nsw i64 %3364, 1
  %3366 = add i64 %3337, %3365
  %3367 = add i64 %3334, 17
  store i64 %3367, i64* %3, align 8
  %3368 = inttoptr i64 %3366 to i16*
  %3369 = load i16, i16* %3368, align 2
  store i16 %3369, i16* %SI.i, align 2
  %3370 = add i64 %3332, -150
  %3371 = add i64 %3334, 24
  store i64 %3371, i64* %3, align 8
  %3372 = inttoptr i64 %3370 to i16*
  store i16 %3369, i16* %3372, align 2
  %3373 = load i64, i64* %RBP.i, align 8
  %3374 = add i64 %3373, -8
  %3375 = load i64, i64* %3, align 8
  %3376 = add i64 %3375, 4
  store i64 %3376, i64* %3, align 8
  %3377 = inttoptr i64 %3374 to i64*
  %3378 = load i64, i64* %3377, align 8
  %3379 = add i64 %3378, 51640
  store i64 %3379, i64* %RAX.i11582.pre-phi, align 8
  %3380 = icmp ugt i64 %3378, -51641
  %3381 = zext i1 %3380 to i8
  store i8 %3381, i8* %14, align 1
  %3382 = trunc i64 %3379 to i32
  %3383 = and i32 %3382, 255
  %3384 = tail call i32 @llvm.ctpop.i32(i32 %3383)
  %3385 = trunc i32 %3384 to i8
  %3386 = and i8 %3385, 1
  %3387 = xor i8 %3386, 1
  store i8 %3387, i8* %21, align 1
  %3388 = xor i64 %3378, 16
  %3389 = xor i64 %3388, %3379
  %3390 = lshr i64 %3389, 4
  %3391 = trunc i64 %3390 to i8
  %3392 = and i8 %3391, 1
  store i8 %3392, i8* %27, align 1
  %3393 = icmp eq i64 %3379, 0
  %3394 = zext i1 %3393 to i8
  store i8 %3394, i8* %30, align 1
  %3395 = lshr i64 %3379, 63
  %3396 = trunc i64 %3395 to i8
  store i8 %3396, i8* %33, align 1
  %3397 = lshr i64 %3378, 63
  %3398 = xor i64 %3395, %3397
  %3399 = add nuw nsw i64 %3398, %3395
  %3400 = icmp eq i64 %3399, 2
  %3401 = zext i1 %3400 to i8
  store i8 %3401, i8* %39, align 1
  %3402 = add i64 %3373, -150
  %3403 = add i64 %3375, 17
  store i64 %3403, i64* %3, align 8
  %3404 = inttoptr i64 %3402 to i16*
  %3405 = load i16, i16* %3404, align 2
  %3406 = zext i16 %3405 to i64
  store i64 %3406, i64* %RCX.i11580, align 8
  %3407 = zext i16 %3405 to i64
  %3408 = shl nuw nsw i64 %3407, 4
  store i64 %3408, i64* %573, align 8
  %3409 = add i64 %3408, %3379
  store i64 %3409, i64* %RAX.i11582.pre-phi, align 8
  %3410 = icmp ult i64 %3409, %3379
  %3411 = icmp ult i64 %3409, %3408
  %3412 = or i1 %3410, %3411
  %3413 = zext i1 %3412 to i8
  store i8 %3413, i8* %14, align 1
  %3414 = trunc i64 %3409 to i32
  %3415 = and i32 %3414, 255
  %3416 = tail call i32 @llvm.ctpop.i32(i32 %3415)
  %3417 = trunc i32 %3416 to i8
  %3418 = and i8 %3417, 1
  %3419 = xor i8 %3418, 1
  store i8 %3419, i8* %21, align 1
  %3420 = xor i64 %3408, %3379
  %3421 = xor i64 %3420, %3409
  %3422 = lshr i64 %3421, 4
  %3423 = trunc i64 %3422 to i8
  %3424 = and i8 %3423, 1
  store i8 %3424, i8* %27, align 1
  %3425 = icmp eq i64 %3409, 0
  %3426 = zext i1 %3425 to i8
  store i8 %3426, i8* %30, align 1
  %3427 = lshr i64 %3409, 63
  %3428 = trunc i64 %3427 to i8
  store i8 %3428, i8* %33, align 1
  %3429 = xor i64 %3427, %3395
  %3430 = add nuw nsw i64 %3429, %3427
  %3431 = icmp eq i64 %3430, 2
  %3432 = zext i1 %3431 to i8
  store i8 %3432, i8* %39, align 1
  %3433 = inttoptr i64 %3409 to i32*
  %3434 = add i64 %3375, 28
  store i64 %3434, i64* %3, align 8
  %3435 = load i32, i32* %3433, align 4
  %3436 = zext i32 %3435 to i64
  store i64 %3436, i64* %RCX.i11580, align 8
  %3437 = load i64, i64* %RBP.i, align 8
  %3438 = add i64 %3437, -140
  %3439 = add i64 %3375, 34
  store i64 %3439, i64* %3, align 8
  %3440 = inttoptr i64 %3438 to i32*
  %3441 = load i32, i32* %3440, align 4
  %3442 = add i32 %3441, %3435
  %3443 = zext i32 %3442 to i64
  store i64 %3443, i64* %RCX.i11580, align 8
  %3444 = icmp ult i32 %3442, %3435
  %3445 = icmp ult i32 %3442, %3441
  %3446 = or i1 %3444, %3445
  %3447 = zext i1 %3446 to i8
  store i8 %3447, i8* %14, align 1
  %3448 = and i32 %3442, 255
  %3449 = tail call i32 @llvm.ctpop.i32(i32 %3448)
  %3450 = trunc i32 %3449 to i8
  %3451 = and i8 %3450, 1
  %3452 = xor i8 %3451, 1
  store i8 %3452, i8* %21, align 1
  %3453 = xor i32 %3441, %3435
  %3454 = xor i32 %3453, %3442
  %3455 = lshr i32 %3454, 4
  %3456 = trunc i32 %3455 to i8
  %3457 = and i8 %3456, 1
  store i8 %3457, i8* %27, align 1
  %3458 = icmp eq i32 %3442, 0
  %3459 = zext i1 %3458 to i8
  store i8 %3459, i8* %30, align 1
  %3460 = lshr i32 %3442, 31
  %3461 = trunc i32 %3460 to i8
  store i8 %3461, i8* %33, align 1
  %3462 = lshr i32 %3435, 31
  %3463 = lshr i32 %3441, 31
  %3464 = xor i32 %3460, %3462
  %3465 = xor i32 %3460, %3463
  %3466 = add nuw nsw i32 %3464, %3465
  %3467 = icmp eq i32 %3466, 2
  %3468 = zext i1 %3467 to i8
  store i8 %3468, i8* %39, align 1
  %3469 = add i64 %3375, 40
  store i64 %3469, i64* %3, align 8
  store i32 %3442, i32* %3440, align 4
  %3470 = load i64, i64* %RBP.i, align 8
  %3471 = add i64 %3470, -8
  %3472 = load i64, i64* %3, align 8
  %3473 = add i64 %3472, 4
  store i64 %3473, i64* %3, align 8
  %3474 = inttoptr i64 %3471 to i64*
  %3475 = load i64, i64* %3474, align 8
  %3476 = add i64 %3475, 51640
  store i64 %3476, i64* %RAX.i11582.pre-phi, align 8
  %3477 = icmp ugt i64 %3475, -51641
  %3478 = zext i1 %3477 to i8
  store i8 %3478, i8* %14, align 1
  %3479 = trunc i64 %3476 to i32
  %3480 = and i32 %3479, 255
  %3481 = tail call i32 @llvm.ctpop.i32(i32 %3480)
  %3482 = trunc i32 %3481 to i8
  %3483 = and i8 %3482, 1
  %3484 = xor i8 %3483, 1
  store i8 %3484, i8* %21, align 1
  %3485 = xor i64 %3475, 16
  %3486 = xor i64 %3485, %3476
  %3487 = lshr i64 %3486, 4
  %3488 = trunc i64 %3487 to i8
  %3489 = and i8 %3488, 1
  store i8 %3489, i8* %27, align 1
  %3490 = icmp eq i64 %3476, 0
  %3491 = zext i1 %3490 to i8
  store i8 %3491, i8* %30, align 1
  %3492 = lshr i64 %3476, 63
  %3493 = trunc i64 %3492 to i8
  store i8 %3493, i8* %33, align 1
  %3494 = lshr i64 %3475, 63
  %3495 = xor i64 %3492, %3494
  %3496 = add nuw nsw i64 %3495, %3492
  %3497 = icmp eq i64 %3496, 2
  %3498 = zext i1 %3497 to i8
  store i8 %3498, i8* %39, align 1
  %3499 = add i64 %3470, -150
  %3500 = add i64 %3472, 17
  store i64 %3500, i64* %3, align 8
  %3501 = inttoptr i64 %3499 to i16*
  %3502 = load i16, i16* %3501, align 2
  %3503 = zext i16 %3502 to i64
  store i64 %3503, i64* %RCX.i11580, align 8
  %3504 = zext i16 %3502 to i64
  %3505 = shl nuw nsw i64 %3504, 4
  store i64 %3505, i64* %573, align 8
  %3506 = add i64 %3505, %3476
  store i64 %3506, i64* %RAX.i11582.pre-phi, align 8
  %3507 = icmp ult i64 %3506, %3476
  %3508 = icmp ult i64 %3506, %3505
  %3509 = or i1 %3507, %3508
  %3510 = zext i1 %3509 to i8
  store i8 %3510, i8* %14, align 1
  %3511 = trunc i64 %3506 to i32
  %3512 = and i32 %3511, 255
  %3513 = tail call i32 @llvm.ctpop.i32(i32 %3512)
  %3514 = trunc i32 %3513 to i8
  %3515 = and i8 %3514, 1
  %3516 = xor i8 %3515, 1
  store i8 %3516, i8* %21, align 1
  %3517 = xor i64 %3505, %3476
  %3518 = xor i64 %3517, %3506
  %3519 = lshr i64 %3518, 4
  %3520 = trunc i64 %3519 to i8
  %3521 = and i8 %3520, 1
  store i8 %3521, i8* %27, align 1
  %3522 = icmp eq i64 %3506, 0
  %3523 = zext i1 %3522 to i8
  store i8 %3523, i8* %30, align 1
  %3524 = lshr i64 %3506, 63
  %3525 = trunc i64 %3524 to i8
  store i8 %3525, i8* %33, align 1
  %3526 = xor i64 %3524, %3492
  %3527 = add nuw nsw i64 %3526, %3524
  %3528 = icmp eq i64 %3527, 2
  %3529 = zext i1 %3528 to i8
  store i8 %3529, i8* %39, align 1
  %3530 = add i64 %3506, 4
  %3531 = add i64 %3472, 29
  store i64 %3531, i64* %3, align 8
  %3532 = inttoptr i64 %3530 to i32*
  %3533 = load i32, i32* %3532, align 4
  %3534 = zext i32 %3533 to i64
  store i64 %3534, i64* %RCX.i11580, align 8
  %3535 = load i64, i64* %RBP.i, align 8
  %3536 = add i64 %3535, -144
  %3537 = add i64 %3472, 35
  store i64 %3537, i64* %3, align 8
  %3538 = inttoptr i64 %3536 to i32*
  %3539 = load i32, i32* %3538, align 4
  %3540 = add i32 %3539, %3533
  %3541 = zext i32 %3540 to i64
  store i64 %3541, i64* %RCX.i11580, align 8
  %3542 = icmp ult i32 %3540, %3533
  %3543 = icmp ult i32 %3540, %3539
  %3544 = or i1 %3542, %3543
  %3545 = zext i1 %3544 to i8
  store i8 %3545, i8* %14, align 1
  %3546 = and i32 %3540, 255
  %3547 = tail call i32 @llvm.ctpop.i32(i32 %3546)
  %3548 = trunc i32 %3547 to i8
  %3549 = and i8 %3548, 1
  %3550 = xor i8 %3549, 1
  store i8 %3550, i8* %21, align 1
  %3551 = xor i32 %3539, %3533
  %3552 = xor i32 %3551, %3540
  %3553 = lshr i32 %3552, 4
  %3554 = trunc i32 %3553 to i8
  %3555 = and i8 %3554, 1
  store i8 %3555, i8* %27, align 1
  %3556 = icmp eq i32 %3540, 0
  %3557 = zext i1 %3556 to i8
  store i8 %3557, i8* %30, align 1
  %3558 = lshr i32 %3540, 31
  %3559 = trunc i32 %3558 to i8
  store i8 %3559, i8* %33, align 1
  %3560 = lshr i32 %3533, 31
  %3561 = lshr i32 %3539, 31
  %3562 = xor i32 %3558, %3560
  %3563 = xor i32 %3558, %3561
  %3564 = add nuw nsw i32 %3562, %3563
  %3565 = icmp eq i32 %3564, 2
  %3566 = zext i1 %3565 to i8
  store i8 %3566, i8* %39, align 1
  %3567 = add i64 %3472, 41
  store i64 %3567, i64* %3, align 8
  store i32 %3540, i32* %3538, align 4
  %3568 = load i64, i64* %RBP.i, align 8
  %3569 = add i64 %3568, -8
  %3570 = load i64, i64* %3, align 8
  %3571 = add i64 %3570, 4
  store i64 %3571, i64* %3, align 8
  %3572 = inttoptr i64 %3569 to i64*
  %3573 = load i64, i64* %3572, align 8
  %3574 = add i64 %3573, 51640
  store i64 %3574, i64* %RAX.i11582.pre-phi, align 8
  %3575 = icmp ugt i64 %3573, -51641
  %3576 = zext i1 %3575 to i8
  store i8 %3576, i8* %14, align 1
  %3577 = trunc i64 %3574 to i32
  %3578 = and i32 %3577, 255
  %3579 = tail call i32 @llvm.ctpop.i32(i32 %3578)
  %3580 = trunc i32 %3579 to i8
  %3581 = and i8 %3580, 1
  %3582 = xor i8 %3581, 1
  store i8 %3582, i8* %21, align 1
  %3583 = xor i64 %3573, 16
  %3584 = xor i64 %3583, %3574
  %3585 = lshr i64 %3584, 4
  %3586 = trunc i64 %3585 to i8
  %3587 = and i8 %3586, 1
  store i8 %3587, i8* %27, align 1
  %3588 = icmp eq i64 %3574, 0
  %3589 = zext i1 %3588 to i8
  store i8 %3589, i8* %30, align 1
  %3590 = lshr i64 %3574, 63
  %3591 = trunc i64 %3590 to i8
  store i8 %3591, i8* %33, align 1
  %3592 = lshr i64 %3573, 63
  %3593 = xor i64 %3590, %3592
  %3594 = add nuw nsw i64 %3593, %3590
  %3595 = icmp eq i64 %3594, 2
  %3596 = zext i1 %3595 to i8
  store i8 %3596, i8* %39, align 1
  %3597 = add i64 %3568, -150
  %3598 = add i64 %3570, 17
  store i64 %3598, i64* %3, align 8
  %3599 = inttoptr i64 %3597 to i16*
  %3600 = load i16, i16* %3599, align 2
  %3601 = zext i16 %3600 to i64
  store i64 %3601, i64* %RCX.i11580, align 8
  %3602 = zext i16 %3600 to i64
  %3603 = shl nuw nsw i64 %3602, 4
  store i64 %3603, i64* %573, align 8
  %3604 = add i64 %3603, %3574
  store i64 %3604, i64* %RAX.i11582.pre-phi, align 8
  %3605 = icmp ult i64 %3604, %3574
  %3606 = icmp ult i64 %3604, %3603
  %3607 = or i1 %3605, %3606
  %3608 = zext i1 %3607 to i8
  store i8 %3608, i8* %14, align 1
  %3609 = trunc i64 %3604 to i32
  %3610 = and i32 %3609, 255
  %3611 = tail call i32 @llvm.ctpop.i32(i32 %3610)
  %3612 = trunc i32 %3611 to i8
  %3613 = and i8 %3612, 1
  %3614 = xor i8 %3613, 1
  store i8 %3614, i8* %21, align 1
  %3615 = xor i64 %3603, %3574
  %3616 = xor i64 %3615, %3604
  %3617 = lshr i64 %3616, 4
  %3618 = trunc i64 %3617 to i8
  %3619 = and i8 %3618, 1
  store i8 %3619, i8* %27, align 1
  %3620 = icmp eq i64 %3604, 0
  %3621 = zext i1 %3620 to i8
  store i8 %3621, i8* %30, align 1
  %3622 = lshr i64 %3604, 63
  %3623 = trunc i64 %3622 to i8
  store i8 %3623, i8* %33, align 1
  %3624 = xor i64 %3622, %3590
  %3625 = add nuw nsw i64 %3624, %3622
  %3626 = icmp eq i64 %3625, 2
  %3627 = zext i1 %3626 to i8
  store i8 %3627, i8* %39, align 1
  %3628 = add i64 %3604, 8
  %3629 = add i64 %3570, 29
  store i64 %3629, i64* %3, align 8
  %3630 = inttoptr i64 %3628 to i32*
  %3631 = load i32, i32* %3630, align 4
  %3632 = zext i32 %3631 to i64
  store i64 %3632, i64* %RCX.i11580, align 8
  %3633 = load i64, i64* %RBP.i, align 8
  %3634 = add i64 %3633, -148
  %3635 = add i64 %3570, 35
  store i64 %3635, i64* %3, align 8
  %3636 = inttoptr i64 %3634 to i32*
  %3637 = load i32, i32* %3636, align 4
  %3638 = add i32 %3637, %3631
  %3639 = zext i32 %3638 to i64
  store i64 %3639, i64* %RCX.i11580, align 8
  %3640 = icmp ult i32 %3638, %3631
  %3641 = icmp ult i32 %3638, %3637
  %3642 = or i1 %3640, %3641
  %3643 = zext i1 %3642 to i8
  store i8 %3643, i8* %14, align 1
  %3644 = and i32 %3638, 255
  %3645 = tail call i32 @llvm.ctpop.i32(i32 %3644)
  %3646 = trunc i32 %3645 to i8
  %3647 = and i8 %3646, 1
  %3648 = xor i8 %3647, 1
  store i8 %3648, i8* %21, align 1
  %3649 = xor i32 %3637, %3631
  %3650 = xor i32 %3649, %3638
  %3651 = lshr i32 %3650, 4
  %3652 = trunc i32 %3651 to i8
  %3653 = and i8 %3652, 1
  store i8 %3653, i8* %27, align 1
  %3654 = icmp eq i32 %3638, 0
  %3655 = zext i1 %3654 to i8
  store i8 %3655, i8* %30, align 1
  %3656 = lshr i32 %3638, 31
  %3657 = trunc i32 %3656 to i8
  store i8 %3657, i8* %33, align 1
  %3658 = lshr i32 %3631, 31
  %3659 = lshr i32 %3637, 31
  %3660 = xor i32 %3656, %3658
  %3661 = xor i32 %3656, %3659
  %3662 = add nuw nsw i32 %3660, %3661
  %3663 = icmp eq i32 %3662, 2
  %3664 = zext i1 %3663 to i8
  store i8 %3664, i8* %39, align 1
  %3665 = add i64 %3570, 41
  store i64 %3665, i64* %3, align 8
  store i32 %3638, i32* %3636, align 4
  %3666 = load i64, i64* %RBP.i, align 8
  %3667 = add i64 %3666, -120
  %3668 = load i64, i64* %3, align 8
  %3669 = add i64 %3668, 4
  store i64 %3669, i64* %3, align 8
  %3670 = inttoptr i64 %3667 to i64*
  %3671 = load i64, i64* %3670, align 8
  store i64 %3671, i64* %RAX.i11582.pre-phi, align 8
  %3672 = add i64 %3666, -28
  %3673 = add i64 %3668, 7
  store i64 %3673, i64* %3, align 8
  %3674 = inttoptr i64 %3672 to i32*
  %3675 = load i32, i32* %3674, align 4
  %3676 = add i32 %3675, 3
  %3677 = zext i32 %3676 to i64
  store i64 %3677, i64* %RCX.i11580, align 8
  %3678 = icmp ugt i32 %3675, -4
  %3679 = zext i1 %3678 to i8
  store i8 %3679, i8* %14, align 1
  %3680 = and i32 %3676, 255
  %3681 = tail call i32 @llvm.ctpop.i32(i32 %3680)
  %3682 = trunc i32 %3681 to i8
  %3683 = and i8 %3682, 1
  %3684 = xor i8 %3683, 1
  store i8 %3684, i8* %21, align 1
  %3685 = xor i32 %3676, %3675
  %3686 = lshr i32 %3685, 4
  %3687 = trunc i32 %3686 to i8
  %3688 = and i8 %3687, 1
  store i8 %3688, i8* %27, align 1
  %3689 = icmp eq i32 %3676, 0
  %3690 = zext i1 %3689 to i8
  store i8 %3690, i8* %30, align 1
  %3691 = lshr i32 %3676, 31
  %3692 = trunc i32 %3691 to i8
  store i8 %3692, i8* %33, align 1
  %3693 = lshr i32 %3675, 31
  %3694 = xor i32 %3691, %3693
  %3695 = add nuw nsw i32 %3694, %3691
  %3696 = icmp eq i32 %3695, 2
  %3697 = zext i1 %3696 to i8
  store i8 %3697, i8* %39, align 1
  %3698 = sext i32 %3676 to i64
  store i64 %3698, i64* %573, align 8
  %3699 = shl nsw i64 %3698, 1
  %3700 = add i64 %3671, %3699
  %3701 = add i64 %3668, 17
  store i64 %3701, i64* %3, align 8
  %3702 = inttoptr i64 %3700 to i16*
  %3703 = load i16, i16* %3702, align 2
  store i16 %3703, i16* %SI.i, align 2
  %3704 = add i64 %3666, -150
  %3705 = add i64 %3668, 24
  store i64 %3705, i64* %3, align 8
  %3706 = inttoptr i64 %3704 to i16*
  store i16 %3703, i16* %3706, align 2
  %3707 = load i64, i64* %RBP.i, align 8
  %3708 = add i64 %3707, -8
  %3709 = load i64, i64* %3, align 8
  %3710 = add i64 %3709, 4
  store i64 %3710, i64* %3, align 8
  %3711 = inttoptr i64 %3708 to i64*
  %3712 = load i64, i64* %3711, align 8
  %3713 = add i64 %3712, 51640
  store i64 %3713, i64* %RAX.i11582.pre-phi, align 8
  %3714 = icmp ugt i64 %3712, -51641
  %3715 = zext i1 %3714 to i8
  store i8 %3715, i8* %14, align 1
  %3716 = trunc i64 %3713 to i32
  %3717 = and i32 %3716, 255
  %3718 = tail call i32 @llvm.ctpop.i32(i32 %3717)
  %3719 = trunc i32 %3718 to i8
  %3720 = and i8 %3719, 1
  %3721 = xor i8 %3720, 1
  store i8 %3721, i8* %21, align 1
  %3722 = xor i64 %3712, 16
  %3723 = xor i64 %3722, %3713
  %3724 = lshr i64 %3723, 4
  %3725 = trunc i64 %3724 to i8
  %3726 = and i8 %3725, 1
  store i8 %3726, i8* %27, align 1
  %3727 = icmp eq i64 %3713, 0
  %3728 = zext i1 %3727 to i8
  store i8 %3728, i8* %30, align 1
  %3729 = lshr i64 %3713, 63
  %3730 = trunc i64 %3729 to i8
  store i8 %3730, i8* %33, align 1
  %3731 = lshr i64 %3712, 63
  %3732 = xor i64 %3729, %3731
  %3733 = add nuw nsw i64 %3732, %3729
  %3734 = icmp eq i64 %3733, 2
  %3735 = zext i1 %3734 to i8
  store i8 %3735, i8* %39, align 1
  %3736 = add i64 %3707, -150
  %3737 = add i64 %3709, 17
  store i64 %3737, i64* %3, align 8
  %3738 = inttoptr i64 %3736 to i16*
  %3739 = load i16, i16* %3738, align 2
  %3740 = zext i16 %3739 to i64
  store i64 %3740, i64* %RCX.i11580, align 8
  %3741 = zext i16 %3739 to i64
  %3742 = shl nuw nsw i64 %3741, 4
  store i64 %3742, i64* %573, align 8
  %3743 = add i64 %3742, %3713
  store i64 %3743, i64* %RAX.i11582.pre-phi, align 8
  %3744 = icmp ult i64 %3743, %3713
  %3745 = icmp ult i64 %3743, %3742
  %3746 = or i1 %3744, %3745
  %3747 = zext i1 %3746 to i8
  store i8 %3747, i8* %14, align 1
  %3748 = trunc i64 %3743 to i32
  %3749 = and i32 %3748, 255
  %3750 = tail call i32 @llvm.ctpop.i32(i32 %3749)
  %3751 = trunc i32 %3750 to i8
  %3752 = and i8 %3751, 1
  %3753 = xor i8 %3752, 1
  store i8 %3753, i8* %21, align 1
  %3754 = xor i64 %3742, %3713
  %3755 = xor i64 %3754, %3743
  %3756 = lshr i64 %3755, 4
  %3757 = trunc i64 %3756 to i8
  %3758 = and i8 %3757, 1
  store i8 %3758, i8* %27, align 1
  %3759 = icmp eq i64 %3743, 0
  %3760 = zext i1 %3759 to i8
  store i8 %3760, i8* %30, align 1
  %3761 = lshr i64 %3743, 63
  %3762 = trunc i64 %3761 to i8
  store i8 %3762, i8* %33, align 1
  %3763 = xor i64 %3761, %3729
  %3764 = add nuw nsw i64 %3763, %3761
  %3765 = icmp eq i64 %3764, 2
  %3766 = zext i1 %3765 to i8
  store i8 %3766, i8* %39, align 1
  %3767 = inttoptr i64 %3743 to i32*
  %3768 = add i64 %3709, 28
  store i64 %3768, i64* %3, align 8
  %3769 = load i32, i32* %3767, align 4
  %3770 = zext i32 %3769 to i64
  store i64 %3770, i64* %RCX.i11580, align 8
  %3771 = load i64, i64* %RBP.i, align 8
  %3772 = add i64 %3771, -140
  %3773 = add i64 %3709, 34
  store i64 %3773, i64* %3, align 8
  %3774 = inttoptr i64 %3772 to i32*
  %3775 = load i32, i32* %3774, align 4
  %3776 = add i32 %3775, %3769
  %3777 = zext i32 %3776 to i64
  store i64 %3777, i64* %RCX.i11580, align 8
  %3778 = icmp ult i32 %3776, %3769
  %3779 = icmp ult i32 %3776, %3775
  %3780 = or i1 %3778, %3779
  %3781 = zext i1 %3780 to i8
  store i8 %3781, i8* %14, align 1
  %3782 = and i32 %3776, 255
  %3783 = tail call i32 @llvm.ctpop.i32(i32 %3782)
  %3784 = trunc i32 %3783 to i8
  %3785 = and i8 %3784, 1
  %3786 = xor i8 %3785, 1
  store i8 %3786, i8* %21, align 1
  %3787 = xor i32 %3775, %3769
  %3788 = xor i32 %3787, %3776
  %3789 = lshr i32 %3788, 4
  %3790 = trunc i32 %3789 to i8
  %3791 = and i8 %3790, 1
  store i8 %3791, i8* %27, align 1
  %3792 = icmp eq i32 %3776, 0
  %3793 = zext i1 %3792 to i8
  store i8 %3793, i8* %30, align 1
  %3794 = lshr i32 %3776, 31
  %3795 = trunc i32 %3794 to i8
  store i8 %3795, i8* %33, align 1
  %3796 = lshr i32 %3769, 31
  %3797 = lshr i32 %3775, 31
  %3798 = xor i32 %3794, %3796
  %3799 = xor i32 %3794, %3797
  %3800 = add nuw nsw i32 %3798, %3799
  %3801 = icmp eq i32 %3800, 2
  %3802 = zext i1 %3801 to i8
  store i8 %3802, i8* %39, align 1
  %3803 = add i64 %3709, 40
  store i64 %3803, i64* %3, align 8
  store i32 %3776, i32* %3774, align 4
  %3804 = load i64, i64* %RBP.i, align 8
  %3805 = add i64 %3804, -8
  %3806 = load i64, i64* %3, align 8
  %3807 = add i64 %3806, 4
  store i64 %3807, i64* %3, align 8
  %3808 = inttoptr i64 %3805 to i64*
  %3809 = load i64, i64* %3808, align 8
  %3810 = add i64 %3809, 51640
  store i64 %3810, i64* %RAX.i11582.pre-phi, align 8
  %3811 = icmp ugt i64 %3809, -51641
  %3812 = zext i1 %3811 to i8
  store i8 %3812, i8* %14, align 1
  %3813 = trunc i64 %3810 to i32
  %3814 = and i32 %3813, 255
  %3815 = tail call i32 @llvm.ctpop.i32(i32 %3814)
  %3816 = trunc i32 %3815 to i8
  %3817 = and i8 %3816, 1
  %3818 = xor i8 %3817, 1
  store i8 %3818, i8* %21, align 1
  %3819 = xor i64 %3809, 16
  %3820 = xor i64 %3819, %3810
  %3821 = lshr i64 %3820, 4
  %3822 = trunc i64 %3821 to i8
  %3823 = and i8 %3822, 1
  store i8 %3823, i8* %27, align 1
  %3824 = icmp eq i64 %3810, 0
  %3825 = zext i1 %3824 to i8
  store i8 %3825, i8* %30, align 1
  %3826 = lshr i64 %3810, 63
  %3827 = trunc i64 %3826 to i8
  store i8 %3827, i8* %33, align 1
  %3828 = lshr i64 %3809, 63
  %3829 = xor i64 %3826, %3828
  %3830 = add nuw nsw i64 %3829, %3826
  %3831 = icmp eq i64 %3830, 2
  %3832 = zext i1 %3831 to i8
  store i8 %3832, i8* %39, align 1
  %3833 = add i64 %3804, -150
  %3834 = add i64 %3806, 17
  store i64 %3834, i64* %3, align 8
  %3835 = inttoptr i64 %3833 to i16*
  %3836 = load i16, i16* %3835, align 2
  %3837 = zext i16 %3836 to i64
  store i64 %3837, i64* %RCX.i11580, align 8
  %3838 = zext i16 %3836 to i64
  %3839 = shl nuw nsw i64 %3838, 4
  store i64 %3839, i64* %573, align 8
  %3840 = add i64 %3839, %3810
  store i64 %3840, i64* %RAX.i11582.pre-phi, align 8
  %3841 = icmp ult i64 %3840, %3810
  %3842 = icmp ult i64 %3840, %3839
  %3843 = or i1 %3841, %3842
  %3844 = zext i1 %3843 to i8
  store i8 %3844, i8* %14, align 1
  %3845 = trunc i64 %3840 to i32
  %3846 = and i32 %3845, 255
  %3847 = tail call i32 @llvm.ctpop.i32(i32 %3846)
  %3848 = trunc i32 %3847 to i8
  %3849 = and i8 %3848, 1
  %3850 = xor i8 %3849, 1
  store i8 %3850, i8* %21, align 1
  %3851 = xor i64 %3839, %3810
  %3852 = xor i64 %3851, %3840
  %3853 = lshr i64 %3852, 4
  %3854 = trunc i64 %3853 to i8
  %3855 = and i8 %3854, 1
  store i8 %3855, i8* %27, align 1
  %3856 = icmp eq i64 %3840, 0
  %3857 = zext i1 %3856 to i8
  store i8 %3857, i8* %30, align 1
  %3858 = lshr i64 %3840, 63
  %3859 = trunc i64 %3858 to i8
  store i8 %3859, i8* %33, align 1
  %3860 = xor i64 %3858, %3826
  %3861 = add nuw nsw i64 %3860, %3858
  %3862 = icmp eq i64 %3861, 2
  %3863 = zext i1 %3862 to i8
  store i8 %3863, i8* %39, align 1
  %3864 = add i64 %3840, 4
  %3865 = add i64 %3806, 29
  store i64 %3865, i64* %3, align 8
  %3866 = inttoptr i64 %3864 to i32*
  %3867 = load i32, i32* %3866, align 4
  %3868 = zext i32 %3867 to i64
  store i64 %3868, i64* %RCX.i11580, align 8
  %3869 = load i64, i64* %RBP.i, align 8
  %3870 = add i64 %3869, -144
  %3871 = add i64 %3806, 35
  store i64 %3871, i64* %3, align 8
  %3872 = inttoptr i64 %3870 to i32*
  %3873 = load i32, i32* %3872, align 4
  %3874 = add i32 %3873, %3867
  %3875 = zext i32 %3874 to i64
  store i64 %3875, i64* %RCX.i11580, align 8
  %3876 = icmp ult i32 %3874, %3867
  %3877 = icmp ult i32 %3874, %3873
  %3878 = or i1 %3876, %3877
  %3879 = zext i1 %3878 to i8
  store i8 %3879, i8* %14, align 1
  %3880 = and i32 %3874, 255
  %3881 = tail call i32 @llvm.ctpop.i32(i32 %3880)
  %3882 = trunc i32 %3881 to i8
  %3883 = and i8 %3882, 1
  %3884 = xor i8 %3883, 1
  store i8 %3884, i8* %21, align 1
  %3885 = xor i32 %3873, %3867
  %3886 = xor i32 %3885, %3874
  %3887 = lshr i32 %3886, 4
  %3888 = trunc i32 %3887 to i8
  %3889 = and i8 %3888, 1
  store i8 %3889, i8* %27, align 1
  %3890 = icmp eq i32 %3874, 0
  %3891 = zext i1 %3890 to i8
  store i8 %3891, i8* %30, align 1
  %3892 = lshr i32 %3874, 31
  %3893 = trunc i32 %3892 to i8
  store i8 %3893, i8* %33, align 1
  %3894 = lshr i32 %3867, 31
  %3895 = lshr i32 %3873, 31
  %3896 = xor i32 %3892, %3894
  %3897 = xor i32 %3892, %3895
  %3898 = add nuw nsw i32 %3896, %3897
  %3899 = icmp eq i32 %3898, 2
  %3900 = zext i1 %3899 to i8
  store i8 %3900, i8* %39, align 1
  %3901 = add i64 %3806, 41
  store i64 %3901, i64* %3, align 8
  store i32 %3874, i32* %3872, align 4
  %3902 = load i64, i64* %RBP.i, align 8
  %3903 = add i64 %3902, -8
  %3904 = load i64, i64* %3, align 8
  %3905 = add i64 %3904, 4
  store i64 %3905, i64* %3, align 8
  %3906 = inttoptr i64 %3903 to i64*
  %3907 = load i64, i64* %3906, align 8
  %3908 = add i64 %3907, 51640
  store i64 %3908, i64* %RAX.i11582.pre-phi, align 8
  %3909 = icmp ugt i64 %3907, -51641
  %3910 = zext i1 %3909 to i8
  store i8 %3910, i8* %14, align 1
  %3911 = trunc i64 %3908 to i32
  %3912 = and i32 %3911, 255
  %3913 = tail call i32 @llvm.ctpop.i32(i32 %3912)
  %3914 = trunc i32 %3913 to i8
  %3915 = and i8 %3914, 1
  %3916 = xor i8 %3915, 1
  store i8 %3916, i8* %21, align 1
  %3917 = xor i64 %3907, 16
  %3918 = xor i64 %3917, %3908
  %3919 = lshr i64 %3918, 4
  %3920 = trunc i64 %3919 to i8
  %3921 = and i8 %3920, 1
  store i8 %3921, i8* %27, align 1
  %3922 = icmp eq i64 %3908, 0
  %3923 = zext i1 %3922 to i8
  store i8 %3923, i8* %30, align 1
  %3924 = lshr i64 %3908, 63
  %3925 = trunc i64 %3924 to i8
  store i8 %3925, i8* %33, align 1
  %3926 = lshr i64 %3907, 63
  %3927 = xor i64 %3924, %3926
  %3928 = add nuw nsw i64 %3927, %3924
  %3929 = icmp eq i64 %3928, 2
  %3930 = zext i1 %3929 to i8
  store i8 %3930, i8* %39, align 1
  %3931 = add i64 %3902, -150
  %3932 = add i64 %3904, 17
  store i64 %3932, i64* %3, align 8
  %3933 = inttoptr i64 %3931 to i16*
  %3934 = load i16, i16* %3933, align 2
  %3935 = zext i16 %3934 to i64
  store i64 %3935, i64* %RCX.i11580, align 8
  %3936 = zext i16 %3934 to i64
  %3937 = shl nuw nsw i64 %3936, 4
  store i64 %3937, i64* %573, align 8
  %3938 = add i64 %3937, %3908
  store i64 %3938, i64* %RAX.i11582.pre-phi, align 8
  %3939 = icmp ult i64 %3938, %3908
  %3940 = icmp ult i64 %3938, %3937
  %3941 = or i1 %3939, %3940
  %3942 = zext i1 %3941 to i8
  store i8 %3942, i8* %14, align 1
  %3943 = trunc i64 %3938 to i32
  %3944 = and i32 %3943, 255
  %3945 = tail call i32 @llvm.ctpop.i32(i32 %3944)
  %3946 = trunc i32 %3945 to i8
  %3947 = and i8 %3946, 1
  %3948 = xor i8 %3947, 1
  store i8 %3948, i8* %21, align 1
  %3949 = xor i64 %3937, %3908
  %3950 = xor i64 %3949, %3938
  %3951 = lshr i64 %3950, 4
  %3952 = trunc i64 %3951 to i8
  %3953 = and i8 %3952, 1
  store i8 %3953, i8* %27, align 1
  %3954 = icmp eq i64 %3938, 0
  %3955 = zext i1 %3954 to i8
  store i8 %3955, i8* %30, align 1
  %3956 = lshr i64 %3938, 63
  %3957 = trunc i64 %3956 to i8
  store i8 %3957, i8* %33, align 1
  %3958 = xor i64 %3956, %3924
  %3959 = add nuw nsw i64 %3958, %3956
  %3960 = icmp eq i64 %3959, 2
  %3961 = zext i1 %3960 to i8
  store i8 %3961, i8* %39, align 1
  %3962 = add i64 %3938, 8
  %3963 = add i64 %3904, 29
  store i64 %3963, i64* %3, align 8
  %3964 = inttoptr i64 %3962 to i32*
  %3965 = load i32, i32* %3964, align 4
  %3966 = zext i32 %3965 to i64
  store i64 %3966, i64* %RCX.i11580, align 8
  %3967 = load i64, i64* %RBP.i, align 8
  %3968 = add i64 %3967, -148
  %3969 = add i64 %3904, 35
  store i64 %3969, i64* %3, align 8
  %3970 = inttoptr i64 %3968 to i32*
  %3971 = load i32, i32* %3970, align 4
  %3972 = add i32 %3971, %3965
  %3973 = zext i32 %3972 to i64
  store i64 %3973, i64* %RCX.i11580, align 8
  %3974 = icmp ult i32 %3972, %3965
  %3975 = icmp ult i32 %3972, %3971
  %3976 = or i1 %3974, %3975
  %3977 = zext i1 %3976 to i8
  store i8 %3977, i8* %14, align 1
  %3978 = and i32 %3972, 255
  %3979 = tail call i32 @llvm.ctpop.i32(i32 %3978)
  %3980 = trunc i32 %3979 to i8
  %3981 = and i8 %3980, 1
  %3982 = xor i8 %3981, 1
  store i8 %3982, i8* %21, align 1
  %3983 = xor i32 %3971, %3965
  %3984 = xor i32 %3983, %3972
  %3985 = lshr i32 %3984, 4
  %3986 = trunc i32 %3985 to i8
  %3987 = and i8 %3986, 1
  store i8 %3987, i8* %27, align 1
  %3988 = icmp eq i32 %3972, 0
  %3989 = zext i1 %3988 to i8
  store i8 %3989, i8* %30, align 1
  %3990 = lshr i32 %3972, 31
  %3991 = trunc i32 %3990 to i8
  store i8 %3991, i8* %33, align 1
  %3992 = lshr i32 %3965, 31
  %3993 = lshr i32 %3971, 31
  %3994 = xor i32 %3990, %3992
  %3995 = xor i32 %3990, %3993
  %3996 = add nuw nsw i32 %3994, %3995
  %3997 = icmp eq i32 %3996, 2
  %3998 = zext i1 %3997 to i8
  store i8 %3998, i8* %39, align 1
  %3999 = add i64 %3904, 41
  store i64 %3999, i64* %3, align 8
  store i32 %3972, i32* %3970, align 4
  %4000 = load i64, i64* %RBP.i, align 8
  %4001 = add i64 %4000, -120
  %4002 = load i64, i64* %3, align 8
  %4003 = add i64 %4002, 4
  store i64 %4003, i64* %3, align 8
  %4004 = inttoptr i64 %4001 to i64*
  %4005 = load i64, i64* %4004, align 8
  store i64 %4005, i64* %RAX.i11582.pre-phi, align 8
  %4006 = add i64 %4000, -28
  %4007 = add i64 %4002, 7
  store i64 %4007, i64* %3, align 8
  %4008 = inttoptr i64 %4006 to i32*
  %4009 = load i32, i32* %4008, align 4
  %4010 = add i32 %4009, 4
  %4011 = zext i32 %4010 to i64
  store i64 %4011, i64* %RCX.i11580, align 8
  %4012 = icmp ugt i32 %4009, -5
  %4013 = zext i1 %4012 to i8
  store i8 %4013, i8* %14, align 1
  %4014 = and i32 %4010, 255
  %4015 = tail call i32 @llvm.ctpop.i32(i32 %4014)
  %4016 = trunc i32 %4015 to i8
  %4017 = and i8 %4016, 1
  %4018 = xor i8 %4017, 1
  store i8 %4018, i8* %21, align 1
  %4019 = xor i32 %4010, %4009
  %4020 = lshr i32 %4019, 4
  %4021 = trunc i32 %4020 to i8
  %4022 = and i8 %4021, 1
  store i8 %4022, i8* %27, align 1
  %4023 = icmp eq i32 %4010, 0
  %4024 = zext i1 %4023 to i8
  store i8 %4024, i8* %30, align 1
  %4025 = lshr i32 %4010, 31
  %4026 = trunc i32 %4025 to i8
  store i8 %4026, i8* %33, align 1
  %4027 = lshr i32 %4009, 31
  %4028 = xor i32 %4025, %4027
  %4029 = add nuw nsw i32 %4028, %4025
  %4030 = icmp eq i32 %4029, 2
  %4031 = zext i1 %4030 to i8
  store i8 %4031, i8* %39, align 1
  %4032 = sext i32 %4010 to i64
  store i64 %4032, i64* %573, align 8
  %4033 = shl nsw i64 %4032, 1
  %4034 = add i64 %4005, %4033
  %4035 = add i64 %4002, 17
  store i64 %4035, i64* %3, align 8
  %4036 = inttoptr i64 %4034 to i16*
  %4037 = load i16, i16* %4036, align 2
  store i16 %4037, i16* %SI.i, align 2
  %4038 = add i64 %4000, -150
  %4039 = add i64 %4002, 24
  store i64 %4039, i64* %3, align 8
  %4040 = inttoptr i64 %4038 to i16*
  store i16 %4037, i16* %4040, align 2
  %4041 = load i64, i64* %RBP.i, align 8
  %4042 = add i64 %4041, -8
  %4043 = load i64, i64* %3, align 8
  %4044 = add i64 %4043, 4
  store i64 %4044, i64* %3, align 8
  %4045 = inttoptr i64 %4042 to i64*
  %4046 = load i64, i64* %4045, align 8
  %4047 = add i64 %4046, 51640
  store i64 %4047, i64* %RAX.i11582.pre-phi, align 8
  %4048 = icmp ugt i64 %4046, -51641
  %4049 = zext i1 %4048 to i8
  store i8 %4049, i8* %14, align 1
  %4050 = trunc i64 %4047 to i32
  %4051 = and i32 %4050, 255
  %4052 = tail call i32 @llvm.ctpop.i32(i32 %4051)
  %4053 = trunc i32 %4052 to i8
  %4054 = and i8 %4053, 1
  %4055 = xor i8 %4054, 1
  store i8 %4055, i8* %21, align 1
  %4056 = xor i64 %4046, 16
  %4057 = xor i64 %4056, %4047
  %4058 = lshr i64 %4057, 4
  %4059 = trunc i64 %4058 to i8
  %4060 = and i8 %4059, 1
  store i8 %4060, i8* %27, align 1
  %4061 = icmp eq i64 %4047, 0
  %4062 = zext i1 %4061 to i8
  store i8 %4062, i8* %30, align 1
  %4063 = lshr i64 %4047, 63
  %4064 = trunc i64 %4063 to i8
  store i8 %4064, i8* %33, align 1
  %4065 = lshr i64 %4046, 63
  %4066 = xor i64 %4063, %4065
  %4067 = add nuw nsw i64 %4066, %4063
  %4068 = icmp eq i64 %4067, 2
  %4069 = zext i1 %4068 to i8
  store i8 %4069, i8* %39, align 1
  %4070 = add i64 %4041, -150
  %4071 = add i64 %4043, 17
  store i64 %4071, i64* %3, align 8
  %4072 = inttoptr i64 %4070 to i16*
  %4073 = load i16, i16* %4072, align 2
  %4074 = zext i16 %4073 to i64
  store i64 %4074, i64* %RCX.i11580, align 8
  %4075 = zext i16 %4073 to i64
  %4076 = shl nuw nsw i64 %4075, 4
  store i64 %4076, i64* %573, align 8
  %4077 = add i64 %4076, %4047
  store i64 %4077, i64* %RAX.i11582.pre-phi, align 8
  %4078 = icmp ult i64 %4077, %4047
  %4079 = icmp ult i64 %4077, %4076
  %4080 = or i1 %4078, %4079
  %4081 = zext i1 %4080 to i8
  store i8 %4081, i8* %14, align 1
  %4082 = trunc i64 %4077 to i32
  %4083 = and i32 %4082, 255
  %4084 = tail call i32 @llvm.ctpop.i32(i32 %4083)
  %4085 = trunc i32 %4084 to i8
  %4086 = and i8 %4085, 1
  %4087 = xor i8 %4086, 1
  store i8 %4087, i8* %21, align 1
  %4088 = xor i64 %4076, %4047
  %4089 = xor i64 %4088, %4077
  %4090 = lshr i64 %4089, 4
  %4091 = trunc i64 %4090 to i8
  %4092 = and i8 %4091, 1
  store i8 %4092, i8* %27, align 1
  %4093 = icmp eq i64 %4077, 0
  %4094 = zext i1 %4093 to i8
  store i8 %4094, i8* %30, align 1
  %4095 = lshr i64 %4077, 63
  %4096 = trunc i64 %4095 to i8
  store i8 %4096, i8* %33, align 1
  %4097 = xor i64 %4095, %4063
  %4098 = add nuw nsw i64 %4097, %4095
  %4099 = icmp eq i64 %4098, 2
  %4100 = zext i1 %4099 to i8
  store i8 %4100, i8* %39, align 1
  %4101 = inttoptr i64 %4077 to i32*
  %4102 = add i64 %4043, 28
  store i64 %4102, i64* %3, align 8
  %4103 = load i32, i32* %4101, align 4
  %4104 = zext i32 %4103 to i64
  store i64 %4104, i64* %RCX.i11580, align 8
  %4105 = load i64, i64* %RBP.i, align 8
  %4106 = add i64 %4105, -140
  %4107 = add i64 %4043, 34
  store i64 %4107, i64* %3, align 8
  %4108 = inttoptr i64 %4106 to i32*
  %4109 = load i32, i32* %4108, align 4
  %4110 = add i32 %4109, %4103
  %4111 = zext i32 %4110 to i64
  store i64 %4111, i64* %RCX.i11580, align 8
  %4112 = icmp ult i32 %4110, %4103
  %4113 = icmp ult i32 %4110, %4109
  %4114 = or i1 %4112, %4113
  %4115 = zext i1 %4114 to i8
  store i8 %4115, i8* %14, align 1
  %4116 = and i32 %4110, 255
  %4117 = tail call i32 @llvm.ctpop.i32(i32 %4116)
  %4118 = trunc i32 %4117 to i8
  %4119 = and i8 %4118, 1
  %4120 = xor i8 %4119, 1
  store i8 %4120, i8* %21, align 1
  %4121 = xor i32 %4109, %4103
  %4122 = xor i32 %4121, %4110
  %4123 = lshr i32 %4122, 4
  %4124 = trunc i32 %4123 to i8
  %4125 = and i8 %4124, 1
  store i8 %4125, i8* %27, align 1
  %4126 = icmp eq i32 %4110, 0
  %4127 = zext i1 %4126 to i8
  store i8 %4127, i8* %30, align 1
  %4128 = lshr i32 %4110, 31
  %4129 = trunc i32 %4128 to i8
  store i8 %4129, i8* %33, align 1
  %4130 = lshr i32 %4103, 31
  %4131 = lshr i32 %4109, 31
  %4132 = xor i32 %4128, %4130
  %4133 = xor i32 %4128, %4131
  %4134 = add nuw nsw i32 %4132, %4133
  %4135 = icmp eq i32 %4134, 2
  %4136 = zext i1 %4135 to i8
  store i8 %4136, i8* %39, align 1
  %4137 = add i64 %4043, 40
  store i64 %4137, i64* %3, align 8
  store i32 %4110, i32* %4108, align 4
  %4138 = load i64, i64* %RBP.i, align 8
  %4139 = add i64 %4138, -8
  %4140 = load i64, i64* %3, align 8
  %4141 = add i64 %4140, 4
  store i64 %4141, i64* %3, align 8
  %4142 = inttoptr i64 %4139 to i64*
  %4143 = load i64, i64* %4142, align 8
  %4144 = add i64 %4143, 51640
  store i64 %4144, i64* %RAX.i11582.pre-phi, align 8
  %4145 = icmp ugt i64 %4143, -51641
  %4146 = zext i1 %4145 to i8
  store i8 %4146, i8* %14, align 1
  %4147 = trunc i64 %4144 to i32
  %4148 = and i32 %4147, 255
  %4149 = tail call i32 @llvm.ctpop.i32(i32 %4148)
  %4150 = trunc i32 %4149 to i8
  %4151 = and i8 %4150, 1
  %4152 = xor i8 %4151, 1
  store i8 %4152, i8* %21, align 1
  %4153 = xor i64 %4143, 16
  %4154 = xor i64 %4153, %4144
  %4155 = lshr i64 %4154, 4
  %4156 = trunc i64 %4155 to i8
  %4157 = and i8 %4156, 1
  store i8 %4157, i8* %27, align 1
  %4158 = icmp eq i64 %4144, 0
  %4159 = zext i1 %4158 to i8
  store i8 %4159, i8* %30, align 1
  %4160 = lshr i64 %4144, 63
  %4161 = trunc i64 %4160 to i8
  store i8 %4161, i8* %33, align 1
  %4162 = lshr i64 %4143, 63
  %4163 = xor i64 %4160, %4162
  %4164 = add nuw nsw i64 %4163, %4160
  %4165 = icmp eq i64 %4164, 2
  %4166 = zext i1 %4165 to i8
  store i8 %4166, i8* %39, align 1
  %4167 = add i64 %4138, -150
  %4168 = add i64 %4140, 17
  store i64 %4168, i64* %3, align 8
  %4169 = inttoptr i64 %4167 to i16*
  %4170 = load i16, i16* %4169, align 2
  %4171 = zext i16 %4170 to i64
  store i64 %4171, i64* %RCX.i11580, align 8
  %4172 = zext i16 %4170 to i64
  %4173 = shl nuw nsw i64 %4172, 4
  store i64 %4173, i64* %573, align 8
  %4174 = add i64 %4173, %4144
  store i64 %4174, i64* %RAX.i11582.pre-phi, align 8
  %4175 = icmp ult i64 %4174, %4144
  %4176 = icmp ult i64 %4174, %4173
  %4177 = or i1 %4175, %4176
  %4178 = zext i1 %4177 to i8
  store i8 %4178, i8* %14, align 1
  %4179 = trunc i64 %4174 to i32
  %4180 = and i32 %4179, 255
  %4181 = tail call i32 @llvm.ctpop.i32(i32 %4180)
  %4182 = trunc i32 %4181 to i8
  %4183 = and i8 %4182, 1
  %4184 = xor i8 %4183, 1
  store i8 %4184, i8* %21, align 1
  %4185 = xor i64 %4173, %4144
  %4186 = xor i64 %4185, %4174
  %4187 = lshr i64 %4186, 4
  %4188 = trunc i64 %4187 to i8
  %4189 = and i8 %4188, 1
  store i8 %4189, i8* %27, align 1
  %4190 = icmp eq i64 %4174, 0
  %4191 = zext i1 %4190 to i8
  store i8 %4191, i8* %30, align 1
  %4192 = lshr i64 %4174, 63
  %4193 = trunc i64 %4192 to i8
  store i8 %4193, i8* %33, align 1
  %4194 = xor i64 %4192, %4160
  %4195 = add nuw nsw i64 %4194, %4192
  %4196 = icmp eq i64 %4195, 2
  %4197 = zext i1 %4196 to i8
  store i8 %4197, i8* %39, align 1
  %4198 = add i64 %4174, 4
  %4199 = add i64 %4140, 29
  store i64 %4199, i64* %3, align 8
  %4200 = inttoptr i64 %4198 to i32*
  %4201 = load i32, i32* %4200, align 4
  %4202 = zext i32 %4201 to i64
  store i64 %4202, i64* %RCX.i11580, align 8
  %4203 = load i64, i64* %RBP.i, align 8
  %4204 = add i64 %4203, -144
  %4205 = add i64 %4140, 35
  store i64 %4205, i64* %3, align 8
  %4206 = inttoptr i64 %4204 to i32*
  %4207 = load i32, i32* %4206, align 4
  %4208 = add i32 %4207, %4201
  %4209 = zext i32 %4208 to i64
  store i64 %4209, i64* %RCX.i11580, align 8
  %4210 = icmp ult i32 %4208, %4201
  %4211 = icmp ult i32 %4208, %4207
  %4212 = or i1 %4210, %4211
  %4213 = zext i1 %4212 to i8
  store i8 %4213, i8* %14, align 1
  %4214 = and i32 %4208, 255
  %4215 = tail call i32 @llvm.ctpop.i32(i32 %4214)
  %4216 = trunc i32 %4215 to i8
  %4217 = and i8 %4216, 1
  %4218 = xor i8 %4217, 1
  store i8 %4218, i8* %21, align 1
  %4219 = xor i32 %4207, %4201
  %4220 = xor i32 %4219, %4208
  %4221 = lshr i32 %4220, 4
  %4222 = trunc i32 %4221 to i8
  %4223 = and i8 %4222, 1
  store i8 %4223, i8* %27, align 1
  %4224 = icmp eq i32 %4208, 0
  %4225 = zext i1 %4224 to i8
  store i8 %4225, i8* %30, align 1
  %4226 = lshr i32 %4208, 31
  %4227 = trunc i32 %4226 to i8
  store i8 %4227, i8* %33, align 1
  %4228 = lshr i32 %4201, 31
  %4229 = lshr i32 %4207, 31
  %4230 = xor i32 %4226, %4228
  %4231 = xor i32 %4226, %4229
  %4232 = add nuw nsw i32 %4230, %4231
  %4233 = icmp eq i32 %4232, 2
  %4234 = zext i1 %4233 to i8
  store i8 %4234, i8* %39, align 1
  %4235 = add i64 %4140, 41
  store i64 %4235, i64* %3, align 8
  store i32 %4208, i32* %4206, align 4
  %4236 = load i64, i64* %RBP.i, align 8
  %4237 = add i64 %4236, -8
  %4238 = load i64, i64* %3, align 8
  %4239 = add i64 %4238, 4
  store i64 %4239, i64* %3, align 8
  %4240 = inttoptr i64 %4237 to i64*
  %4241 = load i64, i64* %4240, align 8
  %4242 = add i64 %4241, 51640
  store i64 %4242, i64* %RAX.i11582.pre-phi, align 8
  %4243 = icmp ugt i64 %4241, -51641
  %4244 = zext i1 %4243 to i8
  store i8 %4244, i8* %14, align 1
  %4245 = trunc i64 %4242 to i32
  %4246 = and i32 %4245, 255
  %4247 = tail call i32 @llvm.ctpop.i32(i32 %4246)
  %4248 = trunc i32 %4247 to i8
  %4249 = and i8 %4248, 1
  %4250 = xor i8 %4249, 1
  store i8 %4250, i8* %21, align 1
  %4251 = xor i64 %4241, 16
  %4252 = xor i64 %4251, %4242
  %4253 = lshr i64 %4252, 4
  %4254 = trunc i64 %4253 to i8
  %4255 = and i8 %4254, 1
  store i8 %4255, i8* %27, align 1
  %4256 = icmp eq i64 %4242, 0
  %4257 = zext i1 %4256 to i8
  store i8 %4257, i8* %30, align 1
  %4258 = lshr i64 %4242, 63
  %4259 = trunc i64 %4258 to i8
  store i8 %4259, i8* %33, align 1
  %4260 = lshr i64 %4241, 63
  %4261 = xor i64 %4258, %4260
  %4262 = add nuw nsw i64 %4261, %4258
  %4263 = icmp eq i64 %4262, 2
  %4264 = zext i1 %4263 to i8
  store i8 %4264, i8* %39, align 1
  %4265 = add i64 %4236, -150
  %4266 = add i64 %4238, 17
  store i64 %4266, i64* %3, align 8
  %4267 = inttoptr i64 %4265 to i16*
  %4268 = load i16, i16* %4267, align 2
  %4269 = zext i16 %4268 to i64
  store i64 %4269, i64* %RCX.i11580, align 8
  %4270 = zext i16 %4268 to i64
  %4271 = shl nuw nsw i64 %4270, 4
  store i64 %4271, i64* %573, align 8
  %4272 = add i64 %4271, %4242
  store i64 %4272, i64* %RAX.i11582.pre-phi, align 8
  %4273 = icmp ult i64 %4272, %4242
  %4274 = icmp ult i64 %4272, %4271
  %4275 = or i1 %4273, %4274
  %4276 = zext i1 %4275 to i8
  store i8 %4276, i8* %14, align 1
  %4277 = trunc i64 %4272 to i32
  %4278 = and i32 %4277, 255
  %4279 = tail call i32 @llvm.ctpop.i32(i32 %4278)
  %4280 = trunc i32 %4279 to i8
  %4281 = and i8 %4280, 1
  %4282 = xor i8 %4281, 1
  store i8 %4282, i8* %21, align 1
  %4283 = xor i64 %4271, %4242
  %4284 = xor i64 %4283, %4272
  %4285 = lshr i64 %4284, 4
  %4286 = trunc i64 %4285 to i8
  %4287 = and i8 %4286, 1
  store i8 %4287, i8* %27, align 1
  %4288 = icmp eq i64 %4272, 0
  %4289 = zext i1 %4288 to i8
  store i8 %4289, i8* %30, align 1
  %4290 = lshr i64 %4272, 63
  %4291 = trunc i64 %4290 to i8
  store i8 %4291, i8* %33, align 1
  %4292 = xor i64 %4290, %4258
  %4293 = add nuw nsw i64 %4292, %4290
  %4294 = icmp eq i64 %4293, 2
  %4295 = zext i1 %4294 to i8
  store i8 %4295, i8* %39, align 1
  %4296 = add i64 %4272, 8
  %4297 = add i64 %4238, 29
  store i64 %4297, i64* %3, align 8
  %4298 = inttoptr i64 %4296 to i32*
  %4299 = load i32, i32* %4298, align 4
  %4300 = zext i32 %4299 to i64
  store i64 %4300, i64* %RCX.i11580, align 8
  %4301 = load i64, i64* %RBP.i, align 8
  %4302 = add i64 %4301, -148
  %4303 = add i64 %4238, 35
  store i64 %4303, i64* %3, align 8
  %4304 = inttoptr i64 %4302 to i32*
  %4305 = load i32, i32* %4304, align 4
  %4306 = add i32 %4305, %4299
  %4307 = zext i32 %4306 to i64
  store i64 %4307, i64* %RCX.i11580, align 8
  %4308 = icmp ult i32 %4306, %4299
  %4309 = icmp ult i32 %4306, %4305
  %4310 = or i1 %4308, %4309
  %4311 = zext i1 %4310 to i8
  store i8 %4311, i8* %14, align 1
  %4312 = and i32 %4306, 255
  %4313 = tail call i32 @llvm.ctpop.i32(i32 %4312)
  %4314 = trunc i32 %4313 to i8
  %4315 = and i8 %4314, 1
  %4316 = xor i8 %4315, 1
  store i8 %4316, i8* %21, align 1
  %4317 = xor i32 %4305, %4299
  %4318 = xor i32 %4317, %4306
  %4319 = lshr i32 %4318, 4
  %4320 = trunc i32 %4319 to i8
  %4321 = and i8 %4320, 1
  store i8 %4321, i8* %27, align 1
  %4322 = icmp eq i32 %4306, 0
  %4323 = zext i1 %4322 to i8
  store i8 %4323, i8* %30, align 1
  %4324 = lshr i32 %4306, 31
  %4325 = trunc i32 %4324 to i8
  store i8 %4325, i8* %33, align 1
  %4326 = lshr i32 %4299, 31
  %4327 = lshr i32 %4305, 31
  %4328 = xor i32 %4324, %4326
  %4329 = xor i32 %4324, %4327
  %4330 = add nuw nsw i32 %4328, %4329
  %4331 = icmp eq i32 %4330, 2
  %4332 = zext i1 %4331 to i8
  store i8 %4332, i8* %39, align 1
  %4333 = add i64 %4238, 41
  store i64 %4333, i64* %3, align 8
  store i32 %4306, i32* %4304, align 4
  %4334 = load i64, i64* %RBP.i, align 8
  %4335 = add i64 %4334, -120
  %4336 = load i64, i64* %3, align 8
  %4337 = add i64 %4336, 4
  store i64 %4337, i64* %3, align 8
  %4338 = inttoptr i64 %4335 to i64*
  %4339 = load i64, i64* %4338, align 8
  store i64 %4339, i64* %RAX.i11582.pre-phi, align 8
  %4340 = add i64 %4334, -28
  %4341 = add i64 %4336, 7
  store i64 %4341, i64* %3, align 8
  %4342 = inttoptr i64 %4340 to i32*
  %4343 = load i32, i32* %4342, align 4
  %4344 = add i32 %4343, 5
  %4345 = zext i32 %4344 to i64
  store i64 %4345, i64* %RCX.i11580, align 8
  %4346 = icmp ugt i32 %4343, -6
  %4347 = zext i1 %4346 to i8
  store i8 %4347, i8* %14, align 1
  %4348 = and i32 %4344, 255
  %4349 = tail call i32 @llvm.ctpop.i32(i32 %4348)
  %4350 = trunc i32 %4349 to i8
  %4351 = and i8 %4350, 1
  %4352 = xor i8 %4351, 1
  store i8 %4352, i8* %21, align 1
  %4353 = xor i32 %4344, %4343
  %4354 = lshr i32 %4353, 4
  %4355 = trunc i32 %4354 to i8
  %4356 = and i8 %4355, 1
  store i8 %4356, i8* %27, align 1
  %4357 = icmp eq i32 %4344, 0
  %4358 = zext i1 %4357 to i8
  store i8 %4358, i8* %30, align 1
  %4359 = lshr i32 %4344, 31
  %4360 = trunc i32 %4359 to i8
  store i8 %4360, i8* %33, align 1
  %4361 = lshr i32 %4343, 31
  %4362 = xor i32 %4359, %4361
  %4363 = add nuw nsw i32 %4362, %4359
  %4364 = icmp eq i32 %4363, 2
  %4365 = zext i1 %4364 to i8
  store i8 %4365, i8* %39, align 1
  %4366 = sext i32 %4344 to i64
  store i64 %4366, i64* %573, align 8
  %4367 = shl nsw i64 %4366, 1
  %4368 = add i64 %4339, %4367
  %4369 = add i64 %4336, 17
  store i64 %4369, i64* %3, align 8
  %4370 = inttoptr i64 %4368 to i16*
  %4371 = load i16, i16* %4370, align 2
  store i16 %4371, i16* %SI.i, align 2
  %4372 = add i64 %4334, -150
  %4373 = add i64 %4336, 24
  store i64 %4373, i64* %3, align 8
  %4374 = inttoptr i64 %4372 to i16*
  store i16 %4371, i16* %4374, align 2
  %4375 = load i64, i64* %RBP.i, align 8
  %4376 = add i64 %4375, -8
  %4377 = load i64, i64* %3, align 8
  %4378 = add i64 %4377, 4
  store i64 %4378, i64* %3, align 8
  %4379 = inttoptr i64 %4376 to i64*
  %4380 = load i64, i64* %4379, align 8
  %4381 = add i64 %4380, 51640
  store i64 %4381, i64* %RAX.i11582.pre-phi, align 8
  %4382 = icmp ugt i64 %4380, -51641
  %4383 = zext i1 %4382 to i8
  store i8 %4383, i8* %14, align 1
  %4384 = trunc i64 %4381 to i32
  %4385 = and i32 %4384, 255
  %4386 = tail call i32 @llvm.ctpop.i32(i32 %4385)
  %4387 = trunc i32 %4386 to i8
  %4388 = and i8 %4387, 1
  %4389 = xor i8 %4388, 1
  store i8 %4389, i8* %21, align 1
  %4390 = xor i64 %4380, 16
  %4391 = xor i64 %4390, %4381
  %4392 = lshr i64 %4391, 4
  %4393 = trunc i64 %4392 to i8
  %4394 = and i8 %4393, 1
  store i8 %4394, i8* %27, align 1
  %4395 = icmp eq i64 %4381, 0
  %4396 = zext i1 %4395 to i8
  store i8 %4396, i8* %30, align 1
  %4397 = lshr i64 %4381, 63
  %4398 = trunc i64 %4397 to i8
  store i8 %4398, i8* %33, align 1
  %4399 = lshr i64 %4380, 63
  %4400 = xor i64 %4397, %4399
  %4401 = add nuw nsw i64 %4400, %4397
  %4402 = icmp eq i64 %4401, 2
  %4403 = zext i1 %4402 to i8
  store i8 %4403, i8* %39, align 1
  %4404 = add i64 %4375, -150
  %4405 = add i64 %4377, 17
  store i64 %4405, i64* %3, align 8
  %4406 = inttoptr i64 %4404 to i16*
  %4407 = load i16, i16* %4406, align 2
  %4408 = zext i16 %4407 to i64
  store i64 %4408, i64* %RCX.i11580, align 8
  %4409 = zext i16 %4407 to i64
  %4410 = shl nuw nsw i64 %4409, 4
  store i64 %4410, i64* %573, align 8
  %4411 = add i64 %4410, %4381
  store i64 %4411, i64* %RAX.i11582.pre-phi, align 8
  %4412 = icmp ult i64 %4411, %4381
  %4413 = icmp ult i64 %4411, %4410
  %4414 = or i1 %4412, %4413
  %4415 = zext i1 %4414 to i8
  store i8 %4415, i8* %14, align 1
  %4416 = trunc i64 %4411 to i32
  %4417 = and i32 %4416, 255
  %4418 = tail call i32 @llvm.ctpop.i32(i32 %4417)
  %4419 = trunc i32 %4418 to i8
  %4420 = and i8 %4419, 1
  %4421 = xor i8 %4420, 1
  store i8 %4421, i8* %21, align 1
  %4422 = xor i64 %4410, %4381
  %4423 = xor i64 %4422, %4411
  %4424 = lshr i64 %4423, 4
  %4425 = trunc i64 %4424 to i8
  %4426 = and i8 %4425, 1
  store i8 %4426, i8* %27, align 1
  %4427 = icmp eq i64 %4411, 0
  %4428 = zext i1 %4427 to i8
  store i8 %4428, i8* %30, align 1
  %4429 = lshr i64 %4411, 63
  %4430 = trunc i64 %4429 to i8
  store i8 %4430, i8* %33, align 1
  %4431 = xor i64 %4429, %4397
  %4432 = add nuw nsw i64 %4431, %4429
  %4433 = icmp eq i64 %4432, 2
  %4434 = zext i1 %4433 to i8
  store i8 %4434, i8* %39, align 1
  %4435 = inttoptr i64 %4411 to i32*
  %4436 = add i64 %4377, 28
  store i64 %4436, i64* %3, align 8
  %4437 = load i32, i32* %4435, align 4
  %4438 = zext i32 %4437 to i64
  store i64 %4438, i64* %RCX.i11580, align 8
  %4439 = load i64, i64* %RBP.i, align 8
  %4440 = add i64 %4439, -140
  %4441 = add i64 %4377, 34
  store i64 %4441, i64* %3, align 8
  %4442 = inttoptr i64 %4440 to i32*
  %4443 = load i32, i32* %4442, align 4
  %4444 = add i32 %4443, %4437
  %4445 = zext i32 %4444 to i64
  store i64 %4445, i64* %RCX.i11580, align 8
  %4446 = icmp ult i32 %4444, %4437
  %4447 = icmp ult i32 %4444, %4443
  %4448 = or i1 %4446, %4447
  %4449 = zext i1 %4448 to i8
  store i8 %4449, i8* %14, align 1
  %4450 = and i32 %4444, 255
  %4451 = tail call i32 @llvm.ctpop.i32(i32 %4450)
  %4452 = trunc i32 %4451 to i8
  %4453 = and i8 %4452, 1
  %4454 = xor i8 %4453, 1
  store i8 %4454, i8* %21, align 1
  %4455 = xor i32 %4443, %4437
  %4456 = xor i32 %4455, %4444
  %4457 = lshr i32 %4456, 4
  %4458 = trunc i32 %4457 to i8
  %4459 = and i8 %4458, 1
  store i8 %4459, i8* %27, align 1
  %4460 = icmp eq i32 %4444, 0
  %4461 = zext i1 %4460 to i8
  store i8 %4461, i8* %30, align 1
  %4462 = lshr i32 %4444, 31
  %4463 = trunc i32 %4462 to i8
  store i8 %4463, i8* %33, align 1
  %4464 = lshr i32 %4437, 31
  %4465 = lshr i32 %4443, 31
  %4466 = xor i32 %4462, %4464
  %4467 = xor i32 %4462, %4465
  %4468 = add nuw nsw i32 %4466, %4467
  %4469 = icmp eq i32 %4468, 2
  %4470 = zext i1 %4469 to i8
  store i8 %4470, i8* %39, align 1
  %4471 = add i64 %4377, 40
  store i64 %4471, i64* %3, align 8
  store i32 %4444, i32* %4442, align 4
  %4472 = load i64, i64* %RBP.i, align 8
  %4473 = add i64 %4472, -8
  %4474 = load i64, i64* %3, align 8
  %4475 = add i64 %4474, 4
  store i64 %4475, i64* %3, align 8
  %4476 = inttoptr i64 %4473 to i64*
  %4477 = load i64, i64* %4476, align 8
  %4478 = add i64 %4477, 51640
  store i64 %4478, i64* %RAX.i11582.pre-phi, align 8
  %4479 = icmp ugt i64 %4477, -51641
  %4480 = zext i1 %4479 to i8
  store i8 %4480, i8* %14, align 1
  %4481 = trunc i64 %4478 to i32
  %4482 = and i32 %4481, 255
  %4483 = tail call i32 @llvm.ctpop.i32(i32 %4482)
  %4484 = trunc i32 %4483 to i8
  %4485 = and i8 %4484, 1
  %4486 = xor i8 %4485, 1
  store i8 %4486, i8* %21, align 1
  %4487 = xor i64 %4477, 16
  %4488 = xor i64 %4487, %4478
  %4489 = lshr i64 %4488, 4
  %4490 = trunc i64 %4489 to i8
  %4491 = and i8 %4490, 1
  store i8 %4491, i8* %27, align 1
  %4492 = icmp eq i64 %4478, 0
  %4493 = zext i1 %4492 to i8
  store i8 %4493, i8* %30, align 1
  %4494 = lshr i64 %4478, 63
  %4495 = trunc i64 %4494 to i8
  store i8 %4495, i8* %33, align 1
  %4496 = lshr i64 %4477, 63
  %4497 = xor i64 %4494, %4496
  %4498 = add nuw nsw i64 %4497, %4494
  %4499 = icmp eq i64 %4498, 2
  %4500 = zext i1 %4499 to i8
  store i8 %4500, i8* %39, align 1
  %4501 = add i64 %4472, -150
  %4502 = add i64 %4474, 17
  store i64 %4502, i64* %3, align 8
  %4503 = inttoptr i64 %4501 to i16*
  %4504 = load i16, i16* %4503, align 2
  %4505 = zext i16 %4504 to i64
  store i64 %4505, i64* %RCX.i11580, align 8
  %4506 = zext i16 %4504 to i64
  %4507 = shl nuw nsw i64 %4506, 4
  store i64 %4507, i64* %573, align 8
  %4508 = add i64 %4507, %4478
  store i64 %4508, i64* %RAX.i11582.pre-phi, align 8
  %4509 = icmp ult i64 %4508, %4478
  %4510 = icmp ult i64 %4508, %4507
  %4511 = or i1 %4509, %4510
  %4512 = zext i1 %4511 to i8
  store i8 %4512, i8* %14, align 1
  %4513 = trunc i64 %4508 to i32
  %4514 = and i32 %4513, 255
  %4515 = tail call i32 @llvm.ctpop.i32(i32 %4514)
  %4516 = trunc i32 %4515 to i8
  %4517 = and i8 %4516, 1
  %4518 = xor i8 %4517, 1
  store i8 %4518, i8* %21, align 1
  %4519 = xor i64 %4507, %4478
  %4520 = xor i64 %4519, %4508
  %4521 = lshr i64 %4520, 4
  %4522 = trunc i64 %4521 to i8
  %4523 = and i8 %4522, 1
  store i8 %4523, i8* %27, align 1
  %4524 = icmp eq i64 %4508, 0
  %4525 = zext i1 %4524 to i8
  store i8 %4525, i8* %30, align 1
  %4526 = lshr i64 %4508, 63
  %4527 = trunc i64 %4526 to i8
  store i8 %4527, i8* %33, align 1
  %4528 = xor i64 %4526, %4494
  %4529 = add nuw nsw i64 %4528, %4526
  %4530 = icmp eq i64 %4529, 2
  %4531 = zext i1 %4530 to i8
  store i8 %4531, i8* %39, align 1
  %4532 = add i64 %4508, 4
  %4533 = add i64 %4474, 29
  store i64 %4533, i64* %3, align 8
  %4534 = inttoptr i64 %4532 to i32*
  %4535 = load i32, i32* %4534, align 4
  %4536 = zext i32 %4535 to i64
  store i64 %4536, i64* %RCX.i11580, align 8
  %4537 = load i64, i64* %RBP.i, align 8
  %4538 = add i64 %4537, -144
  %4539 = add i64 %4474, 35
  store i64 %4539, i64* %3, align 8
  %4540 = inttoptr i64 %4538 to i32*
  %4541 = load i32, i32* %4540, align 4
  %4542 = add i32 %4541, %4535
  %4543 = zext i32 %4542 to i64
  store i64 %4543, i64* %RCX.i11580, align 8
  %4544 = icmp ult i32 %4542, %4535
  %4545 = icmp ult i32 %4542, %4541
  %4546 = or i1 %4544, %4545
  %4547 = zext i1 %4546 to i8
  store i8 %4547, i8* %14, align 1
  %4548 = and i32 %4542, 255
  %4549 = tail call i32 @llvm.ctpop.i32(i32 %4548)
  %4550 = trunc i32 %4549 to i8
  %4551 = and i8 %4550, 1
  %4552 = xor i8 %4551, 1
  store i8 %4552, i8* %21, align 1
  %4553 = xor i32 %4541, %4535
  %4554 = xor i32 %4553, %4542
  %4555 = lshr i32 %4554, 4
  %4556 = trunc i32 %4555 to i8
  %4557 = and i8 %4556, 1
  store i8 %4557, i8* %27, align 1
  %4558 = icmp eq i32 %4542, 0
  %4559 = zext i1 %4558 to i8
  store i8 %4559, i8* %30, align 1
  %4560 = lshr i32 %4542, 31
  %4561 = trunc i32 %4560 to i8
  store i8 %4561, i8* %33, align 1
  %4562 = lshr i32 %4535, 31
  %4563 = lshr i32 %4541, 31
  %4564 = xor i32 %4560, %4562
  %4565 = xor i32 %4560, %4563
  %4566 = add nuw nsw i32 %4564, %4565
  %4567 = icmp eq i32 %4566, 2
  %4568 = zext i1 %4567 to i8
  store i8 %4568, i8* %39, align 1
  %4569 = add i64 %4474, 41
  store i64 %4569, i64* %3, align 8
  store i32 %4542, i32* %4540, align 4
  %4570 = load i64, i64* %RBP.i, align 8
  %4571 = add i64 %4570, -8
  %4572 = load i64, i64* %3, align 8
  %4573 = add i64 %4572, 4
  store i64 %4573, i64* %3, align 8
  %4574 = inttoptr i64 %4571 to i64*
  %4575 = load i64, i64* %4574, align 8
  %4576 = add i64 %4575, 51640
  store i64 %4576, i64* %RAX.i11582.pre-phi, align 8
  %4577 = icmp ugt i64 %4575, -51641
  %4578 = zext i1 %4577 to i8
  store i8 %4578, i8* %14, align 1
  %4579 = trunc i64 %4576 to i32
  %4580 = and i32 %4579, 255
  %4581 = tail call i32 @llvm.ctpop.i32(i32 %4580)
  %4582 = trunc i32 %4581 to i8
  %4583 = and i8 %4582, 1
  %4584 = xor i8 %4583, 1
  store i8 %4584, i8* %21, align 1
  %4585 = xor i64 %4575, 16
  %4586 = xor i64 %4585, %4576
  %4587 = lshr i64 %4586, 4
  %4588 = trunc i64 %4587 to i8
  %4589 = and i8 %4588, 1
  store i8 %4589, i8* %27, align 1
  %4590 = icmp eq i64 %4576, 0
  %4591 = zext i1 %4590 to i8
  store i8 %4591, i8* %30, align 1
  %4592 = lshr i64 %4576, 63
  %4593 = trunc i64 %4592 to i8
  store i8 %4593, i8* %33, align 1
  %4594 = lshr i64 %4575, 63
  %4595 = xor i64 %4592, %4594
  %4596 = add nuw nsw i64 %4595, %4592
  %4597 = icmp eq i64 %4596, 2
  %4598 = zext i1 %4597 to i8
  store i8 %4598, i8* %39, align 1
  %4599 = add i64 %4570, -150
  %4600 = add i64 %4572, 17
  store i64 %4600, i64* %3, align 8
  %4601 = inttoptr i64 %4599 to i16*
  %4602 = load i16, i16* %4601, align 2
  %4603 = zext i16 %4602 to i64
  store i64 %4603, i64* %RCX.i11580, align 8
  %4604 = zext i16 %4602 to i64
  %4605 = shl nuw nsw i64 %4604, 4
  store i64 %4605, i64* %573, align 8
  %4606 = add i64 %4605, %4576
  store i64 %4606, i64* %RAX.i11582.pre-phi, align 8
  %4607 = icmp ult i64 %4606, %4576
  %4608 = icmp ult i64 %4606, %4605
  %4609 = or i1 %4607, %4608
  %4610 = zext i1 %4609 to i8
  store i8 %4610, i8* %14, align 1
  %4611 = trunc i64 %4606 to i32
  %4612 = and i32 %4611, 255
  %4613 = tail call i32 @llvm.ctpop.i32(i32 %4612)
  %4614 = trunc i32 %4613 to i8
  %4615 = and i8 %4614, 1
  %4616 = xor i8 %4615, 1
  store i8 %4616, i8* %21, align 1
  %4617 = xor i64 %4605, %4576
  %4618 = xor i64 %4617, %4606
  %4619 = lshr i64 %4618, 4
  %4620 = trunc i64 %4619 to i8
  %4621 = and i8 %4620, 1
  store i8 %4621, i8* %27, align 1
  %4622 = icmp eq i64 %4606, 0
  %4623 = zext i1 %4622 to i8
  store i8 %4623, i8* %30, align 1
  %4624 = lshr i64 %4606, 63
  %4625 = trunc i64 %4624 to i8
  store i8 %4625, i8* %33, align 1
  %4626 = xor i64 %4624, %4592
  %4627 = add nuw nsw i64 %4626, %4624
  %4628 = icmp eq i64 %4627, 2
  %4629 = zext i1 %4628 to i8
  store i8 %4629, i8* %39, align 1
  %4630 = add i64 %4606, 8
  %4631 = add i64 %4572, 29
  store i64 %4631, i64* %3, align 8
  %4632 = inttoptr i64 %4630 to i32*
  %4633 = load i32, i32* %4632, align 4
  %4634 = zext i32 %4633 to i64
  store i64 %4634, i64* %RCX.i11580, align 8
  %4635 = load i64, i64* %RBP.i, align 8
  %4636 = add i64 %4635, -148
  %4637 = add i64 %4572, 35
  store i64 %4637, i64* %3, align 8
  %4638 = inttoptr i64 %4636 to i32*
  %4639 = load i32, i32* %4638, align 4
  %4640 = add i32 %4639, %4633
  %4641 = zext i32 %4640 to i64
  store i64 %4641, i64* %RCX.i11580, align 8
  %4642 = icmp ult i32 %4640, %4633
  %4643 = icmp ult i32 %4640, %4639
  %4644 = or i1 %4642, %4643
  %4645 = zext i1 %4644 to i8
  store i8 %4645, i8* %14, align 1
  %4646 = and i32 %4640, 255
  %4647 = tail call i32 @llvm.ctpop.i32(i32 %4646)
  %4648 = trunc i32 %4647 to i8
  %4649 = and i8 %4648, 1
  %4650 = xor i8 %4649, 1
  store i8 %4650, i8* %21, align 1
  %4651 = xor i32 %4639, %4633
  %4652 = xor i32 %4651, %4640
  %4653 = lshr i32 %4652, 4
  %4654 = trunc i32 %4653 to i8
  %4655 = and i8 %4654, 1
  store i8 %4655, i8* %27, align 1
  %4656 = icmp eq i32 %4640, 0
  %4657 = zext i1 %4656 to i8
  store i8 %4657, i8* %30, align 1
  %4658 = lshr i32 %4640, 31
  %4659 = trunc i32 %4658 to i8
  store i8 %4659, i8* %33, align 1
  %4660 = lshr i32 %4633, 31
  %4661 = lshr i32 %4639, 31
  %4662 = xor i32 %4658, %4660
  %4663 = xor i32 %4658, %4661
  %4664 = add nuw nsw i32 %4662, %4663
  %4665 = icmp eq i32 %4664, 2
  %4666 = zext i1 %4665 to i8
  store i8 %4666, i8* %39, align 1
  %4667 = add i64 %4572, 41
  store i64 %4667, i64* %3, align 8
  store i32 %4640, i32* %4638, align 4
  %4668 = load i64, i64* %RBP.i, align 8
  %4669 = add i64 %4668, -120
  %4670 = load i64, i64* %3, align 8
  %4671 = add i64 %4670, 4
  store i64 %4671, i64* %3, align 8
  %4672 = inttoptr i64 %4669 to i64*
  %4673 = load i64, i64* %4672, align 8
  store i64 %4673, i64* %RAX.i11582.pre-phi, align 8
  %4674 = add i64 %4668, -28
  %4675 = add i64 %4670, 7
  store i64 %4675, i64* %3, align 8
  %4676 = inttoptr i64 %4674 to i32*
  %4677 = load i32, i32* %4676, align 4
  %4678 = add i32 %4677, 6
  %4679 = zext i32 %4678 to i64
  store i64 %4679, i64* %RCX.i11580, align 8
  %4680 = icmp ugt i32 %4677, -7
  %4681 = zext i1 %4680 to i8
  store i8 %4681, i8* %14, align 1
  %4682 = and i32 %4678, 255
  %4683 = tail call i32 @llvm.ctpop.i32(i32 %4682)
  %4684 = trunc i32 %4683 to i8
  %4685 = and i8 %4684, 1
  %4686 = xor i8 %4685, 1
  store i8 %4686, i8* %21, align 1
  %4687 = xor i32 %4678, %4677
  %4688 = lshr i32 %4687, 4
  %4689 = trunc i32 %4688 to i8
  %4690 = and i8 %4689, 1
  store i8 %4690, i8* %27, align 1
  %4691 = icmp eq i32 %4678, 0
  %4692 = zext i1 %4691 to i8
  store i8 %4692, i8* %30, align 1
  %4693 = lshr i32 %4678, 31
  %4694 = trunc i32 %4693 to i8
  store i8 %4694, i8* %33, align 1
  %4695 = lshr i32 %4677, 31
  %4696 = xor i32 %4693, %4695
  %4697 = add nuw nsw i32 %4696, %4693
  %4698 = icmp eq i32 %4697, 2
  %4699 = zext i1 %4698 to i8
  store i8 %4699, i8* %39, align 1
  %4700 = sext i32 %4678 to i64
  store i64 %4700, i64* %573, align 8
  %4701 = shl nsw i64 %4700, 1
  %4702 = add i64 %4673, %4701
  %4703 = add i64 %4670, 17
  store i64 %4703, i64* %3, align 8
  %4704 = inttoptr i64 %4702 to i16*
  %4705 = load i16, i16* %4704, align 2
  store i16 %4705, i16* %SI.i, align 2
  %4706 = add i64 %4668, -150
  %4707 = add i64 %4670, 24
  store i64 %4707, i64* %3, align 8
  %4708 = inttoptr i64 %4706 to i16*
  store i16 %4705, i16* %4708, align 2
  %4709 = load i64, i64* %RBP.i, align 8
  %4710 = add i64 %4709, -8
  %4711 = load i64, i64* %3, align 8
  %4712 = add i64 %4711, 4
  store i64 %4712, i64* %3, align 8
  %4713 = inttoptr i64 %4710 to i64*
  %4714 = load i64, i64* %4713, align 8
  %4715 = add i64 %4714, 51640
  store i64 %4715, i64* %RAX.i11582.pre-phi, align 8
  %4716 = icmp ugt i64 %4714, -51641
  %4717 = zext i1 %4716 to i8
  store i8 %4717, i8* %14, align 1
  %4718 = trunc i64 %4715 to i32
  %4719 = and i32 %4718, 255
  %4720 = tail call i32 @llvm.ctpop.i32(i32 %4719)
  %4721 = trunc i32 %4720 to i8
  %4722 = and i8 %4721, 1
  %4723 = xor i8 %4722, 1
  store i8 %4723, i8* %21, align 1
  %4724 = xor i64 %4714, 16
  %4725 = xor i64 %4724, %4715
  %4726 = lshr i64 %4725, 4
  %4727 = trunc i64 %4726 to i8
  %4728 = and i8 %4727, 1
  store i8 %4728, i8* %27, align 1
  %4729 = icmp eq i64 %4715, 0
  %4730 = zext i1 %4729 to i8
  store i8 %4730, i8* %30, align 1
  %4731 = lshr i64 %4715, 63
  %4732 = trunc i64 %4731 to i8
  store i8 %4732, i8* %33, align 1
  %4733 = lshr i64 %4714, 63
  %4734 = xor i64 %4731, %4733
  %4735 = add nuw nsw i64 %4734, %4731
  %4736 = icmp eq i64 %4735, 2
  %4737 = zext i1 %4736 to i8
  store i8 %4737, i8* %39, align 1
  %4738 = add i64 %4709, -150
  %4739 = add i64 %4711, 17
  store i64 %4739, i64* %3, align 8
  %4740 = inttoptr i64 %4738 to i16*
  %4741 = load i16, i16* %4740, align 2
  %4742 = zext i16 %4741 to i64
  store i64 %4742, i64* %RCX.i11580, align 8
  %4743 = zext i16 %4741 to i64
  %4744 = shl nuw nsw i64 %4743, 4
  store i64 %4744, i64* %573, align 8
  %4745 = add i64 %4744, %4715
  store i64 %4745, i64* %RAX.i11582.pre-phi, align 8
  %4746 = icmp ult i64 %4745, %4715
  %4747 = icmp ult i64 %4745, %4744
  %4748 = or i1 %4746, %4747
  %4749 = zext i1 %4748 to i8
  store i8 %4749, i8* %14, align 1
  %4750 = trunc i64 %4745 to i32
  %4751 = and i32 %4750, 255
  %4752 = tail call i32 @llvm.ctpop.i32(i32 %4751)
  %4753 = trunc i32 %4752 to i8
  %4754 = and i8 %4753, 1
  %4755 = xor i8 %4754, 1
  store i8 %4755, i8* %21, align 1
  %4756 = xor i64 %4744, %4715
  %4757 = xor i64 %4756, %4745
  %4758 = lshr i64 %4757, 4
  %4759 = trunc i64 %4758 to i8
  %4760 = and i8 %4759, 1
  store i8 %4760, i8* %27, align 1
  %4761 = icmp eq i64 %4745, 0
  %4762 = zext i1 %4761 to i8
  store i8 %4762, i8* %30, align 1
  %4763 = lshr i64 %4745, 63
  %4764 = trunc i64 %4763 to i8
  store i8 %4764, i8* %33, align 1
  %4765 = xor i64 %4763, %4731
  %4766 = add nuw nsw i64 %4765, %4763
  %4767 = icmp eq i64 %4766, 2
  %4768 = zext i1 %4767 to i8
  store i8 %4768, i8* %39, align 1
  %4769 = inttoptr i64 %4745 to i32*
  %4770 = add i64 %4711, 28
  store i64 %4770, i64* %3, align 8
  %4771 = load i32, i32* %4769, align 4
  %4772 = zext i32 %4771 to i64
  store i64 %4772, i64* %RCX.i11580, align 8
  %4773 = load i64, i64* %RBP.i, align 8
  %4774 = add i64 %4773, -140
  %4775 = add i64 %4711, 34
  store i64 %4775, i64* %3, align 8
  %4776 = inttoptr i64 %4774 to i32*
  %4777 = load i32, i32* %4776, align 4
  %4778 = add i32 %4777, %4771
  %4779 = zext i32 %4778 to i64
  store i64 %4779, i64* %RCX.i11580, align 8
  %4780 = icmp ult i32 %4778, %4771
  %4781 = icmp ult i32 %4778, %4777
  %4782 = or i1 %4780, %4781
  %4783 = zext i1 %4782 to i8
  store i8 %4783, i8* %14, align 1
  %4784 = and i32 %4778, 255
  %4785 = tail call i32 @llvm.ctpop.i32(i32 %4784)
  %4786 = trunc i32 %4785 to i8
  %4787 = and i8 %4786, 1
  %4788 = xor i8 %4787, 1
  store i8 %4788, i8* %21, align 1
  %4789 = xor i32 %4777, %4771
  %4790 = xor i32 %4789, %4778
  %4791 = lshr i32 %4790, 4
  %4792 = trunc i32 %4791 to i8
  %4793 = and i8 %4792, 1
  store i8 %4793, i8* %27, align 1
  %4794 = icmp eq i32 %4778, 0
  %4795 = zext i1 %4794 to i8
  store i8 %4795, i8* %30, align 1
  %4796 = lshr i32 %4778, 31
  %4797 = trunc i32 %4796 to i8
  store i8 %4797, i8* %33, align 1
  %4798 = lshr i32 %4771, 31
  %4799 = lshr i32 %4777, 31
  %4800 = xor i32 %4796, %4798
  %4801 = xor i32 %4796, %4799
  %4802 = add nuw nsw i32 %4800, %4801
  %4803 = icmp eq i32 %4802, 2
  %4804 = zext i1 %4803 to i8
  store i8 %4804, i8* %39, align 1
  %4805 = add i64 %4711, 40
  store i64 %4805, i64* %3, align 8
  store i32 %4778, i32* %4776, align 4
  %4806 = load i64, i64* %RBP.i, align 8
  %4807 = add i64 %4806, -8
  %4808 = load i64, i64* %3, align 8
  %4809 = add i64 %4808, 4
  store i64 %4809, i64* %3, align 8
  %4810 = inttoptr i64 %4807 to i64*
  %4811 = load i64, i64* %4810, align 8
  %4812 = add i64 %4811, 51640
  store i64 %4812, i64* %RAX.i11582.pre-phi, align 8
  %4813 = icmp ugt i64 %4811, -51641
  %4814 = zext i1 %4813 to i8
  store i8 %4814, i8* %14, align 1
  %4815 = trunc i64 %4812 to i32
  %4816 = and i32 %4815, 255
  %4817 = tail call i32 @llvm.ctpop.i32(i32 %4816)
  %4818 = trunc i32 %4817 to i8
  %4819 = and i8 %4818, 1
  %4820 = xor i8 %4819, 1
  store i8 %4820, i8* %21, align 1
  %4821 = xor i64 %4811, 16
  %4822 = xor i64 %4821, %4812
  %4823 = lshr i64 %4822, 4
  %4824 = trunc i64 %4823 to i8
  %4825 = and i8 %4824, 1
  store i8 %4825, i8* %27, align 1
  %4826 = icmp eq i64 %4812, 0
  %4827 = zext i1 %4826 to i8
  store i8 %4827, i8* %30, align 1
  %4828 = lshr i64 %4812, 63
  %4829 = trunc i64 %4828 to i8
  store i8 %4829, i8* %33, align 1
  %4830 = lshr i64 %4811, 63
  %4831 = xor i64 %4828, %4830
  %4832 = add nuw nsw i64 %4831, %4828
  %4833 = icmp eq i64 %4832, 2
  %4834 = zext i1 %4833 to i8
  store i8 %4834, i8* %39, align 1
  %4835 = add i64 %4806, -150
  %4836 = add i64 %4808, 17
  store i64 %4836, i64* %3, align 8
  %4837 = inttoptr i64 %4835 to i16*
  %4838 = load i16, i16* %4837, align 2
  %4839 = zext i16 %4838 to i64
  store i64 %4839, i64* %RCX.i11580, align 8
  %4840 = zext i16 %4838 to i64
  %4841 = shl nuw nsw i64 %4840, 4
  store i64 %4841, i64* %573, align 8
  %4842 = add i64 %4841, %4812
  store i64 %4842, i64* %RAX.i11582.pre-phi, align 8
  %4843 = icmp ult i64 %4842, %4812
  %4844 = icmp ult i64 %4842, %4841
  %4845 = or i1 %4843, %4844
  %4846 = zext i1 %4845 to i8
  store i8 %4846, i8* %14, align 1
  %4847 = trunc i64 %4842 to i32
  %4848 = and i32 %4847, 255
  %4849 = tail call i32 @llvm.ctpop.i32(i32 %4848)
  %4850 = trunc i32 %4849 to i8
  %4851 = and i8 %4850, 1
  %4852 = xor i8 %4851, 1
  store i8 %4852, i8* %21, align 1
  %4853 = xor i64 %4841, %4812
  %4854 = xor i64 %4853, %4842
  %4855 = lshr i64 %4854, 4
  %4856 = trunc i64 %4855 to i8
  %4857 = and i8 %4856, 1
  store i8 %4857, i8* %27, align 1
  %4858 = icmp eq i64 %4842, 0
  %4859 = zext i1 %4858 to i8
  store i8 %4859, i8* %30, align 1
  %4860 = lshr i64 %4842, 63
  %4861 = trunc i64 %4860 to i8
  store i8 %4861, i8* %33, align 1
  %4862 = xor i64 %4860, %4828
  %4863 = add nuw nsw i64 %4862, %4860
  %4864 = icmp eq i64 %4863, 2
  %4865 = zext i1 %4864 to i8
  store i8 %4865, i8* %39, align 1
  %4866 = add i64 %4842, 4
  %4867 = add i64 %4808, 29
  store i64 %4867, i64* %3, align 8
  %4868 = inttoptr i64 %4866 to i32*
  %4869 = load i32, i32* %4868, align 4
  %4870 = zext i32 %4869 to i64
  store i64 %4870, i64* %RCX.i11580, align 8
  %4871 = load i64, i64* %RBP.i, align 8
  %4872 = add i64 %4871, -144
  %4873 = add i64 %4808, 35
  store i64 %4873, i64* %3, align 8
  %4874 = inttoptr i64 %4872 to i32*
  %4875 = load i32, i32* %4874, align 4
  %4876 = add i32 %4875, %4869
  %4877 = zext i32 %4876 to i64
  store i64 %4877, i64* %RCX.i11580, align 8
  %4878 = icmp ult i32 %4876, %4869
  %4879 = icmp ult i32 %4876, %4875
  %4880 = or i1 %4878, %4879
  %4881 = zext i1 %4880 to i8
  store i8 %4881, i8* %14, align 1
  %4882 = and i32 %4876, 255
  %4883 = tail call i32 @llvm.ctpop.i32(i32 %4882)
  %4884 = trunc i32 %4883 to i8
  %4885 = and i8 %4884, 1
  %4886 = xor i8 %4885, 1
  store i8 %4886, i8* %21, align 1
  %4887 = xor i32 %4875, %4869
  %4888 = xor i32 %4887, %4876
  %4889 = lshr i32 %4888, 4
  %4890 = trunc i32 %4889 to i8
  %4891 = and i8 %4890, 1
  store i8 %4891, i8* %27, align 1
  %4892 = icmp eq i32 %4876, 0
  %4893 = zext i1 %4892 to i8
  store i8 %4893, i8* %30, align 1
  %4894 = lshr i32 %4876, 31
  %4895 = trunc i32 %4894 to i8
  store i8 %4895, i8* %33, align 1
  %4896 = lshr i32 %4869, 31
  %4897 = lshr i32 %4875, 31
  %4898 = xor i32 %4894, %4896
  %4899 = xor i32 %4894, %4897
  %4900 = add nuw nsw i32 %4898, %4899
  %4901 = icmp eq i32 %4900, 2
  %4902 = zext i1 %4901 to i8
  store i8 %4902, i8* %39, align 1
  %4903 = add i64 %4808, 41
  store i64 %4903, i64* %3, align 8
  store i32 %4876, i32* %4874, align 4
  %4904 = load i64, i64* %RBP.i, align 8
  %4905 = add i64 %4904, -8
  %4906 = load i64, i64* %3, align 8
  %4907 = add i64 %4906, 4
  store i64 %4907, i64* %3, align 8
  %4908 = inttoptr i64 %4905 to i64*
  %4909 = load i64, i64* %4908, align 8
  %4910 = add i64 %4909, 51640
  store i64 %4910, i64* %RAX.i11582.pre-phi, align 8
  %4911 = icmp ugt i64 %4909, -51641
  %4912 = zext i1 %4911 to i8
  store i8 %4912, i8* %14, align 1
  %4913 = trunc i64 %4910 to i32
  %4914 = and i32 %4913, 255
  %4915 = tail call i32 @llvm.ctpop.i32(i32 %4914)
  %4916 = trunc i32 %4915 to i8
  %4917 = and i8 %4916, 1
  %4918 = xor i8 %4917, 1
  store i8 %4918, i8* %21, align 1
  %4919 = xor i64 %4909, 16
  %4920 = xor i64 %4919, %4910
  %4921 = lshr i64 %4920, 4
  %4922 = trunc i64 %4921 to i8
  %4923 = and i8 %4922, 1
  store i8 %4923, i8* %27, align 1
  %4924 = icmp eq i64 %4910, 0
  %4925 = zext i1 %4924 to i8
  store i8 %4925, i8* %30, align 1
  %4926 = lshr i64 %4910, 63
  %4927 = trunc i64 %4926 to i8
  store i8 %4927, i8* %33, align 1
  %4928 = lshr i64 %4909, 63
  %4929 = xor i64 %4926, %4928
  %4930 = add nuw nsw i64 %4929, %4926
  %4931 = icmp eq i64 %4930, 2
  %4932 = zext i1 %4931 to i8
  store i8 %4932, i8* %39, align 1
  %4933 = add i64 %4904, -150
  %4934 = add i64 %4906, 17
  store i64 %4934, i64* %3, align 8
  %4935 = inttoptr i64 %4933 to i16*
  %4936 = load i16, i16* %4935, align 2
  %4937 = zext i16 %4936 to i64
  store i64 %4937, i64* %RCX.i11580, align 8
  %4938 = zext i16 %4936 to i64
  %4939 = shl nuw nsw i64 %4938, 4
  store i64 %4939, i64* %573, align 8
  %4940 = add i64 %4939, %4910
  store i64 %4940, i64* %RAX.i11582.pre-phi, align 8
  %4941 = icmp ult i64 %4940, %4910
  %4942 = icmp ult i64 %4940, %4939
  %4943 = or i1 %4941, %4942
  %4944 = zext i1 %4943 to i8
  store i8 %4944, i8* %14, align 1
  %4945 = trunc i64 %4940 to i32
  %4946 = and i32 %4945, 255
  %4947 = tail call i32 @llvm.ctpop.i32(i32 %4946)
  %4948 = trunc i32 %4947 to i8
  %4949 = and i8 %4948, 1
  %4950 = xor i8 %4949, 1
  store i8 %4950, i8* %21, align 1
  %4951 = xor i64 %4939, %4910
  %4952 = xor i64 %4951, %4940
  %4953 = lshr i64 %4952, 4
  %4954 = trunc i64 %4953 to i8
  %4955 = and i8 %4954, 1
  store i8 %4955, i8* %27, align 1
  %4956 = icmp eq i64 %4940, 0
  %4957 = zext i1 %4956 to i8
  store i8 %4957, i8* %30, align 1
  %4958 = lshr i64 %4940, 63
  %4959 = trunc i64 %4958 to i8
  store i8 %4959, i8* %33, align 1
  %4960 = xor i64 %4958, %4926
  %4961 = add nuw nsw i64 %4960, %4958
  %4962 = icmp eq i64 %4961, 2
  %4963 = zext i1 %4962 to i8
  store i8 %4963, i8* %39, align 1
  %4964 = add i64 %4940, 8
  %4965 = add i64 %4906, 29
  store i64 %4965, i64* %3, align 8
  %4966 = inttoptr i64 %4964 to i32*
  %4967 = load i32, i32* %4966, align 4
  %4968 = zext i32 %4967 to i64
  store i64 %4968, i64* %RCX.i11580, align 8
  %4969 = load i64, i64* %RBP.i, align 8
  %4970 = add i64 %4969, -148
  %4971 = add i64 %4906, 35
  store i64 %4971, i64* %3, align 8
  %4972 = inttoptr i64 %4970 to i32*
  %4973 = load i32, i32* %4972, align 4
  %4974 = add i32 %4973, %4967
  %4975 = zext i32 %4974 to i64
  store i64 %4975, i64* %RCX.i11580, align 8
  %4976 = icmp ult i32 %4974, %4967
  %4977 = icmp ult i32 %4974, %4973
  %4978 = or i1 %4976, %4977
  %4979 = zext i1 %4978 to i8
  store i8 %4979, i8* %14, align 1
  %4980 = and i32 %4974, 255
  %4981 = tail call i32 @llvm.ctpop.i32(i32 %4980)
  %4982 = trunc i32 %4981 to i8
  %4983 = and i8 %4982, 1
  %4984 = xor i8 %4983, 1
  store i8 %4984, i8* %21, align 1
  %4985 = xor i32 %4973, %4967
  %4986 = xor i32 %4985, %4974
  %4987 = lshr i32 %4986, 4
  %4988 = trunc i32 %4987 to i8
  %4989 = and i8 %4988, 1
  store i8 %4989, i8* %27, align 1
  %4990 = icmp eq i32 %4974, 0
  %4991 = zext i1 %4990 to i8
  store i8 %4991, i8* %30, align 1
  %4992 = lshr i32 %4974, 31
  %4993 = trunc i32 %4992 to i8
  store i8 %4993, i8* %33, align 1
  %4994 = lshr i32 %4967, 31
  %4995 = lshr i32 %4973, 31
  %4996 = xor i32 %4992, %4994
  %4997 = xor i32 %4992, %4995
  %4998 = add nuw nsw i32 %4996, %4997
  %4999 = icmp eq i32 %4998, 2
  %5000 = zext i1 %4999 to i8
  store i8 %5000, i8* %39, align 1
  %5001 = add i64 %4906, 41
  store i64 %5001, i64* %3, align 8
  store i32 %4974, i32* %4972, align 4
  %5002 = load i64, i64* %RBP.i, align 8
  %5003 = add i64 %5002, -120
  %5004 = load i64, i64* %3, align 8
  %5005 = add i64 %5004, 4
  store i64 %5005, i64* %3, align 8
  %5006 = inttoptr i64 %5003 to i64*
  %5007 = load i64, i64* %5006, align 8
  store i64 %5007, i64* %RAX.i11582.pre-phi, align 8
  %5008 = add i64 %5002, -28
  %5009 = add i64 %5004, 7
  store i64 %5009, i64* %3, align 8
  %5010 = inttoptr i64 %5008 to i32*
  %5011 = load i32, i32* %5010, align 4
  %5012 = add i32 %5011, 7
  %5013 = zext i32 %5012 to i64
  store i64 %5013, i64* %RCX.i11580, align 8
  %5014 = icmp ugt i32 %5011, -8
  %5015 = zext i1 %5014 to i8
  store i8 %5015, i8* %14, align 1
  %5016 = and i32 %5012, 255
  %5017 = tail call i32 @llvm.ctpop.i32(i32 %5016)
  %5018 = trunc i32 %5017 to i8
  %5019 = and i8 %5018, 1
  %5020 = xor i8 %5019, 1
  store i8 %5020, i8* %21, align 1
  %5021 = xor i32 %5012, %5011
  %5022 = lshr i32 %5021, 4
  %5023 = trunc i32 %5022 to i8
  %5024 = and i8 %5023, 1
  store i8 %5024, i8* %27, align 1
  %5025 = icmp eq i32 %5012, 0
  %5026 = zext i1 %5025 to i8
  store i8 %5026, i8* %30, align 1
  %5027 = lshr i32 %5012, 31
  %5028 = trunc i32 %5027 to i8
  store i8 %5028, i8* %33, align 1
  %5029 = lshr i32 %5011, 31
  %5030 = xor i32 %5027, %5029
  %5031 = add nuw nsw i32 %5030, %5027
  %5032 = icmp eq i32 %5031, 2
  %5033 = zext i1 %5032 to i8
  store i8 %5033, i8* %39, align 1
  %5034 = sext i32 %5012 to i64
  store i64 %5034, i64* %573, align 8
  %5035 = shl nsw i64 %5034, 1
  %5036 = add i64 %5007, %5035
  %5037 = add i64 %5004, 17
  store i64 %5037, i64* %3, align 8
  %5038 = inttoptr i64 %5036 to i16*
  %5039 = load i16, i16* %5038, align 2
  store i16 %5039, i16* %SI.i, align 2
  %5040 = add i64 %5002, -150
  %5041 = add i64 %5004, 24
  store i64 %5041, i64* %3, align 8
  %5042 = inttoptr i64 %5040 to i16*
  store i16 %5039, i16* %5042, align 2
  %5043 = load i64, i64* %RBP.i, align 8
  %5044 = add i64 %5043, -8
  %5045 = load i64, i64* %3, align 8
  %5046 = add i64 %5045, 4
  store i64 %5046, i64* %3, align 8
  %5047 = inttoptr i64 %5044 to i64*
  %5048 = load i64, i64* %5047, align 8
  %5049 = add i64 %5048, 51640
  store i64 %5049, i64* %RAX.i11582.pre-phi, align 8
  %5050 = icmp ugt i64 %5048, -51641
  %5051 = zext i1 %5050 to i8
  store i8 %5051, i8* %14, align 1
  %5052 = trunc i64 %5049 to i32
  %5053 = and i32 %5052, 255
  %5054 = tail call i32 @llvm.ctpop.i32(i32 %5053)
  %5055 = trunc i32 %5054 to i8
  %5056 = and i8 %5055, 1
  %5057 = xor i8 %5056, 1
  store i8 %5057, i8* %21, align 1
  %5058 = xor i64 %5048, 16
  %5059 = xor i64 %5058, %5049
  %5060 = lshr i64 %5059, 4
  %5061 = trunc i64 %5060 to i8
  %5062 = and i8 %5061, 1
  store i8 %5062, i8* %27, align 1
  %5063 = icmp eq i64 %5049, 0
  %5064 = zext i1 %5063 to i8
  store i8 %5064, i8* %30, align 1
  %5065 = lshr i64 %5049, 63
  %5066 = trunc i64 %5065 to i8
  store i8 %5066, i8* %33, align 1
  %5067 = lshr i64 %5048, 63
  %5068 = xor i64 %5065, %5067
  %5069 = add nuw nsw i64 %5068, %5065
  %5070 = icmp eq i64 %5069, 2
  %5071 = zext i1 %5070 to i8
  store i8 %5071, i8* %39, align 1
  %5072 = add i64 %5043, -150
  %5073 = add i64 %5045, 17
  store i64 %5073, i64* %3, align 8
  %5074 = inttoptr i64 %5072 to i16*
  %5075 = load i16, i16* %5074, align 2
  %5076 = zext i16 %5075 to i64
  store i64 %5076, i64* %RCX.i11580, align 8
  %5077 = zext i16 %5075 to i64
  %5078 = shl nuw nsw i64 %5077, 4
  store i64 %5078, i64* %573, align 8
  %5079 = add i64 %5078, %5049
  store i64 %5079, i64* %RAX.i11582.pre-phi, align 8
  %5080 = icmp ult i64 %5079, %5049
  %5081 = icmp ult i64 %5079, %5078
  %5082 = or i1 %5080, %5081
  %5083 = zext i1 %5082 to i8
  store i8 %5083, i8* %14, align 1
  %5084 = trunc i64 %5079 to i32
  %5085 = and i32 %5084, 255
  %5086 = tail call i32 @llvm.ctpop.i32(i32 %5085)
  %5087 = trunc i32 %5086 to i8
  %5088 = and i8 %5087, 1
  %5089 = xor i8 %5088, 1
  store i8 %5089, i8* %21, align 1
  %5090 = xor i64 %5078, %5049
  %5091 = xor i64 %5090, %5079
  %5092 = lshr i64 %5091, 4
  %5093 = trunc i64 %5092 to i8
  %5094 = and i8 %5093, 1
  store i8 %5094, i8* %27, align 1
  %5095 = icmp eq i64 %5079, 0
  %5096 = zext i1 %5095 to i8
  store i8 %5096, i8* %30, align 1
  %5097 = lshr i64 %5079, 63
  %5098 = trunc i64 %5097 to i8
  store i8 %5098, i8* %33, align 1
  %5099 = xor i64 %5097, %5065
  %5100 = add nuw nsw i64 %5099, %5097
  %5101 = icmp eq i64 %5100, 2
  %5102 = zext i1 %5101 to i8
  store i8 %5102, i8* %39, align 1
  %5103 = inttoptr i64 %5079 to i32*
  %5104 = add i64 %5045, 28
  store i64 %5104, i64* %3, align 8
  %5105 = load i32, i32* %5103, align 4
  %5106 = zext i32 %5105 to i64
  store i64 %5106, i64* %RCX.i11580, align 8
  %5107 = load i64, i64* %RBP.i, align 8
  %5108 = add i64 %5107, -140
  %5109 = add i64 %5045, 34
  store i64 %5109, i64* %3, align 8
  %5110 = inttoptr i64 %5108 to i32*
  %5111 = load i32, i32* %5110, align 4
  %5112 = add i32 %5111, %5105
  %5113 = zext i32 %5112 to i64
  store i64 %5113, i64* %RCX.i11580, align 8
  %5114 = icmp ult i32 %5112, %5105
  %5115 = icmp ult i32 %5112, %5111
  %5116 = or i1 %5114, %5115
  %5117 = zext i1 %5116 to i8
  store i8 %5117, i8* %14, align 1
  %5118 = and i32 %5112, 255
  %5119 = tail call i32 @llvm.ctpop.i32(i32 %5118)
  %5120 = trunc i32 %5119 to i8
  %5121 = and i8 %5120, 1
  %5122 = xor i8 %5121, 1
  store i8 %5122, i8* %21, align 1
  %5123 = xor i32 %5111, %5105
  %5124 = xor i32 %5123, %5112
  %5125 = lshr i32 %5124, 4
  %5126 = trunc i32 %5125 to i8
  %5127 = and i8 %5126, 1
  store i8 %5127, i8* %27, align 1
  %5128 = icmp eq i32 %5112, 0
  %5129 = zext i1 %5128 to i8
  store i8 %5129, i8* %30, align 1
  %5130 = lshr i32 %5112, 31
  %5131 = trunc i32 %5130 to i8
  store i8 %5131, i8* %33, align 1
  %5132 = lshr i32 %5105, 31
  %5133 = lshr i32 %5111, 31
  %5134 = xor i32 %5130, %5132
  %5135 = xor i32 %5130, %5133
  %5136 = add nuw nsw i32 %5134, %5135
  %5137 = icmp eq i32 %5136, 2
  %5138 = zext i1 %5137 to i8
  store i8 %5138, i8* %39, align 1
  %5139 = add i64 %5045, 40
  store i64 %5139, i64* %3, align 8
  store i32 %5112, i32* %5110, align 4
  %5140 = load i64, i64* %RBP.i, align 8
  %5141 = add i64 %5140, -8
  %5142 = load i64, i64* %3, align 8
  %5143 = add i64 %5142, 4
  store i64 %5143, i64* %3, align 8
  %5144 = inttoptr i64 %5141 to i64*
  %5145 = load i64, i64* %5144, align 8
  %5146 = add i64 %5145, 51640
  store i64 %5146, i64* %RAX.i11582.pre-phi, align 8
  %5147 = icmp ugt i64 %5145, -51641
  %5148 = zext i1 %5147 to i8
  store i8 %5148, i8* %14, align 1
  %5149 = trunc i64 %5146 to i32
  %5150 = and i32 %5149, 255
  %5151 = tail call i32 @llvm.ctpop.i32(i32 %5150)
  %5152 = trunc i32 %5151 to i8
  %5153 = and i8 %5152, 1
  %5154 = xor i8 %5153, 1
  store i8 %5154, i8* %21, align 1
  %5155 = xor i64 %5145, 16
  %5156 = xor i64 %5155, %5146
  %5157 = lshr i64 %5156, 4
  %5158 = trunc i64 %5157 to i8
  %5159 = and i8 %5158, 1
  store i8 %5159, i8* %27, align 1
  %5160 = icmp eq i64 %5146, 0
  %5161 = zext i1 %5160 to i8
  store i8 %5161, i8* %30, align 1
  %5162 = lshr i64 %5146, 63
  %5163 = trunc i64 %5162 to i8
  store i8 %5163, i8* %33, align 1
  %5164 = lshr i64 %5145, 63
  %5165 = xor i64 %5162, %5164
  %5166 = add nuw nsw i64 %5165, %5162
  %5167 = icmp eq i64 %5166, 2
  %5168 = zext i1 %5167 to i8
  store i8 %5168, i8* %39, align 1
  %5169 = add i64 %5140, -150
  %5170 = add i64 %5142, 17
  store i64 %5170, i64* %3, align 8
  %5171 = inttoptr i64 %5169 to i16*
  %5172 = load i16, i16* %5171, align 2
  %5173 = zext i16 %5172 to i64
  store i64 %5173, i64* %RCX.i11580, align 8
  %5174 = zext i16 %5172 to i64
  %5175 = shl nuw nsw i64 %5174, 4
  store i64 %5175, i64* %573, align 8
  %5176 = add i64 %5175, %5146
  store i64 %5176, i64* %RAX.i11582.pre-phi, align 8
  %5177 = icmp ult i64 %5176, %5146
  %5178 = icmp ult i64 %5176, %5175
  %5179 = or i1 %5177, %5178
  %5180 = zext i1 %5179 to i8
  store i8 %5180, i8* %14, align 1
  %5181 = trunc i64 %5176 to i32
  %5182 = and i32 %5181, 255
  %5183 = tail call i32 @llvm.ctpop.i32(i32 %5182)
  %5184 = trunc i32 %5183 to i8
  %5185 = and i8 %5184, 1
  %5186 = xor i8 %5185, 1
  store i8 %5186, i8* %21, align 1
  %5187 = xor i64 %5175, %5146
  %5188 = xor i64 %5187, %5176
  %5189 = lshr i64 %5188, 4
  %5190 = trunc i64 %5189 to i8
  %5191 = and i8 %5190, 1
  store i8 %5191, i8* %27, align 1
  %5192 = icmp eq i64 %5176, 0
  %5193 = zext i1 %5192 to i8
  store i8 %5193, i8* %30, align 1
  %5194 = lshr i64 %5176, 63
  %5195 = trunc i64 %5194 to i8
  store i8 %5195, i8* %33, align 1
  %5196 = xor i64 %5194, %5162
  %5197 = add nuw nsw i64 %5196, %5194
  %5198 = icmp eq i64 %5197, 2
  %5199 = zext i1 %5198 to i8
  store i8 %5199, i8* %39, align 1
  %5200 = add i64 %5176, 4
  %5201 = add i64 %5142, 29
  store i64 %5201, i64* %3, align 8
  %5202 = inttoptr i64 %5200 to i32*
  %5203 = load i32, i32* %5202, align 4
  %5204 = zext i32 %5203 to i64
  store i64 %5204, i64* %RCX.i11580, align 8
  %5205 = load i64, i64* %RBP.i, align 8
  %5206 = add i64 %5205, -144
  %5207 = add i64 %5142, 35
  store i64 %5207, i64* %3, align 8
  %5208 = inttoptr i64 %5206 to i32*
  %5209 = load i32, i32* %5208, align 4
  %5210 = add i32 %5209, %5203
  %5211 = zext i32 %5210 to i64
  store i64 %5211, i64* %RCX.i11580, align 8
  %5212 = icmp ult i32 %5210, %5203
  %5213 = icmp ult i32 %5210, %5209
  %5214 = or i1 %5212, %5213
  %5215 = zext i1 %5214 to i8
  store i8 %5215, i8* %14, align 1
  %5216 = and i32 %5210, 255
  %5217 = tail call i32 @llvm.ctpop.i32(i32 %5216)
  %5218 = trunc i32 %5217 to i8
  %5219 = and i8 %5218, 1
  %5220 = xor i8 %5219, 1
  store i8 %5220, i8* %21, align 1
  %5221 = xor i32 %5209, %5203
  %5222 = xor i32 %5221, %5210
  %5223 = lshr i32 %5222, 4
  %5224 = trunc i32 %5223 to i8
  %5225 = and i8 %5224, 1
  store i8 %5225, i8* %27, align 1
  %5226 = icmp eq i32 %5210, 0
  %5227 = zext i1 %5226 to i8
  store i8 %5227, i8* %30, align 1
  %5228 = lshr i32 %5210, 31
  %5229 = trunc i32 %5228 to i8
  store i8 %5229, i8* %33, align 1
  %5230 = lshr i32 %5203, 31
  %5231 = lshr i32 %5209, 31
  %5232 = xor i32 %5228, %5230
  %5233 = xor i32 %5228, %5231
  %5234 = add nuw nsw i32 %5232, %5233
  %5235 = icmp eq i32 %5234, 2
  %5236 = zext i1 %5235 to i8
  store i8 %5236, i8* %39, align 1
  %5237 = add i64 %5142, 41
  store i64 %5237, i64* %3, align 8
  store i32 %5210, i32* %5208, align 4
  %5238 = load i64, i64* %RBP.i, align 8
  %5239 = add i64 %5238, -8
  %5240 = load i64, i64* %3, align 8
  %5241 = add i64 %5240, 4
  store i64 %5241, i64* %3, align 8
  %5242 = inttoptr i64 %5239 to i64*
  %5243 = load i64, i64* %5242, align 8
  %5244 = add i64 %5243, 51640
  store i64 %5244, i64* %RAX.i11582.pre-phi, align 8
  %5245 = icmp ugt i64 %5243, -51641
  %5246 = zext i1 %5245 to i8
  store i8 %5246, i8* %14, align 1
  %5247 = trunc i64 %5244 to i32
  %5248 = and i32 %5247, 255
  %5249 = tail call i32 @llvm.ctpop.i32(i32 %5248)
  %5250 = trunc i32 %5249 to i8
  %5251 = and i8 %5250, 1
  %5252 = xor i8 %5251, 1
  store i8 %5252, i8* %21, align 1
  %5253 = xor i64 %5243, 16
  %5254 = xor i64 %5253, %5244
  %5255 = lshr i64 %5254, 4
  %5256 = trunc i64 %5255 to i8
  %5257 = and i8 %5256, 1
  store i8 %5257, i8* %27, align 1
  %5258 = icmp eq i64 %5244, 0
  %5259 = zext i1 %5258 to i8
  store i8 %5259, i8* %30, align 1
  %5260 = lshr i64 %5244, 63
  %5261 = trunc i64 %5260 to i8
  store i8 %5261, i8* %33, align 1
  %5262 = lshr i64 %5243, 63
  %5263 = xor i64 %5260, %5262
  %5264 = add nuw nsw i64 %5263, %5260
  %5265 = icmp eq i64 %5264, 2
  %5266 = zext i1 %5265 to i8
  store i8 %5266, i8* %39, align 1
  %5267 = add i64 %5238, -150
  %5268 = add i64 %5240, 17
  store i64 %5268, i64* %3, align 8
  %5269 = inttoptr i64 %5267 to i16*
  %5270 = load i16, i16* %5269, align 2
  %5271 = zext i16 %5270 to i64
  store i64 %5271, i64* %RCX.i11580, align 8
  %5272 = zext i16 %5270 to i64
  %5273 = shl nuw nsw i64 %5272, 4
  store i64 %5273, i64* %573, align 8
  %5274 = add i64 %5273, %5244
  store i64 %5274, i64* %RAX.i11582.pre-phi, align 8
  %5275 = icmp ult i64 %5274, %5244
  %5276 = icmp ult i64 %5274, %5273
  %5277 = or i1 %5275, %5276
  %5278 = zext i1 %5277 to i8
  store i8 %5278, i8* %14, align 1
  %5279 = trunc i64 %5274 to i32
  %5280 = and i32 %5279, 255
  %5281 = tail call i32 @llvm.ctpop.i32(i32 %5280)
  %5282 = trunc i32 %5281 to i8
  %5283 = and i8 %5282, 1
  %5284 = xor i8 %5283, 1
  store i8 %5284, i8* %21, align 1
  %5285 = xor i64 %5273, %5244
  %5286 = xor i64 %5285, %5274
  %5287 = lshr i64 %5286, 4
  %5288 = trunc i64 %5287 to i8
  %5289 = and i8 %5288, 1
  store i8 %5289, i8* %27, align 1
  %5290 = icmp eq i64 %5274, 0
  %5291 = zext i1 %5290 to i8
  store i8 %5291, i8* %30, align 1
  %5292 = lshr i64 %5274, 63
  %5293 = trunc i64 %5292 to i8
  store i8 %5293, i8* %33, align 1
  %5294 = xor i64 %5292, %5260
  %5295 = add nuw nsw i64 %5294, %5292
  %5296 = icmp eq i64 %5295, 2
  %5297 = zext i1 %5296 to i8
  store i8 %5297, i8* %39, align 1
  %5298 = add i64 %5274, 8
  %5299 = add i64 %5240, 29
  store i64 %5299, i64* %3, align 8
  %5300 = inttoptr i64 %5298 to i32*
  %5301 = load i32, i32* %5300, align 4
  %5302 = zext i32 %5301 to i64
  store i64 %5302, i64* %RCX.i11580, align 8
  %5303 = load i64, i64* %RBP.i, align 8
  %5304 = add i64 %5303, -148
  %5305 = add i64 %5240, 35
  store i64 %5305, i64* %3, align 8
  %5306 = inttoptr i64 %5304 to i32*
  %5307 = load i32, i32* %5306, align 4
  %5308 = add i32 %5307, %5301
  %5309 = zext i32 %5308 to i64
  store i64 %5309, i64* %RCX.i11580, align 8
  %5310 = icmp ult i32 %5308, %5301
  %5311 = icmp ult i32 %5308, %5307
  %5312 = or i1 %5310, %5311
  %5313 = zext i1 %5312 to i8
  store i8 %5313, i8* %14, align 1
  %5314 = and i32 %5308, 255
  %5315 = tail call i32 @llvm.ctpop.i32(i32 %5314)
  %5316 = trunc i32 %5315 to i8
  %5317 = and i8 %5316, 1
  %5318 = xor i8 %5317, 1
  store i8 %5318, i8* %21, align 1
  %5319 = xor i32 %5307, %5301
  %5320 = xor i32 %5319, %5308
  %5321 = lshr i32 %5320, 4
  %5322 = trunc i32 %5321 to i8
  %5323 = and i8 %5322, 1
  store i8 %5323, i8* %27, align 1
  %5324 = icmp eq i32 %5308, 0
  %5325 = zext i1 %5324 to i8
  store i8 %5325, i8* %30, align 1
  %5326 = lshr i32 %5308, 31
  %5327 = trunc i32 %5326 to i8
  store i8 %5327, i8* %33, align 1
  %5328 = lshr i32 %5301, 31
  %5329 = lshr i32 %5307, 31
  %5330 = xor i32 %5326, %5328
  %5331 = xor i32 %5326, %5329
  %5332 = add nuw nsw i32 %5330, %5331
  %5333 = icmp eq i32 %5332, 2
  %5334 = zext i1 %5333 to i8
  store i8 %5334, i8* %39, align 1
  %5335 = add i64 %5240, 41
  store i64 %5335, i64* %3, align 8
  store i32 %5308, i32* %5306, align 4
  %5336 = load i64, i64* %RBP.i, align 8
  %5337 = add i64 %5336, -120
  %5338 = load i64, i64* %3, align 8
  %5339 = add i64 %5338, 4
  store i64 %5339, i64* %3, align 8
  %5340 = inttoptr i64 %5337 to i64*
  %5341 = load i64, i64* %5340, align 8
  store i64 %5341, i64* %RAX.i11582.pre-phi, align 8
  %5342 = add i64 %5336, -28
  %5343 = add i64 %5338, 7
  store i64 %5343, i64* %3, align 8
  %5344 = inttoptr i64 %5342 to i32*
  %5345 = load i32, i32* %5344, align 4
  %5346 = add i32 %5345, 8
  %5347 = zext i32 %5346 to i64
  store i64 %5347, i64* %RCX.i11580, align 8
  %5348 = icmp ugt i32 %5345, -9
  %5349 = zext i1 %5348 to i8
  store i8 %5349, i8* %14, align 1
  %5350 = and i32 %5346, 255
  %5351 = tail call i32 @llvm.ctpop.i32(i32 %5350)
  %5352 = trunc i32 %5351 to i8
  %5353 = and i8 %5352, 1
  %5354 = xor i8 %5353, 1
  store i8 %5354, i8* %21, align 1
  %5355 = xor i32 %5346, %5345
  %5356 = lshr i32 %5355, 4
  %5357 = trunc i32 %5356 to i8
  %5358 = and i8 %5357, 1
  store i8 %5358, i8* %27, align 1
  %5359 = icmp eq i32 %5346, 0
  %5360 = zext i1 %5359 to i8
  store i8 %5360, i8* %30, align 1
  %5361 = lshr i32 %5346, 31
  %5362 = trunc i32 %5361 to i8
  store i8 %5362, i8* %33, align 1
  %5363 = lshr i32 %5345, 31
  %5364 = xor i32 %5361, %5363
  %5365 = add nuw nsw i32 %5364, %5361
  %5366 = icmp eq i32 %5365, 2
  %5367 = zext i1 %5366 to i8
  store i8 %5367, i8* %39, align 1
  %5368 = sext i32 %5346 to i64
  store i64 %5368, i64* %573, align 8
  %5369 = shl nsw i64 %5368, 1
  %5370 = add i64 %5341, %5369
  %5371 = add i64 %5338, 17
  store i64 %5371, i64* %3, align 8
  %5372 = inttoptr i64 %5370 to i16*
  %5373 = load i16, i16* %5372, align 2
  store i16 %5373, i16* %SI.i, align 2
  %5374 = add i64 %5336, -150
  %5375 = add i64 %5338, 24
  store i64 %5375, i64* %3, align 8
  %5376 = inttoptr i64 %5374 to i16*
  store i16 %5373, i16* %5376, align 2
  %5377 = load i64, i64* %RBP.i, align 8
  %5378 = add i64 %5377, -8
  %5379 = load i64, i64* %3, align 8
  %5380 = add i64 %5379, 4
  store i64 %5380, i64* %3, align 8
  %5381 = inttoptr i64 %5378 to i64*
  %5382 = load i64, i64* %5381, align 8
  %5383 = add i64 %5382, 51640
  store i64 %5383, i64* %RAX.i11582.pre-phi, align 8
  %5384 = icmp ugt i64 %5382, -51641
  %5385 = zext i1 %5384 to i8
  store i8 %5385, i8* %14, align 1
  %5386 = trunc i64 %5383 to i32
  %5387 = and i32 %5386, 255
  %5388 = tail call i32 @llvm.ctpop.i32(i32 %5387)
  %5389 = trunc i32 %5388 to i8
  %5390 = and i8 %5389, 1
  %5391 = xor i8 %5390, 1
  store i8 %5391, i8* %21, align 1
  %5392 = xor i64 %5382, 16
  %5393 = xor i64 %5392, %5383
  %5394 = lshr i64 %5393, 4
  %5395 = trunc i64 %5394 to i8
  %5396 = and i8 %5395, 1
  store i8 %5396, i8* %27, align 1
  %5397 = icmp eq i64 %5383, 0
  %5398 = zext i1 %5397 to i8
  store i8 %5398, i8* %30, align 1
  %5399 = lshr i64 %5383, 63
  %5400 = trunc i64 %5399 to i8
  store i8 %5400, i8* %33, align 1
  %5401 = lshr i64 %5382, 63
  %5402 = xor i64 %5399, %5401
  %5403 = add nuw nsw i64 %5402, %5399
  %5404 = icmp eq i64 %5403, 2
  %5405 = zext i1 %5404 to i8
  store i8 %5405, i8* %39, align 1
  %5406 = add i64 %5377, -150
  %5407 = add i64 %5379, 17
  store i64 %5407, i64* %3, align 8
  %5408 = inttoptr i64 %5406 to i16*
  %5409 = load i16, i16* %5408, align 2
  %5410 = zext i16 %5409 to i64
  store i64 %5410, i64* %RCX.i11580, align 8
  %5411 = zext i16 %5409 to i64
  %5412 = shl nuw nsw i64 %5411, 4
  store i64 %5412, i64* %573, align 8
  %5413 = add i64 %5412, %5383
  store i64 %5413, i64* %RAX.i11582.pre-phi, align 8
  %5414 = icmp ult i64 %5413, %5383
  %5415 = icmp ult i64 %5413, %5412
  %5416 = or i1 %5414, %5415
  %5417 = zext i1 %5416 to i8
  store i8 %5417, i8* %14, align 1
  %5418 = trunc i64 %5413 to i32
  %5419 = and i32 %5418, 255
  %5420 = tail call i32 @llvm.ctpop.i32(i32 %5419)
  %5421 = trunc i32 %5420 to i8
  %5422 = and i8 %5421, 1
  %5423 = xor i8 %5422, 1
  store i8 %5423, i8* %21, align 1
  %5424 = xor i64 %5412, %5383
  %5425 = xor i64 %5424, %5413
  %5426 = lshr i64 %5425, 4
  %5427 = trunc i64 %5426 to i8
  %5428 = and i8 %5427, 1
  store i8 %5428, i8* %27, align 1
  %5429 = icmp eq i64 %5413, 0
  %5430 = zext i1 %5429 to i8
  store i8 %5430, i8* %30, align 1
  %5431 = lshr i64 %5413, 63
  %5432 = trunc i64 %5431 to i8
  store i8 %5432, i8* %33, align 1
  %5433 = xor i64 %5431, %5399
  %5434 = add nuw nsw i64 %5433, %5431
  %5435 = icmp eq i64 %5434, 2
  %5436 = zext i1 %5435 to i8
  store i8 %5436, i8* %39, align 1
  %5437 = inttoptr i64 %5413 to i32*
  %5438 = add i64 %5379, 28
  store i64 %5438, i64* %3, align 8
  %5439 = load i32, i32* %5437, align 4
  %5440 = zext i32 %5439 to i64
  store i64 %5440, i64* %RCX.i11580, align 8
  %5441 = load i64, i64* %RBP.i, align 8
  %5442 = add i64 %5441, -140
  %5443 = add i64 %5379, 34
  store i64 %5443, i64* %3, align 8
  %5444 = inttoptr i64 %5442 to i32*
  %5445 = load i32, i32* %5444, align 4
  %5446 = add i32 %5445, %5439
  %5447 = zext i32 %5446 to i64
  store i64 %5447, i64* %RCX.i11580, align 8
  %5448 = icmp ult i32 %5446, %5439
  %5449 = icmp ult i32 %5446, %5445
  %5450 = or i1 %5448, %5449
  %5451 = zext i1 %5450 to i8
  store i8 %5451, i8* %14, align 1
  %5452 = and i32 %5446, 255
  %5453 = tail call i32 @llvm.ctpop.i32(i32 %5452)
  %5454 = trunc i32 %5453 to i8
  %5455 = and i8 %5454, 1
  %5456 = xor i8 %5455, 1
  store i8 %5456, i8* %21, align 1
  %5457 = xor i32 %5445, %5439
  %5458 = xor i32 %5457, %5446
  %5459 = lshr i32 %5458, 4
  %5460 = trunc i32 %5459 to i8
  %5461 = and i8 %5460, 1
  store i8 %5461, i8* %27, align 1
  %5462 = icmp eq i32 %5446, 0
  %5463 = zext i1 %5462 to i8
  store i8 %5463, i8* %30, align 1
  %5464 = lshr i32 %5446, 31
  %5465 = trunc i32 %5464 to i8
  store i8 %5465, i8* %33, align 1
  %5466 = lshr i32 %5439, 31
  %5467 = lshr i32 %5445, 31
  %5468 = xor i32 %5464, %5466
  %5469 = xor i32 %5464, %5467
  %5470 = add nuw nsw i32 %5468, %5469
  %5471 = icmp eq i32 %5470, 2
  %5472 = zext i1 %5471 to i8
  store i8 %5472, i8* %39, align 1
  %5473 = add i64 %5379, 40
  store i64 %5473, i64* %3, align 8
  store i32 %5446, i32* %5444, align 4
  %5474 = load i64, i64* %RBP.i, align 8
  %5475 = add i64 %5474, -8
  %5476 = load i64, i64* %3, align 8
  %5477 = add i64 %5476, 4
  store i64 %5477, i64* %3, align 8
  %5478 = inttoptr i64 %5475 to i64*
  %5479 = load i64, i64* %5478, align 8
  %5480 = add i64 %5479, 51640
  store i64 %5480, i64* %RAX.i11582.pre-phi, align 8
  %5481 = icmp ugt i64 %5479, -51641
  %5482 = zext i1 %5481 to i8
  store i8 %5482, i8* %14, align 1
  %5483 = trunc i64 %5480 to i32
  %5484 = and i32 %5483, 255
  %5485 = tail call i32 @llvm.ctpop.i32(i32 %5484)
  %5486 = trunc i32 %5485 to i8
  %5487 = and i8 %5486, 1
  %5488 = xor i8 %5487, 1
  store i8 %5488, i8* %21, align 1
  %5489 = xor i64 %5479, 16
  %5490 = xor i64 %5489, %5480
  %5491 = lshr i64 %5490, 4
  %5492 = trunc i64 %5491 to i8
  %5493 = and i8 %5492, 1
  store i8 %5493, i8* %27, align 1
  %5494 = icmp eq i64 %5480, 0
  %5495 = zext i1 %5494 to i8
  store i8 %5495, i8* %30, align 1
  %5496 = lshr i64 %5480, 63
  %5497 = trunc i64 %5496 to i8
  store i8 %5497, i8* %33, align 1
  %5498 = lshr i64 %5479, 63
  %5499 = xor i64 %5496, %5498
  %5500 = add nuw nsw i64 %5499, %5496
  %5501 = icmp eq i64 %5500, 2
  %5502 = zext i1 %5501 to i8
  store i8 %5502, i8* %39, align 1
  %5503 = add i64 %5474, -150
  %5504 = add i64 %5476, 17
  store i64 %5504, i64* %3, align 8
  %5505 = inttoptr i64 %5503 to i16*
  %5506 = load i16, i16* %5505, align 2
  %5507 = zext i16 %5506 to i64
  store i64 %5507, i64* %RCX.i11580, align 8
  %5508 = zext i16 %5506 to i64
  %5509 = shl nuw nsw i64 %5508, 4
  store i64 %5509, i64* %573, align 8
  %5510 = add i64 %5509, %5480
  store i64 %5510, i64* %RAX.i11582.pre-phi, align 8
  %5511 = icmp ult i64 %5510, %5480
  %5512 = icmp ult i64 %5510, %5509
  %5513 = or i1 %5511, %5512
  %5514 = zext i1 %5513 to i8
  store i8 %5514, i8* %14, align 1
  %5515 = trunc i64 %5510 to i32
  %5516 = and i32 %5515, 255
  %5517 = tail call i32 @llvm.ctpop.i32(i32 %5516)
  %5518 = trunc i32 %5517 to i8
  %5519 = and i8 %5518, 1
  %5520 = xor i8 %5519, 1
  store i8 %5520, i8* %21, align 1
  %5521 = xor i64 %5509, %5480
  %5522 = xor i64 %5521, %5510
  %5523 = lshr i64 %5522, 4
  %5524 = trunc i64 %5523 to i8
  %5525 = and i8 %5524, 1
  store i8 %5525, i8* %27, align 1
  %5526 = icmp eq i64 %5510, 0
  %5527 = zext i1 %5526 to i8
  store i8 %5527, i8* %30, align 1
  %5528 = lshr i64 %5510, 63
  %5529 = trunc i64 %5528 to i8
  store i8 %5529, i8* %33, align 1
  %5530 = xor i64 %5528, %5496
  %5531 = add nuw nsw i64 %5530, %5528
  %5532 = icmp eq i64 %5531, 2
  %5533 = zext i1 %5532 to i8
  store i8 %5533, i8* %39, align 1
  %5534 = add i64 %5510, 4
  %5535 = add i64 %5476, 29
  store i64 %5535, i64* %3, align 8
  %5536 = inttoptr i64 %5534 to i32*
  %5537 = load i32, i32* %5536, align 4
  %5538 = zext i32 %5537 to i64
  store i64 %5538, i64* %RCX.i11580, align 8
  %5539 = load i64, i64* %RBP.i, align 8
  %5540 = add i64 %5539, -144
  %5541 = add i64 %5476, 35
  store i64 %5541, i64* %3, align 8
  %5542 = inttoptr i64 %5540 to i32*
  %5543 = load i32, i32* %5542, align 4
  %5544 = add i32 %5543, %5537
  %5545 = zext i32 %5544 to i64
  store i64 %5545, i64* %RCX.i11580, align 8
  %5546 = icmp ult i32 %5544, %5537
  %5547 = icmp ult i32 %5544, %5543
  %5548 = or i1 %5546, %5547
  %5549 = zext i1 %5548 to i8
  store i8 %5549, i8* %14, align 1
  %5550 = and i32 %5544, 255
  %5551 = tail call i32 @llvm.ctpop.i32(i32 %5550)
  %5552 = trunc i32 %5551 to i8
  %5553 = and i8 %5552, 1
  %5554 = xor i8 %5553, 1
  store i8 %5554, i8* %21, align 1
  %5555 = xor i32 %5543, %5537
  %5556 = xor i32 %5555, %5544
  %5557 = lshr i32 %5556, 4
  %5558 = trunc i32 %5557 to i8
  %5559 = and i8 %5558, 1
  store i8 %5559, i8* %27, align 1
  %5560 = icmp eq i32 %5544, 0
  %5561 = zext i1 %5560 to i8
  store i8 %5561, i8* %30, align 1
  %5562 = lshr i32 %5544, 31
  %5563 = trunc i32 %5562 to i8
  store i8 %5563, i8* %33, align 1
  %5564 = lshr i32 %5537, 31
  %5565 = lshr i32 %5543, 31
  %5566 = xor i32 %5562, %5564
  %5567 = xor i32 %5562, %5565
  %5568 = add nuw nsw i32 %5566, %5567
  %5569 = icmp eq i32 %5568, 2
  %5570 = zext i1 %5569 to i8
  store i8 %5570, i8* %39, align 1
  %5571 = add i64 %5476, 41
  store i64 %5571, i64* %3, align 8
  store i32 %5544, i32* %5542, align 4
  %5572 = load i64, i64* %RBP.i, align 8
  %5573 = add i64 %5572, -8
  %5574 = load i64, i64* %3, align 8
  %5575 = add i64 %5574, 4
  store i64 %5575, i64* %3, align 8
  %5576 = inttoptr i64 %5573 to i64*
  %5577 = load i64, i64* %5576, align 8
  %5578 = add i64 %5577, 51640
  store i64 %5578, i64* %RAX.i11582.pre-phi, align 8
  %5579 = icmp ugt i64 %5577, -51641
  %5580 = zext i1 %5579 to i8
  store i8 %5580, i8* %14, align 1
  %5581 = trunc i64 %5578 to i32
  %5582 = and i32 %5581, 255
  %5583 = tail call i32 @llvm.ctpop.i32(i32 %5582)
  %5584 = trunc i32 %5583 to i8
  %5585 = and i8 %5584, 1
  %5586 = xor i8 %5585, 1
  store i8 %5586, i8* %21, align 1
  %5587 = xor i64 %5577, 16
  %5588 = xor i64 %5587, %5578
  %5589 = lshr i64 %5588, 4
  %5590 = trunc i64 %5589 to i8
  %5591 = and i8 %5590, 1
  store i8 %5591, i8* %27, align 1
  %5592 = icmp eq i64 %5578, 0
  %5593 = zext i1 %5592 to i8
  store i8 %5593, i8* %30, align 1
  %5594 = lshr i64 %5578, 63
  %5595 = trunc i64 %5594 to i8
  store i8 %5595, i8* %33, align 1
  %5596 = lshr i64 %5577, 63
  %5597 = xor i64 %5594, %5596
  %5598 = add nuw nsw i64 %5597, %5594
  %5599 = icmp eq i64 %5598, 2
  %5600 = zext i1 %5599 to i8
  store i8 %5600, i8* %39, align 1
  %5601 = add i64 %5572, -150
  %5602 = add i64 %5574, 17
  store i64 %5602, i64* %3, align 8
  %5603 = inttoptr i64 %5601 to i16*
  %5604 = load i16, i16* %5603, align 2
  %5605 = zext i16 %5604 to i64
  store i64 %5605, i64* %RCX.i11580, align 8
  %5606 = zext i16 %5604 to i64
  %5607 = shl nuw nsw i64 %5606, 4
  store i64 %5607, i64* %573, align 8
  %5608 = add i64 %5607, %5578
  store i64 %5608, i64* %RAX.i11582.pre-phi, align 8
  %5609 = icmp ult i64 %5608, %5578
  %5610 = icmp ult i64 %5608, %5607
  %5611 = or i1 %5609, %5610
  %5612 = zext i1 %5611 to i8
  store i8 %5612, i8* %14, align 1
  %5613 = trunc i64 %5608 to i32
  %5614 = and i32 %5613, 255
  %5615 = tail call i32 @llvm.ctpop.i32(i32 %5614)
  %5616 = trunc i32 %5615 to i8
  %5617 = and i8 %5616, 1
  %5618 = xor i8 %5617, 1
  store i8 %5618, i8* %21, align 1
  %5619 = xor i64 %5607, %5578
  %5620 = xor i64 %5619, %5608
  %5621 = lshr i64 %5620, 4
  %5622 = trunc i64 %5621 to i8
  %5623 = and i8 %5622, 1
  store i8 %5623, i8* %27, align 1
  %5624 = icmp eq i64 %5608, 0
  %5625 = zext i1 %5624 to i8
  store i8 %5625, i8* %30, align 1
  %5626 = lshr i64 %5608, 63
  %5627 = trunc i64 %5626 to i8
  store i8 %5627, i8* %33, align 1
  %5628 = xor i64 %5626, %5594
  %5629 = add nuw nsw i64 %5628, %5626
  %5630 = icmp eq i64 %5629, 2
  %5631 = zext i1 %5630 to i8
  store i8 %5631, i8* %39, align 1
  %5632 = add i64 %5608, 8
  %5633 = add i64 %5574, 29
  store i64 %5633, i64* %3, align 8
  %5634 = inttoptr i64 %5632 to i32*
  %5635 = load i32, i32* %5634, align 4
  %5636 = zext i32 %5635 to i64
  store i64 %5636, i64* %RCX.i11580, align 8
  %5637 = load i64, i64* %RBP.i, align 8
  %5638 = add i64 %5637, -148
  %5639 = add i64 %5574, 35
  store i64 %5639, i64* %3, align 8
  %5640 = inttoptr i64 %5638 to i32*
  %5641 = load i32, i32* %5640, align 4
  %5642 = add i32 %5641, %5635
  %5643 = zext i32 %5642 to i64
  store i64 %5643, i64* %RCX.i11580, align 8
  %5644 = icmp ult i32 %5642, %5635
  %5645 = icmp ult i32 %5642, %5641
  %5646 = or i1 %5644, %5645
  %5647 = zext i1 %5646 to i8
  store i8 %5647, i8* %14, align 1
  %5648 = and i32 %5642, 255
  %5649 = tail call i32 @llvm.ctpop.i32(i32 %5648)
  %5650 = trunc i32 %5649 to i8
  %5651 = and i8 %5650, 1
  %5652 = xor i8 %5651, 1
  store i8 %5652, i8* %21, align 1
  %5653 = xor i32 %5641, %5635
  %5654 = xor i32 %5653, %5642
  %5655 = lshr i32 %5654, 4
  %5656 = trunc i32 %5655 to i8
  %5657 = and i8 %5656, 1
  store i8 %5657, i8* %27, align 1
  %5658 = icmp eq i32 %5642, 0
  %5659 = zext i1 %5658 to i8
  store i8 %5659, i8* %30, align 1
  %5660 = lshr i32 %5642, 31
  %5661 = trunc i32 %5660 to i8
  store i8 %5661, i8* %33, align 1
  %5662 = lshr i32 %5635, 31
  %5663 = lshr i32 %5641, 31
  %5664 = xor i32 %5660, %5662
  %5665 = xor i32 %5660, %5663
  %5666 = add nuw nsw i32 %5664, %5665
  %5667 = icmp eq i32 %5666, 2
  %5668 = zext i1 %5667 to i8
  store i8 %5668, i8* %39, align 1
  %5669 = add i64 %5574, 41
  store i64 %5669, i64* %3, align 8
  store i32 %5642, i32* %5640, align 4
  %5670 = load i64, i64* %RBP.i, align 8
  %5671 = add i64 %5670, -120
  %5672 = load i64, i64* %3, align 8
  %5673 = add i64 %5672, 4
  store i64 %5673, i64* %3, align 8
  %5674 = inttoptr i64 %5671 to i64*
  %5675 = load i64, i64* %5674, align 8
  store i64 %5675, i64* %RAX.i11582.pre-phi, align 8
  %5676 = add i64 %5670, -28
  %5677 = add i64 %5672, 7
  store i64 %5677, i64* %3, align 8
  %5678 = inttoptr i64 %5676 to i32*
  %5679 = load i32, i32* %5678, align 4
  %5680 = add i32 %5679, 9
  %5681 = zext i32 %5680 to i64
  store i64 %5681, i64* %RCX.i11580, align 8
  %5682 = icmp ugt i32 %5679, -10
  %5683 = zext i1 %5682 to i8
  store i8 %5683, i8* %14, align 1
  %5684 = and i32 %5680, 255
  %5685 = tail call i32 @llvm.ctpop.i32(i32 %5684)
  %5686 = trunc i32 %5685 to i8
  %5687 = and i8 %5686, 1
  %5688 = xor i8 %5687, 1
  store i8 %5688, i8* %21, align 1
  %5689 = xor i32 %5680, %5679
  %5690 = lshr i32 %5689, 4
  %5691 = trunc i32 %5690 to i8
  %5692 = and i8 %5691, 1
  store i8 %5692, i8* %27, align 1
  %5693 = icmp eq i32 %5680, 0
  %5694 = zext i1 %5693 to i8
  store i8 %5694, i8* %30, align 1
  %5695 = lshr i32 %5680, 31
  %5696 = trunc i32 %5695 to i8
  store i8 %5696, i8* %33, align 1
  %5697 = lshr i32 %5679, 31
  %5698 = xor i32 %5695, %5697
  %5699 = add nuw nsw i32 %5698, %5695
  %5700 = icmp eq i32 %5699, 2
  %5701 = zext i1 %5700 to i8
  store i8 %5701, i8* %39, align 1
  %5702 = sext i32 %5680 to i64
  store i64 %5702, i64* %573, align 8
  %5703 = shl nsw i64 %5702, 1
  %5704 = add i64 %5675, %5703
  %5705 = add i64 %5672, 17
  store i64 %5705, i64* %3, align 8
  %5706 = inttoptr i64 %5704 to i16*
  %5707 = load i16, i16* %5706, align 2
  store i16 %5707, i16* %SI.i, align 2
  %5708 = add i64 %5670, -150
  %5709 = add i64 %5672, 24
  store i64 %5709, i64* %3, align 8
  %5710 = inttoptr i64 %5708 to i16*
  store i16 %5707, i16* %5710, align 2
  %5711 = load i64, i64* %RBP.i, align 8
  %5712 = add i64 %5711, -8
  %5713 = load i64, i64* %3, align 8
  %5714 = add i64 %5713, 4
  store i64 %5714, i64* %3, align 8
  %5715 = inttoptr i64 %5712 to i64*
  %5716 = load i64, i64* %5715, align 8
  %5717 = add i64 %5716, 51640
  store i64 %5717, i64* %RAX.i11582.pre-phi, align 8
  %5718 = icmp ugt i64 %5716, -51641
  %5719 = zext i1 %5718 to i8
  store i8 %5719, i8* %14, align 1
  %5720 = trunc i64 %5717 to i32
  %5721 = and i32 %5720, 255
  %5722 = tail call i32 @llvm.ctpop.i32(i32 %5721)
  %5723 = trunc i32 %5722 to i8
  %5724 = and i8 %5723, 1
  %5725 = xor i8 %5724, 1
  store i8 %5725, i8* %21, align 1
  %5726 = xor i64 %5716, 16
  %5727 = xor i64 %5726, %5717
  %5728 = lshr i64 %5727, 4
  %5729 = trunc i64 %5728 to i8
  %5730 = and i8 %5729, 1
  store i8 %5730, i8* %27, align 1
  %5731 = icmp eq i64 %5717, 0
  %5732 = zext i1 %5731 to i8
  store i8 %5732, i8* %30, align 1
  %5733 = lshr i64 %5717, 63
  %5734 = trunc i64 %5733 to i8
  store i8 %5734, i8* %33, align 1
  %5735 = lshr i64 %5716, 63
  %5736 = xor i64 %5733, %5735
  %5737 = add nuw nsw i64 %5736, %5733
  %5738 = icmp eq i64 %5737, 2
  %5739 = zext i1 %5738 to i8
  store i8 %5739, i8* %39, align 1
  %5740 = add i64 %5711, -150
  %5741 = add i64 %5713, 17
  store i64 %5741, i64* %3, align 8
  %5742 = inttoptr i64 %5740 to i16*
  %5743 = load i16, i16* %5742, align 2
  %5744 = zext i16 %5743 to i64
  store i64 %5744, i64* %RCX.i11580, align 8
  %5745 = zext i16 %5743 to i64
  %5746 = shl nuw nsw i64 %5745, 4
  store i64 %5746, i64* %573, align 8
  %5747 = add i64 %5746, %5717
  store i64 %5747, i64* %RAX.i11582.pre-phi, align 8
  %5748 = icmp ult i64 %5747, %5717
  %5749 = icmp ult i64 %5747, %5746
  %5750 = or i1 %5748, %5749
  %5751 = zext i1 %5750 to i8
  store i8 %5751, i8* %14, align 1
  %5752 = trunc i64 %5747 to i32
  %5753 = and i32 %5752, 255
  %5754 = tail call i32 @llvm.ctpop.i32(i32 %5753)
  %5755 = trunc i32 %5754 to i8
  %5756 = and i8 %5755, 1
  %5757 = xor i8 %5756, 1
  store i8 %5757, i8* %21, align 1
  %5758 = xor i64 %5746, %5717
  %5759 = xor i64 %5758, %5747
  %5760 = lshr i64 %5759, 4
  %5761 = trunc i64 %5760 to i8
  %5762 = and i8 %5761, 1
  store i8 %5762, i8* %27, align 1
  %5763 = icmp eq i64 %5747, 0
  %5764 = zext i1 %5763 to i8
  store i8 %5764, i8* %30, align 1
  %5765 = lshr i64 %5747, 63
  %5766 = trunc i64 %5765 to i8
  store i8 %5766, i8* %33, align 1
  %5767 = xor i64 %5765, %5733
  %5768 = add nuw nsw i64 %5767, %5765
  %5769 = icmp eq i64 %5768, 2
  %5770 = zext i1 %5769 to i8
  store i8 %5770, i8* %39, align 1
  %5771 = inttoptr i64 %5747 to i32*
  %5772 = add i64 %5713, 28
  store i64 %5772, i64* %3, align 8
  %5773 = load i32, i32* %5771, align 4
  %5774 = zext i32 %5773 to i64
  store i64 %5774, i64* %RCX.i11580, align 8
  %5775 = load i64, i64* %RBP.i, align 8
  %5776 = add i64 %5775, -140
  %5777 = add i64 %5713, 34
  store i64 %5777, i64* %3, align 8
  %5778 = inttoptr i64 %5776 to i32*
  %5779 = load i32, i32* %5778, align 4
  %5780 = add i32 %5779, %5773
  %5781 = zext i32 %5780 to i64
  store i64 %5781, i64* %RCX.i11580, align 8
  %5782 = icmp ult i32 %5780, %5773
  %5783 = icmp ult i32 %5780, %5779
  %5784 = or i1 %5782, %5783
  %5785 = zext i1 %5784 to i8
  store i8 %5785, i8* %14, align 1
  %5786 = and i32 %5780, 255
  %5787 = tail call i32 @llvm.ctpop.i32(i32 %5786)
  %5788 = trunc i32 %5787 to i8
  %5789 = and i8 %5788, 1
  %5790 = xor i8 %5789, 1
  store i8 %5790, i8* %21, align 1
  %5791 = xor i32 %5779, %5773
  %5792 = xor i32 %5791, %5780
  %5793 = lshr i32 %5792, 4
  %5794 = trunc i32 %5793 to i8
  %5795 = and i8 %5794, 1
  store i8 %5795, i8* %27, align 1
  %5796 = icmp eq i32 %5780, 0
  %5797 = zext i1 %5796 to i8
  store i8 %5797, i8* %30, align 1
  %5798 = lshr i32 %5780, 31
  %5799 = trunc i32 %5798 to i8
  store i8 %5799, i8* %33, align 1
  %5800 = lshr i32 %5773, 31
  %5801 = lshr i32 %5779, 31
  %5802 = xor i32 %5798, %5800
  %5803 = xor i32 %5798, %5801
  %5804 = add nuw nsw i32 %5802, %5803
  %5805 = icmp eq i32 %5804, 2
  %5806 = zext i1 %5805 to i8
  store i8 %5806, i8* %39, align 1
  %5807 = add i64 %5713, 40
  store i64 %5807, i64* %3, align 8
  store i32 %5780, i32* %5778, align 4
  %5808 = load i64, i64* %RBP.i, align 8
  %5809 = add i64 %5808, -8
  %5810 = load i64, i64* %3, align 8
  %5811 = add i64 %5810, 4
  store i64 %5811, i64* %3, align 8
  %5812 = inttoptr i64 %5809 to i64*
  %5813 = load i64, i64* %5812, align 8
  %5814 = add i64 %5813, 51640
  store i64 %5814, i64* %RAX.i11582.pre-phi, align 8
  %5815 = icmp ugt i64 %5813, -51641
  %5816 = zext i1 %5815 to i8
  store i8 %5816, i8* %14, align 1
  %5817 = trunc i64 %5814 to i32
  %5818 = and i32 %5817, 255
  %5819 = tail call i32 @llvm.ctpop.i32(i32 %5818)
  %5820 = trunc i32 %5819 to i8
  %5821 = and i8 %5820, 1
  %5822 = xor i8 %5821, 1
  store i8 %5822, i8* %21, align 1
  %5823 = xor i64 %5813, 16
  %5824 = xor i64 %5823, %5814
  %5825 = lshr i64 %5824, 4
  %5826 = trunc i64 %5825 to i8
  %5827 = and i8 %5826, 1
  store i8 %5827, i8* %27, align 1
  %5828 = icmp eq i64 %5814, 0
  %5829 = zext i1 %5828 to i8
  store i8 %5829, i8* %30, align 1
  %5830 = lshr i64 %5814, 63
  %5831 = trunc i64 %5830 to i8
  store i8 %5831, i8* %33, align 1
  %5832 = lshr i64 %5813, 63
  %5833 = xor i64 %5830, %5832
  %5834 = add nuw nsw i64 %5833, %5830
  %5835 = icmp eq i64 %5834, 2
  %5836 = zext i1 %5835 to i8
  store i8 %5836, i8* %39, align 1
  %5837 = add i64 %5808, -150
  %5838 = add i64 %5810, 17
  store i64 %5838, i64* %3, align 8
  %5839 = inttoptr i64 %5837 to i16*
  %5840 = load i16, i16* %5839, align 2
  %5841 = zext i16 %5840 to i64
  store i64 %5841, i64* %RCX.i11580, align 8
  %5842 = zext i16 %5840 to i64
  %5843 = shl nuw nsw i64 %5842, 4
  store i64 %5843, i64* %573, align 8
  %5844 = add i64 %5843, %5814
  store i64 %5844, i64* %RAX.i11582.pre-phi, align 8
  %5845 = icmp ult i64 %5844, %5814
  %5846 = icmp ult i64 %5844, %5843
  %5847 = or i1 %5845, %5846
  %5848 = zext i1 %5847 to i8
  store i8 %5848, i8* %14, align 1
  %5849 = trunc i64 %5844 to i32
  %5850 = and i32 %5849, 255
  %5851 = tail call i32 @llvm.ctpop.i32(i32 %5850)
  %5852 = trunc i32 %5851 to i8
  %5853 = and i8 %5852, 1
  %5854 = xor i8 %5853, 1
  store i8 %5854, i8* %21, align 1
  %5855 = xor i64 %5843, %5814
  %5856 = xor i64 %5855, %5844
  %5857 = lshr i64 %5856, 4
  %5858 = trunc i64 %5857 to i8
  %5859 = and i8 %5858, 1
  store i8 %5859, i8* %27, align 1
  %5860 = icmp eq i64 %5844, 0
  %5861 = zext i1 %5860 to i8
  store i8 %5861, i8* %30, align 1
  %5862 = lshr i64 %5844, 63
  %5863 = trunc i64 %5862 to i8
  store i8 %5863, i8* %33, align 1
  %5864 = xor i64 %5862, %5830
  %5865 = add nuw nsw i64 %5864, %5862
  %5866 = icmp eq i64 %5865, 2
  %5867 = zext i1 %5866 to i8
  store i8 %5867, i8* %39, align 1
  %5868 = add i64 %5844, 4
  %5869 = add i64 %5810, 29
  store i64 %5869, i64* %3, align 8
  %5870 = inttoptr i64 %5868 to i32*
  %5871 = load i32, i32* %5870, align 4
  %5872 = zext i32 %5871 to i64
  store i64 %5872, i64* %RCX.i11580, align 8
  %5873 = load i64, i64* %RBP.i, align 8
  %5874 = add i64 %5873, -144
  %5875 = add i64 %5810, 35
  store i64 %5875, i64* %3, align 8
  %5876 = inttoptr i64 %5874 to i32*
  %5877 = load i32, i32* %5876, align 4
  %5878 = add i32 %5877, %5871
  %5879 = zext i32 %5878 to i64
  store i64 %5879, i64* %RCX.i11580, align 8
  %5880 = icmp ult i32 %5878, %5871
  %5881 = icmp ult i32 %5878, %5877
  %5882 = or i1 %5880, %5881
  %5883 = zext i1 %5882 to i8
  store i8 %5883, i8* %14, align 1
  %5884 = and i32 %5878, 255
  %5885 = tail call i32 @llvm.ctpop.i32(i32 %5884)
  %5886 = trunc i32 %5885 to i8
  %5887 = and i8 %5886, 1
  %5888 = xor i8 %5887, 1
  store i8 %5888, i8* %21, align 1
  %5889 = xor i32 %5877, %5871
  %5890 = xor i32 %5889, %5878
  %5891 = lshr i32 %5890, 4
  %5892 = trunc i32 %5891 to i8
  %5893 = and i8 %5892, 1
  store i8 %5893, i8* %27, align 1
  %5894 = icmp eq i32 %5878, 0
  %5895 = zext i1 %5894 to i8
  store i8 %5895, i8* %30, align 1
  %5896 = lshr i32 %5878, 31
  %5897 = trunc i32 %5896 to i8
  store i8 %5897, i8* %33, align 1
  %5898 = lshr i32 %5871, 31
  %5899 = lshr i32 %5877, 31
  %5900 = xor i32 %5896, %5898
  %5901 = xor i32 %5896, %5899
  %5902 = add nuw nsw i32 %5900, %5901
  %5903 = icmp eq i32 %5902, 2
  %5904 = zext i1 %5903 to i8
  store i8 %5904, i8* %39, align 1
  %5905 = add i64 %5810, 41
  store i64 %5905, i64* %3, align 8
  store i32 %5878, i32* %5876, align 4
  %5906 = load i64, i64* %RBP.i, align 8
  %5907 = add i64 %5906, -8
  %5908 = load i64, i64* %3, align 8
  %5909 = add i64 %5908, 4
  store i64 %5909, i64* %3, align 8
  %5910 = inttoptr i64 %5907 to i64*
  %5911 = load i64, i64* %5910, align 8
  %5912 = add i64 %5911, 51640
  store i64 %5912, i64* %RAX.i11582.pre-phi, align 8
  %5913 = icmp ugt i64 %5911, -51641
  %5914 = zext i1 %5913 to i8
  store i8 %5914, i8* %14, align 1
  %5915 = trunc i64 %5912 to i32
  %5916 = and i32 %5915, 255
  %5917 = tail call i32 @llvm.ctpop.i32(i32 %5916)
  %5918 = trunc i32 %5917 to i8
  %5919 = and i8 %5918, 1
  %5920 = xor i8 %5919, 1
  store i8 %5920, i8* %21, align 1
  %5921 = xor i64 %5911, 16
  %5922 = xor i64 %5921, %5912
  %5923 = lshr i64 %5922, 4
  %5924 = trunc i64 %5923 to i8
  %5925 = and i8 %5924, 1
  store i8 %5925, i8* %27, align 1
  %5926 = icmp eq i64 %5912, 0
  %5927 = zext i1 %5926 to i8
  store i8 %5927, i8* %30, align 1
  %5928 = lshr i64 %5912, 63
  %5929 = trunc i64 %5928 to i8
  store i8 %5929, i8* %33, align 1
  %5930 = lshr i64 %5911, 63
  %5931 = xor i64 %5928, %5930
  %5932 = add nuw nsw i64 %5931, %5928
  %5933 = icmp eq i64 %5932, 2
  %5934 = zext i1 %5933 to i8
  store i8 %5934, i8* %39, align 1
  %5935 = add i64 %5906, -150
  %5936 = add i64 %5908, 17
  store i64 %5936, i64* %3, align 8
  %5937 = inttoptr i64 %5935 to i16*
  %5938 = load i16, i16* %5937, align 2
  %5939 = zext i16 %5938 to i64
  store i64 %5939, i64* %RCX.i11580, align 8
  %5940 = zext i16 %5938 to i64
  %5941 = shl nuw nsw i64 %5940, 4
  store i64 %5941, i64* %573, align 8
  %5942 = add i64 %5941, %5912
  store i64 %5942, i64* %RAX.i11582.pre-phi, align 8
  %5943 = icmp ult i64 %5942, %5912
  %5944 = icmp ult i64 %5942, %5941
  %5945 = or i1 %5943, %5944
  %5946 = zext i1 %5945 to i8
  store i8 %5946, i8* %14, align 1
  %5947 = trunc i64 %5942 to i32
  %5948 = and i32 %5947, 255
  %5949 = tail call i32 @llvm.ctpop.i32(i32 %5948)
  %5950 = trunc i32 %5949 to i8
  %5951 = and i8 %5950, 1
  %5952 = xor i8 %5951, 1
  store i8 %5952, i8* %21, align 1
  %5953 = xor i64 %5941, %5912
  %5954 = xor i64 %5953, %5942
  %5955 = lshr i64 %5954, 4
  %5956 = trunc i64 %5955 to i8
  %5957 = and i8 %5956, 1
  store i8 %5957, i8* %27, align 1
  %5958 = icmp eq i64 %5942, 0
  %5959 = zext i1 %5958 to i8
  store i8 %5959, i8* %30, align 1
  %5960 = lshr i64 %5942, 63
  %5961 = trunc i64 %5960 to i8
  store i8 %5961, i8* %33, align 1
  %5962 = xor i64 %5960, %5928
  %5963 = add nuw nsw i64 %5962, %5960
  %5964 = icmp eq i64 %5963, 2
  %5965 = zext i1 %5964 to i8
  store i8 %5965, i8* %39, align 1
  %5966 = add i64 %5942, 8
  %5967 = add i64 %5908, 29
  store i64 %5967, i64* %3, align 8
  %5968 = inttoptr i64 %5966 to i32*
  %5969 = load i32, i32* %5968, align 4
  %5970 = zext i32 %5969 to i64
  store i64 %5970, i64* %RCX.i11580, align 8
  %5971 = load i64, i64* %RBP.i, align 8
  %5972 = add i64 %5971, -148
  %5973 = add i64 %5908, 35
  store i64 %5973, i64* %3, align 8
  %5974 = inttoptr i64 %5972 to i32*
  %5975 = load i32, i32* %5974, align 4
  %5976 = add i32 %5975, %5969
  %5977 = zext i32 %5976 to i64
  store i64 %5977, i64* %RCX.i11580, align 8
  %5978 = icmp ult i32 %5976, %5969
  %5979 = icmp ult i32 %5976, %5975
  %5980 = or i1 %5978, %5979
  %5981 = zext i1 %5980 to i8
  store i8 %5981, i8* %14, align 1
  %5982 = and i32 %5976, 255
  %5983 = tail call i32 @llvm.ctpop.i32(i32 %5982)
  %5984 = trunc i32 %5983 to i8
  %5985 = and i8 %5984, 1
  %5986 = xor i8 %5985, 1
  store i8 %5986, i8* %21, align 1
  %5987 = xor i32 %5975, %5969
  %5988 = xor i32 %5987, %5976
  %5989 = lshr i32 %5988, 4
  %5990 = trunc i32 %5989 to i8
  %5991 = and i8 %5990, 1
  store i8 %5991, i8* %27, align 1
  %5992 = icmp eq i32 %5976, 0
  %5993 = zext i1 %5992 to i8
  store i8 %5993, i8* %30, align 1
  %5994 = lshr i32 %5976, 31
  %5995 = trunc i32 %5994 to i8
  store i8 %5995, i8* %33, align 1
  %5996 = lshr i32 %5969, 31
  %5997 = lshr i32 %5975, 31
  %5998 = xor i32 %5994, %5996
  %5999 = xor i32 %5994, %5997
  %6000 = add nuw nsw i32 %5998, %5999
  %6001 = icmp eq i32 %6000, 2
  %6002 = zext i1 %6001 to i8
  store i8 %6002, i8* %39, align 1
  %6003 = add i64 %5908, 41
  store i64 %6003, i64* %3, align 8
  store i32 %5976, i32* %5974, align 4
  %6004 = load i64, i64* %RBP.i, align 8
  %6005 = add i64 %6004, -120
  %6006 = load i64, i64* %3, align 8
  %6007 = add i64 %6006, 4
  store i64 %6007, i64* %3, align 8
  %6008 = inttoptr i64 %6005 to i64*
  %6009 = load i64, i64* %6008, align 8
  store i64 %6009, i64* %RAX.i11582.pre-phi, align 8
  %6010 = add i64 %6004, -28
  %6011 = add i64 %6006, 7
  store i64 %6011, i64* %3, align 8
  %6012 = inttoptr i64 %6010 to i32*
  %6013 = load i32, i32* %6012, align 4
  %6014 = add i32 %6013, 10
  %6015 = zext i32 %6014 to i64
  store i64 %6015, i64* %RCX.i11580, align 8
  %6016 = icmp ugt i32 %6013, -11
  %6017 = zext i1 %6016 to i8
  store i8 %6017, i8* %14, align 1
  %6018 = and i32 %6014, 255
  %6019 = tail call i32 @llvm.ctpop.i32(i32 %6018)
  %6020 = trunc i32 %6019 to i8
  %6021 = and i8 %6020, 1
  %6022 = xor i8 %6021, 1
  store i8 %6022, i8* %21, align 1
  %6023 = xor i32 %6014, %6013
  %6024 = lshr i32 %6023, 4
  %6025 = trunc i32 %6024 to i8
  %6026 = and i8 %6025, 1
  store i8 %6026, i8* %27, align 1
  %6027 = icmp eq i32 %6014, 0
  %6028 = zext i1 %6027 to i8
  store i8 %6028, i8* %30, align 1
  %6029 = lshr i32 %6014, 31
  %6030 = trunc i32 %6029 to i8
  store i8 %6030, i8* %33, align 1
  %6031 = lshr i32 %6013, 31
  %6032 = xor i32 %6029, %6031
  %6033 = add nuw nsw i32 %6032, %6029
  %6034 = icmp eq i32 %6033, 2
  %6035 = zext i1 %6034 to i8
  store i8 %6035, i8* %39, align 1
  %6036 = sext i32 %6014 to i64
  store i64 %6036, i64* %573, align 8
  %6037 = shl nsw i64 %6036, 1
  %6038 = add i64 %6009, %6037
  %6039 = add i64 %6006, 17
  store i64 %6039, i64* %3, align 8
  %6040 = inttoptr i64 %6038 to i16*
  %6041 = load i16, i16* %6040, align 2
  store i16 %6041, i16* %SI.i, align 2
  %6042 = add i64 %6004, -150
  %6043 = add i64 %6006, 24
  store i64 %6043, i64* %3, align 8
  %6044 = inttoptr i64 %6042 to i16*
  store i16 %6041, i16* %6044, align 2
  %6045 = load i64, i64* %RBP.i, align 8
  %6046 = add i64 %6045, -8
  %6047 = load i64, i64* %3, align 8
  %6048 = add i64 %6047, 4
  store i64 %6048, i64* %3, align 8
  %6049 = inttoptr i64 %6046 to i64*
  %6050 = load i64, i64* %6049, align 8
  %6051 = add i64 %6050, 51640
  store i64 %6051, i64* %RAX.i11582.pre-phi, align 8
  %6052 = icmp ugt i64 %6050, -51641
  %6053 = zext i1 %6052 to i8
  store i8 %6053, i8* %14, align 1
  %6054 = trunc i64 %6051 to i32
  %6055 = and i32 %6054, 255
  %6056 = tail call i32 @llvm.ctpop.i32(i32 %6055)
  %6057 = trunc i32 %6056 to i8
  %6058 = and i8 %6057, 1
  %6059 = xor i8 %6058, 1
  store i8 %6059, i8* %21, align 1
  %6060 = xor i64 %6050, 16
  %6061 = xor i64 %6060, %6051
  %6062 = lshr i64 %6061, 4
  %6063 = trunc i64 %6062 to i8
  %6064 = and i8 %6063, 1
  store i8 %6064, i8* %27, align 1
  %6065 = icmp eq i64 %6051, 0
  %6066 = zext i1 %6065 to i8
  store i8 %6066, i8* %30, align 1
  %6067 = lshr i64 %6051, 63
  %6068 = trunc i64 %6067 to i8
  store i8 %6068, i8* %33, align 1
  %6069 = lshr i64 %6050, 63
  %6070 = xor i64 %6067, %6069
  %6071 = add nuw nsw i64 %6070, %6067
  %6072 = icmp eq i64 %6071, 2
  %6073 = zext i1 %6072 to i8
  store i8 %6073, i8* %39, align 1
  %6074 = add i64 %6045, -150
  %6075 = add i64 %6047, 17
  store i64 %6075, i64* %3, align 8
  %6076 = inttoptr i64 %6074 to i16*
  %6077 = load i16, i16* %6076, align 2
  %6078 = zext i16 %6077 to i64
  store i64 %6078, i64* %RCX.i11580, align 8
  %6079 = zext i16 %6077 to i64
  %6080 = shl nuw nsw i64 %6079, 4
  store i64 %6080, i64* %573, align 8
  %6081 = add i64 %6080, %6051
  store i64 %6081, i64* %RAX.i11582.pre-phi, align 8
  %6082 = icmp ult i64 %6081, %6051
  %6083 = icmp ult i64 %6081, %6080
  %6084 = or i1 %6082, %6083
  %6085 = zext i1 %6084 to i8
  store i8 %6085, i8* %14, align 1
  %6086 = trunc i64 %6081 to i32
  %6087 = and i32 %6086, 255
  %6088 = tail call i32 @llvm.ctpop.i32(i32 %6087)
  %6089 = trunc i32 %6088 to i8
  %6090 = and i8 %6089, 1
  %6091 = xor i8 %6090, 1
  store i8 %6091, i8* %21, align 1
  %6092 = xor i64 %6080, %6051
  %6093 = xor i64 %6092, %6081
  %6094 = lshr i64 %6093, 4
  %6095 = trunc i64 %6094 to i8
  %6096 = and i8 %6095, 1
  store i8 %6096, i8* %27, align 1
  %6097 = icmp eq i64 %6081, 0
  %6098 = zext i1 %6097 to i8
  store i8 %6098, i8* %30, align 1
  %6099 = lshr i64 %6081, 63
  %6100 = trunc i64 %6099 to i8
  store i8 %6100, i8* %33, align 1
  %6101 = xor i64 %6099, %6067
  %6102 = add nuw nsw i64 %6101, %6099
  %6103 = icmp eq i64 %6102, 2
  %6104 = zext i1 %6103 to i8
  store i8 %6104, i8* %39, align 1
  %6105 = inttoptr i64 %6081 to i32*
  %6106 = add i64 %6047, 28
  store i64 %6106, i64* %3, align 8
  %6107 = load i32, i32* %6105, align 4
  %6108 = zext i32 %6107 to i64
  store i64 %6108, i64* %RCX.i11580, align 8
  %6109 = load i64, i64* %RBP.i, align 8
  %6110 = add i64 %6109, -140
  %6111 = add i64 %6047, 34
  store i64 %6111, i64* %3, align 8
  %6112 = inttoptr i64 %6110 to i32*
  %6113 = load i32, i32* %6112, align 4
  %6114 = add i32 %6113, %6107
  %6115 = zext i32 %6114 to i64
  store i64 %6115, i64* %RCX.i11580, align 8
  %6116 = icmp ult i32 %6114, %6107
  %6117 = icmp ult i32 %6114, %6113
  %6118 = or i1 %6116, %6117
  %6119 = zext i1 %6118 to i8
  store i8 %6119, i8* %14, align 1
  %6120 = and i32 %6114, 255
  %6121 = tail call i32 @llvm.ctpop.i32(i32 %6120)
  %6122 = trunc i32 %6121 to i8
  %6123 = and i8 %6122, 1
  %6124 = xor i8 %6123, 1
  store i8 %6124, i8* %21, align 1
  %6125 = xor i32 %6113, %6107
  %6126 = xor i32 %6125, %6114
  %6127 = lshr i32 %6126, 4
  %6128 = trunc i32 %6127 to i8
  %6129 = and i8 %6128, 1
  store i8 %6129, i8* %27, align 1
  %6130 = icmp eq i32 %6114, 0
  %6131 = zext i1 %6130 to i8
  store i8 %6131, i8* %30, align 1
  %6132 = lshr i32 %6114, 31
  %6133 = trunc i32 %6132 to i8
  store i8 %6133, i8* %33, align 1
  %6134 = lshr i32 %6107, 31
  %6135 = lshr i32 %6113, 31
  %6136 = xor i32 %6132, %6134
  %6137 = xor i32 %6132, %6135
  %6138 = add nuw nsw i32 %6136, %6137
  %6139 = icmp eq i32 %6138, 2
  %6140 = zext i1 %6139 to i8
  store i8 %6140, i8* %39, align 1
  %6141 = add i64 %6047, 40
  store i64 %6141, i64* %3, align 8
  store i32 %6114, i32* %6112, align 4
  %6142 = load i64, i64* %RBP.i, align 8
  %6143 = add i64 %6142, -8
  %6144 = load i64, i64* %3, align 8
  %6145 = add i64 %6144, 4
  store i64 %6145, i64* %3, align 8
  %6146 = inttoptr i64 %6143 to i64*
  %6147 = load i64, i64* %6146, align 8
  %6148 = add i64 %6147, 51640
  store i64 %6148, i64* %RAX.i11582.pre-phi, align 8
  %6149 = icmp ugt i64 %6147, -51641
  %6150 = zext i1 %6149 to i8
  store i8 %6150, i8* %14, align 1
  %6151 = trunc i64 %6148 to i32
  %6152 = and i32 %6151, 255
  %6153 = tail call i32 @llvm.ctpop.i32(i32 %6152)
  %6154 = trunc i32 %6153 to i8
  %6155 = and i8 %6154, 1
  %6156 = xor i8 %6155, 1
  store i8 %6156, i8* %21, align 1
  %6157 = xor i64 %6147, 16
  %6158 = xor i64 %6157, %6148
  %6159 = lshr i64 %6158, 4
  %6160 = trunc i64 %6159 to i8
  %6161 = and i8 %6160, 1
  store i8 %6161, i8* %27, align 1
  %6162 = icmp eq i64 %6148, 0
  %6163 = zext i1 %6162 to i8
  store i8 %6163, i8* %30, align 1
  %6164 = lshr i64 %6148, 63
  %6165 = trunc i64 %6164 to i8
  store i8 %6165, i8* %33, align 1
  %6166 = lshr i64 %6147, 63
  %6167 = xor i64 %6164, %6166
  %6168 = add nuw nsw i64 %6167, %6164
  %6169 = icmp eq i64 %6168, 2
  %6170 = zext i1 %6169 to i8
  store i8 %6170, i8* %39, align 1
  %6171 = add i64 %6142, -150
  %6172 = add i64 %6144, 17
  store i64 %6172, i64* %3, align 8
  %6173 = inttoptr i64 %6171 to i16*
  %6174 = load i16, i16* %6173, align 2
  %6175 = zext i16 %6174 to i64
  store i64 %6175, i64* %RCX.i11580, align 8
  %6176 = zext i16 %6174 to i64
  %6177 = shl nuw nsw i64 %6176, 4
  store i64 %6177, i64* %573, align 8
  %6178 = add i64 %6177, %6148
  store i64 %6178, i64* %RAX.i11582.pre-phi, align 8
  %6179 = icmp ult i64 %6178, %6148
  %6180 = icmp ult i64 %6178, %6177
  %6181 = or i1 %6179, %6180
  %6182 = zext i1 %6181 to i8
  store i8 %6182, i8* %14, align 1
  %6183 = trunc i64 %6178 to i32
  %6184 = and i32 %6183, 255
  %6185 = tail call i32 @llvm.ctpop.i32(i32 %6184)
  %6186 = trunc i32 %6185 to i8
  %6187 = and i8 %6186, 1
  %6188 = xor i8 %6187, 1
  store i8 %6188, i8* %21, align 1
  %6189 = xor i64 %6177, %6148
  %6190 = xor i64 %6189, %6178
  %6191 = lshr i64 %6190, 4
  %6192 = trunc i64 %6191 to i8
  %6193 = and i8 %6192, 1
  store i8 %6193, i8* %27, align 1
  %6194 = icmp eq i64 %6178, 0
  %6195 = zext i1 %6194 to i8
  store i8 %6195, i8* %30, align 1
  %6196 = lshr i64 %6178, 63
  %6197 = trunc i64 %6196 to i8
  store i8 %6197, i8* %33, align 1
  %6198 = xor i64 %6196, %6164
  %6199 = add nuw nsw i64 %6198, %6196
  %6200 = icmp eq i64 %6199, 2
  %6201 = zext i1 %6200 to i8
  store i8 %6201, i8* %39, align 1
  %6202 = add i64 %6178, 4
  %6203 = add i64 %6144, 29
  store i64 %6203, i64* %3, align 8
  %6204 = inttoptr i64 %6202 to i32*
  %6205 = load i32, i32* %6204, align 4
  %6206 = zext i32 %6205 to i64
  store i64 %6206, i64* %RCX.i11580, align 8
  %6207 = load i64, i64* %RBP.i, align 8
  %6208 = add i64 %6207, -144
  %6209 = add i64 %6144, 35
  store i64 %6209, i64* %3, align 8
  %6210 = inttoptr i64 %6208 to i32*
  %6211 = load i32, i32* %6210, align 4
  %6212 = add i32 %6211, %6205
  %6213 = zext i32 %6212 to i64
  store i64 %6213, i64* %RCX.i11580, align 8
  %6214 = icmp ult i32 %6212, %6205
  %6215 = icmp ult i32 %6212, %6211
  %6216 = or i1 %6214, %6215
  %6217 = zext i1 %6216 to i8
  store i8 %6217, i8* %14, align 1
  %6218 = and i32 %6212, 255
  %6219 = tail call i32 @llvm.ctpop.i32(i32 %6218)
  %6220 = trunc i32 %6219 to i8
  %6221 = and i8 %6220, 1
  %6222 = xor i8 %6221, 1
  store i8 %6222, i8* %21, align 1
  %6223 = xor i32 %6211, %6205
  %6224 = xor i32 %6223, %6212
  %6225 = lshr i32 %6224, 4
  %6226 = trunc i32 %6225 to i8
  %6227 = and i8 %6226, 1
  store i8 %6227, i8* %27, align 1
  %6228 = icmp eq i32 %6212, 0
  %6229 = zext i1 %6228 to i8
  store i8 %6229, i8* %30, align 1
  %6230 = lshr i32 %6212, 31
  %6231 = trunc i32 %6230 to i8
  store i8 %6231, i8* %33, align 1
  %6232 = lshr i32 %6205, 31
  %6233 = lshr i32 %6211, 31
  %6234 = xor i32 %6230, %6232
  %6235 = xor i32 %6230, %6233
  %6236 = add nuw nsw i32 %6234, %6235
  %6237 = icmp eq i32 %6236, 2
  %6238 = zext i1 %6237 to i8
  store i8 %6238, i8* %39, align 1
  %6239 = add i64 %6144, 41
  store i64 %6239, i64* %3, align 8
  store i32 %6212, i32* %6210, align 4
  %6240 = load i64, i64* %RBP.i, align 8
  %6241 = add i64 %6240, -8
  %6242 = load i64, i64* %3, align 8
  %6243 = add i64 %6242, 4
  store i64 %6243, i64* %3, align 8
  %6244 = inttoptr i64 %6241 to i64*
  %6245 = load i64, i64* %6244, align 8
  %6246 = add i64 %6245, 51640
  store i64 %6246, i64* %RAX.i11582.pre-phi, align 8
  %6247 = icmp ugt i64 %6245, -51641
  %6248 = zext i1 %6247 to i8
  store i8 %6248, i8* %14, align 1
  %6249 = trunc i64 %6246 to i32
  %6250 = and i32 %6249, 255
  %6251 = tail call i32 @llvm.ctpop.i32(i32 %6250)
  %6252 = trunc i32 %6251 to i8
  %6253 = and i8 %6252, 1
  %6254 = xor i8 %6253, 1
  store i8 %6254, i8* %21, align 1
  %6255 = xor i64 %6245, 16
  %6256 = xor i64 %6255, %6246
  %6257 = lshr i64 %6256, 4
  %6258 = trunc i64 %6257 to i8
  %6259 = and i8 %6258, 1
  store i8 %6259, i8* %27, align 1
  %6260 = icmp eq i64 %6246, 0
  %6261 = zext i1 %6260 to i8
  store i8 %6261, i8* %30, align 1
  %6262 = lshr i64 %6246, 63
  %6263 = trunc i64 %6262 to i8
  store i8 %6263, i8* %33, align 1
  %6264 = lshr i64 %6245, 63
  %6265 = xor i64 %6262, %6264
  %6266 = add nuw nsw i64 %6265, %6262
  %6267 = icmp eq i64 %6266, 2
  %6268 = zext i1 %6267 to i8
  store i8 %6268, i8* %39, align 1
  %6269 = add i64 %6240, -150
  %6270 = add i64 %6242, 17
  store i64 %6270, i64* %3, align 8
  %6271 = inttoptr i64 %6269 to i16*
  %6272 = load i16, i16* %6271, align 2
  %6273 = zext i16 %6272 to i64
  store i64 %6273, i64* %RCX.i11580, align 8
  %6274 = zext i16 %6272 to i64
  %6275 = shl nuw nsw i64 %6274, 4
  store i64 %6275, i64* %573, align 8
  %6276 = add i64 %6275, %6246
  store i64 %6276, i64* %RAX.i11582.pre-phi, align 8
  %6277 = icmp ult i64 %6276, %6246
  %6278 = icmp ult i64 %6276, %6275
  %6279 = or i1 %6277, %6278
  %6280 = zext i1 %6279 to i8
  store i8 %6280, i8* %14, align 1
  %6281 = trunc i64 %6276 to i32
  %6282 = and i32 %6281, 255
  %6283 = tail call i32 @llvm.ctpop.i32(i32 %6282)
  %6284 = trunc i32 %6283 to i8
  %6285 = and i8 %6284, 1
  %6286 = xor i8 %6285, 1
  store i8 %6286, i8* %21, align 1
  %6287 = xor i64 %6275, %6246
  %6288 = xor i64 %6287, %6276
  %6289 = lshr i64 %6288, 4
  %6290 = trunc i64 %6289 to i8
  %6291 = and i8 %6290, 1
  store i8 %6291, i8* %27, align 1
  %6292 = icmp eq i64 %6276, 0
  %6293 = zext i1 %6292 to i8
  store i8 %6293, i8* %30, align 1
  %6294 = lshr i64 %6276, 63
  %6295 = trunc i64 %6294 to i8
  store i8 %6295, i8* %33, align 1
  %6296 = xor i64 %6294, %6262
  %6297 = add nuw nsw i64 %6296, %6294
  %6298 = icmp eq i64 %6297, 2
  %6299 = zext i1 %6298 to i8
  store i8 %6299, i8* %39, align 1
  %6300 = add i64 %6276, 8
  %6301 = add i64 %6242, 29
  store i64 %6301, i64* %3, align 8
  %6302 = inttoptr i64 %6300 to i32*
  %6303 = load i32, i32* %6302, align 4
  %6304 = zext i32 %6303 to i64
  store i64 %6304, i64* %RCX.i11580, align 8
  %6305 = load i64, i64* %RBP.i, align 8
  %6306 = add i64 %6305, -148
  %6307 = add i64 %6242, 35
  store i64 %6307, i64* %3, align 8
  %6308 = inttoptr i64 %6306 to i32*
  %6309 = load i32, i32* %6308, align 4
  %6310 = add i32 %6309, %6303
  %6311 = zext i32 %6310 to i64
  store i64 %6311, i64* %RCX.i11580, align 8
  %6312 = icmp ult i32 %6310, %6303
  %6313 = icmp ult i32 %6310, %6309
  %6314 = or i1 %6312, %6313
  %6315 = zext i1 %6314 to i8
  store i8 %6315, i8* %14, align 1
  %6316 = and i32 %6310, 255
  %6317 = tail call i32 @llvm.ctpop.i32(i32 %6316)
  %6318 = trunc i32 %6317 to i8
  %6319 = and i8 %6318, 1
  %6320 = xor i8 %6319, 1
  store i8 %6320, i8* %21, align 1
  %6321 = xor i32 %6309, %6303
  %6322 = xor i32 %6321, %6310
  %6323 = lshr i32 %6322, 4
  %6324 = trunc i32 %6323 to i8
  %6325 = and i8 %6324, 1
  store i8 %6325, i8* %27, align 1
  %6326 = icmp eq i32 %6310, 0
  %6327 = zext i1 %6326 to i8
  store i8 %6327, i8* %30, align 1
  %6328 = lshr i32 %6310, 31
  %6329 = trunc i32 %6328 to i8
  store i8 %6329, i8* %33, align 1
  %6330 = lshr i32 %6303, 31
  %6331 = lshr i32 %6309, 31
  %6332 = xor i32 %6328, %6330
  %6333 = xor i32 %6328, %6331
  %6334 = add nuw nsw i32 %6332, %6333
  %6335 = icmp eq i32 %6334, 2
  %6336 = zext i1 %6335 to i8
  store i8 %6336, i8* %39, align 1
  %6337 = add i64 %6242, 41
  store i64 %6337, i64* %3, align 8
  store i32 %6310, i32* %6308, align 4
  %6338 = load i64, i64* %RBP.i, align 8
  %6339 = add i64 %6338, -120
  %6340 = load i64, i64* %3, align 8
  %6341 = add i64 %6340, 4
  store i64 %6341, i64* %3, align 8
  %6342 = inttoptr i64 %6339 to i64*
  %6343 = load i64, i64* %6342, align 8
  store i64 %6343, i64* %RAX.i11582.pre-phi, align 8
  %6344 = add i64 %6338, -28
  %6345 = add i64 %6340, 7
  store i64 %6345, i64* %3, align 8
  %6346 = inttoptr i64 %6344 to i32*
  %6347 = load i32, i32* %6346, align 4
  %6348 = add i32 %6347, 11
  %6349 = zext i32 %6348 to i64
  store i64 %6349, i64* %RCX.i11580, align 8
  %6350 = icmp ugt i32 %6347, -12
  %6351 = zext i1 %6350 to i8
  store i8 %6351, i8* %14, align 1
  %6352 = and i32 %6348, 255
  %6353 = tail call i32 @llvm.ctpop.i32(i32 %6352)
  %6354 = trunc i32 %6353 to i8
  %6355 = and i8 %6354, 1
  %6356 = xor i8 %6355, 1
  store i8 %6356, i8* %21, align 1
  %6357 = xor i32 %6348, %6347
  %6358 = lshr i32 %6357, 4
  %6359 = trunc i32 %6358 to i8
  %6360 = and i8 %6359, 1
  store i8 %6360, i8* %27, align 1
  %6361 = icmp eq i32 %6348, 0
  %6362 = zext i1 %6361 to i8
  store i8 %6362, i8* %30, align 1
  %6363 = lshr i32 %6348, 31
  %6364 = trunc i32 %6363 to i8
  store i8 %6364, i8* %33, align 1
  %6365 = lshr i32 %6347, 31
  %6366 = xor i32 %6363, %6365
  %6367 = add nuw nsw i32 %6366, %6363
  %6368 = icmp eq i32 %6367, 2
  %6369 = zext i1 %6368 to i8
  store i8 %6369, i8* %39, align 1
  %6370 = sext i32 %6348 to i64
  store i64 %6370, i64* %573, align 8
  %6371 = shl nsw i64 %6370, 1
  %6372 = add i64 %6343, %6371
  %6373 = add i64 %6340, 17
  store i64 %6373, i64* %3, align 8
  %6374 = inttoptr i64 %6372 to i16*
  %6375 = load i16, i16* %6374, align 2
  store i16 %6375, i16* %SI.i, align 2
  %6376 = add i64 %6338, -150
  %6377 = add i64 %6340, 24
  store i64 %6377, i64* %3, align 8
  %6378 = inttoptr i64 %6376 to i16*
  store i16 %6375, i16* %6378, align 2
  %6379 = load i64, i64* %RBP.i, align 8
  %6380 = add i64 %6379, -8
  %6381 = load i64, i64* %3, align 8
  %6382 = add i64 %6381, 4
  store i64 %6382, i64* %3, align 8
  %6383 = inttoptr i64 %6380 to i64*
  %6384 = load i64, i64* %6383, align 8
  %6385 = add i64 %6384, 51640
  store i64 %6385, i64* %RAX.i11582.pre-phi, align 8
  %6386 = icmp ugt i64 %6384, -51641
  %6387 = zext i1 %6386 to i8
  store i8 %6387, i8* %14, align 1
  %6388 = trunc i64 %6385 to i32
  %6389 = and i32 %6388, 255
  %6390 = tail call i32 @llvm.ctpop.i32(i32 %6389)
  %6391 = trunc i32 %6390 to i8
  %6392 = and i8 %6391, 1
  %6393 = xor i8 %6392, 1
  store i8 %6393, i8* %21, align 1
  %6394 = xor i64 %6384, 16
  %6395 = xor i64 %6394, %6385
  %6396 = lshr i64 %6395, 4
  %6397 = trunc i64 %6396 to i8
  %6398 = and i8 %6397, 1
  store i8 %6398, i8* %27, align 1
  %6399 = icmp eq i64 %6385, 0
  %6400 = zext i1 %6399 to i8
  store i8 %6400, i8* %30, align 1
  %6401 = lshr i64 %6385, 63
  %6402 = trunc i64 %6401 to i8
  store i8 %6402, i8* %33, align 1
  %6403 = lshr i64 %6384, 63
  %6404 = xor i64 %6401, %6403
  %6405 = add nuw nsw i64 %6404, %6401
  %6406 = icmp eq i64 %6405, 2
  %6407 = zext i1 %6406 to i8
  store i8 %6407, i8* %39, align 1
  %6408 = add i64 %6379, -150
  %6409 = add i64 %6381, 17
  store i64 %6409, i64* %3, align 8
  %6410 = inttoptr i64 %6408 to i16*
  %6411 = load i16, i16* %6410, align 2
  %6412 = zext i16 %6411 to i64
  store i64 %6412, i64* %RCX.i11580, align 8
  %6413 = zext i16 %6411 to i64
  %6414 = shl nuw nsw i64 %6413, 4
  store i64 %6414, i64* %573, align 8
  %6415 = add i64 %6414, %6385
  store i64 %6415, i64* %RAX.i11582.pre-phi, align 8
  %6416 = icmp ult i64 %6415, %6385
  %6417 = icmp ult i64 %6415, %6414
  %6418 = or i1 %6416, %6417
  %6419 = zext i1 %6418 to i8
  store i8 %6419, i8* %14, align 1
  %6420 = trunc i64 %6415 to i32
  %6421 = and i32 %6420, 255
  %6422 = tail call i32 @llvm.ctpop.i32(i32 %6421)
  %6423 = trunc i32 %6422 to i8
  %6424 = and i8 %6423, 1
  %6425 = xor i8 %6424, 1
  store i8 %6425, i8* %21, align 1
  %6426 = xor i64 %6414, %6385
  %6427 = xor i64 %6426, %6415
  %6428 = lshr i64 %6427, 4
  %6429 = trunc i64 %6428 to i8
  %6430 = and i8 %6429, 1
  store i8 %6430, i8* %27, align 1
  %6431 = icmp eq i64 %6415, 0
  %6432 = zext i1 %6431 to i8
  store i8 %6432, i8* %30, align 1
  %6433 = lshr i64 %6415, 63
  %6434 = trunc i64 %6433 to i8
  store i8 %6434, i8* %33, align 1
  %6435 = xor i64 %6433, %6401
  %6436 = add nuw nsw i64 %6435, %6433
  %6437 = icmp eq i64 %6436, 2
  %6438 = zext i1 %6437 to i8
  store i8 %6438, i8* %39, align 1
  %6439 = inttoptr i64 %6415 to i32*
  %6440 = add i64 %6381, 28
  store i64 %6440, i64* %3, align 8
  %6441 = load i32, i32* %6439, align 4
  %6442 = zext i32 %6441 to i64
  store i64 %6442, i64* %RCX.i11580, align 8
  %6443 = load i64, i64* %RBP.i, align 8
  %6444 = add i64 %6443, -140
  %6445 = add i64 %6381, 34
  store i64 %6445, i64* %3, align 8
  %6446 = inttoptr i64 %6444 to i32*
  %6447 = load i32, i32* %6446, align 4
  %6448 = add i32 %6447, %6441
  %6449 = zext i32 %6448 to i64
  store i64 %6449, i64* %RCX.i11580, align 8
  %6450 = icmp ult i32 %6448, %6441
  %6451 = icmp ult i32 %6448, %6447
  %6452 = or i1 %6450, %6451
  %6453 = zext i1 %6452 to i8
  store i8 %6453, i8* %14, align 1
  %6454 = and i32 %6448, 255
  %6455 = tail call i32 @llvm.ctpop.i32(i32 %6454)
  %6456 = trunc i32 %6455 to i8
  %6457 = and i8 %6456, 1
  %6458 = xor i8 %6457, 1
  store i8 %6458, i8* %21, align 1
  %6459 = xor i32 %6447, %6441
  %6460 = xor i32 %6459, %6448
  %6461 = lshr i32 %6460, 4
  %6462 = trunc i32 %6461 to i8
  %6463 = and i8 %6462, 1
  store i8 %6463, i8* %27, align 1
  %6464 = icmp eq i32 %6448, 0
  %6465 = zext i1 %6464 to i8
  store i8 %6465, i8* %30, align 1
  %6466 = lshr i32 %6448, 31
  %6467 = trunc i32 %6466 to i8
  store i8 %6467, i8* %33, align 1
  %6468 = lshr i32 %6441, 31
  %6469 = lshr i32 %6447, 31
  %6470 = xor i32 %6466, %6468
  %6471 = xor i32 %6466, %6469
  %6472 = add nuw nsw i32 %6470, %6471
  %6473 = icmp eq i32 %6472, 2
  %6474 = zext i1 %6473 to i8
  store i8 %6474, i8* %39, align 1
  %6475 = add i64 %6381, 40
  store i64 %6475, i64* %3, align 8
  store i32 %6448, i32* %6446, align 4
  %6476 = load i64, i64* %RBP.i, align 8
  %6477 = add i64 %6476, -8
  %6478 = load i64, i64* %3, align 8
  %6479 = add i64 %6478, 4
  store i64 %6479, i64* %3, align 8
  %6480 = inttoptr i64 %6477 to i64*
  %6481 = load i64, i64* %6480, align 8
  %6482 = add i64 %6481, 51640
  store i64 %6482, i64* %RAX.i11582.pre-phi, align 8
  %6483 = icmp ugt i64 %6481, -51641
  %6484 = zext i1 %6483 to i8
  store i8 %6484, i8* %14, align 1
  %6485 = trunc i64 %6482 to i32
  %6486 = and i32 %6485, 255
  %6487 = tail call i32 @llvm.ctpop.i32(i32 %6486)
  %6488 = trunc i32 %6487 to i8
  %6489 = and i8 %6488, 1
  %6490 = xor i8 %6489, 1
  store i8 %6490, i8* %21, align 1
  %6491 = xor i64 %6481, 16
  %6492 = xor i64 %6491, %6482
  %6493 = lshr i64 %6492, 4
  %6494 = trunc i64 %6493 to i8
  %6495 = and i8 %6494, 1
  store i8 %6495, i8* %27, align 1
  %6496 = icmp eq i64 %6482, 0
  %6497 = zext i1 %6496 to i8
  store i8 %6497, i8* %30, align 1
  %6498 = lshr i64 %6482, 63
  %6499 = trunc i64 %6498 to i8
  store i8 %6499, i8* %33, align 1
  %6500 = lshr i64 %6481, 63
  %6501 = xor i64 %6498, %6500
  %6502 = add nuw nsw i64 %6501, %6498
  %6503 = icmp eq i64 %6502, 2
  %6504 = zext i1 %6503 to i8
  store i8 %6504, i8* %39, align 1
  %6505 = add i64 %6476, -150
  %6506 = add i64 %6478, 17
  store i64 %6506, i64* %3, align 8
  %6507 = inttoptr i64 %6505 to i16*
  %6508 = load i16, i16* %6507, align 2
  %6509 = zext i16 %6508 to i64
  store i64 %6509, i64* %RCX.i11580, align 8
  %6510 = zext i16 %6508 to i64
  %6511 = shl nuw nsw i64 %6510, 4
  store i64 %6511, i64* %573, align 8
  %6512 = add i64 %6511, %6482
  store i64 %6512, i64* %RAX.i11582.pre-phi, align 8
  %6513 = icmp ult i64 %6512, %6482
  %6514 = icmp ult i64 %6512, %6511
  %6515 = or i1 %6513, %6514
  %6516 = zext i1 %6515 to i8
  store i8 %6516, i8* %14, align 1
  %6517 = trunc i64 %6512 to i32
  %6518 = and i32 %6517, 255
  %6519 = tail call i32 @llvm.ctpop.i32(i32 %6518)
  %6520 = trunc i32 %6519 to i8
  %6521 = and i8 %6520, 1
  %6522 = xor i8 %6521, 1
  store i8 %6522, i8* %21, align 1
  %6523 = xor i64 %6511, %6482
  %6524 = xor i64 %6523, %6512
  %6525 = lshr i64 %6524, 4
  %6526 = trunc i64 %6525 to i8
  %6527 = and i8 %6526, 1
  store i8 %6527, i8* %27, align 1
  %6528 = icmp eq i64 %6512, 0
  %6529 = zext i1 %6528 to i8
  store i8 %6529, i8* %30, align 1
  %6530 = lshr i64 %6512, 63
  %6531 = trunc i64 %6530 to i8
  store i8 %6531, i8* %33, align 1
  %6532 = xor i64 %6530, %6498
  %6533 = add nuw nsw i64 %6532, %6530
  %6534 = icmp eq i64 %6533, 2
  %6535 = zext i1 %6534 to i8
  store i8 %6535, i8* %39, align 1
  %6536 = add i64 %6512, 4
  %6537 = add i64 %6478, 29
  store i64 %6537, i64* %3, align 8
  %6538 = inttoptr i64 %6536 to i32*
  %6539 = load i32, i32* %6538, align 4
  %6540 = zext i32 %6539 to i64
  store i64 %6540, i64* %RCX.i11580, align 8
  %6541 = load i64, i64* %RBP.i, align 8
  %6542 = add i64 %6541, -144
  %6543 = add i64 %6478, 35
  store i64 %6543, i64* %3, align 8
  %6544 = inttoptr i64 %6542 to i32*
  %6545 = load i32, i32* %6544, align 4
  %6546 = add i32 %6545, %6539
  %6547 = zext i32 %6546 to i64
  store i64 %6547, i64* %RCX.i11580, align 8
  %6548 = icmp ult i32 %6546, %6539
  %6549 = icmp ult i32 %6546, %6545
  %6550 = or i1 %6548, %6549
  %6551 = zext i1 %6550 to i8
  store i8 %6551, i8* %14, align 1
  %6552 = and i32 %6546, 255
  %6553 = tail call i32 @llvm.ctpop.i32(i32 %6552)
  %6554 = trunc i32 %6553 to i8
  %6555 = and i8 %6554, 1
  %6556 = xor i8 %6555, 1
  store i8 %6556, i8* %21, align 1
  %6557 = xor i32 %6545, %6539
  %6558 = xor i32 %6557, %6546
  %6559 = lshr i32 %6558, 4
  %6560 = trunc i32 %6559 to i8
  %6561 = and i8 %6560, 1
  store i8 %6561, i8* %27, align 1
  %6562 = icmp eq i32 %6546, 0
  %6563 = zext i1 %6562 to i8
  store i8 %6563, i8* %30, align 1
  %6564 = lshr i32 %6546, 31
  %6565 = trunc i32 %6564 to i8
  store i8 %6565, i8* %33, align 1
  %6566 = lshr i32 %6539, 31
  %6567 = lshr i32 %6545, 31
  %6568 = xor i32 %6564, %6566
  %6569 = xor i32 %6564, %6567
  %6570 = add nuw nsw i32 %6568, %6569
  %6571 = icmp eq i32 %6570, 2
  %6572 = zext i1 %6571 to i8
  store i8 %6572, i8* %39, align 1
  %6573 = add i64 %6478, 41
  store i64 %6573, i64* %3, align 8
  store i32 %6546, i32* %6544, align 4
  %6574 = load i64, i64* %RBP.i, align 8
  %6575 = add i64 %6574, -8
  %6576 = load i64, i64* %3, align 8
  %6577 = add i64 %6576, 4
  store i64 %6577, i64* %3, align 8
  %6578 = inttoptr i64 %6575 to i64*
  %6579 = load i64, i64* %6578, align 8
  %6580 = add i64 %6579, 51640
  store i64 %6580, i64* %RAX.i11582.pre-phi, align 8
  %6581 = icmp ugt i64 %6579, -51641
  %6582 = zext i1 %6581 to i8
  store i8 %6582, i8* %14, align 1
  %6583 = trunc i64 %6580 to i32
  %6584 = and i32 %6583, 255
  %6585 = tail call i32 @llvm.ctpop.i32(i32 %6584)
  %6586 = trunc i32 %6585 to i8
  %6587 = and i8 %6586, 1
  %6588 = xor i8 %6587, 1
  store i8 %6588, i8* %21, align 1
  %6589 = xor i64 %6579, 16
  %6590 = xor i64 %6589, %6580
  %6591 = lshr i64 %6590, 4
  %6592 = trunc i64 %6591 to i8
  %6593 = and i8 %6592, 1
  store i8 %6593, i8* %27, align 1
  %6594 = icmp eq i64 %6580, 0
  %6595 = zext i1 %6594 to i8
  store i8 %6595, i8* %30, align 1
  %6596 = lshr i64 %6580, 63
  %6597 = trunc i64 %6596 to i8
  store i8 %6597, i8* %33, align 1
  %6598 = lshr i64 %6579, 63
  %6599 = xor i64 %6596, %6598
  %6600 = add nuw nsw i64 %6599, %6596
  %6601 = icmp eq i64 %6600, 2
  %6602 = zext i1 %6601 to i8
  store i8 %6602, i8* %39, align 1
  %6603 = add i64 %6574, -150
  %6604 = add i64 %6576, 17
  store i64 %6604, i64* %3, align 8
  %6605 = inttoptr i64 %6603 to i16*
  %6606 = load i16, i16* %6605, align 2
  %6607 = zext i16 %6606 to i64
  store i64 %6607, i64* %RCX.i11580, align 8
  %6608 = zext i16 %6606 to i64
  %6609 = shl nuw nsw i64 %6608, 4
  store i64 %6609, i64* %573, align 8
  %6610 = add i64 %6609, %6580
  store i64 %6610, i64* %RAX.i11582.pre-phi, align 8
  %6611 = icmp ult i64 %6610, %6580
  %6612 = icmp ult i64 %6610, %6609
  %6613 = or i1 %6611, %6612
  %6614 = zext i1 %6613 to i8
  store i8 %6614, i8* %14, align 1
  %6615 = trunc i64 %6610 to i32
  %6616 = and i32 %6615, 255
  %6617 = tail call i32 @llvm.ctpop.i32(i32 %6616)
  %6618 = trunc i32 %6617 to i8
  %6619 = and i8 %6618, 1
  %6620 = xor i8 %6619, 1
  store i8 %6620, i8* %21, align 1
  %6621 = xor i64 %6609, %6580
  %6622 = xor i64 %6621, %6610
  %6623 = lshr i64 %6622, 4
  %6624 = trunc i64 %6623 to i8
  %6625 = and i8 %6624, 1
  store i8 %6625, i8* %27, align 1
  %6626 = icmp eq i64 %6610, 0
  %6627 = zext i1 %6626 to i8
  store i8 %6627, i8* %30, align 1
  %6628 = lshr i64 %6610, 63
  %6629 = trunc i64 %6628 to i8
  store i8 %6629, i8* %33, align 1
  %6630 = xor i64 %6628, %6596
  %6631 = add nuw nsw i64 %6630, %6628
  %6632 = icmp eq i64 %6631, 2
  %6633 = zext i1 %6632 to i8
  store i8 %6633, i8* %39, align 1
  %6634 = add i64 %6610, 8
  %6635 = add i64 %6576, 29
  store i64 %6635, i64* %3, align 8
  %6636 = inttoptr i64 %6634 to i32*
  %6637 = load i32, i32* %6636, align 4
  %6638 = zext i32 %6637 to i64
  store i64 %6638, i64* %RCX.i11580, align 8
  %6639 = load i64, i64* %RBP.i, align 8
  %6640 = add i64 %6639, -148
  %6641 = add i64 %6576, 35
  store i64 %6641, i64* %3, align 8
  %6642 = inttoptr i64 %6640 to i32*
  %6643 = load i32, i32* %6642, align 4
  %6644 = add i32 %6643, %6637
  %6645 = zext i32 %6644 to i64
  store i64 %6645, i64* %RCX.i11580, align 8
  %6646 = icmp ult i32 %6644, %6637
  %6647 = icmp ult i32 %6644, %6643
  %6648 = or i1 %6646, %6647
  %6649 = zext i1 %6648 to i8
  store i8 %6649, i8* %14, align 1
  %6650 = and i32 %6644, 255
  %6651 = tail call i32 @llvm.ctpop.i32(i32 %6650)
  %6652 = trunc i32 %6651 to i8
  %6653 = and i8 %6652, 1
  %6654 = xor i8 %6653, 1
  store i8 %6654, i8* %21, align 1
  %6655 = xor i32 %6643, %6637
  %6656 = xor i32 %6655, %6644
  %6657 = lshr i32 %6656, 4
  %6658 = trunc i32 %6657 to i8
  %6659 = and i8 %6658, 1
  store i8 %6659, i8* %27, align 1
  %6660 = icmp eq i32 %6644, 0
  %6661 = zext i1 %6660 to i8
  store i8 %6661, i8* %30, align 1
  %6662 = lshr i32 %6644, 31
  %6663 = trunc i32 %6662 to i8
  store i8 %6663, i8* %33, align 1
  %6664 = lshr i32 %6637, 31
  %6665 = lshr i32 %6643, 31
  %6666 = xor i32 %6662, %6664
  %6667 = xor i32 %6662, %6665
  %6668 = add nuw nsw i32 %6666, %6667
  %6669 = icmp eq i32 %6668, 2
  %6670 = zext i1 %6669 to i8
  store i8 %6670, i8* %39, align 1
  %6671 = add i64 %6576, 41
  store i64 %6671, i64* %3, align 8
  store i32 %6644, i32* %6642, align 4
  %6672 = load i64, i64* %RBP.i, align 8
  %6673 = add i64 %6672, -120
  %6674 = load i64, i64* %3, align 8
  %6675 = add i64 %6674, 4
  store i64 %6675, i64* %3, align 8
  %6676 = inttoptr i64 %6673 to i64*
  %6677 = load i64, i64* %6676, align 8
  store i64 %6677, i64* %RAX.i11582.pre-phi, align 8
  %6678 = add i64 %6672, -28
  %6679 = add i64 %6674, 7
  store i64 %6679, i64* %3, align 8
  %6680 = inttoptr i64 %6678 to i32*
  %6681 = load i32, i32* %6680, align 4
  %6682 = add i32 %6681, 12
  %6683 = zext i32 %6682 to i64
  store i64 %6683, i64* %RCX.i11580, align 8
  %6684 = icmp ugt i32 %6681, -13
  %6685 = zext i1 %6684 to i8
  store i8 %6685, i8* %14, align 1
  %6686 = and i32 %6682, 255
  %6687 = tail call i32 @llvm.ctpop.i32(i32 %6686)
  %6688 = trunc i32 %6687 to i8
  %6689 = and i8 %6688, 1
  %6690 = xor i8 %6689, 1
  store i8 %6690, i8* %21, align 1
  %6691 = xor i32 %6682, %6681
  %6692 = lshr i32 %6691, 4
  %6693 = trunc i32 %6692 to i8
  %6694 = and i8 %6693, 1
  store i8 %6694, i8* %27, align 1
  %6695 = icmp eq i32 %6682, 0
  %6696 = zext i1 %6695 to i8
  store i8 %6696, i8* %30, align 1
  %6697 = lshr i32 %6682, 31
  %6698 = trunc i32 %6697 to i8
  store i8 %6698, i8* %33, align 1
  %6699 = lshr i32 %6681, 31
  %6700 = xor i32 %6697, %6699
  %6701 = add nuw nsw i32 %6700, %6697
  %6702 = icmp eq i32 %6701, 2
  %6703 = zext i1 %6702 to i8
  store i8 %6703, i8* %39, align 1
  %6704 = sext i32 %6682 to i64
  store i64 %6704, i64* %573, align 8
  %6705 = shl nsw i64 %6704, 1
  %6706 = add i64 %6677, %6705
  %6707 = add i64 %6674, 17
  store i64 %6707, i64* %3, align 8
  %6708 = inttoptr i64 %6706 to i16*
  %6709 = load i16, i16* %6708, align 2
  store i16 %6709, i16* %SI.i, align 2
  %6710 = add i64 %6672, -150
  %6711 = add i64 %6674, 24
  store i64 %6711, i64* %3, align 8
  %6712 = inttoptr i64 %6710 to i16*
  store i16 %6709, i16* %6712, align 2
  %6713 = load i64, i64* %RBP.i, align 8
  %6714 = add i64 %6713, -8
  %6715 = load i64, i64* %3, align 8
  %6716 = add i64 %6715, 4
  store i64 %6716, i64* %3, align 8
  %6717 = inttoptr i64 %6714 to i64*
  %6718 = load i64, i64* %6717, align 8
  %6719 = add i64 %6718, 51640
  store i64 %6719, i64* %RAX.i11582.pre-phi, align 8
  %6720 = icmp ugt i64 %6718, -51641
  %6721 = zext i1 %6720 to i8
  store i8 %6721, i8* %14, align 1
  %6722 = trunc i64 %6719 to i32
  %6723 = and i32 %6722, 255
  %6724 = tail call i32 @llvm.ctpop.i32(i32 %6723)
  %6725 = trunc i32 %6724 to i8
  %6726 = and i8 %6725, 1
  %6727 = xor i8 %6726, 1
  store i8 %6727, i8* %21, align 1
  %6728 = xor i64 %6718, 16
  %6729 = xor i64 %6728, %6719
  %6730 = lshr i64 %6729, 4
  %6731 = trunc i64 %6730 to i8
  %6732 = and i8 %6731, 1
  store i8 %6732, i8* %27, align 1
  %6733 = icmp eq i64 %6719, 0
  %6734 = zext i1 %6733 to i8
  store i8 %6734, i8* %30, align 1
  %6735 = lshr i64 %6719, 63
  %6736 = trunc i64 %6735 to i8
  store i8 %6736, i8* %33, align 1
  %6737 = lshr i64 %6718, 63
  %6738 = xor i64 %6735, %6737
  %6739 = add nuw nsw i64 %6738, %6735
  %6740 = icmp eq i64 %6739, 2
  %6741 = zext i1 %6740 to i8
  store i8 %6741, i8* %39, align 1
  %6742 = add i64 %6713, -150
  %6743 = add i64 %6715, 17
  store i64 %6743, i64* %3, align 8
  %6744 = inttoptr i64 %6742 to i16*
  %6745 = load i16, i16* %6744, align 2
  %6746 = zext i16 %6745 to i64
  store i64 %6746, i64* %RCX.i11580, align 8
  %6747 = zext i16 %6745 to i64
  %6748 = shl nuw nsw i64 %6747, 4
  store i64 %6748, i64* %573, align 8
  %6749 = add i64 %6748, %6719
  store i64 %6749, i64* %RAX.i11582.pre-phi, align 8
  %6750 = icmp ult i64 %6749, %6719
  %6751 = icmp ult i64 %6749, %6748
  %6752 = or i1 %6750, %6751
  %6753 = zext i1 %6752 to i8
  store i8 %6753, i8* %14, align 1
  %6754 = trunc i64 %6749 to i32
  %6755 = and i32 %6754, 255
  %6756 = tail call i32 @llvm.ctpop.i32(i32 %6755)
  %6757 = trunc i32 %6756 to i8
  %6758 = and i8 %6757, 1
  %6759 = xor i8 %6758, 1
  store i8 %6759, i8* %21, align 1
  %6760 = xor i64 %6748, %6719
  %6761 = xor i64 %6760, %6749
  %6762 = lshr i64 %6761, 4
  %6763 = trunc i64 %6762 to i8
  %6764 = and i8 %6763, 1
  store i8 %6764, i8* %27, align 1
  %6765 = icmp eq i64 %6749, 0
  %6766 = zext i1 %6765 to i8
  store i8 %6766, i8* %30, align 1
  %6767 = lshr i64 %6749, 63
  %6768 = trunc i64 %6767 to i8
  store i8 %6768, i8* %33, align 1
  %6769 = xor i64 %6767, %6735
  %6770 = add nuw nsw i64 %6769, %6767
  %6771 = icmp eq i64 %6770, 2
  %6772 = zext i1 %6771 to i8
  store i8 %6772, i8* %39, align 1
  %6773 = inttoptr i64 %6749 to i32*
  %6774 = add i64 %6715, 28
  store i64 %6774, i64* %3, align 8
  %6775 = load i32, i32* %6773, align 4
  %6776 = zext i32 %6775 to i64
  store i64 %6776, i64* %RCX.i11580, align 8
  %6777 = load i64, i64* %RBP.i, align 8
  %6778 = add i64 %6777, -140
  %6779 = add i64 %6715, 34
  store i64 %6779, i64* %3, align 8
  %6780 = inttoptr i64 %6778 to i32*
  %6781 = load i32, i32* %6780, align 4
  %6782 = add i32 %6781, %6775
  %6783 = zext i32 %6782 to i64
  store i64 %6783, i64* %RCX.i11580, align 8
  %6784 = icmp ult i32 %6782, %6775
  %6785 = icmp ult i32 %6782, %6781
  %6786 = or i1 %6784, %6785
  %6787 = zext i1 %6786 to i8
  store i8 %6787, i8* %14, align 1
  %6788 = and i32 %6782, 255
  %6789 = tail call i32 @llvm.ctpop.i32(i32 %6788)
  %6790 = trunc i32 %6789 to i8
  %6791 = and i8 %6790, 1
  %6792 = xor i8 %6791, 1
  store i8 %6792, i8* %21, align 1
  %6793 = xor i32 %6781, %6775
  %6794 = xor i32 %6793, %6782
  %6795 = lshr i32 %6794, 4
  %6796 = trunc i32 %6795 to i8
  %6797 = and i8 %6796, 1
  store i8 %6797, i8* %27, align 1
  %6798 = icmp eq i32 %6782, 0
  %6799 = zext i1 %6798 to i8
  store i8 %6799, i8* %30, align 1
  %6800 = lshr i32 %6782, 31
  %6801 = trunc i32 %6800 to i8
  store i8 %6801, i8* %33, align 1
  %6802 = lshr i32 %6775, 31
  %6803 = lshr i32 %6781, 31
  %6804 = xor i32 %6800, %6802
  %6805 = xor i32 %6800, %6803
  %6806 = add nuw nsw i32 %6804, %6805
  %6807 = icmp eq i32 %6806, 2
  %6808 = zext i1 %6807 to i8
  store i8 %6808, i8* %39, align 1
  %6809 = add i64 %6715, 40
  store i64 %6809, i64* %3, align 8
  store i32 %6782, i32* %6780, align 4
  %6810 = load i64, i64* %RBP.i, align 8
  %6811 = add i64 %6810, -8
  %6812 = load i64, i64* %3, align 8
  %6813 = add i64 %6812, 4
  store i64 %6813, i64* %3, align 8
  %6814 = inttoptr i64 %6811 to i64*
  %6815 = load i64, i64* %6814, align 8
  %6816 = add i64 %6815, 51640
  store i64 %6816, i64* %RAX.i11582.pre-phi, align 8
  %6817 = icmp ugt i64 %6815, -51641
  %6818 = zext i1 %6817 to i8
  store i8 %6818, i8* %14, align 1
  %6819 = trunc i64 %6816 to i32
  %6820 = and i32 %6819, 255
  %6821 = tail call i32 @llvm.ctpop.i32(i32 %6820)
  %6822 = trunc i32 %6821 to i8
  %6823 = and i8 %6822, 1
  %6824 = xor i8 %6823, 1
  store i8 %6824, i8* %21, align 1
  %6825 = xor i64 %6815, 16
  %6826 = xor i64 %6825, %6816
  %6827 = lshr i64 %6826, 4
  %6828 = trunc i64 %6827 to i8
  %6829 = and i8 %6828, 1
  store i8 %6829, i8* %27, align 1
  %6830 = icmp eq i64 %6816, 0
  %6831 = zext i1 %6830 to i8
  store i8 %6831, i8* %30, align 1
  %6832 = lshr i64 %6816, 63
  %6833 = trunc i64 %6832 to i8
  store i8 %6833, i8* %33, align 1
  %6834 = lshr i64 %6815, 63
  %6835 = xor i64 %6832, %6834
  %6836 = add nuw nsw i64 %6835, %6832
  %6837 = icmp eq i64 %6836, 2
  %6838 = zext i1 %6837 to i8
  store i8 %6838, i8* %39, align 1
  %6839 = add i64 %6810, -150
  %6840 = add i64 %6812, 17
  store i64 %6840, i64* %3, align 8
  %6841 = inttoptr i64 %6839 to i16*
  %6842 = load i16, i16* %6841, align 2
  %6843 = zext i16 %6842 to i64
  store i64 %6843, i64* %RCX.i11580, align 8
  %6844 = zext i16 %6842 to i64
  %6845 = shl nuw nsw i64 %6844, 4
  store i64 %6845, i64* %573, align 8
  %6846 = add i64 %6845, %6816
  store i64 %6846, i64* %RAX.i11582.pre-phi, align 8
  %6847 = icmp ult i64 %6846, %6816
  %6848 = icmp ult i64 %6846, %6845
  %6849 = or i1 %6847, %6848
  %6850 = zext i1 %6849 to i8
  store i8 %6850, i8* %14, align 1
  %6851 = trunc i64 %6846 to i32
  %6852 = and i32 %6851, 255
  %6853 = tail call i32 @llvm.ctpop.i32(i32 %6852)
  %6854 = trunc i32 %6853 to i8
  %6855 = and i8 %6854, 1
  %6856 = xor i8 %6855, 1
  store i8 %6856, i8* %21, align 1
  %6857 = xor i64 %6845, %6816
  %6858 = xor i64 %6857, %6846
  %6859 = lshr i64 %6858, 4
  %6860 = trunc i64 %6859 to i8
  %6861 = and i8 %6860, 1
  store i8 %6861, i8* %27, align 1
  %6862 = icmp eq i64 %6846, 0
  %6863 = zext i1 %6862 to i8
  store i8 %6863, i8* %30, align 1
  %6864 = lshr i64 %6846, 63
  %6865 = trunc i64 %6864 to i8
  store i8 %6865, i8* %33, align 1
  %6866 = xor i64 %6864, %6832
  %6867 = add nuw nsw i64 %6866, %6864
  %6868 = icmp eq i64 %6867, 2
  %6869 = zext i1 %6868 to i8
  store i8 %6869, i8* %39, align 1
  %6870 = add i64 %6846, 4
  %6871 = add i64 %6812, 29
  store i64 %6871, i64* %3, align 8
  %6872 = inttoptr i64 %6870 to i32*
  %6873 = load i32, i32* %6872, align 4
  %6874 = zext i32 %6873 to i64
  store i64 %6874, i64* %RCX.i11580, align 8
  %6875 = load i64, i64* %RBP.i, align 8
  %6876 = add i64 %6875, -144
  %6877 = add i64 %6812, 35
  store i64 %6877, i64* %3, align 8
  %6878 = inttoptr i64 %6876 to i32*
  %6879 = load i32, i32* %6878, align 4
  %6880 = add i32 %6879, %6873
  %6881 = zext i32 %6880 to i64
  store i64 %6881, i64* %RCX.i11580, align 8
  %6882 = icmp ult i32 %6880, %6873
  %6883 = icmp ult i32 %6880, %6879
  %6884 = or i1 %6882, %6883
  %6885 = zext i1 %6884 to i8
  store i8 %6885, i8* %14, align 1
  %6886 = and i32 %6880, 255
  %6887 = tail call i32 @llvm.ctpop.i32(i32 %6886)
  %6888 = trunc i32 %6887 to i8
  %6889 = and i8 %6888, 1
  %6890 = xor i8 %6889, 1
  store i8 %6890, i8* %21, align 1
  %6891 = xor i32 %6879, %6873
  %6892 = xor i32 %6891, %6880
  %6893 = lshr i32 %6892, 4
  %6894 = trunc i32 %6893 to i8
  %6895 = and i8 %6894, 1
  store i8 %6895, i8* %27, align 1
  %6896 = icmp eq i32 %6880, 0
  %6897 = zext i1 %6896 to i8
  store i8 %6897, i8* %30, align 1
  %6898 = lshr i32 %6880, 31
  %6899 = trunc i32 %6898 to i8
  store i8 %6899, i8* %33, align 1
  %6900 = lshr i32 %6873, 31
  %6901 = lshr i32 %6879, 31
  %6902 = xor i32 %6898, %6900
  %6903 = xor i32 %6898, %6901
  %6904 = add nuw nsw i32 %6902, %6903
  %6905 = icmp eq i32 %6904, 2
  %6906 = zext i1 %6905 to i8
  store i8 %6906, i8* %39, align 1
  %6907 = add i64 %6812, 41
  store i64 %6907, i64* %3, align 8
  store i32 %6880, i32* %6878, align 4
  %6908 = load i64, i64* %RBP.i, align 8
  %6909 = add i64 %6908, -8
  %6910 = load i64, i64* %3, align 8
  %6911 = add i64 %6910, 4
  store i64 %6911, i64* %3, align 8
  %6912 = inttoptr i64 %6909 to i64*
  %6913 = load i64, i64* %6912, align 8
  %6914 = add i64 %6913, 51640
  store i64 %6914, i64* %RAX.i11582.pre-phi, align 8
  %6915 = icmp ugt i64 %6913, -51641
  %6916 = zext i1 %6915 to i8
  store i8 %6916, i8* %14, align 1
  %6917 = trunc i64 %6914 to i32
  %6918 = and i32 %6917, 255
  %6919 = tail call i32 @llvm.ctpop.i32(i32 %6918)
  %6920 = trunc i32 %6919 to i8
  %6921 = and i8 %6920, 1
  %6922 = xor i8 %6921, 1
  store i8 %6922, i8* %21, align 1
  %6923 = xor i64 %6913, 16
  %6924 = xor i64 %6923, %6914
  %6925 = lshr i64 %6924, 4
  %6926 = trunc i64 %6925 to i8
  %6927 = and i8 %6926, 1
  store i8 %6927, i8* %27, align 1
  %6928 = icmp eq i64 %6914, 0
  %6929 = zext i1 %6928 to i8
  store i8 %6929, i8* %30, align 1
  %6930 = lshr i64 %6914, 63
  %6931 = trunc i64 %6930 to i8
  store i8 %6931, i8* %33, align 1
  %6932 = lshr i64 %6913, 63
  %6933 = xor i64 %6930, %6932
  %6934 = add nuw nsw i64 %6933, %6930
  %6935 = icmp eq i64 %6934, 2
  %6936 = zext i1 %6935 to i8
  store i8 %6936, i8* %39, align 1
  %6937 = add i64 %6908, -150
  %6938 = add i64 %6910, 17
  store i64 %6938, i64* %3, align 8
  %6939 = inttoptr i64 %6937 to i16*
  %6940 = load i16, i16* %6939, align 2
  %6941 = zext i16 %6940 to i64
  store i64 %6941, i64* %RCX.i11580, align 8
  %6942 = zext i16 %6940 to i64
  %6943 = shl nuw nsw i64 %6942, 4
  store i64 %6943, i64* %573, align 8
  %6944 = add i64 %6943, %6914
  store i64 %6944, i64* %RAX.i11582.pre-phi, align 8
  %6945 = icmp ult i64 %6944, %6914
  %6946 = icmp ult i64 %6944, %6943
  %6947 = or i1 %6945, %6946
  %6948 = zext i1 %6947 to i8
  store i8 %6948, i8* %14, align 1
  %6949 = trunc i64 %6944 to i32
  %6950 = and i32 %6949, 255
  %6951 = tail call i32 @llvm.ctpop.i32(i32 %6950)
  %6952 = trunc i32 %6951 to i8
  %6953 = and i8 %6952, 1
  %6954 = xor i8 %6953, 1
  store i8 %6954, i8* %21, align 1
  %6955 = xor i64 %6943, %6914
  %6956 = xor i64 %6955, %6944
  %6957 = lshr i64 %6956, 4
  %6958 = trunc i64 %6957 to i8
  %6959 = and i8 %6958, 1
  store i8 %6959, i8* %27, align 1
  %6960 = icmp eq i64 %6944, 0
  %6961 = zext i1 %6960 to i8
  store i8 %6961, i8* %30, align 1
  %6962 = lshr i64 %6944, 63
  %6963 = trunc i64 %6962 to i8
  store i8 %6963, i8* %33, align 1
  %6964 = xor i64 %6962, %6930
  %6965 = add nuw nsw i64 %6964, %6962
  %6966 = icmp eq i64 %6965, 2
  %6967 = zext i1 %6966 to i8
  store i8 %6967, i8* %39, align 1
  %6968 = add i64 %6944, 8
  %6969 = add i64 %6910, 29
  store i64 %6969, i64* %3, align 8
  %6970 = inttoptr i64 %6968 to i32*
  %6971 = load i32, i32* %6970, align 4
  %6972 = zext i32 %6971 to i64
  store i64 %6972, i64* %RCX.i11580, align 8
  %6973 = load i64, i64* %RBP.i, align 8
  %6974 = add i64 %6973, -148
  %6975 = add i64 %6910, 35
  store i64 %6975, i64* %3, align 8
  %6976 = inttoptr i64 %6974 to i32*
  %6977 = load i32, i32* %6976, align 4
  %6978 = add i32 %6977, %6971
  %6979 = zext i32 %6978 to i64
  store i64 %6979, i64* %RCX.i11580, align 8
  %6980 = icmp ult i32 %6978, %6971
  %6981 = icmp ult i32 %6978, %6977
  %6982 = or i1 %6980, %6981
  %6983 = zext i1 %6982 to i8
  store i8 %6983, i8* %14, align 1
  %6984 = and i32 %6978, 255
  %6985 = tail call i32 @llvm.ctpop.i32(i32 %6984)
  %6986 = trunc i32 %6985 to i8
  %6987 = and i8 %6986, 1
  %6988 = xor i8 %6987, 1
  store i8 %6988, i8* %21, align 1
  %6989 = xor i32 %6977, %6971
  %6990 = xor i32 %6989, %6978
  %6991 = lshr i32 %6990, 4
  %6992 = trunc i32 %6991 to i8
  %6993 = and i8 %6992, 1
  store i8 %6993, i8* %27, align 1
  %6994 = icmp eq i32 %6978, 0
  %6995 = zext i1 %6994 to i8
  store i8 %6995, i8* %30, align 1
  %6996 = lshr i32 %6978, 31
  %6997 = trunc i32 %6996 to i8
  store i8 %6997, i8* %33, align 1
  %6998 = lshr i32 %6971, 31
  %6999 = lshr i32 %6977, 31
  %7000 = xor i32 %6996, %6998
  %7001 = xor i32 %6996, %6999
  %7002 = add nuw nsw i32 %7000, %7001
  %7003 = icmp eq i32 %7002, 2
  %7004 = zext i1 %7003 to i8
  store i8 %7004, i8* %39, align 1
  %7005 = add i64 %6910, 41
  store i64 %7005, i64* %3, align 8
  store i32 %6978, i32* %6976, align 4
  %7006 = load i64, i64* %RBP.i, align 8
  %7007 = add i64 %7006, -120
  %7008 = load i64, i64* %3, align 8
  %7009 = add i64 %7008, 4
  store i64 %7009, i64* %3, align 8
  %7010 = inttoptr i64 %7007 to i64*
  %7011 = load i64, i64* %7010, align 8
  store i64 %7011, i64* %RAX.i11582.pre-phi, align 8
  %7012 = add i64 %7006, -28
  %7013 = add i64 %7008, 7
  store i64 %7013, i64* %3, align 8
  %7014 = inttoptr i64 %7012 to i32*
  %7015 = load i32, i32* %7014, align 4
  %7016 = add i32 %7015, 13
  %7017 = zext i32 %7016 to i64
  store i64 %7017, i64* %RCX.i11580, align 8
  %7018 = icmp ugt i32 %7015, -14
  %7019 = zext i1 %7018 to i8
  store i8 %7019, i8* %14, align 1
  %7020 = and i32 %7016, 255
  %7021 = tail call i32 @llvm.ctpop.i32(i32 %7020)
  %7022 = trunc i32 %7021 to i8
  %7023 = and i8 %7022, 1
  %7024 = xor i8 %7023, 1
  store i8 %7024, i8* %21, align 1
  %7025 = xor i32 %7016, %7015
  %7026 = lshr i32 %7025, 4
  %7027 = trunc i32 %7026 to i8
  %7028 = and i8 %7027, 1
  store i8 %7028, i8* %27, align 1
  %7029 = icmp eq i32 %7016, 0
  %7030 = zext i1 %7029 to i8
  store i8 %7030, i8* %30, align 1
  %7031 = lshr i32 %7016, 31
  %7032 = trunc i32 %7031 to i8
  store i8 %7032, i8* %33, align 1
  %7033 = lshr i32 %7015, 31
  %7034 = xor i32 %7031, %7033
  %7035 = add nuw nsw i32 %7034, %7031
  %7036 = icmp eq i32 %7035, 2
  %7037 = zext i1 %7036 to i8
  store i8 %7037, i8* %39, align 1
  %7038 = sext i32 %7016 to i64
  store i64 %7038, i64* %573, align 8
  %7039 = shl nsw i64 %7038, 1
  %7040 = add i64 %7011, %7039
  %7041 = add i64 %7008, 17
  store i64 %7041, i64* %3, align 8
  %7042 = inttoptr i64 %7040 to i16*
  %7043 = load i16, i16* %7042, align 2
  store i16 %7043, i16* %SI.i, align 2
  %7044 = add i64 %7006, -150
  %7045 = add i64 %7008, 24
  store i64 %7045, i64* %3, align 8
  %7046 = inttoptr i64 %7044 to i16*
  store i16 %7043, i16* %7046, align 2
  %7047 = load i64, i64* %RBP.i, align 8
  %7048 = add i64 %7047, -8
  %7049 = load i64, i64* %3, align 8
  %7050 = add i64 %7049, 4
  store i64 %7050, i64* %3, align 8
  %7051 = inttoptr i64 %7048 to i64*
  %7052 = load i64, i64* %7051, align 8
  %7053 = add i64 %7052, 51640
  store i64 %7053, i64* %RAX.i11582.pre-phi, align 8
  %7054 = icmp ugt i64 %7052, -51641
  %7055 = zext i1 %7054 to i8
  store i8 %7055, i8* %14, align 1
  %7056 = trunc i64 %7053 to i32
  %7057 = and i32 %7056, 255
  %7058 = tail call i32 @llvm.ctpop.i32(i32 %7057)
  %7059 = trunc i32 %7058 to i8
  %7060 = and i8 %7059, 1
  %7061 = xor i8 %7060, 1
  store i8 %7061, i8* %21, align 1
  %7062 = xor i64 %7052, 16
  %7063 = xor i64 %7062, %7053
  %7064 = lshr i64 %7063, 4
  %7065 = trunc i64 %7064 to i8
  %7066 = and i8 %7065, 1
  store i8 %7066, i8* %27, align 1
  %7067 = icmp eq i64 %7053, 0
  %7068 = zext i1 %7067 to i8
  store i8 %7068, i8* %30, align 1
  %7069 = lshr i64 %7053, 63
  %7070 = trunc i64 %7069 to i8
  store i8 %7070, i8* %33, align 1
  %7071 = lshr i64 %7052, 63
  %7072 = xor i64 %7069, %7071
  %7073 = add nuw nsw i64 %7072, %7069
  %7074 = icmp eq i64 %7073, 2
  %7075 = zext i1 %7074 to i8
  store i8 %7075, i8* %39, align 1
  %7076 = add i64 %7047, -150
  %7077 = add i64 %7049, 17
  store i64 %7077, i64* %3, align 8
  %7078 = inttoptr i64 %7076 to i16*
  %7079 = load i16, i16* %7078, align 2
  %7080 = zext i16 %7079 to i64
  store i64 %7080, i64* %RCX.i11580, align 8
  %7081 = zext i16 %7079 to i64
  %7082 = shl nuw nsw i64 %7081, 4
  store i64 %7082, i64* %573, align 8
  %7083 = add i64 %7082, %7053
  store i64 %7083, i64* %RAX.i11582.pre-phi, align 8
  %7084 = icmp ult i64 %7083, %7053
  %7085 = icmp ult i64 %7083, %7082
  %7086 = or i1 %7084, %7085
  %7087 = zext i1 %7086 to i8
  store i8 %7087, i8* %14, align 1
  %7088 = trunc i64 %7083 to i32
  %7089 = and i32 %7088, 255
  %7090 = tail call i32 @llvm.ctpop.i32(i32 %7089)
  %7091 = trunc i32 %7090 to i8
  %7092 = and i8 %7091, 1
  %7093 = xor i8 %7092, 1
  store i8 %7093, i8* %21, align 1
  %7094 = xor i64 %7082, %7053
  %7095 = xor i64 %7094, %7083
  %7096 = lshr i64 %7095, 4
  %7097 = trunc i64 %7096 to i8
  %7098 = and i8 %7097, 1
  store i8 %7098, i8* %27, align 1
  %7099 = icmp eq i64 %7083, 0
  %7100 = zext i1 %7099 to i8
  store i8 %7100, i8* %30, align 1
  %7101 = lshr i64 %7083, 63
  %7102 = trunc i64 %7101 to i8
  store i8 %7102, i8* %33, align 1
  %7103 = xor i64 %7101, %7069
  %7104 = add nuw nsw i64 %7103, %7101
  %7105 = icmp eq i64 %7104, 2
  %7106 = zext i1 %7105 to i8
  store i8 %7106, i8* %39, align 1
  %7107 = inttoptr i64 %7083 to i32*
  %7108 = add i64 %7049, 28
  store i64 %7108, i64* %3, align 8
  %7109 = load i32, i32* %7107, align 4
  %7110 = zext i32 %7109 to i64
  store i64 %7110, i64* %RCX.i11580, align 8
  %7111 = load i64, i64* %RBP.i, align 8
  %7112 = add i64 %7111, -140
  %7113 = add i64 %7049, 34
  store i64 %7113, i64* %3, align 8
  %7114 = inttoptr i64 %7112 to i32*
  %7115 = load i32, i32* %7114, align 4
  %7116 = add i32 %7115, %7109
  %7117 = zext i32 %7116 to i64
  store i64 %7117, i64* %RCX.i11580, align 8
  %7118 = icmp ult i32 %7116, %7109
  %7119 = icmp ult i32 %7116, %7115
  %7120 = or i1 %7118, %7119
  %7121 = zext i1 %7120 to i8
  store i8 %7121, i8* %14, align 1
  %7122 = and i32 %7116, 255
  %7123 = tail call i32 @llvm.ctpop.i32(i32 %7122)
  %7124 = trunc i32 %7123 to i8
  %7125 = and i8 %7124, 1
  %7126 = xor i8 %7125, 1
  store i8 %7126, i8* %21, align 1
  %7127 = xor i32 %7115, %7109
  %7128 = xor i32 %7127, %7116
  %7129 = lshr i32 %7128, 4
  %7130 = trunc i32 %7129 to i8
  %7131 = and i8 %7130, 1
  store i8 %7131, i8* %27, align 1
  %7132 = icmp eq i32 %7116, 0
  %7133 = zext i1 %7132 to i8
  store i8 %7133, i8* %30, align 1
  %7134 = lshr i32 %7116, 31
  %7135 = trunc i32 %7134 to i8
  store i8 %7135, i8* %33, align 1
  %7136 = lshr i32 %7109, 31
  %7137 = lshr i32 %7115, 31
  %7138 = xor i32 %7134, %7136
  %7139 = xor i32 %7134, %7137
  %7140 = add nuw nsw i32 %7138, %7139
  %7141 = icmp eq i32 %7140, 2
  %7142 = zext i1 %7141 to i8
  store i8 %7142, i8* %39, align 1
  %7143 = add i64 %7049, 40
  store i64 %7143, i64* %3, align 8
  store i32 %7116, i32* %7114, align 4
  %7144 = load i64, i64* %RBP.i, align 8
  %7145 = add i64 %7144, -8
  %7146 = load i64, i64* %3, align 8
  %7147 = add i64 %7146, 4
  store i64 %7147, i64* %3, align 8
  %7148 = inttoptr i64 %7145 to i64*
  %7149 = load i64, i64* %7148, align 8
  %7150 = add i64 %7149, 51640
  store i64 %7150, i64* %RAX.i11582.pre-phi, align 8
  %7151 = icmp ugt i64 %7149, -51641
  %7152 = zext i1 %7151 to i8
  store i8 %7152, i8* %14, align 1
  %7153 = trunc i64 %7150 to i32
  %7154 = and i32 %7153, 255
  %7155 = tail call i32 @llvm.ctpop.i32(i32 %7154)
  %7156 = trunc i32 %7155 to i8
  %7157 = and i8 %7156, 1
  %7158 = xor i8 %7157, 1
  store i8 %7158, i8* %21, align 1
  %7159 = xor i64 %7149, 16
  %7160 = xor i64 %7159, %7150
  %7161 = lshr i64 %7160, 4
  %7162 = trunc i64 %7161 to i8
  %7163 = and i8 %7162, 1
  store i8 %7163, i8* %27, align 1
  %7164 = icmp eq i64 %7150, 0
  %7165 = zext i1 %7164 to i8
  store i8 %7165, i8* %30, align 1
  %7166 = lshr i64 %7150, 63
  %7167 = trunc i64 %7166 to i8
  store i8 %7167, i8* %33, align 1
  %7168 = lshr i64 %7149, 63
  %7169 = xor i64 %7166, %7168
  %7170 = add nuw nsw i64 %7169, %7166
  %7171 = icmp eq i64 %7170, 2
  %7172 = zext i1 %7171 to i8
  store i8 %7172, i8* %39, align 1
  %7173 = add i64 %7144, -150
  %7174 = add i64 %7146, 17
  store i64 %7174, i64* %3, align 8
  %7175 = inttoptr i64 %7173 to i16*
  %7176 = load i16, i16* %7175, align 2
  %7177 = zext i16 %7176 to i64
  store i64 %7177, i64* %RCX.i11580, align 8
  %7178 = zext i16 %7176 to i64
  %7179 = shl nuw nsw i64 %7178, 4
  store i64 %7179, i64* %573, align 8
  %7180 = add i64 %7179, %7150
  store i64 %7180, i64* %RAX.i11582.pre-phi, align 8
  %7181 = icmp ult i64 %7180, %7150
  %7182 = icmp ult i64 %7180, %7179
  %7183 = or i1 %7181, %7182
  %7184 = zext i1 %7183 to i8
  store i8 %7184, i8* %14, align 1
  %7185 = trunc i64 %7180 to i32
  %7186 = and i32 %7185, 255
  %7187 = tail call i32 @llvm.ctpop.i32(i32 %7186)
  %7188 = trunc i32 %7187 to i8
  %7189 = and i8 %7188, 1
  %7190 = xor i8 %7189, 1
  store i8 %7190, i8* %21, align 1
  %7191 = xor i64 %7179, %7150
  %7192 = xor i64 %7191, %7180
  %7193 = lshr i64 %7192, 4
  %7194 = trunc i64 %7193 to i8
  %7195 = and i8 %7194, 1
  store i8 %7195, i8* %27, align 1
  %7196 = icmp eq i64 %7180, 0
  %7197 = zext i1 %7196 to i8
  store i8 %7197, i8* %30, align 1
  %7198 = lshr i64 %7180, 63
  %7199 = trunc i64 %7198 to i8
  store i8 %7199, i8* %33, align 1
  %7200 = xor i64 %7198, %7166
  %7201 = add nuw nsw i64 %7200, %7198
  %7202 = icmp eq i64 %7201, 2
  %7203 = zext i1 %7202 to i8
  store i8 %7203, i8* %39, align 1
  %7204 = add i64 %7180, 4
  %7205 = add i64 %7146, 29
  store i64 %7205, i64* %3, align 8
  %7206 = inttoptr i64 %7204 to i32*
  %7207 = load i32, i32* %7206, align 4
  %7208 = zext i32 %7207 to i64
  store i64 %7208, i64* %RCX.i11580, align 8
  %7209 = load i64, i64* %RBP.i, align 8
  %7210 = add i64 %7209, -144
  %7211 = add i64 %7146, 35
  store i64 %7211, i64* %3, align 8
  %7212 = inttoptr i64 %7210 to i32*
  %7213 = load i32, i32* %7212, align 4
  %7214 = add i32 %7213, %7207
  %7215 = zext i32 %7214 to i64
  store i64 %7215, i64* %RCX.i11580, align 8
  %7216 = icmp ult i32 %7214, %7207
  %7217 = icmp ult i32 %7214, %7213
  %7218 = or i1 %7216, %7217
  %7219 = zext i1 %7218 to i8
  store i8 %7219, i8* %14, align 1
  %7220 = and i32 %7214, 255
  %7221 = tail call i32 @llvm.ctpop.i32(i32 %7220)
  %7222 = trunc i32 %7221 to i8
  %7223 = and i8 %7222, 1
  %7224 = xor i8 %7223, 1
  store i8 %7224, i8* %21, align 1
  %7225 = xor i32 %7213, %7207
  %7226 = xor i32 %7225, %7214
  %7227 = lshr i32 %7226, 4
  %7228 = trunc i32 %7227 to i8
  %7229 = and i8 %7228, 1
  store i8 %7229, i8* %27, align 1
  %7230 = icmp eq i32 %7214, 0
  %7231 = zext i1 %7230 to i8
  store i8 %7231, i8* %30, align 1
  %7232 = lshr i32 %7214, 31
  %7233 = trunc i32 %7232 to i8
  store i8 %7233, i8* %33, align 1
  %7234 = lshr i32 %7207, 31
  %7235 = lshr i32 %7213, 31
  %7236 = xor i32 %7232, %7234
  %7237 = xor i32 %7232, %7235
  %7238 = add nuw nsw i32 %7236, %7237
  %7239 = icmp eq i32 %7238, 2
  %7240 = zext i1 %7239 to i8
  store i8 %7240, i8* %39, align 1
  %7241 = add i64 %7146, 41
  store i64 %7241, i64* %3, align 8
  store i32 %7214, i32* %7212, align 4
  %7242 = load i64, i64* %RBP.i, align 8
  %7243 = add i64 %7242, -8
  %7244 = load i64, i64* %3, align 8
  %7245 = add i64 %7244, 4
  store i64 %7245, i64* %3, align 8
  %7246 = inttoptr i64 %7243 to i64*
  %7247 = load i64, i64* %7246, align 8
  %7248 = add i64 %7247, 51640
  store i64 %7248, i64* %RAX.i11582.pre-phi, align 8
  %7249 = icmp ugt i64 %7247, -51641
  %7250 = zext i1 %7249 to i8
  store i8 %7250, i8* %14, align 1
  %7251 = trunc i64 %7248 to i32
  %7252 = and i32 %7251, 255
  %7253 = tail call i32 @llvm.ctpop.i32(i32 %7252)
  %7254 = trunc i32 %7253 to i8
  %7255 = and i8 %7254, 1
  %7256 = xor i8 %7255, 1
  store i8 %7256, i8* %21, align 1
  %7257 = xor i64 %7247, 16
  %7258 = xor i64 %7257, %7248
  %7259 = lshr i64 %7258, 4
  %7260 = trunc i64 %7259 to i8
  %7261 = and i8 %7260, 1
  store i8 %7261, i8* %27, align 1
  %7262 = icmp eq i64 %7248, 0
  %7263 = zext i1 %7262 to i8
  store i8 %7263, i8* %30, align 1
  %7264 = lshr i64 %7248, 63
  %7265 = trunc i64 %7264 to i8
  store i8 %7265, i8* %33, align 1
  %7266 = lshr i64 %7247, 63
  %7267 = xor i64 %7264, %7266
  %7268 = add nuw nsw i64 %7267, %7264
  %7269 = icmp eq i64 %7268, 2
  %7270 = zext i1 %7269 to i8
  store i8 %7270, i8* %39, align 1
  %7271 = add i64 %7242, -150
  %7272 = add i64 %7244, 17
  store i64 %7272, i64* %3, align 8
  %7273 = inttoptr i64 %7271 to i16*
  %7274 = load i16, i16* %7273, align 2
  %7275 = zext i16 %7274 to i64
  store i64 %7275, i64* %RCX.i11580, align 8
  %7276 = zext i16 %7274 to i64
  %7277 = shl nuw nsw i64 %7276, 4
  store i64 %7277, i64* %573, align 8
  %7278 = add i64 %7277, %7248
  store i64 %7278, i64* %RAX.i11582.pre-phi, align 8
  %7279 = icmp ult i64 %7278, %7248
  %7280 = icmp ult i64 %7278, %7277
  %7281 = or i1 %7279, %7280
  %7282 = zext i1 %7281 to i8
  store i8 %7282, i8* %14, align 1
  %7283 = trunc i64 %7278 to i32
  %7284 = and i32 %7283, 255
  %7285 = tail call i32 @llvm.ctpop.i32(i32 %7284)
  %7286 = trunc i32 %7285 to i8
  %7287 = and i8 %7286, 1
  %7288 = xor i8 %7287, 1
  store i8 %7288, i8* %21, align 1
  %7289 = xor i64 %7277, %7248
  %7290 = xor i64 %7289, %7278
  %7291 = lshr i64 %7290, 4
  %7292 = trunc i64 %7291 to i8
  %7293 = and i8 %7292, 1
  store i8 %7293, i8* %27, align 1
  %7294 = icmp eq i64 %7278, 0
  %7295 = zext i1 %7294 to i8
  store i8 %7295, i8* %30, align 1
  %7296 = lshr i64 %7278, 63
  %7297 = trunc i64 %7296 to i8
  store i8 %7297, i8* %33, align 1
  %7298 = xor i64 %7296, %7264
  %7299 = add nuw nsw i64 %7298, %7296
  %7300 = icmp eq i64 %7299, 2
  %7301 = zext i1 %7300 to i8
  store i8 %7301, i8* %39, align 1
  %7302 = add i64 %7278, 8
  %7303 = add i64 %7244, 29
  store i64 %7303, i64* %3, align 8
  %7304 = inttoptr i64 %7302 to i32*
  %7305 = load i32, i32* %7304, align 4
  %7306 = zext i32 %7305 to i64
  store i64 %7306, i64* %RCX.i11580, align 8
  %7307 = load i64, i64* %RBP.i, align 8
  %7308 = add i64 %7307, -148
  %7309 = add i64 %7244, 35
  store i64 %7309, i64* %3, align 8
  %7310 = inttoptr i64 %7308 to i32*
  %7311 = load i32, i32* %7310, align 4
  %7312 = add i32 %7311, %7305
  %7313 = zext i32 %7312 to i64
  store i64 %7313, i64* %RCX.i11580, align 8
  %7314 = icmp ult i32 %7312, %7305
  %7315 = icmp ult i32 %7312, %7311
  %7316 = or i1 %7314, %7315
  %7317 = zext i1 %7316 to i8
  store i8 %7317, i8* %14, align 1
  %7318 = and i32 %7312, 255
  %7319 = tail call i32 @llvm.ctpop.i32(i32 %7318)
  %7320 = trunc i32 %7319 to i8
  %7321 = and i8 %7320, 1
  %7322 = xor i8 %7321, 1
  store i8 %7322, i8* %21, align 1
  %7323 = xor i32 %7311, %7305
  %7324 = xor i32 %7323, %7312
  %7325 = lshr i32 %7324, 4
  %7326 = trunc i32 %7325 to i8
  %7327 = and i8 %7326, 1
  store i8 %7327, i8* %27, align 1
  %7328 = icmp eq i32 %7312, 0
  %7329 = zext i1 %7328 to i8
  store i8 %7329, i8* %30, align 1
  %7330 = lshr i32 %7312, 31
  %7331 = trunc i32 %7330 to i8
  store i8 %7331, i8* %33, align 1
  %7332 = lshr i32 %7305, 31
  %7333 = lshr i32 %7311, 31
  %7334 = xor i32 %7330, %7332
  %7335 = xor i32 %7330, %7333
  %7336 = add nuw nsw i32 %7334, %7335
  %7337 = icmp eq i32 %7336, 2
  %7338 = zext i1 %7337 to i8
  store i8 %7338, i8* %39, align 1
  %7339 = add i64 %7244, 41
  store i64 %7339, i64* %3, align 8
  store i32 %7312, i32* %7310, align 4
  %7340 = load i64, i64* %RBP.i, align 8
  %7341 = add i64 %7340, -120
  %7342 = load i64, i64* %3, align 8
  %7343 = add i64 %7342, 4
  store i64 %7343, i64* %3, align 8
  %7344 = inttoptr i64 %7341 to i64*
  %7345 = load i64, i64* %7344, align 8
  store i64 %7345, i64* %RAX.i11582.pre-phi, align 8
  %7346 = add i64 %7340, -28
  %7347 = add i64 %7342, 7
  store i64 %7347, i64* %3, align 8
  %7348 = inttoptr i64 %7346 to i32*
  %7349 = load i32, i32* %7348, align 4
  %7350 = add i32 %7349, 14
  %7351 = zext i32 %7350 to i64
  store i64 %7351, i64* %RCX.i11580, align 8
  %7352 = icmp ugt i32 %7349, -15
  %7353 = zext i1 %7352 to i8
  store i8 %7353, i8* %14, align 1
  %7354 = and i32 %7350, 255
  %7355 = tail call i32 @llvm.ctpop.i32(i32 %7354)
  %7356 = trunc i32 %7355 to i8
  %7357 = and i8 %7356, 1
  %7358 = xor i8 %7357, 1
  store i8 %7358, i8* %21, align 1
  %7359 = xor i32 %7350, %7349
  %7360 = lshr i32 %7359, 4
  %7361 = trunc i32 %7360 to i8
  %7362 = and i8 %7361, 1
  store i8 %7362, i8* %27, align 1
  %7363 = icmp eq i32 %7350, 0
  %7364 = zext i1 %7363 to i8
  store i8 %7364, i8* %30, align 1
  %7365 = lshr i32 %7350, 31
  %7366 = trunc i32 %7365 to i8
  store i8 %7366, i8* %33, align 1
  %7367 = lshr i32 %7349, 31
  %7368 = xor i32 %7365, %7367
  %7369 = add nuw nsw i32 %7368, %7365
  %7370 = icmp eq i32 %7369, 2
  %7371 = zext i1 %7370 to i8
  store i8 %7371, i8* %39, align 1
  %7372 = sext i32 %7350 to i64
  store i64 %7372, i64* %573, align 8
  %7373 = shl nsw i64 %7372, 1
  %7374 = add i64 %7345, %7373
  %7375 = add i64 %7342, 17
  store i64 %7375, i64* %3, align 8
  %7376 = inttoptr i64 %7374 to i16*
  %7377 = load i16, i16* %7376, align 2
  store i16 %7377, i16* %SI.i, align 2
  %7378 = add i64 %7340, -150
  %7379 = add i64 %7342, 24
  store i64 %7379, i64* %3, align 8
  %7380 = inttoptr i64 %7378 to i16*
  store i16 %7377, i16* %7380, align 2
  %7381 = load i64, i64* %RBP.i, align 8
  %7382 = add i64 %7381, -8
  %7383 = load i64, i64* %3, align 8
  %7384 = add i64 %7383, 4
  store i64 %7384, i64* %3, align 8
  %7385 = inttoptr i64 %7382 to i64*
  %7386 = load i64, i64* %7385, align 8
  %7387 = add i64 %7386, 51640
  store i64 %7387, i64* %RAX.i11582.pre-phi, align 8
  %7388 = icmp ugt i64 %7386, -51641
  %7389 = zext i1 %7388 to i8
  store i8 %7389, i8* %14, align 1
  %7390 = trunc i64 %7387 to i32
  %7391 = and i32 %7390, 255
  %7392 = tail call i32 @llvm.ctpop.i32(i32 %7391)
  %7393 = trunc i32 %7392 to i8
  %7394 = and i8 %7393, 1
  %7395 = xor i8 %7394, 1
  store i8 %7395, i8* %21, align 1
  %7396 = xor i64 %7386, 16
  %7397 = xor i64 %7396, %7387
  %7398 = lshr i64 %7397, 4
  %7399 = trunc i64 %7398 to i8
  %7400 = and i8 %7399, 1
  store i8 %7400, i8* %27, align 1
  %7401 = icmp eq i64 %7387, 0
  %7402 = zext i1 %7401 to i8
  store i8 %7402, i8* %30, align 1
  %7403 = lshr i64 %7387, 63
  %7404 = trunc i64 %7403 to i8
  store i8 %7404, i8* %33, align 1
  %7405 = lshr i64 %7386, 63
  %7406 = xor i64 %7403, %7405
  %7407 = add nuw nsw i64 %7406, %7403
  %7408 = icmp eq i64 %7407, 2
  %7409 = zext i1 %7408 to i8
  store i8 %7409, i8* %39, align 1
  %7410 = add i64 %7381, -150
  %7411 = add i64 %7383, 17
  store i64 %7411, i64* %3, align 8
  %7412 = inttoptr i64 %7410 to i16*
  %7413 = load i16, i16* %7412, align 2
  %7414 = zext i16 %7413 to i64
  store i64 %7414, i64* %RCX.i11580, align 8
  %7415 = zext i16 %7413 to i64
  %7416 = shl nuw nsw i64 %7415, 4
  store i64 %7416, i64* %573, align 8
  %7417 = add i64 %7416, %7387
  store i64 %7417, i64* %RAX.i11582.pre-phi, align 8
  %7418 = icmp ult i64 %7417, %7387
  %7419 = icmp ult i64 %7417, %7416
  %7420 = or i1 %7418, %7419
  %7421 = zext i1 %7420 to i8
  store i8 %7421, i8* %14, align 1
  %7422 = trunc i64 %7417 to i32
  %7423 = and i32 %7422, 255
  %7424 = tail call i32 @llvm.ctpop.i32(i32 %7423)
  %7425 = trunc i32 %7424 to i8
  %7426 = and i8 %7425, 1
  %7427 = xor i8 %7426, 1
  store i8 %7427, i8* %21, align 1
  %7428 = xor i64 %7416, %7387
  %7429 = xor i64 %7428, %7417
  %7430 = lshr i64 %7429, 4
  %7431 = trunc i64 %7430 to i8
  %7432 = and i8 %7431, 1
  store i8 %7432, i8* %27, align 1
  %7433 = icmp eq i64 %7417, 0
  %7434 = zext i1 %7433 to i8
  store i8 %7434, i8* %30, align 1
  %7435 = lshr i64 %7417, 63
  %7436 = trunc i64 %7435 to i8
  store i8 %7436, i8* %33, align 1
  %7437 = xor i64 %7435, %7403
  %7438 = add nuw nsw i64 %7437, %7435
  %7439 = icmp eq i64 %7438, 2
  %7440 = zext i1 %7439 to i8
  store i8 %7440, i8* %39, align 1
  %7441 = inttoptr i64 %7417 to i32*
  %7442 = add i64 %7383, 28
  store i64 %7442, i64* %3, align 8
  %7443 = load i32, i32* %7441, align 4
  %7444 = zext i32 %7443 to i64
  store i64 %7444, i64* %RCX.i11580, align 8
  %7445 = load i64, i64* %RBP.i, align 8
  %7446 = add i64 %7445, -140
  %7447 = add i64 %7383, 34
  store i64 %7447, i64* %3, align 8
  %7448 = inttoptr i64 %7446 to i32*
  %7449 = load i32, i32* %7448, align 4
  %7450 = add i32 %7449, %7443
  %7451 = zext i32 %7450 to i64
  store i64 %7451, i64* %RCX.i11580, align 8
  %7452 = icmp ult i32 %7450, %7443
  %7453 = icmp ult i32 %7450, %7449
  %7454 = or i1 %7452, %7453
  %7455 = zext i1 %7454 to i8
  store i8 %7455, i8* %14, align 1
  %7456 = and i32 %7450, 255
  %7457 = tail call i32 @llvm.ctpop.i32(i32 %7456)
  %7458 = trunc i32 %7457 to i8
  %7459 = and i8 %7458, 1
  %7460 = xor i8 %7459, 1
  store i8 %7460, i8* %21, align 1
  %7461 = xor i32 %7449, %7443
  %7462 = xor i32 %7461, %7450
  %7463 = lshr i32 %7462, 4
  %7464 = trunc i32 %7463 to i8
  %7465 = and i8 %7464, 1
  store i8 %7465, i8* %27, align 1
  %7466 = icmp eq i32 %7450, 0
  %7467 = zext i1 %7466 to i8
  store i8 %7467, i8* %30, align 1
  %7468 = lshr i32 %7450, 31
  %7469 = trunc i32 %7468 to i8
  store i8 %7469, i8* %33, align 1
  %7470 = lshr i32 %7443, 31
  %7471 = lshr i32 %7449, 31
  %7472 = xor i32 %7468, %7470
  %7473 = xor i32 %7468, %7471
  %7474 = add nuw nsw i32 %7472, %7473
  %7475 = icmp eq i32 %7474, 2
  %7476 = zext i1 %7475 to i8
  store i8 %7476, i8* %39, align 1
  %7477 = add i64 %7383, 40
  store i64 %7477, i64* %3, align 8
  store i32 %7450, i32* %7448, align 4
  %7478 = load i64, i64* %RBP.i, align 8
  %7479 = add i64 %7478, -8
  %7480 = load i64, i64* %3, align 8
  %7481 = add i64 %7480, 4
  store i64 %7481, i64* %3, align 8
  %7482 = inttoptr i64 %7479 to i64*
  %7483 = load i64, i64* %7482, align 8
  %7484 = add i64 %7483, 51640
  store i64 %7484, i64* %RAX.i11582.pre-phi, align 8
  %7485 = icmp ugt i64 %7483, -51641
  %7486 = zext i1 %7485 to i8
  store i8 %7486, i8* %14, align 1
  %7487 = trunc i64 %7484 to i32
  %7488 = and i32 %7487, 255
  %7489 = tail call i32 @llvm.ctpop.i32(i32 %7488)
  %7490 = trunc i32 %7489 to i8
  %7491 = and i8 %7490, 1
  %7492 = xor i8 %7491, 1
  store i8 %7492, i8* %21, align 1
  %7493 = xor i64 %7483, 16
  %7494 = xor i64 %7493, %7484
  %7495 = lshr i64 %7494, 4
  %7496 = trunc i64 %7495 to i8
  %7497 = and i8 %7496, 1
  store i8 %7497, i8* %27, align 1
  %7498 = icmp eq i64 %7484, 0
  %7499 = zext i1 %7498 to i8
  store i8 %7499, i8* %30, align 1
  %7500 = lshr i64 %7484, 63
  %7501 = trunc i64 %7500 to i8
  store i8 %7501, i8* %33, align 1
  %7502 = lshr i64 %7483, 63
  %7503 = xor i64 %7500, %7502
  %7504 = add nuw nsw i64 %7503, %7500
  %7505 = icmp eq i64 %7504, 2
  %7506 = zext i1 %7505 to i8
  store i8 %7506, i8* %39, align 1
  %7507 = add i64 %7478, -150
  %7508 = add i64 %7480, 17
  store i64 %7508, i64* %3, align 8
  %7509 = inttoptr i64 %7507 to i16*
  %7510 = load i16, i16* %7509, align 2
  %7511 = zext i16 %7510 to i64
  store i64 %7511, i64* %RCX.i11580, align 8
  %7512 = zext i16 %7510 to i64
  %7513 = shl nuw nsw i64 %7512, 4
  store i64 %7513, i64* %573, align 8
  %7514 = add i64 %7513, %7484
  store i64 %7514, i64* %RAX.i11582.pre-phi, align 8
  %7515 = icmp ult i64 %7514, %7484
  %7516 = icmp ult i64 %7514, %7513
  %7517 = or i1 %7515, %7516
  %7518 = zext i1 %7517 to i8
  store i8 %7518, i8* %14, align 1
  %7519 = trunc i64 %7514 to i32
  %7520 = and i32 %7519, 255
  %7521 = tail call i32 @llvm.ctpop.i32(i32 %7520)
  %7522 = trunc i32 %7521 to i8
  %7523 = and i8 %7522, 1
  %7524 = xor i8 %7523, 1
  store i8 %7524, i8* %21, align 1
  %7525 = xor i64 %7513, %7484
  %7526 = xor i64 %7525, %7514
  %7527 = lshr i64 %7526, 4
  %7528 = trunc i64 %7527 to i8
  %7529 = and i8 %7528, 1
  store i8 %7529, i8* %27, align 1
  %7530 = icmp eq i64 %7514, 0
  %7531 = zext i1 %7530 to i8
  store i8 %7531, i8* %30, align 1
  %7532 = lshr i64 %7514, 63
  %7533 = trunc i64 %7532 to i8
  store i8 %7533, i8* %33, align 1
  %7534 = xor i64 %7532, %7500
  %7535 = add nuw nsw i64 %7534, %7532
  %7536 = icmp eq i64 %7535, 2
  %7537 = zext i1 %7536 to i8
  store i8 %7537, i8* %39, align 1
  %7538 = add i64 %7514, 4
  %7539 = add i64 %7480, 29
  store i64 %7539, i64* %3, align 8
  %7540 = inttoptr i64 %7538 to i32*
  %7541 = load i32, i32* %7540, align 4
  %7542 = zext i32 %7541 to i64
  store i64 %7542, i64* %RCX.i11580, align 8
  %7543 = load i64, i64* %RBP.i, align 8
  %7544 = add i64 %7543, -144
  %7545 = add i64 %7480, 35
  store i64 %7545, i64* %3, align 8
  %7546 = inttoptr i64 %7544 to i32*
  %7547 = load i32, i32* %7546, align 4
  %7548 = add i32 %7547, %7541
  %7549 = zext i32 %7548 to i64
  store i64 %7549, i64* %RCX.i11580, align 8
  %7550 = icmp ult i32 %7548, %7541
  %7551 = icmp ult i32 %7548, %7547
  %7552 = or i1 %7550, %7551
  %7553 = zext i1 %7552 to i8
  store i8 %7553, i8* %14, align 1
  %7554 = and i32 %7548, 255
  %7555 = tail call i32 @llvm.ctpop.i32(i32 %7554)
  %7556 = trunc i32 %7555 to i8
  %7557 = and i8 %7556, 1
  %7558 = xor i8 %7557, 1
  store i8 %7558, i8* %21, align 1
  %7559 = xor i32 %7547, %7541
  %7560 = xor i32 %7559, %7548
  %7561 = lshr i32 %7560, 4
  %7562 = trunc i32 %7561 to i8
  %7563 = and i8 %7562, 1
  store i8 %7563, i8* %27, align 1
  %7564 = icmp eq i32 %7548, 0
  %7565 = zext i1 %7564 to i8
  store i8 %7565, i8* %30, align 1
  %7566 = lshr i32 %7548, 31
  %7567 = trunc i32 %7566 to i8
  store i8 %7567, i8* %33, align 1
  %7568 = lshr i32 %7541, 31
  %7569 = lshr i32 %7547, 31
  %7570 = xor i32 %7566, %7568
  %7571 = xor i32 %7566, %7569
  %7572 = add nuw nsw i32 %7570, %7571
  %7573 = icmp eq i32 %7572, 2
  %7574 = zext i1 %7573 to i8
  store i8 %7574, i8* %39, align 1
  %7575 = add i64 %7480, 41
  store i64 %7575, i64* %3, align 8
  store i32 %7548, i32* %7546, align 4
  %7576 = load i64, i64* %RBP.i, align 8
  %7577 = add i64 %7576, -8
  %7578 = load i64, i64* %3, align 8
  %7579 = add i64 %7578, 4
  store i64 %7579, i64* %3, align 8
  %7580 = inttoptr i64 %7577 to i64*
  %7581 = load i64, i64* %7580, align 8
  %7582 = add i64 %7581, 51640
  store i64 %7582, i64* %RAX.i11582.pre-phi, align 8
  %7583 = icmp ugt i64 %7581, -51641
  %7584 = zext i1 %7583 to i8
  store i8 %7584, i8* %14, align 1
  %7585 = trunc i64 %7582 to i32
  %7586 = and i32 %7585, 255
  %7587 = tail call i32 @llvm.ctpop.i32(i32 %7586)
  %7588 = trunc i32 %7587 to i8
  %7589 = and i8 %7588, 1
  %7590 = xor i8 %7589, 1
  store i8 %7590, i8* %21, align 1
  %7591 = xor i64 %7581, 16
  %7592 = xor i64 %7591, %7582
  %7593 = lshr i64 %7592, 4
  %7594 = trunc i64 %7593 to i8
  %7595 = and i8 %7594, 1
  store i8 %7595, i8* %27, align 1
  %7596 = icmp eq i64 %7582, 0
  %7597 = zext i1 %7596 to i8
  store i8 %7597, i8* %30, align 1
  %7598 = lshr i64 %7582, 63
  %7599 = trunc i64 %7598 to i8
  store i8 %7599, i8* %33, align 1
  %7600 = lshr i64 %7581, 63
  %7601 = xor i64 %7598, %7600
  %7602 = add nuw nsw i64 %7601, %7598
  %7603 = icmp eq i64 %7602, 2
  %7604 = zext i1 %7603 to i8
  store i8 %7604, i8* %39, align 1
  %7605 = add i64 %7576, -150
  %7606 = add i64 %7578, 17
  store i64 %7606, i64* %3, align 8
  %7607 = inttoptr i64 %7605 to i16*
  %7608 = load i16, i16* %7607, align 2
  %7609 = zext i16 %7608 to i64
  store i64 %7609, i64* %RCX.i11580, align 8
  %7610 = zext i16 %7608 to i64
  %7611 = shl nuw nsw i64 %7610, 4
  store i64 %7611, i64* %573, align 8
  %7612 = add i64 %7611, %7582
  store i64 %7612, i64* %RAX.i11582.pre-phi, align 8
  %7613 = icmp ult i64 %7612, %7582
  %7614 = icmp ult i64 %7612, %7611
  %7615 = or i1 %7613, %7614
  %7616 = zext i1 %7615 to i8
  store i8 %7616, i8* %14, align 1
  %7617 = trunc i64 %7612 to i32
  %7618 = and i32 %7617, 255
  %7619 = tail call i32 @llvm.ctpop.i32(i32 %7618)
  %7620 = trunc i32 %7619 to i8
  %7621 = and i8 %7620, 1
  %7622 = xor i8 %7621, 1
  store i8 %7622, i8* %21, align 1
  %7623 = xor i64 %7611, %7582
  %7624 = xor i64 %7623, %7612
  %7625 = lshr i64 %7624, 4
  %7626 = trunc i64 %7625 to i8
  %7627 = and i8 %7626, 1
  store i8 %7627, i8* %27, align 1
  %7628 = icmp eq i64 %7612, 0
  %7629 = zext i1 %7628 to i8
  store i8 %7629, i8* %30, align 1
  %7630 = lshr i64 %7612, 63
  %7631 = trunc i64 %7630 to i8
  store i8 %7631, i8* %33, align 1
  %7632 = xor i64 %7630, %7598
  %7633 = add nuw nsw i64 %7632, %7630
  %7634 = icmp eq i64 %7633, 2
  %7635 = zext i1 %7634 to i8
  store i8 %7635, i8* %39, align 1
  %7636 = add i64 %7612, 8
  %7637 = add i64 %7578, 29
  store i64 %7637, i64* %3, align 8
  %7638 = inttoptr i64 %7636 to i32*
  %7639 = load i32, i32* %7638, align 4
  %7640 = zext i32 %7639 to i64
  store i64 %7640, i64* %RCX.i11580, align 8
  %7641 = load i64, i64* %RBP.i, align 8
  %7642 = add i64 %7641, -148
  %7643 = add i64 %7578, 35
  store i64 %7643, i64* %3, align 8
  %7644 = inttoptr i64 %7642 to i32*
  %7645 = load i32, i32* %7644, align 4
  %7646 = add i32 %7645, %7639
  %7647 = zext i32 %7646 to i64
  store i64 %7647, i64* %RCX.i11580, align 8
  %7648 = icmp ult i32 %7646, %7639
  %7649 = icmp ult i32 %7646, %7645
  %7650 = or i1 %7648, %7649
  %7651 = zext i1 %7650 to i8
  store i8 %7651, i8* %14, align 1
  %7652 = and i32 %7646, 255
  %7653 = tail call i32 @llvm.ctpop.i32(i32 %7652)
  %7654 = trunc i32 %7653 to i8
  %7655 = and i8 %7654, 1
  %7656 = xor i8 %7655, 1
  store i8 %7656, i8* %21, align 1
  %7657 = xor i32 %7645, %7639
  %7658 = xor i32 %7657, %7646
  %7659 = lshr i32 %7658, 4
  %7660 = trunc i32 %7659 to i8
  %7661 = and i8 %7660, 1
  store i8 %7661, i8* %27, align 1
  %7662 = icmp eq i32 %7646, 0
  %7663 = zext i1 %7662 to i8
  store i8 %7663, i8* %30, align 1
  %7664 = lshr i32 %7646, 31
  %7665 = trunc i32 %7664 to i8
  store i8 %7665, i8* %33, align 1
  %7666 = lshr i32 %7639, 31
  %7667 = lshr i32 %7645, 31
  %7668 = xor i32 %7664, %7666
  %7669 = xor i32 %7664, %7667
  %7670 = add nuw nsw i32 %7668, %7669
  %7671 = icmp eq i32 %7670, 2
  %7672 = zext i1 %7671 to i8
  store i8 %7672, i8* %39, align 1
  %7673 = add i64 %7578, 41
  store i64 %7673, i64* %3, align 8
  store i32 %7646, i32* %7644, align 4
  %7674 = load i64, i64* %RBP.i, align 8
  %7675 = add i64 %7674, -120
  %7676 = load i64, i64* %3, align 8
  %7677 = add i64 %7676, 4
  store i64 %7677, i64* %3, align 8
  %7678 = inttoptr i64 %7675 to i64*
  %7679 = load i64, i64* %7678, align 8
  store i64 %7679, i64* %RAX.i11582.pre-phi, align 8
  %7680 = add i64 %7674, -28
  %7681 = add i64 %7676, 7
  store i64 %7681, i64* %3, align 8
  %7682 = inttoptr i64 %7680 to i32*
  %7683 = load i32, i32* %7682, align 4
  %7684 = add i32 %7683, 15
  %7685 = zext i32 %7684 to i64
  store i64 %7685, i64* %RCX.i11580, align 8
  %7686 = icmp ugt i32 %7683, -16
  %7687 = zext i1 %7686 to i8
  store i8 %7687, i8* %14, align 1
  %7688 = and i32 %7684, 255
  %7689 = tail call i32 @llvm.ctpop.i32(i32 %7688)
  %7690 = trunc i32 %7689 to i8
  %7691 = and i8 %7690, 1
  %7692 = xor i8 %7691, 1
  store i8 %7692, i8* %21, align 1
  %7693 = xor i32 %7684, %7683
  %7694 = lshr i32 %7693, 4
  %7695 = trunc i32 %7694 to i8
  %7696 = and i8 %7695, 1
  store i8 %7696, i8* %27, align 1
  %7697 = icmp eq i32 %7684, 0
  %7698 = zext i1 %7697 to i8
  store i8 %7698, i8* %30, align 1
  %7699 = lshr i32 %7684, 31
  %7700 = trunc i32 %7699 to i8
  store i8 %7700, i8* %33, align 1
  %7701 = lshr i32 %7683, 31
  %7702 = xor i32 %7699, %7701
  %7703 = add nuw nsw i32 %7702, %7699
  %7704 = icmp eq i32 %7703, 2
  %7705 = zext i1 %7704 to i8
  store i8 %7705, i8* %39, align 1
  %7706 = sext i32 %7684 to i64
  store i64 %7706, i64* %573, align 8
  %7707 = shl nsw i64 %7706, 1
  %7708 = add i64 %7679, %7707
  %7709 = add i64 %7676, 17
  store i64 %7709, i64* %3, align 8
  %7710 = inttoptr i64 %7708 to i16*
  %7711 = load i16, i16* %7710, align 2
  store i16 %7711, i16* %SI.i, align 2
  %7712 = add i64 %7674, -150
  %7713 = add i64 %7676, 24
  store i64 %7713, i64* %3, align 8
  %7714 = inttoptr i64 %7712 to i16*
  store i16 %7711, i16* %7714, align 2
  %7715 = load i64, i64* %RBP.i, align 8
  %7716 = add i64 %7715, -8
  %7717 = load i64, i64* %3, align 8
  %7718 = add i64 %7717, 4
  store i64 %7718, i64* %3, align 8
  %7719 = inttoptr i64 %7716 to i64*
  %7720 = load i64, i64* %7719, align 8
  %7721 = add i64 %7720, 51640
  store i64 %7721, i64* %RAX.i11582.pre-phi, align 8
  %7722 = icmp ugt i64 %7720, -51641
  %7723 = zext i1 %7722 to i8
  store i8 %7723, i8* %14, align 1
  %7724 = trunc i64 %7721 to i32
  %7725 = and i32 %7724, 255
  %7726 = tail call i32 @llvm.ctpop.i32(i32 %7725)
  %7727 = trunc i32 %7726 to i8
  %7728 = and i8 %7727, 1
  %7729 = xor i8 %7728, 1
  store i8 %7729, i8* %21, align 1
  %7730 = xor i64 %7720, 16
  %7731 = xor i64 %7730, %7721
  %7732 = lshr i64 %7731, 4
  %7733 = trunc i64 %7732 to i8
  %7734 = and i8 %7733, 1
  store i8 %7734, i8* %27, align 1
  %7735 = icmp eq i64 %7721, 0
  %7736 = zext i1 %7735 to i8
  store i8 %7736, i8* %30, align 1
  %7737 = lshr i64 %7721, 63
  %7738 = trunc i64 %7737 to i8
  store i8 %7738, i8* %33, align 1
  %7739 = lshr i64 %7720, 63
  %7740 = xor i64 %7737, %7739
  %7741 = add nuw nsw i64 %7740, %7737
  %7742 = icmp eq i64 %7741, 2
  %7743 = zext i1 %7742 to i8
  store i8 %7743, i8* %39, align 1
  %7744 = add i64 %7715, -150
  %7745 = add i64 %7717, 17
  store i64 %7745, i64* %3, align 8
  %7746 = inttoptr i64 %7744 to i16*
  %7747 = load i16, i16* %7746, align 2
  %7748 = zext i16 %7747 to i64
  store i64 %7748, i64* %RCX.i11580, align 8
  %7749 = zext i16 %7747 to i64
  %7750 = shl nuw nsw i64 %7749, 4
  store i64 %7750, i64* %573, align 8
  %7751 = add i64 %7750, %7721
  store i64 %7751, i64* %RAX.i11582.pre-phi, align 8
  %7752 = icmp ult i64 %7751, %7721
  %7753 = icmp ult i64 %7751, %7750
  %7754 = or i1 %7752, %7753
  %7755 = zext i1 %7754 to i8
  store i8 %7755, i8* %14, align 1
  %7756 = trunc i64 %7751 to i32
  %7757 = and i32 %7756, 255
  %7758 = tail call i32 @llvm.ctpop.i32(i32 %7757)
  %7759 = trunc i32 %7758 to i8
  %7760 = and i8 %7759, 1
  %7761 = xor i8 %7760, 1
  store i8 %7761, i8* %21, align 1
  %7762 = xor i64 %7750, %7721
  %7763 = xor i64 %7762, %7751
  %7764 = lshr i64 %7763, 4
  %7765 = trunc i64 %7764 to i8
  %7766 = and i8 %7765, 1
  store i8 %7766, i8* %27, align 1
  %7767 = icmp eq i64 %7751, 0
  %7768 = zext i1 %7767 to i8
  store i8 %7768, i8* %30, align 1
  %7769 = lshr i64 %7751, 63
  %7770 = trunc i64 %7769 to i8
  store i8 %7770, i8* %33, align 1
  %7771 = xor i64 %7769, %7737
  %7772 = add nuw nsw i64 %7771, %7769
  %7773 = icmp eq i64 %7772, 2
  %7774 = zext i1 %7773 to i8
  store i8 %7774, i8* %39, align 1
  %7775 = inttoptr i64 %7751 to i32*
  %7776 = add i64 %7717, 28
  store i64 %7776, i64* %3, align 8
  %7777 = load i32, i32* %7775, align 4
  %7778 = zext i32 %7777 to i64
  store i64 %7778, i64* %RCX.i11580, align 8
  %7779 = load i64, i64* %RBP.i, align 8
  %7780 = add i64 %7779, -140
  %7781 = add i64 %7717, 34
  store i64 %7781, i64* %3, align 8
  %7782 = inttoptr i64 %7780 to i32*
  %7783 = load i32, i32* %7782, align 4
  %7784 = add i32 %7783, %7777
  %7785 = zext i32 %7784 to i64
  store i64 %7785, i64* %RCX.i11580, align 8
  %7786 = icmp ult i32 %7784, %7777
  %7787 = icmp ult i32 %7784, %7783
  %7788 = or i1 %7786, %7787
  %7789 = zext i1 %7788 to i8
  store i8 %7789, i8* %14, align 1
  %7790 = and i32 %7784, 255
  %7791 = tail call i32 @llvm.ctpop.i32(i32 %7790)
  %7792 = trunc i32 %7791 to i8
  %7793 = and i8 %7792, 1
  %7794 = xor i8 %7793, 1
  store i8 %7794, i8* %21, align 1
  %7795 = xor i32 %7783, %7777
  %7796 = xor i32 %7795, %7784
  %7797 = lshr i32 %7796, 4
  %7798 = trunc i32 %7797 to i8
  %7799 = and i8 %7798, 1
  store i8 %7799, i8* %27, align 1
  %7800 = icmp eq i32 %7784, 0
  %7801 = zext i1 %7800 to i8
  store i8 %7801, i8* %30, align 1
  %7802 = lshr i32 %7784, 31
  %7803 = trunc i32 %7802 to i8
  store i8 %7803, i8* %33, align 1
  %7804 = lshr i32 %7777, 31
  %7805 = lshr i32 %7783, 31
  %7806 = xor i32 %7802, %7804
  %7807 = xor i32 %7802, %7805
  %7808 = add nuw nsw i32 %7806, %7807
  %7809 = icmp eq i32 %7808, 2
  %7810 = zext i1 %7809 to i8
  store i8 %7810, i8* %39, align 1
  %7811 = add i64 %7717, 40
  store i64 %7811, i64* %3, align 8
  store i32 %7784, i32* %7782, align 4
  %7812 = load i64, i64* %RBP.i, align 8
  %7813 = add i64 %7812, -8
  %7814 = load i64, i64* %3, align 8
  %7815 = add i64 %7814, 4
  store i64 %7815, i64* %3, align 8
  %7816 = inttoptr i64 %7813 to i64*
  %7817 = load i64, i64* %7816, align 8
  %7818 = add i64 %7817, 51640
  store i64 %7818, i64* %RAX.i11582.pre-phi, align 8
  %7819 = icmp ugt i64 %7817, -51641
  %7820 = zext i1 %7819 to i8
  store i8 %7820, i8* %14, align 1
  %7821 = trunc i64 %7818 to i32
  %7822 = and i32 %7821, 255
  %7823 = tail call i32 @llvm.ctpop.i32(i32 %7822)
  %7824 = trunc i32 %7823 to i8
  %7825 = and i8 %7824, 1
  %7826 = xor i8 %7825, 1
  store i8 %7826, i8* %21, align 1
  %7827 = xor i64 %7817, 16
  %7828 = xor i64 %7827, %7818
  %7829 = lshr i64 %7828, 4
  %7830 = trunc i64 %7829 to i8
  %7831 = and i8 %7830, 1
  store i8 %7831, i8* %27, align 1
  %7832 = icmp eq i64 %7818, 0
  %7833 = zext i1 %7832 to i8
  store i8 %7833, i8* %30, align 1
  %7834 = lshr i64 %7818, 63
  %7835 = trunc i64 %7834 to i8
  store i8 %7835, i8* %33, align 1
  %7836 = lshr i64 %7817, 63
  %7837 = xor i64 %7834, %7836
  %7838 = add nuw nsw i64 %7837, %7834
  %7839 = icmp eq i64 %7838, 2
  %7840 = zext i1 %7839 to i8
  store i8 %7840, i8* %39, align 1
  %7841 = add i64 %7812, -150
  %7842 = add i64 %7814, 17
  store i64 %7842, i64* %3, align 8
  %7843 = inttoptr i64 %7841 to i16*
  %7844 = load i16, i16* %7843, align 2
  %7845 = zext i16 %7844 to i64
  store i64 %7845, i64* %RCX.i11580, align 8
  %7846 = zext i16 %7844 to i64
  %7847 = shl nuw nsw i64 %7846, 4
  store i64 %7847, i64* %573, align 8
  %7848 = add i64 %7847, %7818
  store i64 %7848, i64* %RAX.i11582.pre-phi, align 8
  %7849 = icmp ult i64 %7848, %7818
  %7850 = icmp ult i64 %7848, %7847
  %7851 = or i1 %7849, %7850
  %7852 = zext i1 %7851 to i8
  store i8 %7852, i8* %14, align 1
  %7853 = trunc i64 %7848 to i32
  %7854 = and i32 %7853, 255
  %7855 = tail call i32 @llvm.ctpop.i32(i32 %7854)
  %7856 = trunc i32 %7855 to i8
  %7857 = and i8 %7856, 1
  %7858 = xor i8 %7857, 1
  store i8 %7858, i8* %21, align 1
  %7859 = xor i64 %7847, %7818
  %7860 = xor i64 %7859, %7848
  %7861 = lshr i64 %7860, 4
  %7862 = trunc i64 %7861 to i8
  %7863 = and i8 %7862, 1
  store i8 %7863, i8* %27, align 1
  %7864 = icmp eq i64 %7848, 0
  %7865 = zext i1 %7864 to i8
  store i8 %7865, i8* %30, align 1
  %7866 = lshr i64 %7848, 63
  %7867 = trunc i64 %7866 to i8
  store i8 %7867, i8* %33, align 1
  %7868 = xor i64 %7866, %7834
  %7869 = add nuw nsw i64 %7868, %7866
  %7870 = icmp eq i64 %7869, 2
  %7871 = zext i1 %7870 to i8
  store i8 %7871, i8* %39, align 1
  %7872 = add i64 %7848, 4
  %7873 = add i64 %7814, 29
  store i64 %7873, i64* %3, align 8
  %7874 = inttoptr i64 %7872 to i32*
  %7875 = load i32, i32* %7874, align 4
  %7876 = zext i32 %7875 to i64
  store i64 %7876, i64* %RCX.i11580, align 8
  %7877 = load i64, i64* %RBP.i, align 8
  %7878 = add i64 %7877, -144
  %7879 = add i64 %7814, 35
  store i64 %7879, i64* %3, align 8
  %7880 = inttoptr i64 %7878 to i32*
  %7881 = load i32, i32* %7880, align 4
  %7882 = add i32 %7881, %7875
  %7883 = zext i32 %7882 to i64
  store i64 %7883, i64* %RCX.i11580, align 8
  %7884 = icmp ult i32 %7882, %7875
  %7885 = icmp ult i32 %7882, %7881
  %7886 = or i1 %7884, %7885
  %7887 = zext i1 %7886 to i8
  store i8 %7887, i8* %14, align 1
  %7888 = and i32 %7882, 255
  %7889 = tail call i32 @llvm.ctpop.i32(i32 %7888)
  %7890 = trunc i32 %7889 to i8
  %7891 = and i8 %7890, 1
  %7892 = xor i8 %7891, 1
  store i8 %7892, i8* %21, align 1
  %7893 = xor i32 %7881, %7875
  %7894 = xor i32 %7893, %7882
  %7895 = lshr i32 %7894, 4
  %7896 = trunc i32 %7895 to i8
  %7897 = and i8 %7896, 1
  store i8 %7897, i8* %27, align 1
  %7898 = icmp eq i32 %7882, 0
  %7899 = zext i1 %7898 to i8
  store i8 %7899, i8* %30, align 1
  %7900 = lshr i32 %7882, 31
  %7901 = trunc i32 %7900 to i8
  store i8 %7901, i8* %33, align 1
  %7902 = lshr i32 %7875, 31
  %7903 = lshr i32 %7881, 31
  %7904 = xor i32 %7900, %7902
  %7905 = xor i32 %7900, %7903
  %7906 = add nuw nsw i32 %7904, %7905
  %7907 = icmp eq i32 %7906, 2
  %7908 = zext i1 %7907 to i8
  store i8 %7908, i8* %39, align 1
  %7909 = add i64 %7814, 41
  store i64 %7909, i64* %3, align 8
  store i32 %7882, i32* %7880, align 4
  %7910 = load i64, i64* %RBP.i, align 8
  %7911 = add i64 %7910, -8
  %7912 = load i64, i64* %3, align 8
  %7913 = add i64 %7912, 4
  store i64 %7913, i64* %3, align 8
  %7914 = inttoptr i64 %7911 to i64*
  %7915 = load i64, i64* %7914, align 8
  %7916 = add i64 %7915, 51640
  store i64 %7916, i64* %RAX.i11582.pre-phi, align 8
  %7917 = icmp ugt i64 %7915, -51641
  %7918 = zext i1 %7917 to i8
  store i8 %7918, i8* %14, align 1
  %7919 = trunc i64 %7916 to i32
  %7920 = and i32 %7919, 255
  %7921 = tail call i32 @llvm.ctpop.i32(i32 %7920)
  %7922 = trunc i32 %7921 to i8
  %7923 = and i8 %7922, 1
  %7924 = xor i8 %7923, 1
  store i8 %7924, i8* %21, align 1
  %7925 = xor i64 %7915, 16
  %7926 = xor i64 %7925, %7916
  %7927 = lshr i64 %7926, 4
  %7928 = trunc i64 %7927 to i8
  %7929 = and i8 %7928, 1
  store i8 %7929, i8* %27, align 1
  %7930 = icmp eq i64 %7916, 0
  %7931 = zext i1 %7930 to i8
  store i8 %7931, i8* %30, align 1
  %7932 = lshr i64 %7916, 63
  %7933 = trunc i64 %7932 to i8
  store i8 %7933, i8* %33, align 1
  %7934 = lshr i64 %7915, 63
  %7935 = xor i64 %7932, %7934
  %7936 = add nuw nsw i64 %7935, %7932
  %7937 = icmp eq i64 %7936, 2
  %7938 = zext i1 %7937 to i8
  store i8 %7938, i8* %39, align 1
  %7939 = add i64 %7910, -150
  %7940 = add i64 %7912, 17
  store i64 %7940, i64* %3, align 8
  %7941 = inttoptr i64 %7939 to i16*
  %7942 = load i16, i16* %7941, align 2
  %7943 = zext i16 %7942 to i64
  store i64 %7943, i64* %RCX.i11580, align 8
  %7944 = zext i16 %7942 to i64
  %7945 = shl nuw nsw i64 %7944, 4
  store i64 %7945, i64* %573, align 8
  %7946 = add i64 %7945, %7916
  store i64 %7946, i64* %RAX.i11582.pre-phi, align 8
  %7947 = icmp ult i64 %7946, %7916
  %7948 = icmp ult i64 %7946, %7945
  %7949 = or i1 %7947, %7948
  %7950 = zext i1 %7949 to i8
  store i8 %7950, i8* %14, align 1
  %7951 = trunc i64 %7946 to i32
  %7952 = and i32 %7951, 255
  %7953 = tail call i32 @llvm.ctpop.i32(i32 %7952)
  %7954 = trunc i32 %7953 to i8
  %7955 = and i8 %7954, 1
  %7956 = xor i8 %7955, 1
  store i8 %7956, i8* %21, align 1
  %7957 = xor i64 %7945, %7916
  %7958 = xor i64 %7957, %7946
  %7959 = lshr i64 %7958, 4
  %7960 = trunc i64 %7959 to i8
  %7961 = and i8 %7960, 1
  store i8 %7961, i8* %27, align 1
  %7962 = icmp eq i64 %7946, 0
  %7963 = zext i1 %7962 to i8
  store i8 %7963, i8* %30, align 1
  %7964 = lshr i64 %7946, 63
  %7965 = trunc i64 %7964 to i8
  store i8 %7965, i8* %33, align 1
  %7966 = xor i64 %7964, %7932
  %7967 = add nuw nsw i64 %7966, %7964
  %7968 = icmp eq i64 %7967, 2
  %7969 = zext i1 %7968 to i8
  store i8 %7969, i8* %39, align 1
  %7970 = add i64 %7946, 8
  %7971 = add i64 %7912, 29
  store i64 %7971, i64* %3, align 8
  %7972 = inttoptr i64 %7970 to i32*
  %7973 = load i32, i32* %7972, align 4
  %7974 = zext i32 %7973 to i64
  store i64 %7974, i64* %RCX.i11580, align 8
  %7975 = load i64, i64* %RBP.i, align 8
  %7976 = add i64 %7975, -148
  %7977 = add i64 %7912, 35
  store i64 %7977, i64* %3, align 8
  %7978 = inttoptr i64 %7976 to i32*
  %7979 = load i32, i32* %7978, align 4
  %7980 = add i32 %7979, %7973
  %7981 = zext i32 %7980 to i64
  store i64 %7981, i64* %RCX.i11580, align 8
  %7982 = icmp ult i32 %7980, %7973
  %7983 = icmp ult i32 %7980, %7979
  %7984 = or i1 %7982, %7983
  %7985 = zext i1 %7984 to i8
  store i8 %7985, i8* %14, align 1
  %7986 = and i32 %7980, 255
  %7987 = tail call i32 @llvm.ctpop.i32(i32 %7986)
  %7988 = trunc i32 %7987 to i8
  %7989 = and i8 %7988, 1
  %7990 = xor i8 %7989, 1
  store i8 %7990, i8* %21, align 1
  %7991 = xor i32 %7979, %7973
  %7992 = xor i32 %7991, %7980
  %7993 = lshr i32 %7992, 4
  %7994 = trunc i32 %7993 to i8
  %7995 = and i8 %7994, 1
  store i8 %7995, i8* %27, align 1
  %7996 = icmp eq i32 %7980, 0
  %7997 = zext i1 %7996 to i8
  store i8 %7997, i8* %30, align 1
  %7998 = lshr i32 %7980, 31
  %7999 = trunc i32 %7998 to i8
  store i8 %7999, i8* %33, align 1
  %8000 = lshr i32 %7973, 31
  %8001 = lshr i32 %7979, 31
  %8002 = xor i32 %7998, %8000
  %8003 = xor i32 %7998, %8001
  %8004 = add nuw nsw i32 %8002, %8003
  %8005 = icmp eq i32 %8004, 2
  %8006 = zext i1 %8005 to i8
  store i8 %8006, i8* %39, align 1
  %8007 = add i64 %7912, 41
  store i64 %8007, i64* %3, align 8
  store i32 %7980, i32* %7978, align 4
  %8008 = load i64, i64* %RBP.i, align 8
  %8009 = add i64 %8008, -120
  %8010 = load i64, i64* %3, align 8
  %8011 = add i64 %8010, 4
  store i64 %8011, i64* %3, align 8
  %8012 = inttoptr i64 %8009 to i64*
  %8013 = load i64, i64* %8012, align 8
  store i64 %8013, i64* %RAX.i11582.pre-phi, align 8
  %8014 = add i64 %8008, -28
  %8015 = add i64 %8010, 7
  store i64 %8015, i64* %3, align 8
  %8016 = inttoptr i64 %8014 to i32*
  %8017 = load i32, i32* %8016, align 4
  %8018 = add i32 %8017, 16
  %8019 = zext i32 %8018 to i64
  store i64 %8019, i64* %RCX.i11580, align 8
  %8020 = icmp ugt i32 %8017, -17
  %8021 = zext i1 %8020 to i8
  store i8 %8021, i8* %14, align 1
  %8022 = and i32 %8018, 255
  %8023 = tail call i32 @llvm.ctpop.i32(i32 %8022)
  %8024 = trunc i32 %8023 to i8
  %8025 = and i8 %8024, 1
  %8026 = xor i8 %8025, 1
  store i8 %8026, i8* %21, align 1
  %8027 = xor i32 %8017, 16
  %8028 = xor i32 %8027, %8018
  %8029 = lshr i32 %8028, 4
  %8030 = trunc i32 %8029 to i8
  %8031 = and i8 %8030, 1
  store i8 %8031, i8* %27, align 1
  %8032 = icmp eq i32 %8018, 0
  %8033 = zext i1 %8032 to i8
  store i8 %8033, i8* %30, align 1
  %8034 = lshr i32 %8018, 31
  %8035 = trunc i32 %8034 to i8
  store i8 %8035, i8* %33, align 1
  %8036 = lshr i32 %8017, 31
  %8037 = xor i32 %8034, %8036
  %8038 = add nuw nsw i32 %8037, %8034
  %8039 = icmp eq i32 %8038, 2
  %8040 = zext i1 %8039 to i8
  store i8 %8040, i8* %39, align 1
  %8041 = sext i32 %8018 to i64
  store i64 %8041, i64* %573, align 8
  %8042 = shl nsw i64 %8041, 1
  %8043 = add i64 %8013, %8042
  %8044 = add i64 %8010, 17
  store i64 %8044, i64* %3, align 8
  %8045 = inttoptr i64 %8043 to i16*
  %8046 = load i16, i16* %8045, align 2
  store i16 %8046, i16* %SI.i, align 2
  %8047 = add i64 %8008, -150
  %8048 = add i64 %8010, 24
  store i64 %8048, i64* %3, align 8
  %8049 = inttoptr i64 %8047 to i16*
  store i16 %8046, i16* %8049, align 2
  %8050 = load i64, i64* %RBP.i, align 8
  %8051 = add i64 %8050, -8
  %8052 = load i64, i64* %3, align 8
  %8053 = add i64 %8052, 4
  store i64 %8053, i64* %3, align 8
  %8054 = inttoptr i64 %8051 to i64*
  %8055 = load i64, i64* %8054, align 8
  %8056 = add i64 %8055, 51640
  store i64 %8056, i64* %RAX.i11582.pre-phi, align 8
  %8057 = icmp ugt i64 %8055, -51641
  %8058 = zext i1 %8057 to i8
  store i8 %8058, i8* %14, align 1
  %8059 = trunc i64 %8056 to i32
  %8060 = and i32 %8059, 255
  %8061 = tail call i32 @llvm.ctpop.i32(i32 %8060)
  %8062 = trunc i32 %8061 to i8
  %8063 = and i8 %8062, 1
  %8064 = xor i8 %8063, 1
  store i8 %8064, i8* %21, align 1
  %8065 = xor i64 %8055, 16
  %8066 = xor i64 %8065, %8056
  %8067 = lshr i64 %8066, 4
  %8068 = trunc i64 %8067 to i8
  %8069 = and i8 %8068, 1
  store i8 %8069, i8* %27, align 1
  %8070 = icmp eq i64 %8056, 0
  %8071 = zext i1 %8070 to i8
  store i8 %8071, i8* %30, align 1
  %8072 = lshr i64 %8056, 63
  %8073 = trunc i64 %8072 to i8
  store i8 %8073, i8* %33, align 1
  %8074 = lshr i64 %8055, 63
  %8075 = xor i64 %8072, %8074
  %8076 = add nuw nsw i64 %8075, %8072
  %8077 = icmp eq i64 %8076, 2
  %8078 = zext i1 %8077 to i8
  store i8 %8078, i8* %39, align 1
  %8079 = add i64 %8050, -150
  %8080 = add i64 %8052, 17
  store i64 %8080, i64* %3, align 8
  %8081 = inttoptr i64 %8079 to i16*
  %8082 = load i16, i16* %8081, align 2
  %8083 = zext i16 %8082 to i64
  store i64 %8083, i64* %RCX.i11580, align 8
  %8084 = zext i16 %8082 to i64
  %8085 = shl nuw nsw i64 %8084, 4
  store i64 %8085, i64* %573, align 8
  %8086 = add i64 %8085, %8056
  store i64 %8086, i64* %RAX.i11582.pre-phi, align 8
  %8087 = icmp ult i64 %8086, %8056
  %8088 = icmp ult i64 %8086, %8085
  %8089 = or i1 %8087, %8088
  %8090 = zext i1 %8089 to i8
  store i8 %8090, i8* %14, align 1
  %8091 = trunc i64 %8086 to i32
  %8092 = and i32 %8091, 255
  %8093 = tail call i32 @llvm.ctpop.i32(i32 %8092)
  %8094 = trunc i32 %8093 to i8
  %8095 = and i8 %8094, 1
  %8096 = xor i8 %8095, 1
  store i8 %8096, i8* %21, align 1
  %8097 = xor i64 %8085, %8056
  %8098 = xor i64 %8097, %8086
  %8099 = lshr i64 %8098, 4
  %8100 = trunc i64 %8099 to i8
  %8101 = and i8 %8100, 1
  store i8 %8101, i8* %27, align 1
  %8102 = icmp eq i64 %8086, 0
  %8103 = zext i1 %8102 to i8
  store i8 %8103, i8* %30, align 1
  %8104 = lshr i64 %8086, 63
  %8105 = trunc i64 %8104 to i8
  store i8 %8105, i8* %33, align 1
  %8106 = xor i64 %8104, %8072
  %8107 = add nuw nsw i64 %8106, %8104
  %8108 = icmp eq i64 %8107, 2
  %8109 = zext i1 %8108 to i8
  store i8 %8109, i8* %39, align 1
  %8110 = inttoptr i64 %8086 to i32*
  %8111 = add i64 %8052, 28
  store i64 %8111, i64* %3, align 8
  %8112 = load i32, i32* %8110, align 4
  %8113 = zext i32 %8112 to i64
  store i64 %8113, i64* %RCX.i11580, align 8
  %8114 = load i64, i64* %RBP.i, align 8
  %8115 = add i64 %8114, -140
  %8116 = add i64 %8052, 34
  store i64 %8116, i64* %3, align 8
  %8117 = inttoptr i64 %8115 to i32*
  %8118 = load i32, i32* %8117, align 4
  %8119 = add i32 %8118, %8112
  %8120 = zext i32 %8119 to i64
  store i64 %8120, i64* %RCX.i11580, align 8
  %8121 = icmp ult i32 %8119, %8112
  %8122 = icmp ult i32 %8119, %8118
  %8123 = or i1 %8121, %8122
  %8124 = zext i1 %8123 to i8
  store i8 %8124, i8* %14, align 1
  %8125 = and i32 %8119, 255
  %8126 = tail call i32 @llvm.ctpop.i32(i32 %8125)
  %8127 = trunc i32 %8126 to i8
  %8128 = and i8 %8127, 1
  %8129 = xor i8 %8128, 1
  store i8 %8129, i8* %21, align 1
  %8130 = xor i32 %8118, %8112
  %8131 = xor i32 %8130, %8119
  %8132 = lshr i32 %8131, 4
  %8133 = trunc i32 %8132 to i8
  %8134 = and i8 %8133, 1
  store i8 %8134, i8* %27, align 1
  %8135 = icmp eq i32 %8119, 0
  %8136 = zext i1 %8135 to i8
  store i8 %8136, i8* %30, align 1
  %8137 = lshr i32 %8119, 31
  %8138 = trunc i32 %8137 to i8
  store i8 %8138, i8* %33, align 1
  %8139 = lshr i32 %8112, 31
  %8140 = lshr i32 %8118, 31
  %8141 = xor i32 %8137, %8139
  %8142 = xor i32 %8137, %8140
  %8143 = add nuw nsw i32 %8141, %8142
  %8144 = icmp eq i32 %8143, 2
  %8145 = zext i1 %8144 to i8
  store i8 %8145, i8* %39, align 1
  %8146 = add i64 %8052, 40
  store i64 %8146, i64* %3, align 8
  store i32 %8119, i32* %8117, align 4
  %8147 = load i64, i64* %RBP.i, align 8
  %8148 = add i64 %8147, -8
  %8149 = load i64, i64* %3, align 8
  %8150 = add i64 %8149, 4
  store i64 %8150, i64* %3, align 8
  %8151 = inttoptr i64 %8148 to i64*
  %8152 = load i64, i64* %8151, align 8
  %8153 = add i64 %8152, 51640
  store i64 %8153, i64* %RAX.i11582.pre-phi, align 8
  %8154 = icmp ugt i64 %8152, -51641
  %8155 = zext i1 %8154 to i8
  store i8 %8155, i8* %14, align 1
  %8156 = trunc i64 %8153 to i32
  %8157 = and i32 %8156, 255
  %8158 = tail call i32 @llvm.ctpop.i32(i32 %8157)
  %8159 = trunc i32 %8158 to i8
  %8160 = and i8 %8159, 1
  %8161 = xor i8 %8160, 1
  store i8 %8161, i8* %21, align 1
  %8162 = xor i64 %8152, 16
  %8163 = xor i64 %8162, %8153
  %8164 = lshr i64 %8163, 4
  %8165 = trunc i64 %8164 to i8
  %8166 = and i8 %8165, 1
  store i8 %8166, i8* %27, align 1
  %8167 = icmp eq i64 %8153, 0
  %8168 = zext i1 %8167 to i8
  store i8 %8168, i8* %30, align 1
  %8169 = lshr i64 %8153, 63
  %8170 = trunc i64 %8169 to i8
  store i8 %8170, i8* %33, align 1
  %8171 = lshr i64 %8152, 63
  %8172 = xor i64 %8169, %8171
  %8173 = add nuw nsw i64 %8172, %8169
  %8174 = icmp eq i64 %8173, 2
  %8175 = zext i1 %8174 to i8
  store i8 %8175, i8* %39, align 1
  %8176 = add i64 %8147, -150
  %8177 = add i64 %8149, 17
  store i64 %8177, i64* %3, align 8
  %8178 = inttoptr i64 %8176 to i16*
  %8179 = load i16, i16* %8178, align 2
  %8180 = zext i16 %8179 to i64
  store i64 %8180, i64* %RCX.i11580, align 8
  %8181 = zext i16 %8179 to i64
  %8182 = shl nuw nsw i64 %8181, 4
  store i64 %8182, i64* %573, align 8
  %8183 = add i64 %8182, %8153
  store i64 %8183, i64* %RAX.i11582.pre-phi, align 8
  %8184 = icmp ult i64 %8183, %8153
  %8185 = icmp ult i64 %8183, %8182
  %8186 = or i1 %8184, %8185
  %8187 = zext i1 %8186 to i8
  store i8 %8187, i8* %14, align 1
  %8188 = trunc i64 %8183 to i32
  %8189 = and i32 %8188, 255
  %8190 = tail call i32 @llvm.ctpop.i32(i32 %8189)
  %8191 = trunc i32 %8190 to i8
  %8192 = and i8 %8191, 1
  %8193 = xor i8 %8192, 1
  store i8 %8193, i8* %21, align 1
  %8194 = xor i64 %8182, %8153
  %8195 = xor i64 %8194, %8183
  %8196 = lshr i64 %8195, 4
  %8197 = trunc i64 %8196 to i8
  %8198 = and i8 %8197, 1
  store i8 %8198, i8* %27, align 1
  %8199 = icmp eq i64 %8183, 0
  %8200 = zext i1 %8199 to i8
  store i8 %8200, i8* %30, align 1
  %8201 = lshr i64 %8183, 63
  %8202 = trunc i64 %8201 to i8
  store i8 %8202, i8* %33, align 1
  %8203 = xor i64 %8201, %8169
  %8204 = add nuw nsw i64 %8203, %8201
  %8205 = icmp eq i64 %8204, 2
  %8206 = zext i1 %8205 to i8
  store i8 %8206, i8* %39, align 1
  %8207 = add i64 %8183, 4
  %8208 = add i64 %8149, 29
  store i64 %8208, i64* %3, align 8
  %8209 = inttoptr i64 %8207 to i32*
  %8210 = load i32, i32* %8209, align 4
  %8211 = zext i32 %8210 to i64
  store i64 %8211, i64* %RCX.i11580, align 8
  %8212 = load i64, i64* %RBP.i, align 8
  %8213 = add i64 %8212, -144
  %8214 = add i64 %8149, 35
  store i64 %8214, i64* %3, align 8
  %8215 = inttoptr i64 %8213 to i32*
  %8216 = load i32, i32* %8215, align 4
  %8217 = add i32 %8216, %8210
  %8218 = zext i32 %8217 to i64
  store i64 %8218, i64* %RCX.i11580, align 8
  %8219 = icmp ult i32 %8217, %8210
  %8220 = icmp ult i32 %8217, %8216
  %8221 = or i1 %8219, %8220
  %8222 = zext i1 %8221 to i8
  store i8 %8222, i8* %14, align 1
  %8223 = and i32 %8217, 255
  %8224 = tail call i32 @llvm.ctpop.i32(i32 %8223)
  %8225 = trunc i32 %8224 to i8
  %8226 = and i8 %8225, 1
  %8227 = xor i8 %8226, 1
  store i8 %8227, i8* %21, align 1
  %8228 = xor i32 %8216, %8210
  %8229 = xor i32 %8228, %8217
  %8230 = lshr i32 %8229, 4
  %8231 = trunc i32 %8230 to i8
  %8232 = and i8 %8231, 1
  store i8 %8232, i8* %27, align 1
  %8233 = icmp eq i32 %8217, 0
  %8234 = zext i1 %8233 to i8
  store i8 %8234, i8* %30, align 1
  %8235 = lshr i32 %8217, 31
  %8236 = trunc i32 %8235 to i8
  store i8 %8236, i8* %33, align 1
  %8237 = lshr i32 %8210, 31
  %8238 = lshr i32 %8216, 31
  %8239 = xor i32 %8235, %8237
  %8240 = xor i32 %8235, %8238
  %8241 = add nuw nsw i32 %8239, %8240
  %8242 = icmp eq i32 %8241, 2
  %8243 = zext i1 %8242 to i8
  store i8 %8243, i8* %39, align 1
  %8244 = add i64 %8149, 41
  store i64 %8244, i64* %3, align 8
  store i32 %8217, i32* %8215, align 4
  %8245 = load i64, i64* %RBP.i, align 8
  %8246 = add i64 %8245, -8
  %8247 = load i64, i64* %3, align 8
  %8248 = add i64 %8247, 4
  store i64 %8248, i64* %3, align 8
  %8249 = inttoptr i64 %8246 to i64*
  %8250 = load i64, i64* %8249, align 8
  %8251 = add i64 %8250, 51640
  store i64 %8251, i64* %RAX.i11582.pre-phi, align 8
  %8252 = icmp ugt i64 %8250, -51641
  %8253 = zext i1 %8252 to i8
  store i8 %8253, i8* %14, align 1
  %8254 = trunc i64 %8251 to i32
  %8255 = and i32 %8254, 255
  %8256 = tail call i32 @llvm.ctpop.i32(i32 %8255)
  %8257 = trunc i32 %8256 to i8
  %8258 = and i8 %8257, 1
  %8259 = xor i8 %8258, 1
  store i8 %8259, i8* %21, align 1
  %8260 = xor i64 %8250, 16
  %8261 = xor i64 %8260, %8251
  %8262 = lshr i64 %8261, 4
  %8263 = trunc i64 %8262 to i8
  %8264 = and i8 %8263, 1
  store i8 %8264, i8* %27, align 1
  %8265 = icmp eq i64 %8251, 0
  %8266 = zext i1 %8265 to i8
  store i8 %8266, i8* %30, align 1
  %8267 = lshr i64 %8251, 63
  %8268 = trunc i64 %8267 to i8
  store i8 %8268, i8* %33, align 1
  %8269 = lshr i64 %8250, 63
  %8270 = xor i64 %8267, %8269
  %8271 = add nuw nsw i64 %8270, %8267
  %8272 = icmp eq i64 %8271, 2
  %8273 = zext i1 %8272 to i8
  store i8 %8273, i8* %39, align 1
  %8274 = add i64 %8245, -150
  %8275 = add i64 %8247, 17
  store i64 %8275, i64* %3, align 8
  %8276 = inttoptr i64 %8274 to i16*
  %8277 = load i16, i16* %8276, align 2
  %8278 = zext i16 %8277 to i64
  store i64 %8278, i64* %RCX.i11580, align 8
  %8279 = zext i16 %8277 to i64
  %8280 = shl nuw nsw i64 %8279, 4
  store i64 %8280, i64* %573, align 8
  %8281 = add i64 %8280, %8251
  store i64 %8281, i64* %RAX.i11582.pre-phi, align 8
  %8282 = icmp ult i64 %8281, %8251
  %8283 = icmp ult i64 %8281, %8280
  %8284 = or i1 %8282, %8283
  %8285 = zext i1 %8284 to i8
  store i8 %8285, i8* %14, align 1
  %8286 = trunc i64 %8281 to i32
  %8287 = and i32 %8286, 255
  %8288 = tail call i32 @llvm.ctpop.i32(i32 %8287)
  %8289 = trunc i32 %8288 to i8
  %8290 = and i8 %8289, 1
  %8291 = xor i8 %8290, 1
  store i8 %8291, i8* %21, align 1
  %8292 = xor i64 %8280, %8251
  %8293 = xor i64 %8292, %8281
  %8294 = lshr i64 %8293, 4
  %8295 = trunc i64 %8294 to i8
  %8296 = and i8 %8295, 1
  store i8 %8296, i8* %27, align 1
  %8297 = icmp eq i64 %8281, 0
  %8298 = zext i1 %8297 to i8
  store i8 %8298, i8* %30, align 1
  %8299 = lshr i64 %8281, 63
  %8300 = trunc i64 %8299 to i8
  store i8 %8300, i8* %33, align 1
  %8301 = xor i64 %8299, %8267
  %8302 = add nuw nsw i64 %8301, %8299
  %8303 = icmp eq i64 %8302, 2
  %8304 = zext i1 %8303 to i8
  store i8 %8304, i8* %39, align 1
  %8305 = add i64 %8281, 8
  %8306 = add i64 %8247, 29
  store i64 %8306, i64* %3, align 8
  %8307 = inttoptr i64 %8305 to i32*
  %8308 = load i32, i32* %8307, align 4
  %8309 = zext i32 %8308 to i64
  store i64 %8309, i64* %RCX.i11580, align 8
  %8310 = load i64, i64* %RBP.i, align 8
  %8311 = add i64 %8310, -148
  %8312 = add i64 %8247, 35
  store i64 %8312, i64* %3, align 8
  %8313 = inttoptr i64 %8311 to i32*
  %8314 = load i32, i32* %8313, align 4
  %8315 = add i32 %8314, %8308
  %8316 = zext i32 %8315 to i64
  store i64 %8316, i64* %RCX.i11580, align 8
  %8317 = icmp ult i32 %8315, %8308
  %8318 = icmp ult i32 %8315, %8314
  %8319 = or i1 %8317, %8318
  %8320 = zext i1 %8319 to i8
  store i8 %8320, i8* %14, align 1
  %8321 = and i32 %8315, 255
  %8322 = tail call i32 @llvm.ctpop.i32(i32 %8321)
  %8323 = trunc i32 %8322 to i8
  %8324 = and i8 %8323, 1
  %8325 = xor i8 %8324, 1
  store i8 %8325, i8* %21, align 1
  %8326 = xor i32 %8314, %8308
  %8327 = xor i32 %8326, %8315
  %8328 = lshr i32 %8327, 4
  %8329 = trunc i32 %8328 to i8
  %8330 = and i8 %8329, 1
  store i8 %8330, i8* %27, align 1
  %8331 = icmp eq i32 %8315, 0
  %8332 = zext i1 %8331 to i8
  store i8 %8332, i8* %30, align 1
  %8333 = lshr i32 %8315, 31
  %8334 = trunc i32 %8333 to i8
  store i8 %8334, i8* %33, align 1
  %8335 = lshr i32 %8308, 31
  %8336 = lshr i32 %8314, 31
  %8337 = xor i32 %8333, %8335
  %8338 = xor i32 %8333, %8336
  %8339 = add nuw nsw i32 %8337, %8338
  %8340 = icmp eq i32 %8339, 2
  %8341 = zext i1 %8340 to i8
  store i8 %8341, i8* %39, align 1
  %8342 = add i64 %8247, 41
  store i64 %8342, i64* %3, align 8
  store i32 %8315, i32* %8313, align 4
  %8343 = load i64, i64* %RBP.i, align 8
  %8344 = add i64 %8343, -120
  %8345 = load i64, i64* %3, align 8
  %8346 = add i64 %8345, 4
  store i64 %8346, i64* %3, align 8
  %8347 = inttoptr i64 %8344 to i64*
  %8348 = load i64, i64* %8347, align 8
  store i64 %8348, i64* %RAX.i11582.pre-phi, align 8
  %8349 = add i64 %8343, -28
  %8350 = add i64 %8345, 7
  store i64 %8350, i64* %3, align 8
  %8351 = inttoptr i64 %8349 to i32*
  %8352 = load i32, i32* %8351, align 4
  %8353 = add i32 %8352, 17
  %8354 = zext i32 %8353 to i64
  store i64 %8354, i64* %RCX.i11580, align 8
  %8355 = icmp ugt i32 %8352, -18
  %8356 = zext i1 %8355 to i8
  store i8 %8356, i8* %14, align 1
  %8357 = and i32 %8353, 255
  %8358 = tail call i32 @llvm.ctpop.i32(i32 %8357)
  %8359 = trunc i32 %8358 to i8
  %8360 = and i8 %8359, 1
  %8361 = xor i8 %8360, 1
  store i8 %8361, i8* %21, align 1
  %8362 = xor i32 %8352, 16
  %8363 = xor i32 %8362, %8353
  %8364 = lshr i32 %8363, 4
  %8365 = trunc i32 %8364 to i8
  %8366 = and i8 %8365, 1
  store i8 %8366, i8* %27, align 1
  %8367 = icmp eq i32 %8353, 0
  %8368 = zext i1 %8367 to i8
  store i8 %8368, i8* %30, align 1
  %8369 = lshr i32 %8353, 31
  %8370 = trunc i32 %8369 to i8
  store i8 %8370, i8* %33, align 1
  %8371 = lshr i32 %8352, 31
  %8372 = xor i32 %8369, %8371
  %8373 = add nuw nsw i32 %8372, %8369
  %8374 = icmp eq i32 %8373, 2
  %8375 = zext i1 %8374 to i8
  store i8 %8375, i8* %39, align 1
  %8376 = sext i32 %8353 to i64
  store i64 %8376, i64* %573, align 8
  %8377 = shl nsw i64 %8376, 1
  %8378 = add i64 %8348, %8377
  %8379 = add i64 %8345, 17
  store i64 %8379, i64* %3, align 8
  %8380 = inttoptr i64 %8378 to i16*
  %8381 = load i16, i16* %8380, align 2
  store i16 %8381, i16* %SI.i, align 2
  %8382 = add i64 %8343, -150
  %8383 = add i64 %8345, 24
  store i64 %8383, i64* %3, align 8
  %8384 = inttoptr i64 %8382 to i16*
  store i16 %8381, i16* %8384, align 2
  %8385 = load i64, i64* %RBP.i, align 8
  %8386 = add i64 %8385, -8
  %8387 = load i64, i64* %3, align 8
  %8388 = add i64 %8387, 4
  store i64 %8388, i64* %3, align 8
  %8389 = inttoptr i64 %8386 to i64*
  %8390 = load i64, i64* %8389, align 8
  %8391 = add i64 %8390, 51640
  store i64 %8391, i64* %RAX.i11582.pre-phi, align 8
  %8392 = icmp ugt i64 %8390, -51641
  %8393 = zext i1 %8392 to i8
  store i8 %8393, i8* %14, align 1
  %8394 = trunc i64 %8391 to i32
  %8395 = and i32 %8394, 255
  %8396 = tail call i32 @llvm.ctpop.i32(i32 %8395)
  %8397 = trunc i32 %8396 to i8
  %8398 = and i8 %8397, 1
  %8399 = xor i8 %8398, 1
  store i8 %8399, i8* %21, align 1
  %8400 = xor i64 %8390, 16
  %8401 = xor i64 %8400, %8391
  %8402 = lshr i64 %8401, 4
  %8403 = trunc i64 %8402 to i8
  %8404 = and i8 %8403, 1
  store i8 %8404, i8* %27, align 1
  %8405 = icmp eq i64 %8391, 0
  %8406 = zext i1 %8405 to i8
  store i8 %8406, i8* %30, align 1
  %8407 = lshr i64 %8391, 63
  %8408 = trunc i64 %8407 to i8
  store i8 %8408, i8* %33, align 1
  %8409 = lshr i64 %8390, 63
  %8410 = xor i64 %8407, %8409
  %8411 = add nuw nsw i64 %8410, %8407
  %8412 = icmp eq i64 %8411, 2
  %8413 = zext i1 %8412 to i8
  store i8 %8413, i8* %39, align 1
  %8414 = add i64 %8385, -150
  %8415 = add i64 %8387, 17
  store i64 %8415, i64* %3, align 8
  %8416 = inttoptr i64 %8414 to i16*
  %8417 = load i16, i16* %8416, align 2
  %8418 = zext i16 %8417 to i64
  store i64 %8418, i64* %RCX.i11580, align 8
  %8419 = zext i16 %8417 to i64
  %8420 = shl nuw nsw i64 %8419, 4
  store i64 %8420, i64* %573, align 8
  %8421 = add i64 %8420, %8391
  store i64 %8421, i64* %RAX.i11582.pre-phi, align 8
  %8422 = icmp ult i64 %8421, %8391
  %8423 = icmp ult i64 %8421, %8420
  %8424 = or i1 %8422, %8423
  %8425 = zext i1 %8424 to i8
  store i8 %8425, i8* %14, align 1
  %8426 = trunc i64 %8421 to i32
  %8427 = and i32 %8426, 255
  %8428 = tail call i32 @llvm.ctpop.i32(i32 %8427)
  %8429 = trunc i32 %8428 to i8
  %8430 = and i8 %8429, 1
  %8431 = xor i8 %8430, 1
  store i8 %8431, i8* %21, align 1
  %8432 = xor i64 %8420, %8391
  %8433 = xor i64 %8432, %8421
  %8434 = lshr i64 %8433, 4
  %8435 = trunc i64 %8434 to i8
  %8436 = and i8 %8435, 1
  store i8 %8436, i8* %27, align 1
  %8437 = icmp eq i64 %8421, 0
  %8438 = zext i1 %8437 to i8
  store i8 %8438, i8* %30, align 1
  %8439 = lshr i64 %8421, 63
  %8440 = trunc i64 %8439 to i8
  store i8 %8440, i8* %33, align 1
  %8441 = xor i64 %8439, %8407
  %8442 = add nuw nsw i64 %8441, %8439
  %8443 = icmp eq i64 %8442, 2
  %8444 = zext i1 %8443 to i8
  store i8 %8444, i8* %39, align 1
  %8445 = inttoptr i64 %8421 to i32*
  %8446 = add i64 %8387, 28
  store i64 %8446, i64* %3, align 8
  %8447 = load i32, i32* %8445, align 4
  %8448 = zext i32 %8447 to i64
  store i64 %8448, i64* %RCX.i11580, align 8
  %8449 = load i64, i64* %RBP.i, align 8
  %8450 = add i64 %8449, -140
  %8451 = add i64 %8387, 34
  store i64 %8451, i64* %3, align 8
  %8452 = inttoptr i64 %8450 to i32*
  %8453 = load i32, i32* %8452, align 4
  %8454 = add i32 %8453, %8447
  %8455 = zext i32 %8454 to i64
  store i64 %8455, i64* %RCX.i11580, align 8
  %8456 = icmp ult i32 %8454, %8447
  %8457 = icmp ult i32 %8454, %8453
  %8458 = or i1 %8456, %8457
  %8459 = zext i1 %8458 to i8
  store i8 %8459, i8* %14, align 1
  %8460 = and i32 %8454, 255
  %8461 = tail call i32 @llvm.ctpop.i32(i32 %8460)
  %8462 = trunc i32 %8461 to i8
  %8463 = and i8 %8462, 1
  %8464 = xor i8 %8463, 1
  store i8 %8464, i8* %21, align 1
  %8465 = xor i32 %8453, %8447
  %8466 = xor i32 %8465, %8454
  %8467 = lshr i32 %8466, 4
  %8468 = trunc i32 %8467 to i8
  %8469 = and i8 %8468, 1
  store i8 %8469, i8* %27, align 1
  %8470 = icmp eq i32 %8454, 0
  %8471 = zext i1 %8470 to i8
  store i8 %8471, i8* %30, align 1
  %8472 = lshr i32 %8454, 31
  %8473 = trunc i32 %8472 to i8
  store i8 %8473, i8* %33, align 1
  %8474 = lshr i32 %8447, 31
  %8475 = lshr i32 %8453, 31
  %8476 = xor i32 %8472, %8474
  %8477 = xor i32 %8472, %8475
  %8478 = add nuw nsw i32 %8476, %8477
  %8479 = icmp eq i32 %8478, 2
  %8480 = zext i1 %8479 to i8
  store i8 %8480, i8* %39, align 1
  %8481 = add i64 %8387, 40
  store i64 %8481, i64* %3, align 8
  store i32 %8454, i32* %8452, align 4
  %8482 = load i64, i64* %RBP.i, align 8
  %8483 = add i64 %8482, -8
  %8484 = load i64, i64* %3, align 8
  %8485 = add i64 %8484, 4
  store i64 %8485, i64* %3, align 8
  %8486 = inttoptr i64 %8483 to i64*
  %8487 = load i64, i64* %8486, align 8
  %8488 = add i64 %8487, 51640
  store i64 %8488, i64* %RAX.i11582.pre-phi, align 8
  %8489 = icmp ugt i64 %8487, -51641
  %8490 = zext i1 %8489 to i8
  store i8 %8490, i8* %14, align 1
  %8491 = trunc i64 %8488 to i32
  %8492 = and i32 %8491, 255
  %8493 = tail call i32 @llvm.ctpop.i32(i32 %8492)
  %8494 = trunc i32 %8493 to i8
  %8495 = and i8 %8494, 1
  %8496 = xor i8 %8495, 1
  store i8 %8496, i8* %21, align 1
  %8497 = xor i64 %8487, 16
  %8498 = xor i64 %8497, %8488
  %8499 = lshr i64 %8498, 4
  %8500 = trunc i64 %8499 to i8
  %8501 = and i8 %8500, 1
  store i8 %8501, i8* %27, align 1
  %8502 = icmp eq i64 %8488, 0
  %8503 = zext i1 %8502 to i8
  store i8 %8503, i8* %30, align 1
  %8504 = lshr i64 %8488, 63
  %8505 = trunc i64 %8504 to i8
  store i8 %8505, i8* %33, align 1
  %8506 = lshr i64 %8487, 63
  %8507 = xor i64 %8504, %8506
  %8508 = add nuw nsw i64 %8507, %8504
  %8509 = icmp eq i64 %8508, 2
  %8510 = zext i1 %8509 to i8
  store i8 %8510, i8* %39, align 1
  %8511 = add i64 %8482, -150
  %8512 = add i64 %8484, 17
  store i64 %8512, i64* %3, align 8
  %8513 = inttoptr i64 %8511 to i16*
  %8514 = load i16, i16* %8513, align 2
  %8515 = zext i16 %8514 to i64
  store i64 %8515, i64* %RCX.i11580, align 8
  %8516 = zext i16 %8514 to i64
  %8517 = shl nuw nsw i64 %8516, 4
  store i64 %8517, i64* %573, align 8
  %8518 = add i64 %8517, %8488
  store i64 %8518, i64* %RAX.i11582.pre-phi, align 8
  %8519 = icmp ult i64 %8518, %8488
  %8520 = icmp ult i64 %8518, %8517
  %8521 = or i1 %8519, %8520
  %8522 = zext i1 %8521 to i8
  store i8 %8522, i8* %14, align 1
  %8523 = trunc i64 %8518 to i32
  %8524 = and i32 %8523, 255
  %8525 = tail call i32 @llvm.ctpop.i32(i32 %8524)
  %8526 = trunc i32 %8525 to i8
  %8527 = and i8 %8526, 1
  %8528 = xor i8 %8527, 1
  store i8 %8528, i8* %21, align 1
  %8529 = xor i64 %8517, %8488
  %8530 = xor i64 %8529, %8518
  %8531 = lshr i64 %8530, 4
  %8532 = trunc i64 %8531 to i8
  %8533 = and i8 %8532, 1
  store i8 %8533, i8* %27, align 1
  %8534 = icmp eq i64 %8518, 0
  %8535 = zext i1 %8534 to i8
  store i8 %8535, i8* %30, align 1
  %8536 = lshr i64 %8518, 63
  %8537 = trunc i64 %8536 to i8
  store i8 %8537, i8* %33, align 1
  %8538 = xor i64 %8536, %8504
  %8539 = add nuw nsw i64 %8538, %8536
  %8540 = icmp eq i64 %8539, 2
  %8541 = zext i1 %8540 to i8
  store i8 %8541, i8* %39, align 1
  %8542 = add i64 %8518, 4
  %8543 = add i64 %8484, 29
  store i64 %8543, i64* %3, align 8
  %8544 = inttoptr i64 %8542 to i32*
  %8545 = load i32, i32* %8544, align 4
  %8546 = zext i32 %8545 to i64
  store i64 %8546, i64* %RCX.i11580, align 8
  %8547 = load i64, i64* %RBP.i, align 8
  %8548 = add i64 %8547, -144
  %8549 = add i64 %8484, 35
  store i64 %8549, i64* %3, align 8
  %8550 = inttoptr i64 %8548 to i32*
  %8551 = load i32, i32* %8550, align 4
  %8552 = add i32 %8551, %8545
  %8553 = zext i32 %8552 to i64
  store i64 %8553, i64* %RCX.i11580, align 8
  %8554 = icmp ult i32 %8552, %8545
  %8555 = icmp ult i32 %8552, %8551
  %8556 = or i1 %8554, %8555
  %8557 = zext i1 %8556 to i8
  store i8 %8557, i8* %14, align 1
  %8558 = and i32 %8552, 255
  %8559 = tail call i32 @llvm.ctpop.i32(i32 %8558)
  %8560 = trunc i32 %8559 to i8
  %8561 = and i8 %8560, 1
  %8562 = xor i8 %8561, 1
  store i8 %8562, i8* %21, align 1
  %8563 = xor i32 %8551, %8545
  %8564 = xor i32 %8563, %8552
  %8565 = lshr i32 %8564, 4
  %8566 = trunc i32 %8565 to i8
  %8567 = and i8 %8566, 1
  store i8 %8567, i8* %27, align 1
  %8568 = icmp eq i32 %8552, 0
  %8569 = zext i1 %8568 to i8
  store i8 %8569, i8* %30, align 1
  %8570 = lshr i32 %8552, 31
  %8571 = trunc i32 %8570 to i8
  store i8 %8571, i8* %33, align 1
  %8572 = lshr i32 %8545, 31
  %8573 = lshr i32 %8551, 31
  %8574 = xor i32 %8570, %8572
  %8575 = xor i32 %8570, %8573
  %8576 = add nuw nsw i32 %8574, %8575
  %8577 = icmp eq i32 %8576, 2
  %8578 = zext i1 %8577 to i8
  store i8 %8578, i8* %39, align 1
  %8579 = add i64 %8484, 41
  store i64 %8579, i64* %3, align 8
  store i32 %8552, i32* %8550, align 4
  %8580 = load i64, i64* %RBP.i, align 8
  %8581 = add i64 %8580, -8
  %8582 = load i64, i64* %3, align 8
  %8583 = add i64 %8582, 4
  store i64 %8583, i64* %3, align 8
  %8584 = inttoptr i64 %8581 to i64*
  %8585 = load i64, i64* %8584, align 8
  %8586 = add i64 %8585, 51640
  store i64 %8586, i64* %RAX.i11582.pre-phi, align 8
  %8587 = icmp ugt i64 %8585, -51641
  %8588 = zext i1 %8587 to i8
  store i8 %8588, i8* %14, align 1
  %8589 = trunc i64 %8586 to i32
  %8590 = and i32 %8589, 255
  %8591 = tail call i32 @llvm.ctpop.i32(i32 %8590)
  %8592 = trunc i32 %8591 to i8
  %8593 = and i8 %8592, 1
  %8594 = xor i8 %8593, 1
  store i8 %8594, i8* %21, align 1
  %8595 = xor i64 %8585, 16
  %8596 = xor i64 %8595, %8586
  %8597 = lshr i64 %8596, 4
  %8598 = trunc i64 %8597 to i8
  %8599 = and i8 %8598, 1
  store i8 %8599, i8* %27, align 1
  %8600 = icmp eq i64 %8586, 0
  %8601 = zext i1 %8600 to i8
  store i8 %8601, i8* %30, align 1
  %8602 = lshr i64 %8586, 63
  %8603 = trunc i64 %8602 to i8
  store i8 %8603, i8* %33, align 1
  %8604 = lshr i64 %8585, 63
  %8605 = xor i64 %8602, %8604
  %8606 = add nuw nsw i64 %8605, %8602
  %8607 = icmp eq i64 %8606, 2
  %8608 = zext i1 %8607 to i8
  store i8 %8608, i8* %39, align 1
  %8609 = add i64 %8580, -150
  %8610 = add i64 %8582, 17
  store i64 %8610, i64* %3, align 8
  %8611 = inttoptr i64 %8609 to i16*
  %8612 = load i16, i16* %8611, align 2
  %8613 = zext i16 %8612 to i64
  store i64 %8613, i64* %RCX.i11580, align 8
  %8614 = zext i16 %8612 to i64
  %8615 = shl nuw nsw i64 %8614, 4
  store i64 %8615, i64* %573, align 8
  %8616 = add i64 %8615, %8586
  store i64 %8616, i64* %RAX.i11582.pre-phi, align 8
  %8617 = icmp ult i64 %8616, %8586
  %8618 = icmp ult i64 %8616, %8615
  %8619 = or i1 %8617, %8618
  %8620 = zext i1 %8619 to i8
  store i8 %8620, i8* %14, align 1
  %8621 = trunc i64 %8616 to i32
  %8622 = and i32 %8621, 255
  %8623 = tail call i32 @llvm.ctpop.i32(i32 %8622)
  %8624 = trunc i32 %8623 to i8
  %8625 = and i8 %8624, 1
  %8626 = xor i8 %8625, 1
  store i8 %8626, i8* %21, align 1
  %8627 = xor i64 %8615, %8586
  %8628 = xor i64 %8627, %8616
  %8629 = lshr i64 %8628, 4
  %8630 = trunc i64 %8629 to i8
  %8631 = and i8 %8630, 1
  store i8 %8631, i8* %27, align 1
  %8632 = icmp eq i64 %8616, 0
  %8633 = zext i1 %8632 to i8
  store i8 %8633, i8* %30, align 1
  %8634 = lshr i64 %8616, 63
  %8635 = trunc i64 %8634 to i8
  store i8 %8635, i8* %33, align 1
  %8636 = xor i64 %8634, %8602
  %8637 = add nuw nsw i64 %8636, %8634
  %8638 = icmp eq i64 %8637, 2
  %8639 = zext i1 %8638 to i8
  store i8 %8639, i8* %39, align 1
  %8640 = add i64 %8616, 8
  %8641 = add i64 %8582, 29
  store i64 %8641, i64* %3, align 8
  %8642 = inttoptr i64 %8640 to i32*
  %8643 = load i32, i32* %8642, align 4
  %8644 = zext i32 %8643 to i64
  store i64 %8644, i64* %RCX.i11580, align 8
  %8645 = load i64, i64* %RBP.i, align 8
  %8646 = add i64 %8645, -148
  %8647 = add i64 %8582, 35
  store i64 %8647, i64* %3, align 8
  %8648 = inttoptr i64 %8646 to i32*
  %8649 = load i32, i32* %8648, align 4
  %8650 = add i32 %8649, %8643
  %8651 = zext i32 %8650 to i64
  store i64 %8651, i64* %RCX.i11580, align 8
  %8652 = icmp ult i32 %8650, %8643
  %8653 = icmp ult i32 %8650, %8649
  %8654 = or i1 %8652, %8653
  %8655 = zext i1 %8654 to i8
  store i8 %8655, i8* %14, align 1
  %8656 = and i32 %8650, 255
  %8657 = tail call i32 @llvm.ctpop.i32(i32 %8656)
  %8658 = trunc i32 %8657 to i8
  %8659 = and i8 %8658, 1
  %8660 = xor i8 %8659, 1
  store i8 %8660, i8* %21, align 1
  %8661 = xor i32 %8649, %8643
  %8662 = xor i32 %8661, %8650
  %8663 = lshr i32 %8662, 4
  %8664 = trunc i32 %8663 to i8
  %8665 = and i8 %8664, 1
  store i8 %8665, i8* %27, align 1
  %8666 = icmp eq i32 %8650, 0
  %8667 = zext i1 %8666 to i8
  store i8 %8667, i8* %30, align 1
  %8668 = lshr i32 %8650, 31
  %8669 = trunc i32 %8668 to i8
  store i8 %8669, i8* %33, align 1
  %8670 = lshr i32 %8643, 31
  %8671 = lshr i32 %8649, 31
  %8672 = xor i32 %8668, %8670
  %8673 = xor i32 %8668, %8671
  %8674 = add nuw nsw i32 %8672, %8673
  %8675 = icmp eq i32 %8674, 2
  %8676 = zext i1 %8675 to i8
  store i8 %8676, i8* %39, align 1
  %8677 = add i64 %8582, 41
  store i64 %8677, i64* %3, align 8
  store i32 %8650, i32* %8648, align 4
  %8678 = load i64, i64* %RBP.i, align 8
  %8679 = add i64 %8678, -120
  %8680 = load i64, i64* %3, align 8
  %8681 = add i64 %8680, 4
  store i64 %8681, i64* %3, align 8
  %8682 = inttoptr i64 %8679 to i64*
  %8683 = load i64, i64* %8682, align 8
  store i64 %8683, i64* %RAX.i11582.pre-phi, align 8
  %8684 = add i64 %8678, -28
  %8685 = add i64 %8680, 7
  store i64 %8685, i64* %3, align 8
  %8686 = inttoptr i64 %8684 to i32*
  %8687 = load i32, i32* %8686, align 4
  %8688 = add i32 %8687, 18
  %8689 = zext i32 %8688 to i64
  store i64 %8689, i64* %RCX.i11580, align 8
  %8690 = icmp ugt i32 %8687, -19
  %8691 = zext i1 %8690 to i8
  store i8 %8691, i8* %14, align 1
  %8692 = and i32 %8688, 255
  %8693 = tail call i32 @llvm.ctpop.i32(i32 %8692)
  %8694 = trunc i32 %8693 to i8
  %8695 = and i8 %8694, 1
  %8696 = xor i8 %8695, 1
  store i8 %8696, i8* %21, align 1
  %8697 = xor i32 %8687, 16
  %8698 = xor i32 %8697, %8688
  %8699 = lshr i32 %8698, 4
  %8700 = trunc i32 %8699 to i8
  %8701 = and i8 %8700, 1
  store i8 %8701, i8* %27, align 1
  %8702 = icmp eq i32 %8688, 0
  %8703 = zext i1 %8702 to i8
  store i8 %8703, i8* %30, align 1
  %8704 = lshr i32 %8688, 31
  %8705 = trunc i32 %8704 to i8
  store i8 %8705, i8* %33, align 1
  %8706 = lshr i32 %8687, 31
  %8707 = xor i32 %8704, %8706
  %8708 = add nuw nsw i32 %8707, %8704
  %8709 = icmp eq i32 %8708, 2
  %8710 = zext i1 %8709 to i8
  store i8 %8710, i8* %39, align 1
  %8711 = sext i32 %8688 to i64
  store i64 %8711, i64* %573, align 8
  %8712 = shl nsw i64 %8711, 1
  %8713 = add i64 %8683, %8712
  %8714 = add i64 %8680, 17
  store i64 %8714, i64* %3, align 8
  %8715 = inttoptr i64 %8713 to i16*
  %8716 = load i16, i16* %8715, align 2
  store i16 %8716, i16* %SI.i, align 2
  %8717 = add i64 %8678, -150
  %8718 = add i64 %8680, 24
  store i64 %8718, i64* %3, align 8
  %8719 = inttoptr i64 %8717 to i16*
  store i16 %8716, i16* %8719, align 2
  %8720 = load i64, i64* %RBP.i, align 8
  %8721 = add i64 %8720, -8
  %8722 = load i64, i64* %3, align 8
  %8723 = add i64 %8722, 4
  store i64 %8723, i64* %3, align 8
  %8724 = inttoptr i64 %8721 to i64*
  %8725 = load i64, i64* %8724, align 8
  %8726 = add i64 %8725, 51640
  store i64 %8726, i64* %RAX.i11582.pre-phi, align 8
  %8727 = icmp ugt i64 %8725, -51641
  %8728 = zext i1 %8727 to i8
  store i8 %8728, i8* %14, align 1
  %8729 = trunc i64 %8726 to i32
  %8730 = and i32 %8729, 255
  %8731 = tail call i32 @llvm.ctpop.i32(i32 %8730)
  %8732 = trunc i32 %8731 to i8
  %8733 = and i8 %8732, 1
  %8734 = xor i8 %8733, 1
  store i8 %8734, i8* %21, align 1
  %8735 = xor i64 %8725, 16
  %8736 = xor i64 %8735, %8726
  %8737 = lshr i64 %8736, 4
  %8738 = trunc i64 %8737 to i8
  %8739 = and i8 %8738, 1
  store i8 %8739, i8* %27, align 1
  %8740 = icmp eq i64 %8726, 0
  %8741 = zext i1 %8740 to i8
  store i8 %8741, i8* %30, align 1
  %8742 = lshr i64 %8726, 63
  %8743 = trunc i64 %8742 to i8
  store i8 %8743, i8* %33, align 1
  %8744 = lshr i64 %8725, 63
  %8745 = xor i64 %8742, %8744
  %8746 = add nuw nsw i64 %8745, %8742
  %8747 = icmp eq i64 %8746, 2
  %8748 = zext i1 %8747 to i8
  store i8 %8748, i8* %39, align 1
  %8749 = add i64 %8720, -150
  %8750 = add i64 %8722, 17
  store i64 %8750, i64* %3, align 8
  %8751 = inttoptr i64 %8749 to i16*
  %8752 = load i16, i16* %8751, align 2
  %8753 = zext i16 %8752 to i64
  store i64 %8753, i64* %RCX.i11580, align 8
  %8754 = zext i16 %8752 to i64
  %8755 = shl nuw nsw i64 %8754, 4
  store i64 %8755, i64* %573, align 8
  %8756 = add i64 %8755, %8726
  store i64 %8756, i64* %RAX.i11582.pre-phi, align 8
  %8757 = icmp ult i64 %8756, %8726
  %8758 = icmp ult i64 %8756, %8755
  %8759 = or i1 %8757, %8758
  %8760 = zext i1 %8759 to i8
  store i8 %8760, i8* %14, align 1
  %8761 = trunc i64 %8756 to i32
  %8762 = and i32 %8761, 255
  %8763 = tail call i32 @llvm.ctpop.i32(i32 %8762)
  %8764 = trunc i32 %8763 to i8
  %8765 = and i8 %8764, 1
  %8766 = xor i8 %8765, 1
  store i8 %8766, i8* %21, align 1
  %8767 = xor i64 %8755, %8726
  %8768 = xor i64 %8767, %8756
  %8769 = lshr i64 %8768, 4
  %8770 = trunc i64 %8769 to i8
  %8771 = and i8 %8770, 1
  store i8 %8771, i8* %27, align 1
  %8772 = icmp eq i64 %8756, 0
  %8773 = zext i1 %8772 to i8
  store i8 %8773, i8* %30, align 1
  %8774 = lshr i64 %8756, 63
  %8775 = trunc i64 %8774 to i8
  store i8 %8775, i8* %33, align 1
  %8776 = xor i64 %8774, %8742
  %8777 = add nuw nsw i64 %8776, %8774
  %8778 = icmp eq i64 %8777, 2
  %8779 = zext i1 %8778 to i8
  store i8 %8779, i8* %39, align 1
  %8780 = inttoptr i64 %8756 to i32*
  %8781 = add i64 %8722, 28
  store i64 %8781, i64* %3, align 8
  %8782 = load i32, i32* %8780, align 4
  %8783 = zext i32 %8782 to i64
  store i64 %8783, i64* %RCX.i11580, align 8
  %8784 = load i64, i64* %RBP.i, align 8
  %8785 = add i64 %8784, -140
  %8786 = add i64 %8722, 34
  store i64 %8786, i64* %3, align 8
  %8787 = inttoptr i64 %8785 to i32*
  %8788 = load i32, i32* %8787, align 4
  %8789 = add i32 %8788, %8782
  %8790 = zext i32 %8789 to i64
  store i64 %8790, i64* %RCX.i11580, align 8
  %8791 = icmp ult i32 %8789, %8782
  %8792 = icmp ult i32 %8789, %8788
  %8793 = or i1 %8791, %8792
  %8794 = zext i1 %8793 to i8
  store i8 %8794, i8* %14, align 1
  %8795 = and i32 %8789, 255
  %8796 = tail call i32 @llvm.ctpop.i32(i32 %8795)
  %8797 = trunc i32 %8796 to i8
  %8798 = and i8 %8797, 1
  %8799 = xor i8 %8798, 1
  store i8 %8799, i8* %21, align 1
  %8800 = xor i32 %8788, %8782
  %8801 = xor i32 %8800, %8789
  %8802 = lshr i32 %8801, 4
  %8803 = trunc i32 %8802 to i8
  %8804 = and i8 %8803, 1
  store i8 %8804, i8* %27, align 1
  %8805 = icmp eq i32 %8789, 0
  %8806 = zext i1 %8805 to i8
  store i8 %8806, i8* %30, align 1
  %8807 = lshr i32 %8789, 31
  %8808 = trunc i32 %8807 to i8
  store i8 %8808, i8* %33, align 1
  %8809 = lshr i32 %8782, 31
  %8810 = lshr i32 %8788, 31
  %8811 = xor i32 %8807, %8809
  %8812 = xor i32 %8807, %8810
  %8813 = add nuw nsw i32 %8811, %8812
  %8814 = icmp eq i32 %8813, 2
  %8815 = zext i1 %8814 to i8
  store i8 %8815, i8* %39, align 1
  %8816 = add i64 %8722, 40
  store i64 %8816, i64* %3, align 8
  store i32 %8789, i32* %8787, align 4
  %8817 = load i64, i64* %RBP.i, align 8
  %8818 = add i64 %8817, -8
  %8819 = load i64, i64* %3, align 8
  %8820 = add i64 %8819, 4
  store i64 %8820, i64* %3, align 8
  %8821 = inttoptr i64 %8818 to i64*
  %8822 = load i64, i64* %8821, align 8
  %8823 = add i64 %8822, 51640
  store i64 %8823, i64* %RAX.i11582.pre-phi, align 8
  %8824 = icmp ugt i64 %8822, -51641
  %8825 = zext i1 %8824 to i8
  store i8 %8825, i8* %14, align 1
  %8826 = trunc i64 %8823 to i32
  %8827 = and i32 %8826, 255
  %8828 = tail call i32 @llvm.ctpop.i32(i32 %8827)
  %8829 = trunc i32 %8828 to i8
  %8830 = and i8 %8829, 1
  %8831 = xor i8 %8830, 1
  store i8 %8831, i8* %21, align 1
  %8832 = xor i64 %8822, 16
  %8833 = xor i64 %8832, %8823
  %8834 = lshr i64 %8833, 4
  %8835 = trunc i64 %8834 to i8
  %8836 = and i8 %8835, 1
  store i8 %8836, i8* %27, align 1
  %8837 = icmp eq i64 %8823, 0
  %8838 = zext i1 %8837 to i8
  store i8 %8838, i8* %30, align 1
  %8839 = lshr i64 %8823, 63
  %8840 = trunc i64 %8839 to i8
  store i8 %8840, i8* %33, align 1
  %8841 = lshr i64 %8822, 63
  %8842 = xor i64 %8839, %8841
  %8843 = add nuw nsw i64 %8842, %8839
  %8844 = icmp eq i64 %8843, 2
  %8845 = zext i1 %8844 to i8
  store i8 %8845, i8* %39, align 1
  %8846 = add i64 %8817, -150
  %8847 = add i64 %8819, 17
  store i64 %8847, i64* %3, align 8
  %8848 = inttoptr i64 %8846 to i16*
  %8849 = load i16, i16* %8848, align 2
  %8850 = zext i16 %8849 to i64
  store i64 %8850, i64* %RCX.i11580, align 8
  %8851 = zext i16 %8849 to i64
  %8852 = shl nuw nsw i64 %8851, 4
  store i64 %8852, i64* %573, align 8
  %8853 = add i64 %8852, %8823
  store i64 %8853, i64* %RAX.i11582.pre-phi, align 8
  %8854 = icmp ult i64 %8853, %8823
  %8855 = icmp ult i64 %8853, %8852
  %8856 = or i1 %8854, %8855
  %8857 = zext i1 %8856 to i8
  store i8 %8857, i8* %14, align 1
  %8858 = trunc i64 %8853 to i32
  %8859 = and i32 %8858, 255
  %8860 = tail call i32 @llvm.ctpop.i32(i32 %8859)
  %8861 = trunc i32 %8860 to i8
  %8862 = and i8 %8861, 1
  %8863 = xor i8 %8862, 1
  store i8 %8863, i8* %21, align 1
  %8864 = xor i64 %8852, %8823
  %8865 = xor i64 %8864, %8853
  %8866 = lshr i64 %8865, 4
  %8867 = trunc i64 %8866 to i8
  %8868 = and i8 %8867, 1
  store i8 %8868, i8* %27, align 1
  %8869 = icmp eq i64 %8853, 0
  %8870 = zext i1 %8869 to i8
  store i8 %8870, i8* %30, align 1
  %8871 = lshr i64 %8853, 63
  %8872 = trunc i64 %8871 to i8
  store i8 %8872, i8* %33, align 1
  %8873 = xor i64 %8871, %8839
  %8874 = add nuw nsw i64 %8873, %8871
  %8875 = icmp eq i64 %8874, 2
  %8876 = zext i1 %8875 to i8
  store i8 %8876, i8* %39, align 1
  %8877 = add i64 %8853, 4
  %8878 = add i64 %8819, 29
  store i64 %8878, i64* %3, align 8
  %8879 = inttoptr i64 %8877 to i32*
  %8880 = load i32, i32* %8879, align 4
  %8881 = zext i32 %8880 to i64
  store i64 %8881, i64* %RCX.i11580, align 8
  %8882 = load i64, i64* %RBP.i, align 8
  %8883 = add i64 %8882, -144
  %8884 = add i64 %8819, 35
  store i64 %8884, i64* %3, align 8
  %8885 = inttoptr i64 %8883 to i32*
  %8886 = load i32, i32* %8885, align 4
  %8887 = add i32 %8886, %8880
  %8888 = zext i32 %8887 to i64
  store i64 %8888, i64* %RCX.i11580, align 8
  %8889 = icmp ult i32 %8887, %8880
  %8890 = icmp ult i32 %8887, %8886
  %8891 = or i1 %8889, %8890
  %8892 = zext i1 %8891 to i8
  store i8 %8892, i8* %14, align 1
  %8893 = and i32 %8887, 255
  %8894 = tail call i32 @llvm.ctpop.i32(i32 %8893)
  %8895 = trunc i32 %8894 to i8
  %8896 = and i8 %8895, 1
  %8897 = xor i8 %8896, 1
  store i8 %8897, i8* %21, align 1
  %8898 = xor i32 %8886, %8880
  %8899 = xor i32 %8898, %8887
  %8900 = lshr i32 %8899, 4
  %8901 = trunc i32 %8900 to i8
  %8902 = and i8 %8901, 1
  store i8 %8902, i8* %27, align 1
  %8903 = icmp eq i32 %8887, 0
  %8904 = zext i1 %8903 to i8
  store i8 %8904, i8* %30, align 1
  %8905 = lshr i32 %8887, 31
  %8906 = trunc i32 %8905 to i8
  store i8 %8906, i8* %33, align 1
  %8907 = lshr i32 %8880, 31
  %8908 = lshr i32 %8886, 31
  %8909 = xor i32 %8905, %8907
  %8910 = xor i32 %8905, %8908
  %8911 = add nuw nsw i32 %8909, %8910
  %8912 = icmp eq i32 %8911, 2
  %8913 = zext i1 %8912 to i8
  store i8 %8913, i8* %39, align 1
  %8914 = add i64 %8819, 41
  store i64 %8914, i64* %3, align 8
  store i32 %8887, i32* %8885, align 4
  %8915 = load i64, i64* %RBP.i, align 8
  %8916 = add i64 %8915, -8
  %8917 = load i64, i64* %3, align 8
  %8918 = add i64 %8917, 4
  store i64 %8918, i64* %3, align 8
  %8919 = inttoptr i64 %8916 to i64*
  %8920 = load i64, i64* %8919, align 8
  %8921 = add i64 %8920, 51640
  store i64 %8921, i64* %RAX.i11582.pre-phi, align 8
  %8922 = icmp ugt i64 %8920, -51641
  %8923 = zext i1 %8922 to i8
  store i8 %8923, i8* %14, align 1
  %8924 = trunc i64 %8921 to i32
  %8925 = and i32 %8924, 255
  %8926 = tail call i32 @llvm.ctpop.i32(i32 %8925)
  %8927 = trunc i32 %8926 to i8
  %8928 = and i8 %8927, 1
  %8929 = xor i8 %8928, 1
  store i8 %8929, i8* %21, align 1
  %8930 = xor i64 %8920, 16
  %8931 = xor i64 %8930, %8921
  %8932 = lshr i64 %8931, 4
  %8933 = trunc i64 %8932 to i8
  %8934 = and i8 %8933, 1
  store i8 %8934, i8* %27, align 1
  %8935 = icmp eq i64 %8921, 0
  %8936 = zext i1 %8935 to i8
  store i8 %8936, i8* %30, align 1
  %8937 = lshr i64 %8921, 63
  %8938 = trunc i64 %8937 to i8
  store i8 %8938, i8* %33, align 1
  %8939 = lshr i64 %8920, 63
  %8940 = xor i64 %8937, %8939
  %8941 = add nuw nsw i64 %8940, %8937
  %8942 = icmp eq i64 %8941, 2
  %8943 = zext i1 %8942 to i8
  store i8 %8943, i8* %39, align 1
  %8944 = add i64 %8915, -150
  %8945 = add i64 %8917, 17
  store i64 %8945, i64* %3, align 8
  %8946 = inttoptr i64 %8944 to i16*
  %8947 = load i16, i16* %8946, align 2
  %8948 = zext i16 %8947 to i64
  store i64 %8948, i64* %RCX.i11580, align 8
  %8949 = zext i16 %8947 to i64
  %8950 = shl nuw nsw i64 %8949, 4
  store i64 %8950, i64* %573, align 8
  %8951 = add i64 %8950, %8921
  store i64 %8951, i64* %RAX.i11582.pre-phi, align 8
  %8952 = icmp ult i64 %8951, %8921
  %8953 = icmp ult i64 %8951, %8950
  %8954 = or i1 %8952, %8953
  %8955 = zext i1 %8954 to i8
  store i8 %8955, i8* %14, align 1
  %8956 = trunc i64 %8951 to i32
  %8957 = and i32 %8956, 255
  %8958 = tail call i32 @llvm.ctpop.i32(i32 %8957)
  %8959 = trunc i32 %8958 to i8
  %8960 = and i8 %8959, 1
  %8961 = xor i8 %8960, 1
  store i8 %8961, i8* %21, align 1
  %8962 = xor i64 %8950, %8921
  %8963 = xor i64 %8962, %8951
  %8964 = lshr i64 %8963, 4
  %8965 = trunc i64 %8964 to i8
  %8966 = and i8 %8965, 1
  store i8 %8966, i8* %27, align 1
  %8967 = icmp eq i64 %8951, 0
  %8968 = zext i1 %8967 to i8
  store i8 %8968, i8* %30, align 1
  %8969 = lshr i64 %8951, 63
  %8970 = trunc i64 %8969 to i8
  store i8 %8970, i8* %33, align 1
  %8971 = xor i64 %8969, %8937
  %8972 = add nuw nsw i64 %8971, %8969
  %8973 = icmp eq i64 %8972, 2
  %8974 = zext i1 %8973 to i8
  store i8 %8974, i8* %39, align 1
  %8975 = add i64 %8951, 8
  %8976 = add i64 %8917, 29
  store i64 %8976, i64* %3, align 8
  %8977 = inttoptr i64 %8975 to i32*
  %8978 = load i32, i32* %8977, align 4
  %8979 = zext i32 %8978 to i64
  store i64 %8979, i64* %RCX.i11580, align 8
  %8980 = load i64, i64* %RBP.i, align 8
  %8981 = add i64 %8980, -148
  %8982 = add i64 %8917, 35
  store i64 %8982, i64* %3, align 8
  %8983 = inttoptr i64 %8981 to i32*
  %8984 = load i32, i32* %8983, align 4
  %8985 = add i32 %8984, %8978
  %8986 = zext i32 %8985 to i64
  store i64 %8986, i64* %RCX.i11580, align 8
  %8987 = icmp ult i32 %8985, %8978
  %8988 = icmp ult i32 %8985, %8984
  %8989 = or i1 %8987, %8988
  %8990 = zext i1 %8989 to i8
  store i8 %8990, i8* %14, align 1
  %8991 = and i32 %8985, 255
  %8992 = tail call i32 @llvm.ctpop.i32(i32 %8991)
  %8993 = trunc i32 %8992 to i8
  %8994 = and i8 %8993, 1
  %8995 = xor i8 %8994, 1
  store i8 %8995, i8* %21, align 1
  %8996 = xor i32 %8984, %8978
  %8997 = xor i32 %8996, %8985
  %8998 = lshr i32 %8997, 4
  %8999 = trunc i32 %8998 to i8
  %9000 = and i8 %8999, 1
  store i8 %9000, i8* %27, align 1
  %9001 = icmp eq i32 %8985, 0
  %9002 = zext i1 %9001 to i8
  store i8 %9002, i8* %30, align 1
  %9003 = lshr i32 %8985, 31
  %9004 = trunc i32 %9003 to i8
  store i8 %9004, i8* %33, align 1
  %9005 = lshr i32 %8978, 31
  %9006 = lshr i32 %8984, 31
  %9007 = xor i32 %9003, %9005
  %9008 = xor i32 %9003, %9006
  %9009 = add nuw nsw i32 %9007, %9008
  %9010 = icmp eq i32 %9009, 2
  %9011 = zext i1 %9010 to i8
  store i8 %9011, i8* %39, align 1
  %9012 = add i64 %8917, 41
  store i64 %9012, i64* %3, align 8
  store i32 %8985, i32* %8983, align 4
  %9013 = load i64, i64* %RBP.i, align 8
  %9014 = add i64 %9013, -120
  %9015 = load i64, i64* %3, align 8
  %9016 = add i64 %9015, 4
  store i64 %9016, i64* %3, align 8
  %9017 = inttoptr i64 %9014 to i64*
  %9018 = load i64, i64* %9017, align 8
  store i64 %9018, i64* %RAX.i11582.pre-phi, align 8
  %9019 = add i64 %9013, -28
  %9020 = add i64 %9015, 7
  store i64 %9020, i64* %3, align 8
  %9021 = inttoptr i64 %9019 to i32*
  %9022 = load i32, i32* %9021, align 4
  %9023 = add i32 %9022, 19
  %9024 = zext i32 %9023 to i64
  store i64 %9024, i64* %RCX.i11580, align 8
  %9025 = icmp ugt i32 %9022, -20
  %9026 = zext i1 %9025 to i8
  store i8 %9026, i8* %14, align 1
  %9027 = and i32 %9023, 255
  %9028 = tail call i32 @llvm.ctpop.i32(i32 %9027)
  %9029 = trunc i32 %9028 to i8
  %9030 = and i8 %9029, 1
  %9031 = xor i8 %9030, 1
  store i8 %9031, i8* %21, align 1
  %9032 = xor i32 %9022, 16
  %9033 = xor i32 %9032, %9023
  %9034 = lshr i32 %9033, 4
  %9035 = trunc i32 %9034 to i8
  %9036 = and i8 %9035, 1
  store i8 %9036, i8* %27, align 1
  %9037 = icmp eq i32 %9023, 0
  %9038 = zext i1 %9037 to i8
  store i8 %9038, i8* %30, align 1
  %9039 = lshr i32 %9023, 31
  %9040 = trunc i32 %9039 to i8
  store i8 %9040, i8* %33, align 1
  %9041 = lshr i32 %9022, 31
  %9042 = xor i32 %9039, %9041
  %9043 = add nuw nsw i32 %9042, %9039
  %9044 = icmp eq i32 %9043, 2
  %9045 = zext i1 %9044 to i8
  store i8 %9045, i8* %39, align 1
  %9046 = sext i32 %9023 to i64
  store i64 %9046, i64* %573, align 8
  %9047 = shl nsw i64 %9046, 1
  %9048 = add i64 %9018, %9047
  %9049 = add i64 %9015, 17
  store i64 %9049, i64* %3, align 8
  %9050 = inttoptr i64 %9048 to i16*
  %9051 = load i16, i16* %9050, align 2
  store i16 %9051, i16* %SI.i, align 2
  %9052 = add i64 %9013, -150
  %9053 = add i64 %9015, 24
  store i64 %9053, i64* %3, align 8
  %9054 = inttoptr i64 %9052 to i16*
  store i16 %9051, i16* %9054, align 2
  %9055 = load i64, i64* %RBP.i, align 8
  %9056 = add i64 %9055, -8
  %9057 = load i64, i64* %3, align 8
  %9058 = add i64 %9057, 4
  store i64 %9058, i64* %3, align 8
  %9059 = inttoptr i64 %9056 to i64*
  %9060 = load i64, i64* %9059, align 8
  %9061 = add i64 %9060, 51640
  store i64 %9061, i64* %RAX.i11582.pre-phi, align 8
  %9062 = icmp ugt i64 %9060, -51641
  %9063 = zext i1 %9062 to i8
  store i8 %9063, i8* %14, align 1
  %9064 = trunc i64 %9061 to i32
  %9065 = and i32 %9064, 255
  %9066 = tail call i32 @llvm.ctpop.i32(i32 %9065)
  %9067 = trunc i32 %9066 to i8
  %9068 = and i8 %9067, 1
  %9069 = xor i8 %9068, 1
  store i8 %9069, i8* %21, align 1
  %9070 = xor i64 %9060, 16
  %9071 = xor i64 %9070, %9061
  %9072 = lshr i64 %9071, 4
  %9073 = trunc i64 %9072 to i8
  %9074 = and i8 %9073, 1
  store i8 %9074, i8* %27, align 1
  %9075 = icmp eq i64 %9061, 0
  %9076 = zext i1 %9075 to i8
  store i8 %9076, i8* %30, align 1
  %9077 = lshr i64 %9061, 63
  %9078 = trunc i64 %9077 to i8
  store i8 %9078, i8* %33, align 1
  %9079 = lshr i64 %9060, 63
  %9080 = xor i64 %9077, %9079
  %9081 = add nuw nsw i64 %9080, %9077
  %9082 = icmp eq i64 %9081, 2
  %9083 = zext i1 %9082 to i8
  store i8 %9083, i8* %39, align 1
  %9084 = add i64 %9055, -150
  %9085 = add i64 %9057, 17
  store i64 %9085, i64* %3, align 8
  %9086 = inttoptr i64 %9084 to i16*
  %9087 = load i16, i16* %9086, align 2
  %9088 = zext i16 %9087 to i64
  store i64 %9088, i64* %RCX.i11580, align 8
  %9089 = zext i16 %9087 to i64
  %9090 = shl nuw nsw i64 %9089, 4
  store i64 %9090, i64* %573, align 8
  %9091 = add i64 %9090, %9061
  store i64 %9091, i64* %RAX.i11582.pre-phi, align 8
  %9092 = icmp ult i64 %9091, %9061
  %9093 = icmp ult i64 %9091, %9090
  %9094 = or i1 %9092, %9093
  %9095 = zext i1 %9094 to i8
  store i8 %9095, i8* %14, align 1
  %9096 = trunc i64 %9091 to i32
  %9097 = and i32 %9096, 255
  %9098 = tail call i32 @llvm.ctpop.i32(i32 %9097)
  %9099 = trunc i32 %9098 to i8
  %9100 = and i8 %9099, 1
  %9101 = xor i8 %9100, 1
  store i8 %9101, i8* %21, align 1
  %9102 = xor i64 %9090, %9061
  %9103 = xor i64 %9102, %9091
  %9104 = lshr i64 %9103, 4
  %9105 = trunc i64 %9104 to i8
  %9106 = and i8 %9105, 1
  store i8 %9106, i8* %27, align 1
  %9107 = icmp eq i64 %9091, 0
  %9108 = zext i1 %9107 to i8
  store i8 %9108, i8* %30, align 1
  %9109 = lshr i64 %9091, 63
  %9110 = trunc i64 %9109 to i8
  store i8 %9110, i8* %33, align 1
  %9111 = xor i64 %9109, %9077
  %9112 = add nuw nsw i64 %9111, %9109
  %9113 = icmp eq i64 %9112, 2
  %9114 = zext i1 %9113 to i8
  store i8 %9114, i8* %39, align 1
  %9115 = inttoptr i64 %9091 to i32*
  %9116 = add i64 %9057, 28
  store i64 %9116, i64* %3, align 8
  %9117 = load i32, i32* %9115, align 4
  %9118 = zext i32 %9117 to i64
  store i64 %9118, i64* %RCX.i11580, align 8
  %9119 = load i64, i64* %RBP.i, align 8
  %9120 = add i64 %9119, -140
  %9121 = add i64 %9057, 34
  store i64 %9121, i64* %3, align 8
  %9122 = inttoptr i64 %9120 to i32*
  %9123 = load i32, i32* %9122, align 4
  %9124 = add i32 %9123, %9117
  %9125 = zext i32 %9124 to i64
  store i64 %9125, i64* %RCX.i11580, align 8
  %9126 = icmp ult i32 %9124, %9117
  %9127 = icmp ult i32 %9124, %9123
  %9128 = or i1 %9126, %9127
  %9129 = zext i1 %9128 to i8
  store i8 %9129, i8* %14, align 1
  %9130 = and i32 %9124, 255
  %9131 = tail call i32 @llvm.ctpop.i32(i32 %9130)
  %9132 = trunc i32 %9131 to i8
  %9133 = and i8 %9132, 1
  %9134 = xor i8 %9133, 1
  store i8 %9134, i8* %21, align 1
  %9135 = xor i32 %9123, %9117
  %9136 = xor i32 %9135, %9124
  %9137 = lshr i32 %9136, 4
  %9138 = trunc i32 %9137 to i8
  %9139 = and i8 %9138, 1
  store i8 %9139, i8* %27, align 1
  %9140 = icmp eq i32 %9124, 0
  %9141 = zext i1 %9140 to i8
  store i8 %9141, i8* %30, align 1
  %9142 = lshr i32 %9124, 31
  %9143 = trunc i32 %9142 to i8
  store i8 %9143, i8* %33, align 1
  %9144 = lshr i32 %9117, 31
  %9145 = lshr i32 %9123, 31
  %9146 = xor i32 %9142, %9144
  %9147 = xor i32 %9142, %9145
  %9148 = add nuw nsw i32 %9146, %9147
  %9149 = icmp eq i32 %9148, 2
  %9150 = zext i1 %9149 to i8
  store i8 %9150, i8* %39, align 1
  %9151 = add i64 %9057, 40
  store i64 %9151, i64* %3, align 8
  store i32 %9124, i32* %9122, align 4
  %9152 = load i64, i64* %RBP.i, align 8
  %9153 = add i64 %9152, -8
  %9154 = load i64, i64* %3, align 8
  %9155 = add i64 %9154, 4
  store i64 %9155, i64* %3, align 8
  %9156 = inttoptr i64 %9153 to i64*
  %9157 = load i64, i64* %9156, align 8
  %9158 = add i64 %9157, 51640
  store i64 %9158, i64* %RAX.i11582.pre-phi, align 8
  %9159 = icmp ugt i64 %9157, -51641
  %9160 = zext i1 %9159 to i8
  store i8 %9160, i8* %14, align 1
  %9161 = trunc i64 %9158 to i32
  %9162 = and i32 %9161, 255
  %9163 = tail call i32 @llvm.ctpop.i32(i32 %9162)
  %9164 = trunc i32 %9163 to i8
  %9165 = and i8 %9164, 1
  %9166 = xor i8 %9165, 1
  store i8 %9166, i8* %21, align 1
  %9167 = xor i64 %9157, 16
  %9168 = xor i64 %9167, %9158
  %9169 = lshr i64 %9168, 4
  %9170 = trunc i64 %9169 to i8
  %9171 = and i8 %9170, 1
  store i8 %9171, i8* %27, align 1
  %9172 = icmp eq i64 %9158, 0
  %9173 = zext i1 %9172 to i8
  store i8 %9173, i8* %30, align 1
  %9174 = lshr i64 %9158, 63
  %9175 = trunc i64 %9174 to i8
  store i8 %9175, i8* %33, align 1
  %9176 = lshr i64 %9157, 63
  %9177 = xor i64 %9174, %9176
  %9178 = add nuw nsw i64 %9177, %9174
  %9179 = icmp eq i64 %9178, 2
  %9180 = zext i1 %9179 to i8
  store i8 %9180, i8* %39, align 1
  %9181 = add i64 %9152, -150
  %9182 = add i64 %9154, 17
  store i64 %9182, i64* %3, align 8
  %9183 = inttoptr i64 %9181 to i16*
  %9184 = load i16, i16* %9183, align 2
  %9185 = zext i16 %9184 to i64
  store i64 %9185, i64* %RCX.i11580, align 8
  %9186 = zext i16 %9184 to i64
  %9187 = shl nuw nsw i64 %9186, 4
  store i64 %9187, i64* %573, align 8
  %9188 = add i64 %9187, %9158
  store i64 %9188, i64* %RAX.i11582.pre-phi, align 8
  %9189 = icmp ult i64 %9188, %9158
  %9190 = icmp ult i64 %9188, %9187
  %9191 = or i1 %9189, %9190
  %9192 = zext i1 %9191 to i8
  store i8 %9192, i8* %14, align 1
  %9193 = trunc i64 %9188 to i32
  %9194 = and i32 %9193, 255
  %9195 = tail call i32 @llvm.ctpop.i32(i32 %9194)
  %9196 = trunc i32 %9195 to i8
  %9197 = and i8 %9196, 1
  %9198 = xor i8 %9197, 1
  store i8 %9198, i8* %21, align 1
  %9199 = xor i64 %9187, %9158
  %9200 = xor i64 %9199, %9188
  %9201 = lshr i64 %9200, 4
  %9202 = trunc i64 %9201 to i8
  %9203 = and i8 %9202, 1
  store i8 %9203, i8* %27, align 1
  %9204 = icmp eq i64 %9188, 0
  %9205 = zext i1 %9204 to i8
  store i8 %9205, i8* %30, align 1
  %9206 = lshr i64 %9188, 63
  %9207 = trunc i64 %9206 to i8
  store i8 %9207, i8* %33, align 1
  %9208 = xor i64 %9206, %9174
  %9209 = add nuw nsw i64 %9208, %9206
  %9210 = icmp eq i64 %9209, 2
  %9211 = zext i1 %9210 to i8
  store i8 %9211, i8* %39, align 1
  %9212 = add i64 %9188, 4
  %9213 = add i64 %9154, 29
  store i64 %9213, i64* %3, align 8
  %9214 = inttoptr i64 %9212 to i32*
  %9215 = load i32, i32* %9214, align 4
  %9216 = zext i32 %9215 to i64
  store i64 %9216, i64* %RCX.i11580, align 8
  %9217 = load i64, i64* %RBP.i, align 8
  %9218 = add i64 %9217, -144
  %9219 = add i64 %9154, 35
  store i64 %9219, i64* %3, align 8
  %9220 = inttoptr i64 %9218 to i32*
  %9221 = load i32, i32* %9220, align 4
  %9222 = add i32 %9221, %9215
  %9223 = zext i32 %9222 to i64
  store i64 %9223, i64* %RCX.i11580, align 8
  %9224 = icmp ult i32 %9222, %9215
  %9225 = icmp ult i32 %9222, %9221
  %9226 = or i1 %9224, %9225
  %9227 = zext i1 %9226 to i8
  store i8 %9227, i8* %14, align 1
  %9228 = and i32 %9222, 255
  %9229 = tail call i32 @llvm.ctpop.i32(i32 %9228)
  %9230 = trunc i32 %9229 to i8
  %9231 = and i8 %9230, 1
  %9232 = xor i8 %9231, 1
  store i8 %9232, i8* %21, align 1
  %9233 = xor i32 %9221, %9215
  %9234 = xor i32 %9233, %9222
  %9235 = lshr i32 %9234, 4
  %9236 = trunc i32 %9235 to i8
  %9237 = and i8 %9236, 1
  store i8 %9237, i8* %27, align 1
  %9238 = icmp eq i32 %9222, 0
  %9239 = zext i1 %9238 to i8
  store i8 %9239, i8* %30, align 1
  %9240 = lshr i32 %9222, 31
  %9241 = trunc i32 %9240 to i8
  store i8 %9241, i8* %33, align 1
  %9242 = lshr i32 %9215, 31
  %9243 = lshr i32 %9221, 31
  %9244 = xor i32 %9240, %9242
  %9245 = xor i32 %9240, %9243
  %9246 = add nuw nsw i32 %9244, %9245
  %9247 = icmp eq i32 %9246, 2
  %9248 = zext i1 %9247 to i8
  store i8 %9248, i8* %39, align 1
  %9249 = add i64 %9154, 41
  store i64 %9249, i64* %3, align 8
  store i32 %9222, i32* %9220, align 4
  %9250 = load i64, i64* %RBP.i, align 8
  %9251 = add i64 %9250, -8
  %9252 = load i64, i64* %3, align 8
  %9253 = add i64 %9252, 4
  store i64 %9253, i64* %3, align 8
  %9254 = inttoptr i64 %9251 to i64*
  %9255 = load i64, i64* %9254, align 8
  %9256 = add i64 %9255, 51640
  store i64 %9256, i64* %RAX.i11582.pre-phi, align 8
  %9257 = icmp ugt i64 %9255, -51641
  %9258 = zext i1 %9257 to i8
  store i8 %9258, i8* %14, align 1
  %9259 = trunc i64 %9256 to i32
  %9260 = and i32 %9259, 255
  %9261 = tail call i32 @llvm.ctpop.i32(i32 %9260)
  %9262 = trunc i32 %9261 to i8
  %9263 = and i8 %9262, 1
  %9264 = xor i8 %9263, 1
  store i8 %9264, i8* %21, align 1
  %9265 = xor i64 %9255, 16
  %9266 = xor i64 %9265, %9256
  %9267 = lshr i64 %9266, 4
  %9268 = trunc i64 %9267 to i8
  %9269 = and i8 %9268, 1
  store i8 %9269, i8* %27, align 1
  %9270 = icmp eq i64 %9256, 0
  %9271 = zext i1 %9270 to i8
  store i8 %9271, i8* %30, align 1
  %9272 = lshr i64 %9256, 63
  %9273 = trunc i64 %9272 to i8
  store i8 %9273, i8* %33, align 1
  %9274 = lshr i64 %9255, 63
  %9275 = xor i64 %9272, %9274
  %9276 = add nuw nsw i64 %9275, %9272
  %9277 = icmp eq i64 %9276, 2
  %9278 = zext i1 %9277 to i8
  store i8 %9278, i8* %39, align 1
  %9279 = add i64 %9250, -150
  %9280 = add i64 %9252, 17
  store i64 %9280, i64* %3, align 8
  %9281 = inttoptr i64 %9279 to i16*
  %9282 = load i16, i16* %9281, align 2
  %9283 = zext i16 %9282 to i64
  store i64 %9283, i64* %RCX.i11580, align 8
  %9284 = zext i16 %9282 to i64
  %9285 = shl nuw nsw i64 %9284, 4
  store i64 %9285, i64* %573, align 8
  %9286 = add i64 %9285, %9256
  store i64 %9286, i64* %RAX.i11582.pre-phi, align 8
  %9287 = icmp ult i64 %9286, %9256
  %9288 = icmp ult i64 %9286, %9285
  %9289 = or i1 %9287, %9288
  %9290 = zext i1 %9289 to i8
  store i8 %9290, i8* %14, align 1
  %9291 = trunc i64 %9286 to i32
  %9292 = and i32 %9291, 255
  %9293 = tail call i32 @llvm.ctpop.i32(i32 %9292)
  %9294 = trunc i32 %9293 to i8
  %9295 = and i8 %9294, 1
  %9296 = xor i8 %9295, 1
  store i8 %9296, i8* %21, align 1
  %9297 = xor i64 %9285, %9256
  %9298 = xor i64 %9297, %9286
  %9299 = lshr i64 %9298, 4
  %9300 = trunc i64 %9299 to i8
  %9301 = and i8 %9300, 1
  store i8 %9301, i8* %27, align 1
  %9302 = icmp eq i64 %9286, 0
  %9303 = zext i1 %9302 to i8
  store i8 %9303, i8* %30, align 1
  %9304 = lshr i64 %9286, 63
  %9305 = trunc i64 %9304 to i8
  store i8 %9305, i8* %33, align 1
  %9306 = xor i64 %9304, %9272
  %9307 = add nuw nsw i64 %9306, %9304
  %9308 = icmp eq i64 %9307, 2
  %9309 = zext i1 %9308 to i8
  store i8 %9309, i8* %39, align 1
  %9310 = add i64 %9286, 8
  %9311 = add i64 %9252, 29
  store i64 %9311, i64* %3, align 8
  %9312 = inttoptr i64 %9310 to i32*
  %9313 = load i32, i32* %9312, align 4
  %9314 = zext i32 %9313 to i64
  store i64 %9314, i64* %RCX.i11580, align 8
  %9315 = load i64, i64* %RBP.i, align 8
  %9316 = add i64 %9315, -148
  %9317 = add i64 %9252, 35
  store i64 %9317, i64* %3, align 8
  %9318 = inttoptr i64 %9316 to i32*
  %9319 = load i32, i32* %9318, align 4
  %9320 = add i32 %9319, %9313
  %9321 = zext i32 %9320 to i64
  store i64 %9321, i64* %RCX.i11580, align 8
  %9322 = icmp ult i32 %9320, %9313
  %9323 = icmp ult i32 %9320, %9319
  %9324 = or i1 %9322, %9323
  %9325 = zext i1 %9324 to i8
  store i8 %9325, i8* %14, align 1
  %9326 = and i32 %9320, 255
  %9327 = tail call i32 @llvm.ctpop.i32(i32 %9326)
  %9328 = trunc i32 %9327 to i8
  %9329 = and i8 %9328, 1
  %9330 = xor i8 %9329, 1
  store i8 %9330, i8* %21, align 1
  %9331 = xor i32 %9319, %9313
  %9332 = xor i32 %9331, %9320
  %9333 = lshr i32 %9332, 4
  %9334 = trunc i32 %9333 to i8
  %9335 = and i8 %9334, 1
  store i8 %9335, i8* %27, align 1
  %9336 = icmp eq i32 %9320, 0
  %9337 = zext i1 %9336 to i8
  store i8 %9337, i8* %30, align 1
  %9338 = lshr i32 %9320, 31
  %9339 = trunc i32 %9338 to i8
  store i8 %9339, i8* %33, align 1
  %9340 = lshr i32 %9313, 31
  %9341 = lshr i32 %9319, 31
  %9342 = xor i32 %9338, %9340
  %9343 = xor i32 %9338, %9341
  %9344 = add nuw nsw i32 %9342, %9343
  %9345 = icmp eq i32 %9344, 2
  %9346 = zext i1 %9345 to i8
  store i8 %9346, i8* %39, align 1
  %9347 = add i64 %9252, 41
  store i64 %9347, i64* %3, align 8
  store i32 %9320, i32* %9318, align 4
  %9348 = load i64, i64* %RBP.i, align 8
  %9349 = add i64 %9348, -120
  %9350 = load i64, i64* %3, align 8
  %9351 = add i64 %9350, 4
  store i64 %9351, i64* %3, align 8
  %9352 = inttoptr i64 %9349 to i64*
  %9353 = load i64, i64* %9352, align 8
  store i64 %9353, i64* %RAX.i11582.pre-phi, align 8
  %9354 = add i64 %9348, -28
  %9355 = add i64 %9350, 7
  store i64 %9355, i64* %3, align 8
  %9356 = inttoptr i64 %9354 to i32*
  %9357 = load i32, i32* %9356, align 4
  %9358 = add i32 %9357, 20
  %9359 = zext i32 %9358 to i64
  store i64 %9359, i64* %RCX.i11580, align 8
  %9360 = icmp ugt i32 %9357, -21
  %9361 = zext i1 %9360 to i8
  store i8 %9361, i8* %14, align 1
  %9362 = and i32 %9358, 255
  %9363 = tail call i32 @llvm.ctpop.i32(i32 %9362)
  %9364 = trunc i32 %9363 to i8
  %9365 = and i8 %9364, 1
  %9366 = xor i8 %9365, 1
  store i8 %9366, i8* %21, align 1
  %9367 = xor i32 %9357, 16
  %9368 = xor i32 %9367, %9358
  %9369 = lshr i32 %9368, 4
  %9370 = trunc i32 %9369 to i8
  %9371 = and i8 %9370, 1
  store i8 %9371, i8* %27, align 1
  %9372 = icmp eq i32 %9358, 0
  %9373 = zext i1 %9372 to i8
  store i8 %9373, i8* %30, align 1
  %9374 = lshr i32 %9358, 31
  %9375 = trunc i32 %9374 to i8
  store i8 %9375, i8* %33, align 1
  %9376 = lshr i32 %9357, 31
  %9377 = xor i32 %9374, %9376
  %9378 = add nuw nsw i32 %9377, %9374
  %9379 = icmp eq i32 %9378, 2
  %9380 = zext i1 %9379 to i8
  store i8 %9380, i8* %39, align 1
  %9381 = sext i32 %9358 to i64
  store i64 %9381, i64* %573, align 8
  %9382 = shl nsw i64 %9381, 1
  %9383 = add i64 %9353, %9382
  %9384 = add i64 %9350, 17
  store i64 %9384, i64* %3, align 8
  %9385 = inttoptr i64 %9383 to i16*
  %9386 = load i16, i16* %9385, align 2
  store i16 %9386, i16* %SI.i, align 2
  %9387 = add i64 %9348, -150
  %9388 = add i64 %9350, 24
  store i64 %9388, i64* %3, align 8
  %9389 = inttoptr i64 %9387 to i16*
  store i16 %9386, i16* %9389, align 2
  %9390 = load i64, i64* %RBP.i, align 8
  %9391 = add i64 %9390, -8
  %9392 = load i64, i64* %3, align 8
  %9393 = add i64 %9392, 4
  store i64 %9393, i64* %3, align 8
  %9394 = inttoptr i64 %9391 to i64*
  %9395 = load i64, i64* %9394, align 8
  %9396 = add i64 %9395, 51640
  store i64 %9396, i64* %RAX.i11582.pre-phi, align 8
  %9397 = icmp ugt i64 %9395, -51641
  %9398 = zext i1 %9397 to i8
  store i8 %9398, i8* %14, align 1
  %9399 = trunc i64 %9396 to i32
  %9400 = and i32 %9399, 255
  %9401 = tail call i32 @llvm.ctpop.i32(i32 %9400)
  %9402 = trunc i32 %9401 to i8
  %9403 = and i8 %9402, 1
  %9404 = xor i8 %9403, 1
  store i8 %9404, i8* %21, align 1
  %9405 = xor i64 %9395, 16
  %9406 = xor i64 %9405, %9396
  %9407 = lshr i64 %9406, 4
  %9408 = trunc i64 %9407 to i8
  %9409 = and i8 %9408, 1
  store i8 %9409, i8* %27, align 1
  %9410 = icmp eq i64 %9396, 0
  %9411 = zext i1 %9410 to i8
  store i8 %9411, i8* %30, align 1
  %9412 = lshr i64 %9396, 63
  %9413 = trunc i64 %9412 to i8
  store i8 %9413, i8* %33, align 1
  %9414 = lshr i64 %9395, 63
  %9415 = xor i64 %9412, %9414
  %9416 = add nuw nsw i64 %9415, %9412
  %9417 = icmp eq i64 %9416, 2
  %9418 = zext i1 %9417 to i8
  store i8 %9418, i8* %39, align 1
  %9419 = add i64 %9390, -150
  %9420 = add i64 %9392, 17
  store i64 %9420, i64* %3, align 8
  %9421 = inttoptr i64 %9419 to i16*
  %9422 = load i16, i16* %9421, align 2
  %9423 = zext i16 %9422 to i64
  store i64 %9423, i64* %RCX.i11580, align 8
  %9424 = zext i16 %9422 to i64
  %9425 = shl nuw nsw i64 %9424, 4
  store i64 %9425, i64* %573, align 8
  %9426 = add i64 %9425, %9396
  store i64 %9426, i64* %RAX.i11582.pre-phi, align 8
  %9427 = icmp ult i64 %9426, %9396
  %9428 = icmp ult i64 %9426, %9425
  %9429 = or i1 %9427, %9428
  %9430 = zext i1 %9429 to i8
  store i8 %9430, i8* %14, align 1
  %9431 = trunc i64 %9426 to i32
  %9432 = and i32 %9431, 255
  %9433 = tail call i32 @llvm.ctpop.i32(i32 %9432)
  %9434 = trunc i32 %9433 to i8
  %9435 = and i8 %9434, 1
  %9436 = xor i8 %9435, 1
  store i8 %9436, i8* %21, align 1
  %9437 = xor i64 %9425, %9396
  %9438 = xor i64 %9437, %9426
  %9439 = lshr i64 %9438, 4
  %9440 = trunc i64 %9439 to i8
  %9441 = and i8 %9440, 1
  store i8 %9441, i8* %27, align 1
  %9442 = icmp eq i64 %9426, 0
  %9443 = zext i1 %9442 to i8
  store i8 %9443, i8* %30, align 1
  %9444 = lshr i64 %9426, 63
  %9445 = trunc i64 %9444 to i8
  store i8 %9445, i8* %33, align 1
  %9446 = xor i64 %9444, %9412
  %9447 = add nuw nsw i64 %9446, %9444
  %9448 = icmp eq i64 %9447, 2
  %9449 = zext i1 %9448 to i8
  store i8 %9449, i8* %39, align 1
  %9450 = inttoptr i64 %9426 to i32*
  %9451 = add i64 %9392, 28
  store i64 %9451, i64* %3, align 8
  %9452 = load i32, i32* %9450, align 4
  %9453 = zext i32 %9452 to i64
  store i64 %9453, i64* %RCX.i11580, align 8
  %9454 = load i64, i64* %RBP.i, align 8
  %9455 = add i64 %9454, -140
  %9456 = add i64 %9392, 34
  store i64 %9456, i64* %3, align 8
  %9457 = inttoptr i64 %9455 to i32*
  %9458 = load i32, i32* %9457, align 4
  %9459 = add i32 %9458, %9452
  %9460 = zext i32 %9459 to i64
  store i64 %9460, i64* %RCX.i11580, align 8
  %9461 = icmp ult i32 %9459, %9452
  %9462 = icmp ult i32 %9459, %9458
  %9463 = or i1 %9461, %9462
  %9464 = zext i1 %9463 to i8
  store i8 %9464, i8* %14, align 1
  %9465 = and i32 %9459, 255
  %9466 = tail call i32 @llvm.ctpop.i32(i32 %9465)
  %9467 = trunc i32 %9466 to i8
  %9468 = and i8 %9467, 1
  %9469 = xor i8 %9468, 1
  store i8 %9469, i8* %21, align 1
  %9470 = xor i32 %9458, %9452
  %9471 = xor i32 %9470, %9459
  %9472 = lshr i32 %9471, 4
  %9473 = trunc i32 %9472 to i8
  %9474 = and i8 %9473, 1
  store i8 %9474, i8* %27, align 1
  %9475 = icmp eq i32 %9459, 0
  %9476 = zext i1 %9475 to i8
  store i8 %9476, i8* %30, align 1
  %9477 = lshr i32 %9459, 31
  %9478 = trunc i32 %9477 to i8
  store i8 %9478, i8* %33, align 1
  %9479 = lshr i32 %9452, 31
  %9480 = lshr i32 %9458, 31
  %9481 = xor i32 %9477, %9479
  %9482 = xor i32 %9477, %9480
  %9483 = add nuw nsw i32 %9481, %9482
  %9484 = icmp eq i32 %9483, 2
  %9485 = zext i1 %9484 to i8
  store i8 %9485, i8* %39, align 1
  %9486 = add i64 %9392, 40
  store i64 %9486, i64* %3, align 8
  store i32 %9459, i32* %9457, align 4
  %9487 = load i64, i64* %RBP.i, align 8
  %9488 = add i64 %9487, -8
  %9489 = load i64, i64* %3, align 8
  %9490 = add i64 %9489, 4
  store i64 %9490, i64* %3, align 8
  %9491 = inttoptr i64 %9488 to i64*
  %9492 = load i64, i64* %9491, align 8
  %9493 = add i64 %9492, 51640
  store i64 %9493, i64* %RAX.i11582.pre-phi, align 8
  %9494 = icmp ugt i64 %9492, -51641
  %9495 = zext i1 %9494 to i8
  store i8 %9495, i8* %14, align 1
  %9496 = trunc i64 %9493 to i32
  %9497 = and i32 %9496, 255
  %9498 = tail call i32 @llvm.ctpop.i32(i32 %9497)
  %9499 = trunc i32 %9498 to i8
  %9500 = and i8 %9499, 1
  %9501 = xor i8 %9500, 1
  store i8 %9501, i8* %21, align 1
  %9502 = xor i64 %9492, 16
  %9503 = xor i64 %9502, %9493
  %9504 = lshr i64 %9503, 4
  %9505 = trunc i64 %9504 to i8
  %9506 = and i8 %9505, 1
  store i8 %9506, i8* %27, align 1
  %9507 = icmp eq i64 %9493, 0
  %9508 = zext i1 %9507 to i8
  store i8 %9508, i8* %30, align 1
  %9509 = lshr i64 %9493, 63
  %9510 = trunc i64 %9509 to i8
  store i8 %9510, i8* %33, align 1
  %9511 = lshr i64 %9492, 63
  %9512 = xor i64 %9509, %9511
  %9513 = add nuw nsw i64 %9512, %9509
  %9514 = icmp eq i64 %9513, 2
  %9515 = zext i1 %9514 to i8
  store i8 %9515, i8* %39, align 1
  %9516 = add i64 %9487, -150
  %9517 = add i64 %9489, 17
  store i64 %9517, i64* %3, align 8
  %9518 = inttoptr i64 %9516 to i16*
  %9519 = load i16, i16* %9518, align 2
  %9520 = zext i16 %9519 to i64
  store i64 %9520, i64* %RCX.i11580, align 8
  %9521 = zext i16 %9519 to i64
  %9522 = shl nuw nsw i64 %9521, 4
  store i64 %9522, i64* %573, align 8
  %9523 = add i64 %9522, %9493
  store i64 %9523, i64* %RAX.i11582.pre-phi, align 8
  %9524 = icmp ult i64 %9523, %9493
  %9525 = icmp ult i64 %9523, %9522
  %9526 = or i1 %9524, %9525
  %9527 = zext i1 %9526 to i8
  store i8 %9527, i8* %14, align 1
  %9528 = trunc i64 %9523 to i32
  %9529 = and i32 %9528, 255
  %9530 = tail call i32 @llvm.ctpop.i32(i32 %9529)
  %9531 = trunc i32 %9530 to i8
  %9532 = and i8 %9531, 1
  %9533 = xor i8 %9532, 1
  store i8 %9533, i8* %21, align 1
  %9534 = xor i64 %9522, %9493
  %9535 = xor i64 %9534, %9523
  %9536 = lshr i64 %9535, 4
  %9537 = trunc i64 %9536 to i8
  %9538 = and i8 %9537, 1
  store i8 %9538, i8* %27, align 1
  %9539 = icmp eq i64 %9523, 0
  %9540 = zext i1 %9539 to i8
  store i8 %9540, i8* %30, align 1
  %9541 = lshr i64 %9523, 63
  %9542 = trunc i64 %9541 to i8
  store i8 %9542, i8* %33, align 1
  %9543 = xor i64 %9541, %9509
  %9544 = add nuw nsw i64 %9543, %9541
  %9545 = icmp eq i64 %9544, 2
  %9546 = zext i1 %9545 to i8
  store i8 %9546, i8* %39, align 1
  %9547 = add i64 %9523, 4
  %9548 = add i64 %9489, 29
  store i64 %9548, i64* %3, align 8
  %9549 = inttoptr i64 %9547 to i32*
  %9550 = load i32, i32* %9549, align 4
  %9551 = zext i32 %9550 to i64
  store i64 %9551, i64* %RCX.i11580, align 8
  %9552 = load i64, i64* %RBP.i, align 8
  %9553 = add i64 %9552, -144
  %9554 = add i64 %9489, 35
  store i64 %9554, i64* %3, align 8
  %9555 = inttoptr i64 %9553 to i32*
  %9556 = load i32, i32* %9555, align 4
  %9557 = add i32 %9556, %9550
  %9558 = zext i32 %9557 to i64
  store i64 %9558, i64* %RCX.i11580, align 8
  %9559 = icmp ult i32 %9557, %9550
  %9560 = icmp ult i32 %9557, %9556
  %9561 = or i1 %9559, %9560
  %9562 = zext i1 %9561 to i8
  store i8 %9562, i8* %14, align 1
  %9563 = and i32 %9557, 255
  %9564 = tail call i32 @llvm.ctpop.i32(i32 %9563)
  %9565 = trunc i32 %9564 to i8
  %9566 = and i8 %9565, 1
  %9567 = xor i8 %9566, 1
  store i8 %9567, i8* %21, align 1
  %9568 = xor i32 %9556, %9550
  %9569 = xor i32 %9568, %9557
  %9570 = lshr i32 %9569, 4
  %9571 = trunc i32 %9570 to i8
  %9572 = and i8 %9571, 1
  store i8 %9572, i8* %27, align 1
  %9573 = icmp eq i32 %9557, 0
  %9574 = zext i1 %9573 to i8
  store i8 %9574, i8* %30, align 1
  %9575 = lshr i32 %9557, 31
  %9576 = trunc i32 %9575 to i8
  store i8 %9576, i8* %33, align 1
  %9577 = lshr i32 %9550, 31
  %9578 = lshr i32 %9556, 31
  %9579 = xor i32 %9575, %9577
  %9580 = xor i32 %9575, %9578
  %9581 = add nuw nsw i32 %9579, %9580
  %9582 = icmp eq i32 %9581, 2
  %9583 = zext i1 %9582 to i8
  store i8 %9583, i8* %39, align 1
  %9584 = add i64 %9489, 41
  store i64 %9584, i64* %3, align 8
  store i32 %9557, i32* %9555, align 4
  %9585 = load i64, i64* %RBP.i, align 8
  %9586 = add i64 %9585, -8
  %9587 = load i64, i64* %3, align 8
  %9588 = add i64 %9587, 4
  store i64 %9588, i64* %3, align 8
  %9589 = inttoptr i64 %9586 to i64*
  %9590 = load i64, i64* %9589, align 8
  %9591 = add i64 %9590, 51640
  store i64 %9591, i64* %RAX.i11582.pre-phi, align 8
  %9592 = icmp ugt i64 %9590, -51641
  %9593 = zext i1 %9592 to i8
  store i8 %9593, i8* %14, align 1
  %9594 = trunc i64 %9591 to i32
  %9595 = and i32 %9594, 255
  %9596 = tail call i32 @llvm.ctpop.i32(i32 %9595)
  %9597 = trunc i32 %9596 to i8
  %9598 = and i8 %9597, 1
  %9599 = xor i8 %9598, 1
  store i8 %9599, i8* %21, align 1
  %9600 = xor i64 %9590, 16
  %9601 = xor i64 %9600, %9591
  %9602 = lshr i64 %9601, 4
  %9603 = trunc i64 %9602 to i8
  %9604 = and i8 %9603, 1
  store i8 %9604, i8* %27, align 1
  %9605 = icmp eq i64 %9591, 0
  %9606 = zext i1 %9605 to i8
  store i8 %9606, i8* %30, align 1
  %9607 = lshr i64 %9591, 63
  %9608 = trunc i64 %9607 to i8
  store i8 %9608, i8* %33, align 1
  %9609 = lshr i64 %9590, 63
  %9610 = xor i64 %9607, %9609
  %9611 = add nuw nsw i64 %9610, %9607
  %9612 = icmp eq i64 %9611, 2
  %9613 = zext i1 %9612 to i8
  store i8 %9613, i8* %39, align 1
  %9614 = add i64 %9585, -150
  %9615 = add i64 %9587, 17
  store i64 %9615, i64* %3, align 8
  %9616 = inttoptr i64 %9614 to i16*
  %9617 = load i16, i16* %9616, align 2
  %9618 = zext i16 %9617 to i64
  store i64 %9618, i64* %RCX.i11580, align 8
  %9619 = zext i16 %9617 to i64
  %9620 = shl nuw nsw i64 %9619, 4
  store i64 %9620, i64* %573, align 8
  %9621 = add i64 %9620, %9591
  store i64 %9621, i64* %RAX.i11582.pre-phi, align 8
  %9622 = icmp ult i64 %9621, %9591
  %9623 = icmp ult i64 %9621, %9620
  %9624 = or i1 %9622, %9623
  %9625 = zext i1 %9624 to i8
  store i8 %9625, i8* %14, align 1
  %9626 = trunc i64 %9621 to i32
  %9627 = and i32 %9626, 255
  %9628 = tail call i32 @llvm.ctpop.i32(i32 %9627)
  %9629 = trunc i32 %9628 to i8
  %9630 = and i8 %9629, 1
  %9631 = xor i8 %9630, 1
  store i8 %9631, i8* %21, align 1
  %9632 = xor i64 %9620, %9591
  %9633 = xor i64 %9632, %9621
  %9634 = lshr i64 %9633, 4
  %9635 = trunc i64 %9634 to i8
  %9636 = and i8 %9635, 1
  store i8 %9636, i8* %27, align 1
  %9637 = icmp eq i64 %9621, 0
  %9638 = zext i1 %9637 to i8
  store i8 %9638, i8* %30, align 1
  %9639 = lshr i64 %9621, 63
  %9640 = trunc i64 %9639 to i8
  store i8 %9640, i8* %33, align 1
  %9641 = xor i64 %9639, %9607
  %9642 = add nuw nsw i64 %9641, %9639
  %9643 = icmp eq i64 %9642, 2
  %9644 = zext i1 %9643 to i8
  store i8 %9644, i8* %39, align 1
  %9645 = add i64 %9621, 8
  %9646 = add i64 %9587, 29
  store i64 %9646, i64* %3, align 8
  %9647 = inttoptr i64 %9645 to i32*
  %9648 = load i32, i32* %9647, align 4
  %9649 = zext i32 %9648 to i64
  store i64 %9649, i64* %RCX.i11580, align 8
  %9650 = load i64, i64* %RBP.i, align 8
  %9651 = add i64 %9650, -148
  %9652 = add i64 %9587, 35
  store i64 %9652, i64* %3, align 8
  %9653 = inttoptr i64 %9651 to i32*
  %9654 = load i32, i32* %9653, align 4
  %9655 = add i32 %9654, %9648
  %9656 = zext i32 %9655 to i64
  store i64 %9656, i64* %RCX.i11580, align 8
  %9657 = icmp ult i32 %9655, %9648
  %9658 = icmp ult i32 %9655, %9654
  %9659 = or i1 %9657, %9658
  %9660 = zext i1 %9659 to i8
  store i8 %9660, i8* %14, align 1
  %9661 = and i32 %9655, 255
  %9662 = tail call i32 @llvm.ctpop.i32(i32 %9661)
  %9663 = trunc i32 %9662 to i8
  %9664 = and i8 %9663, 1
  %9665 = xor i8 %9664, 1
  store i8 %9665, i8* %21, align 1
  %9666 = xor i32 %9654, %9648
  %9667 = xor i32 %9666, %9655
  %9668 = lshr i32 %9667, 4
  %9669 = trunc i32 %9668 to i8
  %9670 = and i8 %9669, 1
  store i8 %9670, i8* %27, align 1
  %9671 = icmp eq i32 %9655, 0
  %9672 = zext i1 %9671 to i8
  store i8 %9672, i8* %30, align 1
  %9673 = lshr i32 %9655, 31
  %9674 = trunc i32 %9673 to i8
  store i8 %9674, i8* %33, align 1
  %9675 = lshr i32 %9648, 31
  %9676 = lshr i32 %9654, 31
  %9677 = xor i32 %9673, %9675
  %9678 = xor i32 %9673, %9676
  %9679 = add nuw nsw i32 %9677, %9678
  %9680 = icmp eq i32 %9679, 2
  %9681 = zext i1 %9680 to i8
  store i8 %9681, i8* %39, align 1
  %9682 = add i64 %9587, 41
  store i64 %9682, i64* %3, align 8
  store i32 %9655, i32* %9653, align 4
  %9683 = load i64, i64* %RBP.i, align 8
  %9684 = add i64 %9683, -120
  %9685 = load i64, i64* %3, align 8
  %9686 = add i64 %9685, 4
  store i64 %9686, i64* %3, align 8
  %9687 = inttoptr i64 %9684 to i64*
  %9688 = load i64, i64* %9687, align 8
  store i64 %9688, i64* %RAX.i11582.pre-phi, align 8
  %9689 = add i64 %9683, -28
  %9690 = add i64 %9685, 7
  store i64 %9690, i64* %3, align 8
  %9691 = inttoptr i64 %9689 to i32*
  %9692 = load i32, i32* %9691, align 4
  %9693 = add i32 %9692, 21
  %9694 = zext i32 %9693 to i64
  store i64 %9694, i64* %RCX.i11580, align 8
  %9695 = icmp ugt i32 %9692, -22
  %9696 = zext i1 %9695 to i8
  store i8 %9696, i8* %14, align 1
  %9697 = and i32 %9693, 255
  %9698 = tail call i32 @llvm.ctpop.i32(i32 %9697)
  %9699 = trunc i32 %9698 to i8
  %9700 = and i8 %9699, 1
  %9701 = xor i8 %9700, 1
  store i8 %9701, i8* %21, align 1
  %9702 = xor i32 %9692, 16
  %9703 = xor i32 %9702, %9693
  %9704 = lshr i32 %9703, 4
  %9705 = trunc i32 %9704 to i8
  %9706 = and i8 %9705, 1
  store i8 %9706, i8* %27, align 1
  %9707 = icmp eq i32 %9693, 0
  %9708 = zext i1 %9707 to i8
  store i8 %9708, i8* %30, align 1
  %9709 = lshr i32 %9693, 31
  %9710 = trunc i32 %9709 to i8
  store i8 %9710, i8* %33, align 1
  %9711 = lshr i32 %9692, 31
  %9712 = xor i32 %9709, %9711
  %9713 = add nuw nsw i32 %9712, %9709
  %9714 = icmp eq i32 %9713, 2
  %9715 = zext i1 %9714 to i8
  store i8 %9715, i8* %39, align 1
  %9716 = sext i32 %9693 to i64
  store i64 %9716, i64* %573, align 8
  %9717 = shl nsw i64 %9716, 1
  %9718 = add i64 %9688, %9717
  %9719 = add i64 %9685, 17
  store i64 %9719, i64* %3, align 8
  %9720 = inttoptr i64 %9718 to i16*
  %9721 = load i16, i16* %9720, align 2
  store i16 %9721, i16* %SI.i, align 2
  %9722 = add i64 %9683, -150
  %9723 = add i64 %9685, 24
  store i64 %9723, i64* %3, align 8
  %9724 = inttoptr i64 %9722 to i16*
  store i16 %9721, i16* %9724, align 2
  %9725 = load i64, i64* %RBP.i, align 8
  %9726 = add i64 %9725, -8
  %9727 = load i64, i64* %3, align 8
  %9728 = add i64 %9727, 4
  store i64 %9728, i64* %3, align 8
  %9729 = inttoptr i64 %9726 to i64*
  %9730 = load i64, i64* %9729, align 8
  %9731 = add i64 %9730, 51640
  store i64 %9731, i64* %RAX.i11582.pre-phi, align 8
  %9732 = icmp ugt i64 %9730, -51641
  %9733 = zext i1 %9732 to i8
  store i8 %9733, i8* %14, align 1
  %9734 = trunc i64 %9731 to i32
  %9735 = and i32 %9734, 255
  %9736 = tail call i32 @llvm.ctpop.i32(i32 %9735)
  %9737 = trunc i32 %9736 to i8
  %9738 = and i8 %9737, 1
  %9739 = xor i8 %9738, 1
  store i8 %9739, i8* %21, align 1
  %9740 = xor i64 %9730, 16
  %9741 = xor i64 %9740, %9731
  %9742 = lshr i64 %9741, 4
  %9743 = trunc i64 %9742 to i8
  %9744 = and i8 %9743, 1
  store i8 %9744, i8* %27, align 1
  %9745 = icmp eq i64 %9731, 0
  %9746 = zext i1 %9745 to i8
  store i8 %9746, i8* %30, align 1
  %9747 = lshr i64 %9731, 63
  %9748 = trunc i64 %9747 to i8
  store i8 %9748, i8* %33, align 1
  %9749 = lshr i64 %9730, 63
  %9750 = xor i64 %9747, %9749
  %9751 = add nuw nsw i64 %9750, %9747
  %9752 = icmp eq i64 %9751, 2
  %9753 = zext i1 %9752 to i8
  store i8 %9753, i8* %39, align 1
  %9754 = add i64 %9725, -150
  %9755 = add i64 %9727, 17
  store i64 %9755, i64* %3, align 8
  %9756 = inttoptr i64 %9754 to i16*
  %9757 = load i16, i16* %9756, align 2
  %9758 = zext i16 %9757 to i64
  store i64 %9758, i64* %RCX.i11580, align 8
  %9759 = zext i16 %9757 to i64
  %9760 = shl nuw nsw i64 %9759, 4
  store i64 %9760, i64* %573, align 8
  %9761 = add i64 %9760, %9731
  store i64 %9761, i64* %RAX.i11582.pre-phi, align 8
  %9762 = icmp ult i64 %9761, %9731
  %9763 = icmp ult i64 %9761, %9760
  %9764 = or i1 %9762, %9763
  %9765 = zext i1 %9764 to i8
  store i8 %9765, i8* %14, align 1
  %9766 = trunc i64 %9761 to i32
  %9767 = and i32 %9766, 255
  %9768 = tail call i32 @llvm.ctpop.i32(i32 %9767)
  %9769 = trunc i32 %9768 to i8
  %9770 = and i8 %9769, 1
  %9771 = xor i8 %9770, 1
  store i8 %9771, i8* %21, align 1
  %9772 = xor i64 %9760, %9731
  %9773 = xor i64 %9772, %9761
  %9774 = lshr i64 %9773, 4
  %9775 = trunc i64 %9774 to i8
  %9776 = and i8 %9775, 1
  store i8 %9776, i8* %27, align 1
  %9777 = icmp eq i64 %9761, 0
  %9778 = zext i1 %9777 to i8
  store i8 %9778, i8* %30, align 1
  %9779 = lshr i64 %9761, 63
  %9780 = trunc i64 %9779 to i8
  store i8 %9780, i8* %33, align 1
  %9781 = xor i64 %9779, %9747
  %9782 = add nuw nsw i64 %9781, %9779
  %9783 = icmp eq i64 %9782, 2
  %9784 = zext i1 %9783 to i8
  store i8 %9784, i8* %39, align 1
  %9785 = inttoptr i64 %9761 to i32*
  %9786 = add i64 %9727, 28
  store i64 %9786, i64* %3, align 8
  %9787 = load i32, i32* %9785, align 4
  %9788 = zext i32 %9787 to i64
  store i64 %9788, i64* %RCX.i11580, align 8
  %9789 = load i64, i64* %RBP.i, align 8
  %9790 = add i64 %9789, -140
  %9791 = add i64 %9727, 34
  store i64 %9791, i64* %3, align 8
  %9792 = inttoptr i64 %9790 to i32*
  %9793 = load i32, i32* %9792, align 4
  %9794 = add i32 %9793, %9787
  %9795 = zext i32 %9794 to i64
  store i64 %9795, i64* %RCX.i11580, align 8
  %9796 = icmp ult i32 %9794, %9787
  %9797 = icmp ult i32 %9794, %9793
  %9798 = or i1 %9796, %9797
  %9799 = zext i1 %9798 to i8
  store i8 %9799, i8* %14, align 1
  %9800 = and i32 %9794, 255
  %9801 = tail call i32 @llvm.ctpop.i32(i32 %9800)
  %9802 = trunc i32 %9801 to i8
  %9803 = and i8 %9802, 1
  %9804 = xor i8 %9803, 1
  store i8 %9804, i8* %21, align 1
  %9805 = xor i32 %9793, %9787
  %9806 = xor i32 %9805, %9794
  %9807 = lshr i32 %9806, 4
  %9808 = trunc i32 %9807 to i8
  %9809 = and i8 %9808, 1
  store i8 %9809, i8* %27, align 1
  %9810 = icmp eq i32 %9794, 0
  %9811 = zext i1 %9810 to i8
  store i8 %9811, i8* %30, align 1
  %9812 = lshr i32 %9794, 31
  %9813 = trunc i32 %9812 to i8
  store i8 %9813, i8* %33, align 1
  %9814 = lshr i32 %9787, 31
  %9815 = lshr i32 %9793, 31
  %9816 = xor i32 %9812, %9814
  %9817 = xor i32 %9812, %9815
  %9818 = add nuw nsw i32 %9816, %9817
  %9819 = icmp eq i32 %9818, 2
  %9820 = zext i1 %9819 to i8
  store i8 %9820, i8* %39, align 1
  %9821 = add i64 %9727, 40
  store i64 %9821, i64* %3, align 8
  store i32 %9794, i32* %9792, align 4
  %9822 = load i64, i64* %RBP.i, align 8
  %9823 = add i64 %9822, -8
  %9824 = load i64, i64* %3, align 8
  %9825 = add i64 %9824, 4
  store i64 %9825, i64* %3, align 8
  %9826 = inttoptr i64 %9823 to i64*
  %9827 = load i64, i64* %9826, align 8
  %9828 = add i64 %9827, 51640
  store i64 %9828, i64* %RAX.i11582.pre-phi, align 8
  %9829 = icmp ugt i64 %9827, -51641
  %9830 = zext i1 %9829 to i8
  store i8 %9830, i8* %14, align 1
  %9831 = trunc i64 %9828 to i32
  %9832 = and i32 %9831, 255
  %9833 = tail call i32 @llvm.ctpop.i32(i32 %9832)
  %9834 = trunc i32 %9833 to i8
  %9835 = and i8 %9834, 1
  %9836 = xor i8 %9835, 1
  store i8 %9836, i8* %21, align 1
  %9837 = xor i64 %9827, 16
  %9838 = xor i64 %9837, %9828
  %9839 = lshr i64 %9838, 4
  %9840 = trunc i64 %9839 to i8
  %9841 = and i8 %9840, 1
  store i8 %9841, i8* %27, align 1
  %9842 = icmp eq i64 %9828, 0
  %9843 = zext i1 %9842 to i8
  store i8 %9843, i8* %30, align 1
  %9844 = lshr i64 %9828, 63
  %9845 = trunc i64 %9844 to i8
  store i8 %9845, i8* %33, align 1
  %9846 = lshr i64 %9827, 63
  %9847 = xor i64 %9844, %9846
  %9848 = add nuw nsw i64 %9847, %9844
  %9849 = icmp eq i64 %9848, 2
  %9850 = zext i1 %9849 to i8
  store i8 %9850, i8* %39, align 1
  %9851 = add i64 %9822, -150
  %9852 = add i64 %9824, 17
  store i64 %9852, i64* %3, align 8
  %9853 = inttoptr i64 %9851 to i16*
  %9854 = load i16, i16* %9853, align 2
  %9855 = zext i16 %9854 to i64
  store i64 %9855, i64* %RCX.i11580, align 8
  %9856 = zext i16 %9854 to i64
  %9857 = shl nuw nsw i64 %9856, 4
  store i64 %9857, i64* %573, align 8
  %9858 = add i64 %9857, %9828
  store i64 %9858, i64* %RAX.i11582.pre-phi, align 8
  %9859 = icmp ult i64 %9858, %9828
  %9860 = icmp ult i64 %9858, %9857
  %9861 = or i1 %9859, %9860
  %9862 = zext i1 %9861 to i8
  store i8 %9862, i8* %14, align 1
  %9863 = trunc i64 %9858 to i32
  %9864 = and i32 %9863, 255
  %9865 = tail call i32 @llvm.ctpop.i32(i32 %9864)
  %9866 = trunc i32 %9865 to i8
  %9867 = and i8 %9866, 1
  %9868 = xor i8 %9867, 1
  store i8 %9868, i8* %21, align 1
  %9869 = xor i64 %9857, %9828
  %9870 = xor i64 %9869, %9858
  %9871 = lshr i64 %9870, 4
  %9872 = trunc i64 %9871 to i8
  %9873 = and i8 %9872, 1
  store i8 %9873, i8* %27, align 1
  %9874 = icmp eq i64 %9858, 0
  %9875 = zext i1 %9874 to i8
  store i8 %9875, i8* %30, align 1
  %9876 = lshr i64 %9858, 63
  %9877 = trunc i64 %9876 to i8
  store i8 %9877, i8* %33, align 1
  %9878 = xor i64 %9876, %9844
  %9879 = add nuw nsw i64 %9878, %9876
  %9880 = icmp eq i64 %9879, 2
  %9881 = zext i1 %9880 to i8
  store i8 %9881, i8* %39, align 1
  %9882 = add i64 %9858, 4
  %9883 = add i64 %9824, 29
  store i64 %9883, i64* %3, align 8
  %9884 = inttoptr i64 %9882 to i32*
  %9885 = load i32, i32* %9884, align 4
  %9886 = zext i32 %9885 to i64
  store i64 %9886, i64* %RCX.i11580, align 8
  %9887 = load i64, i64* %RBP.i, align 8
  %9888 = add i64 %9887, -144
  %9889 = add i64 %9824, 35
  store i64 %9889, i64* %3, align 8
  %9890 = inttoptr i64 %9888 to i32*
  %9891 = load i32, i32* %9890, align 4
  %9892 = add i32 %9891, %9885
  %9893 = zext i32 %9892 to i64
  store i64 %9893, i64* %RCX.i11580, align 8
  %9894 = icmp ult i32 %9892, %9885
  %9895 = icmp ult i32 %9892, %9891
  %9896 = or i1 %9894, %9895
  %9897 = zext i1 %9896 to i8
  store i8 %9897, i8* %14, align 1
  %9898 = and i32 %9892, 255
  %9899 = tail call i32 @llvm.ctpop.i32(i32 %9898)
  %9900 = trunc i32 %9899 to i8
  %9901 = and i8 %9900, 1
  %9902 = xor i8 %9901, 1
  store i8 %9902, i8* %21, align 1
  %9903 = xor i32 %9891, %9885
  %9904 = xor i32 %9903, %9892
  %9905 = lshr i32 %9904, 4
  %9906 = trunc i32 %9905 to i8
  %9907 = and i8 %9906, 1
  store i8 %9907, i8* %27, align 1
  %9908 = icmp eq i32 %9892, 0
  %9909 = zext i1 %9908 to i8
  store i8 %9909, i8* %30, align 1
  %9910 = lshr i32 %9892, 31
  %9911 = trunc i32 %9910 to i8
  store i8 %9911, i8* %33, align 1
  %9912 = lshr i32 %9885, 31
  %9913 = lshr i32 %9891, 31
  %9914 = xor i32 %9910, %9912
  %9915 = xor i32 %9910, %9913
  %9916 = add nuw nsw i32 %9914, %9915
  %9917 = icmp eq i32 %9916, 2
  %9918 = zext i1 %9917 to i8
  store i8 %9918, i8* %39, align 1
  %9919 = add i64 %9824, 41
  store i64 %9919, i64* %3, align 8
  store i32 %9892, i32* %9890, align 4
  %9920 = load i64, i64* %RBP.i, align 8
  %9921 = add i64 %9920, -8
  %9922 = load i64, i64* %3, align 8
  %9923 = add i64 %9922, 4
  store i64 %9923, i64* %3, align 8
  %9924 = inttoptr i64 %9921 to i64*
  %9925 = load i64, i64* %9924, align 8
  %9926 = add i64 %9925, 51640
  store i64 %9926, i64* %RAX.i11582.pre-phi, align 8
  %9927 = icmp ugt i64 %9925, -51641
  %9928 = zext i1 %9927 to i8
  store i8 %9928, i8* %14, align 1
  %9929 = trunc i64 %9926 to i32
  %9930 = and i32 %9929, 255
  %9931 = tail call i32 @llvm.ctpop.i32(i32 %9930)
  %9932 = trunc i32 %9931 to i8
  %9933 = and i8 %9932, 1
  %9934 = xor i8 %9933, 1
  store i8 %9934, i8* %21, align 1
  %9935 = xor i64 %9925, 16
  %9936 = xor i64 %9935, %9926
  %9937 = lshr i64 %9936, 4
  %9938 = trunc i64 %9937 to i8
  %9939 = and i8 %9938, 1
  store i8 %9939, i8* %27, align 1
  %9940 = icmp eq i64 %9926, 0
  %9941 = zext i1 %9940 to i8
  store i8 %9941, i8* %30, align 1
  %9942 = lshr i64 %9926, 63
  %9943 = trunc i64 %9942 to i8
  store i8 %9943, i8* %33, align 1
  %9944 = lshr i64 %9925, 63
  %9945 = xor i64 %9942, %9944
  %9946 = add nuw nsw i64 %9945, %9942
  %9947 = icmp eq i64 %9946, 2
  %9948 = zext i1 %9947 to i8
  store i8 %9948, i8* %39, align 1
  %9949 = add i64 %9920, -150
  %9950 = add i64 %9922, 17
  store i64 %9950, i64* %3, align 8
  %9951 = inttoptr i64 %9949 to i16*
  %9952 = load i16, i16* %9951, align 2
  %9953 = zext i16 %9952 to i64
  store i64 %9953, i64* %RCX.i11580, align 8
  %9954 = zext i16 %9952 to i64
  %9955 = shl nuw nsw i64 %9954, 4
  store i64 %9955, i64* %573, align 8
  %9956 = add i64 %9955, %9926
  store i64 %9956, i64* %RAX.i11582.pre-phi, align 8
  %9957 = icmp ult i64 %9956, %9926
  %9958 = icmp ult i64 %9956, %9955
  %9959 = or i1 %9957, %9958
  %9960 = zext i1 %9959 to i8
  store i8 %9960, i8* %14, align 1
  %9961 = trunc i64 %9956 to i32
  %9962 = and i32 %9961, 255
  %9963 = tail call i32 @llvm.ctpop.i32(i32 %9962)
  %9964 = trunc i32 %9963 to i8
  %9965 = and i8 %9964, 1
  %9966 = xor i8 %9965, 1
  store i8 %9966, i8* %21, align 1
  %9967 = xor i64 %9955, %9926
  %9968 = xor i64 %9967, %9956
  %9969 = lshr i64 %9968, 4
  %9970 = trunc i64 %9969 to i8
  %9971 = and i8 %9970, 1
  store i8 %9971, i8* %27, align 1
  %9972 = icmp eq i64 %9956, 0
  %9973 = zext i1 %9972 to i8
  store i8 %9973, i8* %30, align 1
  %9974 = lshr i64 %9956, 63
  %9975 = trunc i64 %9974 to i8
  store i8 %9975, i8* %33, align 1
  %9976 = xor i64 %9974, %9942
  %9977 = add nuw nsw i64 %9976, %9974
  %9978 = icmp eq i64 %9977, 2
  %9979 = zext i1 %9978 to i8
  store i8 %9979, i8* %39, align 1
  %9980 = add i64 %9956, 8
  %9981 = add i64 %9922, 29
  store i64 %9981, i64* %3, align 8
  %9982 = inttoptr i64 %9980 to i32*
  %9983 = load i32, i32* %9982, align 4
  %9984 = zext i32 %9983 to i64
  store i64 %9984, i64* %RCX.i11580, align 8
  %9985 = load i64, i64* %RBP.i, align 8
  %9986 = add i64 %9985, -148
  %9987 = add i64 %9922, 35
  store i64 %9987, i64* %3, align 8
  %9988 = inttoptr i64 %9986 to i32*
  %9989 = load i32, i32* %9988, align 4
  %9990 = add i32 %9989, %9983
  %9991 = zext i32 %9990 to i64
  store i64 %9991, i64* %RCX.i11580, align 8
  %9992 = icmp ult i32 %9990, %9983
  %9993 = icmp ult i32 %9990, %9989
  %9994 = or i1 %9992, %9993
  %9995 = zext i1 %9994 to i8
  store i8 %9995, i8* %14, align 1
  %9996 = and i32 %9990, 255
  %9997 = tail call i32 @llvm.ctpop.i32(i32 %9996)
  %9998 = trunc i32 %9997 to i8
  %9999 = and i8 %9998, 1
  %10000 = xor i8 %9999, 1
  store i8 %10000, i8* %21, align 1
  %10001 = xor i32 %9989, %9983
  %10002 = xor i32 %10001, %9990
  %10003 = lshr i32 %10002, 4
  %10004 = trunc i32 %10003 to i8
  %10005 = and i8 %10004, 1
  store i8 %10005, i8* %27, align 1
  %10006 = icmp eq i32 %9990, 0
  %10007 = zext i1 %10006 to i8
  store i8 %10007, i8* %30, align 1
  %10008 = lshr i32 %9990, 31
  %10009 = trunc i32 %10008 to i8
  store i8 %10009, i8* %33, align 1
  %10010 = lshr i32 %9983, 31
  %10011 = lshr i32 %9989, 31
  %10012 = xor i32 %10008, %10010
  %10013 = xor i32 %10008, %10011
  %10014 = add nuw nsw i32 %10012, %10013
  %10015 = icmp eq i32 %10014, 2
  %10016 = zext i1 %10015 to i8
  store i8 %10016, i8* %39, align 1
  %10017 = add i64 %9922, 41
  store i64 %10017, i64* %3, align 8
  store i32 %9990, i32* %9988, align 4
  %10018 = load i64, i64* %RBP.i, align 8
  %10019 = add i64 %10018, -120
  %10020 = load i64, i64* %3, align 8
  %10021 = add i64 %10020, 4
  store i64 %10021, i64* %3, align 8
  %10022 = inttoptr i64 %10019 to i64*
  %10023 = load i64, i64* %10022, align 8
  store i64 %10023, i64* %RAX.i11582.pre-phi, align 8
  %10024 = add i64 %10018, -28
  %10025 = add i64 %10020, 7
  store i64 %10025, i64* %3, align 8
  %10026 = inttoptr i64 %10024 to i32*
  %10027 = load i32, i32* %10026, align 4
  %10028 = add i32 %10027, 22
  %10029 = zext i32 %10028 to i64
  store i64 %10029, i64* %RCX.i11580, align 8
  %10030 = icmp ugt i32 %10027, -23
  %10031 = zext i1 %10030 to i8
  store i8 %10031, i8* %14, align 1
  %10032 = and i32 %10028, 255
  %10033 = tail call i32 @llvm.ctpop.i32(i32 %10032)
  %10034 = trunc i32 %10033 to i8
  %10035 = and i8 %10034, 1
  %10036 = xor i8 %10035, 1
  store i8 %10036, i8* %21, align 1
  %10037 = xor i32 %10027, 16
  %10038 = xor i32 %10037, %10028
  %10039 = lshr i32 %10038, 4
  %10040 = trunc i32 %10039 to i8
  %10041 = and i8 %10040, 1
  store i8 %10041, i8* %27, align 1
  %10042 = icmp eq i32 %10028, 0
  %10043 = zext i1 %10042 to i8
  store i8 %10043, i8* %30, align 1
  %10044 = lshr i32 %10028, 31
  %10045 = trunc i32 %10044 to i8
  store i8 %10045, i8* %33, align 1
  %10046 = lshr i32 %10027, 31
  %10047 = xor i32 %10044, %10046
  %10048 = add nuw nsw i32 %10047, %10044
  %10049 = icmp eq i32 %10048, 2
  %10050 = zext i1 %10049 to i8
  store i8 %10050, i8* %39, align 1
  %10051 = sext i32 %10028 to i64
  store i64 %10051, i64* %573, align 8
  %10052 = shl nsw i64 %10051, 1
  %10053 = add i64 %10023, %10052
  %10054 = add i64 %10020, 17
  store i64 %10054, i64* %3, align 8
  %10055 = inttoptr i64 %10053 to i16*
  %10056 = load i16, i16* %10055, align 2
  store i16 %10056, i16* %SI.i, align 2
  %10057 = add i64 %10018, -150
  %10058 = add i64 %10020, 24
  store i64 %10058, i64* %3, align 8
  %10059 = inttoptr i64 %10057 to i16*
  store i16 %10056, i16* %10059, align 2
  %10060 = load i64, i64* %RBP.i, align 8
  %10061 = add i64 %10060, -8
  %10062 = load i64, i64* %3, align 8
  %10063 = add i64 %10062, 4
  store i64 %10063, i64* %3, align 8
  %10064 = inttoptr i64 %10061 to i64*
  %10065 = load i64, i64* %10064, align 8
  %10066 = add i64 %10065, 51640
  store i64 %10066, i64* %RAX.i11582.pre-phi, align 8
  %10067 = icmp ugt i64 %10065, -51641
  %10068 = zext i1 %10067 to i8
  store i8 %10068, i8* %14, align 1
  %10069 = trunc i64 %10066 to i32
  %10070 = and i32 %10069, 255
  %10071 = tail call i32 @llvm.ctpop.i32(i32 %10070)
  %10072 = trunc i32 %10071 to i8
  %10073 = and i8 %10072, 1
  %10074 = xor i8 %10073, 1
  store i8 %10074, i8* %21, align 1
  %10075 = xor i64 %10065, 16
  %10076 = xor i64 %10075, %10066
  %10077 = lshr i64 %10076, 4
  %10078 = trunc i64 %10077 to i8
  %10079 = and i8 %10078, 1
  store i8 %10079, i8* %27, align 1
  %10080 = icmp eq i64 %10066, 0
  %10081 = zext i1 %10080 to i8
  store i8 %10081, i8* %30, align 1
  %10082 = lshr i64 %10066, 63
  %10083 = trunc i64 %10082 to i8
  store i8 %10083, i8* %33, align 1
  %10084 = lshr i64 %10065, 63
  %10085 = xor i64 %10082, %10084
  %10086 = add nuw nsw i64 %10085, %10082
  %10087 = icmp eq i64 %10086, 2
  %10088 = zext i1 %10087 to i8
  store i8 %10088, i8* %39, align 1
  %10089 = add i64 %10060, -150
  %10090 = add i64 %10062, 17
  store i64 %10090, i64* %3, align 8
  %10091 = inttoptr i64 %10089 to i16*
  %10092 = load i16, i16* %10091, align 2
  %10093 = zext i16 %10092 to i64
  store i64 %10093, i64* %RCX.i11580, align 8
  %10094 = zext i16 %10092 to i64
  %10095 = shl nuw nsw i64 %10094, 4
  store i64 %10095, i64* %573, align 8
  %10096 = add i64 %10095, %10066
  store i64 %10096, i64* %RAX.i11582.pre-phi, align 8
  %10097 = icmp ult i64 %10096, %10066
  %10098 = icmp ult i64 %10096, %10095
  %10099 = or i1 %10097, %10098
  %10100 = zext i1 %10099 to i8
  store i8 %10100, i8* %14, align 1
  %10101 = trunc i64 %10096 to i32
  %10102 = and i32 %10101, 255
  %10103 = tail call i32 @llvm.ctpop.i32(i32 %10102)
  %10104 = trunc i32 %10103 to i8
  %10105 = and i8 %10104, 1
  %10106 = xor i8 %10105, 1
  store i8 %10106, i8* %21, align 1
  %10107 = xor i64 %10095, %10066
  %10108 = xor i64 %10107, %10096
  %10109 = lshr i64 %10108, 4
  %10110 = trunc i64 %10109 to i8
  %10111 = and i8 %10110, 1
  store i8 %10111, i8* %27, align 1
  %10112 = icmp eq i64 %10096, 0
  %10113 = zext i1 %10112 to i8
  store i8 %10113, i8* %30, align 1
  %10114 = lshr i64 %10096, 63
  %10115 = trunc i64 %10114 to i8
  store i8 %10115, i8* %33, align 1
  %10116 = xor i64 %10114, %10082
  %10117 = add nuw nsw i64 %10116, %10114
  %10118 = icmp eq i64 %10117, 2
  %10119 = zext i1 %10118 to i8
  store i8 %10119, i8* %39, align 1
  %10120 = inttoptr i64 %10096 to i32*
  %10121 = add i64 %10062, 28
  store i64 %10121, i64* %3, align 8
  %10122 = load i32, i32* %10120, align 4
  %10123 = zext i32 %10122 to i64
  store i64 %10123, i64* %RCX.i11580, align 8
  %10124 = load i64, i64* %RBP.i, align 8
  %10125 = add i64 %10124, -140
  %10126 = add i64 %10062, 34
  store i64 %10126, i64* %3, align 8
  %10127 = inttoptr i64 %10125 to i32*
  %10128 = load i32, i32* %10127, align 4
  %10129 = add i32 %10128, %10122
  %10130 = zext i32 %10129 to i64
  store i64 %10130, i64* %RCX.i11580, align 8
  %10131 = icmp ult i32 %10129, %10122
  %10132 = icmp ult i32 %10129, %10128
  %10133 = or i1 %10131, %10132
  %10134 = zext i1 %10133 to i8
  store i8 %10134, i8* %14, align 1
  %10135 = and i32 %10129, 255
  %10136 = tail call i32 @llvm.ctpop.i32(i32 %10135)
  %10137 = trunc i32 %10136 to i8
  %10138 = and i8 %10137, 1
  %10139 = xor i8 %10138, 1
  store i8 %10139, i8* %21, align 1
  %10140 = xor i32 %10128, %10122
  %10141 = xor i32 %10140, %10129
  %10142 = lshr i32 %10141, 4
  %10143 = trunc i32 %10142 to i8
  %10144 = and i8 %10143, 1
  store i8 %10144, i8* %27, align 1
  %10145 = icmp eq i32 %10129, 0
  %10146 = zext i1 %10145 to i8
  store i8 %10146, i8* %30, align 1
  %10147 = lshr i32 %10129, 31
  %10148 = trunc i32 %10147 to i8
  store i8 %10148, i8* %33, align 1
  %10149 = lshr i32 %10122, 31
  %10150 = lshr i32 %10128, 31
  %10151 = xor i32 %10147, %10149
  %10152 = xor i32 %10147, %10150
  %10153 = add nuw nsw i32 %10151, %10152
  %10154 = icmp eq i32 %10153, 2
  %10155 = zext i1 %10154 to i8
  store i8 %10155, i8* %39, align 1
  %10156 = add i64 %10062, 40
  store i64 %10156, i64* %3, align 8
  store i32 %10129, i32* %10127, align 4
  %10157 = load i64, i64* %RBP.i, align 8
  %10158 = add i64 %10157, -8
  %10159 = load i64, i64* %3, align 8
  %10160 = add i64 %10159, 4
  store i64 %10160, i64* %3, align 8
  %10161 = inttoptr i64 %10158 to i64*
  %10162 = load i64, i64* %10161, align 8
  %10163 = add i64 %10162, 51640
  store i64 %10163, i64* %RAX.i11582.pre-phi, align 8
  %10164 = icmp ugt i64 %10162, -51641
  %10165 = zext i1 %10164 to i8
  store i8 %10165, i8* %14, align 1
  %10166 = trunc i64 %10163 to i32
  %10167 = and i32 %10166, 255
  %10168 = tail call i32 @llvm.ctpop.i32(i32 %10167)
  %10169 = trunc i32 %10168 to i8
  %10170 = and i8 %10169, 1
  %10171 = xor i8 %10170, 1
  store i8 %10171, i8* %21, align 1
  %10172 = xor i64 %10162, 16
  %10173 = xor i64 %10172, %10163
  %10174 = lshr i64 %10173, 4
  %10175 = trunc i64 %10174 to i8
  %10176 = and i8 %10175, 1
  store i8 %10176, i8* %27, align 1
  %10177 = icmp eq i64 %10163, 0
  %10178 = zext i1 %10177 to i8
  store i8 %10178, i8* %30, align 1
  %10179 = lshr i64 %10163, 63
  %10180 = trunc i64 %10179 to i8
  store i8 %10180, i8* %33, align 1
  %10181 = lshr i64 %10162, 63
  %10182 = xor i64 %10179, %10181
  %10183 = add nuw nsw i64 %10182, %10179
  %10184 = icmp eq i64 %10183, 2
  %10185 = zext i1 %10184 to i8
  store i8 %10185, i8* %39, align 1
  %10186 = add i64 %10157, -150
  %10187 = add i64 %10159, 17
  store i64 %10187, i64* %3, align 8
  %10188 = inttoptr i64 %10186 to i16*
  %10189 = load i16, i16* %10188, align 2
  %10190 = zext i16 %10189 to i64
  store i64 %10190, i64* %RCX.i11580, align 8
  %10191 = zext i16 %10189 to i64
  %10192 = shl nuw nsw i64 %10191, 4
  store i64 %10192, i64* %573, align 8
  %10193 = add i64 %10192, %10163
  store i64 %10193, i64* %RAX.i11582.pre-phi, align 8
  %10194 = icmp ult i64 %10193, %10163
  %10195 = icmp ult i64 %10193, %10192
  %10196 = or i1 %10194, %10195
  %10197 = zext i1 %10196 to i8
  store i8 %10197, i8* %14, align 1
  %10198 = trunc i64 %10193 to i32
  %10199 = and i32 %10198, 255
  %10200 = tail call i32 @llvm.ctpop.i32(i32 %10199)
  %10201 = trunc i32 %10200 to i8
  %10202 = and i8 %10201, 1
  %10203 = xor i8 %10202, 1
  store i8 %10203, i8* %21, align 1
  %10204 = xor i64 %10192, %10163
  %10205 = xor i64 %10204, %10193
  %10206 = lshr i64 %10205, 4
  %10207 = trunc i64 %10206 to i8
  %10208 = and i8 %10207, 1
  store i8 %10208, i8* %27, align 1
  %10209 = icmp eq i64 %10193, 0
  %10210 = zext i1 %10209 to i8
  store i8 %10210, i8* %30, align 1
  %10211 = lshr i64 %10193, 63
  %10212 = trunc i64 %10211 to i8
  store i8 %10212, i8* %33, align 1
  %10213 = xor i64 %10211, %10179
  %10214 = add nuw nsw i64 %10213, %10211
  %10215 = icmp eq i64 %10214, 2
  %10216 = zext i1 %10215 to i8
  store i8 %10216, i8* %39, align 1
  %10217 = add i64 %10193, 4
  %10218 = add i64 %10159, 29
  store i64 %10218, i64* %3, align 8
  %10219 = inttoptr i64 %10217 to i32*
  %10220 = load i32, i32* %10219, align 4
  %10221 = zext i32 %10220 to i64
  store i64 %10221, i64* %RCX.i11580, align 8
  %10222 = load i64, i64* %RBP.i, align 8
  %10223 = add i64 %10222, -144
  %10224 = add i64 %10159, 35
  store i64 %10224, i64* %3, align 8
  %10225 = inttoptr i64 %10223 to i32*
  %10226 = load i32, i32* %10225, align 4
  %10227 = add i32 %10226, %10220
  %10228 = zext i32 %10227 to i64
  store i64 %10228, i64* %RCX.i11580, align 8
  %10229 = icmp ult i32 %10227, %10220
  %10230 = icmp ult i32 %10227, %10226
  %10231 = or i1 %10229, %10230
  %10232 = zext i1 %10231 to i8
  store i8 %10232, i8* %14, align 1
  %10233 = and i32 %10227, 255
  %10234 = tail call i32 @llvm.ctpop.i32(i32 %10233)
  %10235 = trunc i32 %10234 to i8
  %10236 = and i8 %10235, 1
  %10237 = xor i8 %10236, 1
  store i8 %10237, i8* %21, align 1
  %10238 = xor i32 %10226, %10220
  %10239 = xor i32 %10238, %10227
  %10240 = lshr i32 %10239, 4
  %10241 = trunc i32 %10240 to i8
  %10242 = and i8 %10241, 1
  store i8 %10242, i8* %27, align 1
  %10243 = icmp eq i32 %10227, 0
  %10244 = zext i1 %10243 to i8
  store i8 %10244, i8* %30, align 1
  %10245 = lshr i32 %10227, 31
  %10246 = trunc i32 %10245 to i8
  store i8 %10246, i8* %33, align 1
  %10247 = lshr i32 %10220, 31
  %10248 = lshr i32 %10226, 31
  %10249 = xor i32 %10245, %10247
  %10250 = xor i32 %10245, %10248
  %10251 = add nuw nsw i32 %10249, %10250
  %10252 = icmp eq i32 %10251, 2
  %10253 = zext i1 %10252 to i8
  store i8 %10253, i8* %39, align 1
  %10254 = add i64 %10159, 41
  store i64 %10254, i64* %3, align 8
  store i32 %10227, i32* %10225, align 4
  %10255 = load i64, i64* %RBP.i, align 8
  %10256 = add i64 %10255, -8
  %10257 = load i64, i64* %3, align 8
  %10258 = add i64 %10257, 4
  store i64 %10258, i64* %3, align 8
  %10259 = inttoptr i64 %10256 to i64*
  %10260 = load i64, i64* %10259, align 8
  %10261 = add i64 %10260, 51640
  store i64 %10261, i64* %RAX.i11582.pre-phi, align 8
  %10262 = icmp ugt i64 %10260, -51641
  %10263 = zext i1 %10262 to i8
  store i8 %10263, i8* %14, align 1
  %10264 = trunc i64 %10261 to i32
  %10265 = and i32 %10264, 255
  %10266 = tail call i32 @llvm.ctpop.i32(i32 %10265)
  %10267 = trunc i32 %10266 to i8
  %10268 = and i8 %10267, 1
  %10269 = xor i8 %10268, 1
  store i8 %10269, i8* %21, align 1
  %10270 = xor i64 %10260, 16
  %10271 = xor i64 %10270, %10261
  %10272 = lshr i64 %10271, 4
  %10273 = trunc i64 %10272 to i8
  %10274 = and i8 %10273, 1
  store i8 %10274, i8* %27, align 1
  %10275 = icmp eq i64 %10261, 0
  %10276 = zext i1 %10275 to i8
  store i8 %10276, i8* %30, align 1
  %10277 = lshr i64 %10261, 63
  %10278 = trunc i64 %10277 to i8
  store i8 %10278, i8* %33, align 1
  %10279 = lshr i64 %10260, 63
  %10280 = xor i64 %10277, %10279
  %10281 = add nuw nsw i64 %10280, %10277
  %10282 = icmp eq i64 %10281, 2
  %10283 = zext i1 %10282 to i8
  store i8 %10283, i8* %39, align 1
  %10284 = add i64 %10255, -150
  %10285 = add i64 %10257, 17
  store i64 %10285, i64* %3, align 8
  %10286 = inttoptr i64 %10284 to i16*
  %10287 = load i16, i16* %10286, align 2
  %10288 = zext i16 %10287 to i64
  store i64 %10288, i64* %RCX.i11580, align 8
  %10289 = zext i16 %10287 to i64
  %10290 = shl nuw nsw i64 %10289, 4
  store i64 %10290, i64* %573, align 8
  %10291 = add i64 %10290, %10261
  store i64 %10291, i64* %RAX.i11582.pre-phi, align 8
  %10292 = icmp ult i64 %10291, %10261
  %10293 = icmp ult i64 %10291, %10290
  %10294 = or i1 %10292, %10293
  %10295 = zext i1 %10294 to i8
  store i8 %10295, i8* %14, align 1
  %10296 = trunc i64 %10291 to i32
  %10297 = and i32 %10296, 255
  %10298 = tail call i32 @llvm.ctpop.i32(i32 %10297)
  %10299 = trunc i32 %10298 to i8
  %10300 = and i8 %10299, 1
  %10301 = xor i8 %10300, 1
  store i8 %10301, i8* %21, align 1
  %10302 = xor i64 %10290, %10261
  %10303 = xor i64 %10302, %10291
  %10304 = lshr i64 %10303, 4
  %10305 = trunc i64 %10304 to i8
  %10306 = and i8 %10305, 1
  store i8 %10306, i8* %27, align 1
  %10307 = icmp eq i64 %10291, 0
  %10308 = zext i1 %10307 to i8
  store i8 %10308, i8* %30, align 1
  %10309 = lshr i64 %10291, 63
  %10310 = trunc i64 %10309 to i8
  store i8 %10310, i8* %33, align 1
  %10311 = xor i64 %10309, %10277
  %10312 = add nuw nsw i64 %10311, %10309
  %10313 = icmp eq i64 %10312, 2
  %10314 = zext i1 %10313 to i8
  store i8 %10314, i8* %39, align 1
  %10315 = add i64 %10291, 8
  %10316 = add i64 %10257, 29
  store i64 %10316, i64* %3, align 8
  %10317 = inttoptr i64 %10315 to i32*
  %10318 = load i32, i32* %10317, align 4
  %10319 = zext i32 %10318 to i64
  store i64 %10319, i64* %RCX.i11580, align 8
  %10320 = load i64, i64* %RBP.i, align 8
  %10321 = add i64 %10320, -148
  %10322 = add i64 %10257, 35
  store i64 %10322, i64* %3, align 8
  %10323 = inttoptr i64 %10321 to i32*
  %10324 = load i32, i32* %10323, align 4
  %10325 = add i32 %10324, %10318
  %10326 = zext i32 %10325 to i64
  store i64 %10326, i64* %RCX.i11580, align 8
  %10327 = icmp ult i32 %10325, %10318
  %10328 = icmp ult i32 %10325, %10324
  %10329 = or i1 %10327, %10328
  %10330 = zext i1 %10329 to i8
  store i8 %10330, i8* %14, align 1
  %10331 = and i32 %10325, 255
  %10332 = tail call i32 @llvm.ctpop.i32(i32 %10331)
  %10333 = trunc i32 %10332 to i8
  %10334 = and i8 %10333, 1
  %10335 = xor i8 %10334, 1
  store i8 %10335, i8* %21, align 1
  %10336 = xor i32 %10324, %10318
  %10337 = xor i32 %10336, %10325
  %10338 = lshr i32 %10337, 4
  %10339 = trunc i32 %10338 to i8
  %10340 = and i8 %10339, 1
  store i8 %10340, i8* %27, align 1
  %10341 = icmp eq i32 %10325, 0
  %10342 = zext i1 %10341 to i8
  store i8 %10342, i8* %30, align 1
  %10343 = lshr i32 %10325, 31
  %10344 = trunc i32 %10343 to i8
  store i8 %10344, i8* %33, align 1
  %10345 = lshr i32 %10318, 31
  %10346 = lshr i32 %10324, 31
  %10347 = xor i32 %10343, %10345
  %10348 = xor i32 %10343, %10346
  %10349 = add nuw nsw i32 %10347, %10348
  %10350 = icmp eq i32 %10349, 2
  %10351 = zext i1 %10350 to i8
  store i8 %10351, i8* %39, align 1
  %10352 = add i64 %10257, 41
  store i64 %10352, i64* %3, align 8
  store i32 %10325, i32* %10323, align 4
  %10353 = load i64, i64* %RBP.i, align 8
  %10354 = add i64 %10353, -120
  %10355 = load i64, i64* %3, align 8
  %10356 = add i64 %10355, 4
  store i64 %10356, i64* %3, align 8
  %10357 = inttoptr i64 %10354 to i64*
  %10358 = load i64, i64* %10357, align 8
  store i64 %10358, i64* %RAX.i11582.pre-phi, align 8
  %10359 = add i64 %10353, -28
  %10360 = add i64 %10355, 7
  store i64 %10360, i64* %3, align 8
  %10361 = inttoptr i64 %10359 to i32*
  %10362 = load i32, i32* %10361, align 4
  %10363 = add i32 %10362, 23
  %10364 = zext i32 %10363 to i64
  store i64 %10364, i64* %RCX.i11580, align 8
  %10365 = icmp ugt i32 %10362, -24
  %10366 = zext i1 %10365 to i8
  store i8 %10366, i8* %14, align 1
  %10367 = and i32 %10363, 255
  %10368 = tail call i32 @llvm.ctpop.i32(i32 %10367)
  %10369 = trunc i32 %10368 to i8
  %10370 = and i8 %10369, 1
  %10371 = xor i8 %10370, 1
  store i8 %10371, i8* %21, align 1
  %10372 = xor i32 %10362, 16
  %10373 = xor i32 %10372, %10363
  %10374 = lshr i32 %10373, 4
  %10375 = trunc i32 %10374 to i8
  %10376 = and i8 %10375, 1
  store i8 %10376, i8* %27, align 1
  %10377 = icmp eq i32 %10363, 0
  %10378 = zext i1 %10377 to i8
  store i8 %10378, i8* %30, align 1
  %10379 = lshr i32 %10363, 31
  %10380 = trunc i32 %10379 to i8
  store i8 %10380, i8* %33, align 1
  %10381 = lshr i32 %10362, 31
  %10382 = xor i32 %10379, %10381
  %10383 = add nuw nsw i32 %10382, %10379
  %10384 = icmp eq i32 %10383, 2
  %10385 = zext i1 %10384 to i8
  store i8 %10385, i8* %39, align 1
  %10386 = sext i32 %10363 to i64
  store i64 %10386, i64* %573, align 8
  %10387 = shl nsw i64 %10386, 1
  %10388 = add i64 %10358, %10387
  %10389 = add i64 %10355, 17
  store i64 %10389, i64* %3, align 8
  %10390 = inttoptr i64 %10388 to i16*
  %10391 = load i16, i16* %10390, align 2
  store i16 %10391, i16* %SI.i, align 2
  %10392 = add i64 %10353, -150
  %10393 = add i64 %10355, 24
  store i64 %10393, i64* %3, align 8
  %10394 = inttoptr i64 %10392 to i16*
  store i16 %10391, i16* %10394, align 2
  %10395 = load i64, i64* %RBP.i, align 8
  %10396 = add i64 %10395, -8
  %10397 = load i64, i64* %3, align 8
  %10398 = add i64 %10397, 4
  store i64 %10398, i64* %3, align 8
  %10399 = inttoptr i64 %10396 to i64*
  %10400 = load i64, i64* %10399, align 8
  %10401 = add i64 %10400, 51640
  store i64 %10401, i64* %RAX.i11582.pre-phi, align 8
  %10402 = icmp ugt i64 %10400, -51641
  %10403 = zext i1 %10402 to i8
  store i8 %10403, i8* %14, align 1
  %10404 = trunc i64 %10401 to i32
  %10405 = and i32 %10404, 255
  %10406 = tail call i32 @llvm.ctpop.i32(i32 %10405)
  %10407 = trunc i32 %10406 to i8
  %10408 = and i8 %10407, 1
  %10409 = xor i8 %10408, 1
  store i8 %10409, i8* %21, align 1
  %10410 = xor i64 %10400, 16
  %10411 = xor i64 %10410, %10401
  %10412 = lshr i64 %10411, 4
  %10413 = trunc i64 %10412 to i8
  %10414 = and i8 %10413, 1
  store i8 %10414, i8* %27, align 1
  %10415 = icmp eq i64 %10401, 0
  %10416 = zext i1 %10415 to i8
  store i8 %10416, i8* %30, align 1
  %10417 = lshr i64 %10401, 63
  %10418 = trunc i64 %10417 to i8
  store i8 %10418, i8* %33, align 1
  %10419 = lshr i64 %10400, 63
  %10420 = xor i64 %10417, %10419
  %10421 = add nuw nsw i64 %10420, %10417
  %10422 = icmp eq i64 %10421, 2
  %10423 = zext i1 %10422 to i8
  store i8 %10423, i8* %39, align 1
  %10424 = add i64 %10395, -150
  %10425 = add i64 %10397, 17
  store i64 %10425, i64* %3, align 8
  %10426 = inttoptr i64 %10424 to i16*
  %10427 = load i16, i16* %10426, align 2
  %10428 = zext i16 %10427 to i64
  store i64 %10428, i64* %RCX.i11580, align 8
  %10429 = zext i16 %10427 to i64
  %10430 = shl nuw nsw i64 %10429, 4
  store i64 %10430, i64* %573, align 8
  %10431 = add i64 %10430, %10401
  store i64 %10431, i64* %RAX.i11582.pre-phi, align 8
  %10432 = icmp ult i64 %10431, %10401
  %10433 = icmp ult i64 %10431, %10430
  %10434 = or i1 %10432, %10433
  %10435 = zext i1 %10434 to i8
  store i8 %10435, i8* %14, align 1
  %10436 = trunc i64 %10431 to i32
  %10437 = and i32 %10436, 255
  %10438 = tail call i32 @llvm.ctpop.i32(i32 %10437)
  %10439 = trunc i32 %10438 to i8
  %10440 = and i8 %10439, 1
  %10441 = xor i8 %10440, 1
  store i8 %10441, i8* %21, align 1
  %10442 = xor i64 %10430, %10401
  %10443 = xor i64 %10442, %10431
  %10444 = lshr i64 %10443, 4
  %10445 = trunc i64 %10444 to i8
  %10446 = and i8 %10445, 1
  store i8 %10446, i8* %27, align 1
  %10447 = icmp eq i64 %10431, 0
  %10448 = zext i1 %10447 to i8
  store i8 %10448, i8* %30, align 1
  %10449 = lshr i64 %10431, 63
  %10450 = trunc i64 %10449 to i8
  store i8 %10450, i8* %33, align 1
  %10451 = xor i64 %10449, %10417
  %10452 = add nuw nsw i64 %10451, %10449
  %10453 = icmp eq i64 %10452, 2
  %10454 = zext i1 %10453 to i8
  store i8 %10454, i8* %39, align 1
  %10455 = inttoptr i64 %10431 to i32*
  %10456 = add i64 %10397, 28
  store i64 %10456, i64* %3, align 8
  %10457 = load i32, i32* %10455, align 4
  %10458 = zext i32 %10457 to i64
  store i64 %10458, i64* %RCX.i11580, align 8
  %10459 = load i64, i64* %RBP.i, align 8
  %10460 = add i64 %10459, -140
  %10461 = add i64 %10397, 34
  store i64 %10461, i64* %3, align 8
  %10462 = inttoptr i64 %10460 to i32*
  %10463 = load i32, i32* %10462, align 4
  %10464 = add i32 %10463, %10457
  %10465 = zext i32 %10464 to i64
  store i64 %10465, i64* %RCX.i11580, align 8
  %10466 = icmp ult i32 %10464, %10457
  %10467 = icmp ult i32 %10464, %10463
  %10468 = or i1 %10466, %10467
  %10469 = zext i1 %10468 to i8
  store i8 %10469, i8* %14, align 1
  %10470 = and i32 %10464, 255
  %10471 = tail call i32 @llvm.ctpop.i32(i32 %10470)
  %10472 = trunc i32 %10471 to i8
  %10473 = and i8 %10472, 1
  %10474 = xor i8 %10473, 1
  store i8 %10474, i8* %21, align 1
  %10475 = xor i32 %10463, %10457
  %10476 = xor i32 %10475, %10464
  %10477 = lshr i32 %10476, 4
  %10478 = trunc i32 %10477 to i8
  %10479 = and i8 %10478, 1
  store i8 %10479, i8* %27, align 1
  %10480 = icmp eq i32 %10464, 0
  %10481 = zext i1 %10480 to i8
  store i8 %10481, i8* %30, align 1
  %10482 = lshr i32 %10464, 31
  %10483 = trunc i32 %10482 to i8
  store i8 %10483, i8* %33, align 1
  %10484 = lshr i32 %10457, 31
  %10485 = lshr i32 %10463, 31
  %10486 = xor i32 %10482, %10484
  %10487 = xor i32 %10482, %10485
  %10488 = add nuw nsw i32 %10486, %10487
  %10489 = icmp eq i32 %10488, 2
  %10490 = zext i1 %10489 to i8
  store i8 %10490, i8* %39, align 1
  %10491 = add i64 %10397, 40
  store i64 %10491, i64* %3, align 8
  store i32 %10464, i32* %10462, align 4
  %10492 = load i64, i64* %RBP.i, align 8
  %10493 = add i64 %10492, -8
  %10494 = load i64, i64* %3, align 8
  %10495 = add i64 %10494, 4
  store i64 %10495, i64* %3, align 8
  %10496 = inttoptr i64 %10493 to i64*
  %10497 = load i64, i64* %10496, align 8
  %10498 = add i64 %10497, 51640
  store i64 %10498, i64* %RAX.i11582.pre-phi, align 8
  %10499 = icmp ugt i64 %10497, -51641
  %10500 = zext i1 %10499 to i8
  store i8 %10500, i8* %14, align 1
  %10501 = trunc i64 %10498 to i32
  %10502 = and i32 %10501, 255
  %10503 = tail call i32 @llvm.ctpop.i32(i32 %10502)
  %10504 = trunc i32 %10503 to i8
  %10505 = and i8 %10504, 1
  %10506 = xor i8 %10505, 1
  store i8 %10506, i8* %21, align 1
  %10507 = xor i64 %10497, 16
  %10508 = xor i64 %10507, %10498
  %10509 = lshr i64 %10508, 4
  %10510 = trunc i64 %10509 to i8
  %10511 = and i8 %10510, 1
  store i8 %10511, i8* %27, align 1
  %10512 = icmp eq i64 %10498, 0
  %10513 = zext i1 %10512 to i8
  store i8 %10513, i8* %30, align 1
  %10514 = lshr i64 %10498, 63
  %10515 = trunc i64 %10514 to i8
  store i8 %10515, i8* %33, align 1
  %10516 = lshr i64 %10497, 63
  %10517 = xor i64 %10514, %10516
  %10518 = add nuw nsw i64 %10517, %10514
  %10519 = icmp eq i64 %10518, 2
  %10520 = zext i1 %10519 to i8
  store i8 %10520, i8* %39, align 1
  %10521 = add i64 %10492, -150
  %10522 = add i64 %10494, 17
  store i64 %10522, i64* %3, align 8
  %10523 = inttoptr i64 %10521 to i16*
  %10524 = load i16, i16* %10523, align 2
  %10525 = zext i16 %10524 to i64
  store i64 %10525, i64* %RCX.i11580, align 8
  %10526 = zext i16 %10524 to i64
  %10527 = shl nuw nsw i64 %10526, 4
  store i64 %10527, i64* %573, align 8
  %10528 = add i64 %10527, %10498
  store i64 %10528, i64* %RAX.i11582.pre-phi, align 8
  %10529 = icmp ult i64 %10528, %10498
  %10530 = icmp ult i64 %10528, %10527
  %10531 = or i1 %10529, %10530
  %10532 = zext i1 %10531 to i8
  store i8 %10532, i8* %14, align 1
  %10533 = trunc i64 %10528 to i32
  %10534 = and i32 %10533, 255
  %10535 = tail call i32 @llvm.ctpop.i32(i32 %10534)
  %10536 = trunc i32 %10535 to i8
  %10537 = and i8 %10536, 1
  %10538 = xor i8 %10537, 1
  store i8 %10538, i8* %21, align 1
  %10539 = xor i64 %10527, %10498
  %10540 = xor i64 %10539, %10528
  %10541 = lshr i64 %10540, 4
  %10542 = trunc i64 %10541 to i8
  %10543 = and i8 %10542, 1
  store i8 %10543, i8* %27, align 1
  %10544 = icmp eq i64 %10528, 0
  %10545 = zext i1 %10544 to i8
  store i8 %10545, i8* %30, align 1
  %10546 = lshr i64 %10528, 63
  %10547 = trunc i64 %10546 to i8
  store i8 %10547, i8* %33, align 1
  %10548 = xor i64 %10546, %10514
  %10549 = add nuw nsw i64 %10548, %10546
  %10550 = icmp eq i64 %10549, 2
  %10551 = zext i1 %10550 to i8
  store i8 %10551, i8* %39, align 1
  %10552 = add i64 %10528, 4
  %10553 = add i64 %10494, 29
  store i64 %10553, i64* %3, align 8
  %10554 = inttoptr i64 %10552 to i32*
  %10555 = load i32, i32* %10554, align 4
  %10556 = zext i32 %10555 to i64
  store i64 %10556, i64* %RCX.i11580, align 8
  %10557 = load i64, i64* %RBP.i, align 8
  %10558 = add i64 %10557, -144
  %10559 = add i64 %10494, 35
  store i64 %10559, i64* %3, align 8
  %10560 = inttoptr i64 %10558 to i32*
  %10561 = load i32, i32* %10560, align 4
  %10562 = add i32 %10561, %10555
  %10563 = zext i32 %10562 to i64
  store i64 %10563, i64* %RCX.i11580, align 8
  %10564 = icmp ult i32 %10562, %10555
  %10565 = icmp ult i32 %10562, %10561
  %10566 = or i1 %10564, %10565
  %10567 = zext i1 %10566 to i8
  store i8 %10567, i8* %14, align 1
  %10568 = and i32 %10562, 255
  %10569 = tail call i32 @llvm.ctpop.i32(i32 %10568)
  %10570 = trunc i32 %10569 to i8
  %10571 = and i8 %10570, 1
  %10572 = xor i8 %10571, 1
  store i8 %10572, i8* %21, align 1
  %10573 = xor i32 %10561, %10555
  %10574 = xor i32 %10573, %10562
  %10575 = lshr i32 %10574, 4
  %10576 = trunc i32 %10575 to i8
  %10577 = and i8 %10576, 1
  store i8 %10577, i8* %27, align 1
  %10578 = icmp eq i32 %10562, 0
  %10579 = zext i1 %10578 to i8
  store i8 %10579, i8* %30, align 1
  %10580 = lshr i32 %10562, 31
  %10581 = trunc i32 %10580 to i8
  store i8 %10581, i8* %33, align 1
  %10582 = lshr i32 %10555, 31
  %10583 = lshr i32 %10561, 31
  %10584 = xor i32 %10580, %10582
  %10585 = xor i32 %10580, %10583
  %10586 = add nuw nsw i32 %10584, %10585
  %10587 = icmp eq i32 %10586, 2
  %10588 = zext i1 %10587 to i8
  store i8 %10588, i8* %39, align 1
  %10589 = add i64 %10494, 41
  store i64 %10589, i64* %3, align 8
  store i32 %10562, i32* %10560, align 4
  %10590 = load i64, i64* %RBP.i, align 8
  %10591 = add i64 %10590, -8
  %10592 = load i64, i64* %3, align 8
  %10593 = add i64 %10592, 4
  store i64 %10593, i64* %3, align 8
  %10594 = inttoptr i64 %10591 to i64*
  %10595 = load i64, i64* %10594, align 8
  %10596 = add i64 %10595, 51640
  store i64 %10596, i64* %RAX.i11582.pre-phi, align 8
  %10597 = icmp ugt i64 %10595, -51641
  %10598 = zext i1 %10597 to i8
  store i8 %10598, i8* %14, align 1
  %10599 = trunc i64 %10596 to i32
  %10600 = and i32 %10599, 255
  %10601 = tail call i32 @llvm.ctpop.i32(i32 %10600)
  %10602 = trunc i32 %10601 to i8
  %10603 = and i8 %10602, 1
  %10604 = xor i8 %10603, 1
  store i8 %10604, i8* %21, align 1
  %10605 = xor i64 %10595, 16
  %10606 = xor i64 %10605, %10596
  %10607 = lshr i64 %10606, 4
  %10608 = trunc i64 %10607 to i8
  %10609 = and i8 %10608, 1
  store i8 %10609, i8* %27, align 1
  %10610 = icmp eq i64 %10596, 0
  %10611 = zext i1 %10610 to i8
  store i8 %10611, i8* %30, align 1
  %10612 = lshr i64 %10596, 63
  %10613 = trunc i64 %10612 to i8
  store i8 %10613, i8* %33, align 1
  %10614 = lshr i64 %10595, 63
  %10615 = xor i64 %10612, %10614
  %10616 = add nuw nsw i64 %10615, %10612
  %10617 = icmp eq i64 %10616, 2
  %10618 = zext i1 %10617 to i8
  store i8 %10618, i8* %39, align 1
  %10619 = add i64 %10590, -150
  %10620 = add i64 %10592, 17
  store i64 %10620, i64* %3, align 8
  %10621 = inttoptr i64 %10619 to i16*
  %10622 = load i16, i16* %10621, align 2
  %10623 = zext i16 %10622 to i64
  store i64 %10623, i64* %RCX.i11580, align 8
  %10624 = zext i16 %10622 to i64
  %10625 = shl nuw nsw i64 %10624, 4
  store i64 %10625, i64* %573, align 8
  %10626 = add i64 %10625, %10596
  store i64 %10626, i64* %RAX.i11582.pre-phi, align 8
  %10627 = icmp ult i64 %10626, %10596
  %10628 = icmp ult i64 %10626, %10625
  %10629 = or i1 %10627, %10628
  %10630 = zext i1 %10629 to i8
  store i8 %10630, i8* %14, align 1
  %10631 = trunc i64 %10626 to i32
  %10632 = and i32 %10631, 255
  %10633 = tail call i32 @llvm.ctpop.i32(i32 %10632)
  %10634 = trunc i32 %10633 to i8
  %10635 = and i8 %10634, 1
  %10636 = xor i8 %10635, 1
  store i8 %10636, i8* %21, align 1
  %10637 = xor i64 %10625, %10596
  %10638 = xor i64 %10637, %10626
  %10639 = lshr i64 %10638, 4
  %10640 = trunc i64 %10639 to i8
  %10641 = and i8 %10640, 1
  store i8 %10641, i8* %27, align 1
  %10642 = icmp eq i64 %10626, 0
  %10643 = zext i1 %10642 to i8
  store i8 %10643, i8* %30, align 1
  %10644 = lshr i64 %10626, 63
  %10645 = trunc i64 %10644 to i8
  store i8 %10645, i8* %33, align 1
  %10646 = xor i64 %10644, %10612
  %10647 = add nuw nsw i64 %10646, %10644
  %10648 = icmp eq i64 %10647, 2
  %10649 = zext i1 %10648 to i8
  store i8 %10649, i8* %39, align 1
  %10650 = add i64 %10626, 8
  %10651 = add i64 %10592, 29
  store i64 %10651, i64* %3, align 8
  %10652 = inttoptr i64 %10650 to i32*
  %10653 = load i32, i32* %10652, align 4
  %10654 = zext i32 %10653 to i64
  store i64 %10654, i64* %RCX.i11580, align 8
  %10655 = load i64, i64* %RBP.i, align 8
  %10656 = add i64 %10655, -148
  %10657 = add i64 %10592, 35
  store i64 %10657, i64* %3, align 8
  %10658 = inttoptr i64 %10656 to i32*
  %10659 = load i32, i32* %10658, align 4
  %10660 = add i32 %10659, %10653
  %10661 = zext i32 %10660 to i64
  store i64 %10661, i64* %RCX.i11580, align 8
  %10662 = icmp ult i32 %10660, %10653
  %10663 = icmp ult i32 %10660, %10659
  %10664 = or i1 %10662, %10663
  %10665 = zext i1 %10664 to i8
  store i8 %10665, i8* %14, align 1
  %10666 = and i32 %10660, 255
  %10667 = tail call i32 @llvm.ctpop.i32(i32 %10666)
  %10668 = trunc i32 %10667 to i8
  %10669 = and i8 %10668, 1
  %10670 = xor i8 %10669, 1
  store i8 %10670, i8* %21, align 1
  %10671 = xor i32 %10659, %10653
  %10672 = xor i32 %10671, %10660
  %10673 = lshr i32 %10672, 4
  %10674 = trunc i32 %10673 to i8
  %10675 = and i8 %10674, 1
  store i8 %10675, i8* %27, align 1
  %10676 = icmp eq i32 %10660, 0
  %10677 = zext i1 %10676 to i8
  store i8 %10677, i8* %30, align 1
  %10678 = lshr i32 %10660, 31
  %10679 = trunc i32 %10678 to i8
  store i8 %10679, i8* %33, align 1
  %10680 = lshr i32 %10653, 31
  %10681 = lshr i32 %10659, 31
  %10682 = xor i32 %10678, %10680
  %10683 = xor i32 %10678, %10681
  %10684 = add nuw nsw i32 %10682, %10683
  %10685 = icmp eq i32 %10684, 2
  %10686 = zext i1 %10685 to i8
  store i8 %10686, i8* %39, align 1
  %10687 = add i64 %10592, 41
  store i64 %10687, i64* %3, align 8
  store i32 %10660, i32* %10658, align 4
  %10688 = load i64, i64* %RBP.i, align 8
  %10689 = add i64 %10688, -120
  %10690 = load i64, i64* %3, align 8
  %10691 = add i64 %10690, 4
  store i64 %10691, i64* %3, align 8
  %10692 = inttoptr i64 %10689 to i64*
  %10693 = load i64, i64* %10692, align 8
  store i64 %10693, i64* %RAX.i11582.pre-phi, align 8
  %10694 = add i64 %10688, -28
  %10695 = add i64 %10690, 7
  store i64 %10695, i64* %3, align 8
  %10696 = inttoptr i64 %10694 to i32*
  %10697 = load i32, i32* %10696, align 4
  %10698 = add i32 %10697, 24
  %10699 = zext i32 %10698 to i64
  store i64 %10699, i64* %RCX.i11580, align 8
  %10700 = icmp ugt i32 %10697, -25
  %10701 = zext i1 %10700 to i8
  store i8 %10701, i8* %14, align 1
  %10702 = and i32 %10698, 255
  %10703 = tail call i32 @llvm.ctpop.i32(i32 %10702)
  %10704 = trunc i32 %10703 to i8
  %10705 = and i8 %10704, 1
  %10706 = xor i8 %10705, 1
  store i8 %10706, i8* %21, align 1
  %10707 = xor i32 %10697, 16
  %10708 = xor i32 %10707, %10698
  %10709 = lshr i32 %10708, 4
  %10710 = trunc i32 %10709 to i8
  %10711 = and i8 %10710, 1
  store i8 %10711, i8* %27, align 1
  %10712 = icmp eq i32 %10698, 0
  %10713 = zext i1 %10712 to i8
  store i8 %10713, i8* %30, align 1
  %10714 = lshr i32 %10698, 31
  %10715 = trunc i32 %10714 to i8
  store i8 %10715, i8* %33, align 1
  %10716 = lshr i32 %10697, 31
  %10717 = xor i32 %10714, %10716
  %10718 = add nuw nsw i32 %10717, %10714
  %10719 = icmp eq i32 %10718, 2
  %10720 = zext i1 %10719 to i8
  store i8 %10720, i8* %39, align 1
  %10721 = sext i32 %10698 to i64
  store i64 %10721, i64* %573, align 8
  %10722 = shl nsw i64 %10721, 1
  %10723 = add i64 %10693, %10722
  %10724 = add i64 %10690, 17
  store i64 %10724, i64* %3, align 8
  %10725 = inttoptr i64 %10723 to i16*
  %10726 = load i16, i16* %10725, align 2
  store i16 %10726, i16* %SI.i, align 2
  %10727 = add i64 %10688, -150
  %10728 = add i64 %10690, 24
  store i64 %10728, i64* %3, align 8
  %10729 = inttoptr i64 %10727 to i16*
  store i16 %10726, i16* %10729, align 2
  %10730 = load i64, i64* %RBP.i, align 8
  %10731 = add i64 %10730, -8
  %10732 = load i64, i64* %3, align 8
  %10733 = add i64 %10732, 4
  store i64 %10733, i64* %3, align 8
  %10734 = inttoptr i64 %10731 to i64*
  %10735 = load i64, i64* %10734, align 8
  %10736 = add i64 %10735, 51640
  store i64 %10736, i64* %RAX.i11582.pre-phi, align 8
  %10737 = icmp ugt i64 %10735, -51641
  %10738 = zext i1 %10737 to i8
  store i8 %10738, i8* %14, align 1
  %10739 = trunc i64 %10736 to i32
  %10740 = and i32 %10739, 255
  %10741 = tail call i32 @llvm.ctpop.i32(i32 %10740)
  %10742 = trunc i32 %10741 to i8
  %10743 = and i8 %10742, 1
  %10744 = xor i8 %10743, 1
  store i8 %10744, i8* %21, align 1
  %10745 = xor i64 %10735, 16
  %10746 = xor i64 %10745, %10736
  %10747 = lshr i64 %10746, 4
  %10748 = trunc i64 %10747 to i8
  %10749 = and i8 %10748, 1
  store i8 %10749, i8* %27, align 1
  %10750 = icmp eq i64 %10736, 0
  %10751 = zext i1 %10750 to i8
  store i8 %10751, i8* %30, align 1
  %10752 = lshr i64 %10736, 63
  %10753 = trunc i64 %10752 to i8
  store i8 %10753, i8* %33, align 1
  %10754 = lshr i64 %10735, 63
  %10755 = xor i64 %10752, %10754
  %10756 = add nuw nsw i64 %10755, %10752
  %10757 = icmp eq i64 %10756, 2
  %10758 = zext i1 %10757 to i8
  store i8 %10758, i8* %39, align 1
  %10759 = add i64 %10730, -150
  %10760 = add i64 %10732, 17
  store i64 %10760, i64* %3, align 8
  %10761 = inttoptr i64 %10759 to i16*
  %10762 = load i16, i16* %10761, align 2
  %10763 = zext i16 %10762 to i64
  store i64 %10763, i64* %RCX.i11580, align 8
  %10764 = zext i16 %10762 to i64
  %10765 = shl nuw nsw i64 %10764, 4
  store i64 %10765, i64* %573, align 8
  %10766 = add i64 %10765, %10736
  store i64 %10766, i64* %RAX.i11582.pre-phi, align 8
  %10767 = icmp ult i64 %10766, %10736
  %10768 = icmp ult i64 %10766, %10765
  %10769 = or i1 %10767, %10768
  %10770 = zext i1 %10769 to i8
  store i8 %10770, i8* %14, align 1
  %10771 = trunc i64 %10766 to i32
  %10772 = and i32 %10771, 255
  %10773 = tail call i32 @llvm.ctpop.i32(i32 %10772)
  %10774 = trunc i32 %10773 to i8
  %10775 = and i8 %10774, 1
  %10776 = xor i8 %10775, 1
  store i8 %10776, i8* %21, align 1
  %10777 = xor i64 %10765, %10736
  %10778 = xor i64 %10777, %10766
  %10779 = lshr i64 %10778, 4
  %10780 = trunc i64 %10779 to i8
  %10781 = and i8 %10780, 1
  store i8 %10781, i8* %27, align 1
  %10782 = icmp eq i64 %10766, 0
  %10783 = zext i1 %10782 to i8
  store i8 %10783, i8* %30, align 1
  %10784 = lshr i64 %10766, 63
  %10785 = trunc i64 %10784 to i8
  store i8 %10785, i8* %33, align 1
  %10786 = xor i64 %10784, %10752
  %10787 = add nuw nsw i64 %10786, %10784
  %10788 = icmp eq i64 %10787, 2
  %10789 = zext i1 %10788 to i8
  store i8 %10789, i8* %39, align 1
  %10790 = inttoptr i64 %10766 to i32*
  %10791 = add i64 %10732, 28
  store i64 %10791, i64* %3, align 8
  %10792 = load i32, i32* %10790, align 4
  %10793 = zext i32 %10792 to i64
  store i64 %10793, i64* %RCX.i11580, align 8
  %10794 = load i64, i64* %RBP.i, align 8
  %10795 = add i64 %10794, -140
  %10796 = add i64 %10732, 34
  store i64 %10796, i64* %3, align 8
  %10797 = inttoptr i64 %10795 to i32*
  %10798 = load i32, i32* %10797, align 4
  %10799 = add i32 %10798, %10792
  %10800 = zext i32 %10799 to i64
  store i64 %10800, i64* %RCX.i11580, align 8
  %10801 = icmp ult i32 %10799, %10792
  %10802 = icmp ult i32 %10799, %10798
  %10803 = or i1 %10801, %10802
  %10804 = zext i1 %10803 to i8
  store i8 %10804, i8* %14, align 1
  %10805 = and i32 %10799, 255
  %10806 = tail call i32 @llvm.ctpop.i32(i32 %10805)
  %10807 = trunc i32 %10806 to i8
  %10808 = and i8 %10807, 1
  %10809 = xor i8 %10808, 1
  store i8 %10809, i8* %21, align 1
  %10810 = xor i32 %10798, %10792
  %10811 = xor i32 %10810, %10799
  %10812 = lshr i32 %10811, 4
  %10813 = trunc i32 %10812 to i8
  %10814 = and i8 %10813, 1
  store i8 %10814, i8* %27, align 1
  %10815 = icmp eq i32 %10799, 0
  %10816 = zext i1 %10815 to i8
  store i8 %10816, i8* %30, align 1
  %10817 = lshr i32 %10799, 31
  %10818 = trunc i32 %10817 to i8
  store i8 %10818, i8* %33, align 1
  %10819 = lshr i32 %10792, 31
  %10820 = lshr i32 %10798, 31
  %10821 = xor i32 %10817, %10819
  %10822 = xor i32 %10817, %10820
  %10823 = add nuw nsw i32 %10821, %10822
  %10824 = icmp eq i32 %10823, 2
  %10825 = zext i1 %10824 to i8
  store i8 %10825, i8* %39, align 1
  %10826 = add i64 %10732, 40
  store i64 %10826, i64* %3, align 8
  store i32 %10799, i32* %10797, align 4
  %10827 = load i64, i64* %RBP.i, align 8
  %10828 = add i64 %10827, -8
  %10829 = load i64, i64* %3, align 8
  %10830 = add i64 %10829, 4
  store i64 %10830, i64* %3, align 8
  %10831 = inttoptr i64 %10828 to i64*
  %10832 = load i64, i64* %10831, align 8
  %10833 = add i64 %10832, 51640
  store i64 %10833, i64* %RAX.i11582.pre-phi, align 8
  %10834 = icmp ugt i64 %10832, -51641
  %10835 = zext i1 %10834 to i8
  store i8 %10835, i8* %14, align 1
  %10836 = trunc i64 %10833 to i32
  %10837 = and i32 %10836, 255
  %10838 = tail call i32 @llvm.ctpop.i32(i32 %10837)
  %10839 = trunc i32 %10838 to i8
  %10840 = and i8 %10839, 1
  %10841 = xor i8 %10840, 1
  store i8 %10841, i8* %21, align 1
  %10842 = xor i64 %10832, 16
  %10843 = xor i64 %10842, %10833
  %10844 = lshr i64 %10843, 4
  %10845 = trunc i64 %10844 to i8
  %10846 = and i8 %10845, 1
  store i8 %10846, i8* %27, align 1
  %10847 = icmp eq i64 %10833, 0
  %10848 = zext i1 %10847 to i8
  store i8 %10848, i8* %30, align 1
  %10849 = lshr i64 %10833, 63
  %10850 = trunc i64 %10849 to i8
  store i8 %10850, i8* %33, align 1
  %10851 = lshr i64 %10832, 63
  %10852 = xor i64 %10849, %10851
  %10853 = add nuw nsw i64 %10852, %10849
  %10854 = icmp eq i64 %10853, 2
  %10855 = zext i1 %10854 to i8
  store i8 %10855, i8* %39, align 1
  %10856 = add i64 %10827, -150
  %10857 = add i64 %10829, 17
  store i64 %10857, i64* %3, align 8
  %10858 = inttoptr i64 %10856 to i16*
  %10859 = load i16, i16* %10858, align 2
  %10860 = zext i16 %10859 to i64
  store i64 %10860, i64* %RCX.i11580, align 8
  %10861 = zext i16 %10859 to i64
  %10862 = shl nuw nsw i64 %10861, 4
  store i64 %10862, i64* %573, align 8
  %10863 = add i64 %10862, %10833
  store i64 %10863, i64* %RAX.i11582.pre-phi, align 8
  %10864 = icmp ult i64 %10863, %10833
  %10865 = icmp ult i64 %10863, %10862
  %10866 = or i1 %10864, %10865
  %10867 = zext i1 %10866 to i8
  store i8 %10867, i8* %14, align 1
  %10868 = trunc i64 %10863 to i32
  %10869 = and i32 %10868, 255
  %10870 = tail call i32 @llvm.ctpop.i32(i32 %10869)
  %10871 = trunc i32 %10870 to i8
  %10872 = and i8 %10871, 1
  %10873 = xor i8 %10872, 1
  store i8 %10873, i8* %21, align 1
  %10874 = xor i64 %10862, %10833
  %10875 = xor i64 %10874, %10863
  %10876 = lshr i64 %10875, 4
  %10877 = trunc i64 %10876 to i8
  %10878 = and i8 %10877, 1
  store i8 %10878, i8* %27, align 1
  %10879 = icmp eq i64 %10863, 0
  %10880 = zext i1 %10879 to i8
  store i8 %10880, i8* %30, align 1
  %10881 = lshr i64 %10863, 63
  %10882 = trunc i64 %10881 to i8
  store i8 %10882, i8* %33, align 1
  %10883 = xor i64 %10881, %10849
  %10884 = add nuw nsw i64 %10883, %10881
  %10885 = icmp eq i64 %10884, 2
  %10886 = zext i1 %10885 to i8
  store i8 %10886, i8* %39, align 1
  %10887 = add i64 %10863, 4
  %10888 = add i64 %10829, 29
  store i64 %10888, i64* %3, align 8
  %10889 = inttoptr i64 %10887 to i32*
  %10890 = load i32, i32* %10889, align 4
  %10891 = zext i32 %10890 to i64
  store i64 %10891, i64* %RCX.i11580, align 8
  %10892 = load i64, i64* %RBP.i, align 8
  %10893 = add i64 %10892, -144
  %10894 = add i64 %10829, 35
  store i64 %10894, i64* %3, align 8
  %10895 = inttoptr i64 %10893 to i32*
  %10896 = load i32, i32* %10895, align 4
  %10897 = add i32 %10896, %10890
  %10898 = zext i32 %10897 to i64
  store i64 %10898, i64* %RCX.i11580, align 8
  %10899 = icmp ult i32 %10897, %10890
  %10900 = icmp ult i32 %10897, %10896
  %10901 = or i1 %10899, %10900
  %10902 = zext i1 %10901 to i8
  store i8 %10902, i8* %14, align 1
  %10903 = and i32 %10897, 255
  %10904 = tail call i32 @llvm.ctpop.i32(i32 %10903)
  %10905 = trunc i32 %10904 to i8
  %10906 = and i8 %10905, 1
  %10907 = xor i8 %10906, 1
  store i8 %10907, i8* %21, align 1
  %10908 = xor i32 %10896, %10890
  %10909 = xor i32 %10908, %10897
  %10910 = lshr i32 %10909, 4
  %10911 = trunc i32 %10910 to i8
  %10912 = and i8 %10911, 1
  store i8 %10912, i8* %27, align 1
  %10913 = icmp eq i32 %10897, 0
  %10914 = zext i1 %10913 to i8
  store i8 %10914, i8* %30, align 1
  %10915 = lshr i32 %10897, 31
  %10916 = trunc i32 %10915 to i8
  store i8 %10916, i8* %33, align 1
  %10917 = lshr i32 %10890, 31
  %10918 = lshr i32 %10896, 31
  %10919 = xor i32 %10915, %10917
  %10920 = xor i32 %10915, %10918
  %10921 = add nuw nsw i32 %10919, %10920
  %10922 = icmp eq i32 %10921, 2
  %10923 = zext i1 %10922 to i8
  store i8 %10923, i8* %39, align 1
  %10924 = add i64 %10829, 41
  store i64 %10924, i64* %3, align 8
  store i32 %10897, i32* %10895, align 4
  %10925 = load i64, i64* %RBP.i, align 8
  %10926 = add i64 %10925, -8
  %10927 = load i64, i64* %3, align 8
  %10928 = add i64 %10927, 4
  store i64 %10928, i64* %3, align 8
  %10929 = inttoptr i64 %10926 to i64*
  %10930 = load i64, i64* %10929, align 8
  %10931 = add i64 %10930, 51640
  store i64 %10931, i64* %RAX.i11582.pre-phi, align 8
  %10932 = icmp ugt i64 %10930, -51641
  %10933 = zext i1 %10932 to i8
  store i8 %10933, i8* %14, align 1
  %10934 = trunc i64 %10931 to i32
  %10935 = and i32 %10934, 255
  %10936 = tail call i32 @llvm.ctpop.i32(i32 %10935)
  %10937 = trunc i32 %10936 to i8
  %10938 = and i8 %10937, 1
  %10939 = xor i8 %10938, 1
  store i8 %10939, i8* %21, align 1
  %10940 = xor i64 %10930, 16
  %10941 = xor i64 %10940, %10931
  %10942 = lshr i64 %10941, 4
  %10943 = trunc i64 %10942 to i8
  %10944 = and i8 %10943, 1
  store i8 %10944, i8* %27, align 1
  %10945 = icmp eq i64 %10931, 0
  %10946 = zext i1 %10945 to i8
  store i8 %10946, i8* %30, align 1
  %10947 = lshr i64 %10931, 63
  %10948 = trunc i64 %10947 to i8
  store i8 %10948, i8* %33, align 1
  %10949 = lshr i64 %10930, 63
  %10950 = xor i64 %10947, %10949
  %10951 = add nuw nsw i64 %10950, %10947
  %10952 = icmp eq i64 %10951, 2
  %10953 = zext i1 %10952 to i8
  store i8 %10953, i8* %39, align 1
  %10954 = add i64 %10925, -150
  %10955 = add i64 %10927, 17
  store i64 %10955, i64* %3, align 8
  %10956 = inttoptr i64 %10954 to i16*
  %10957 = load i16, i16* %10956, align 2
  %10958 = zext i16 %10957 to i64
  store i64 %10958, i64* %RCX.i11580, align 8
  %10959 = zext i16 %10957 to i64
  %10960 = shl nuw nsw i64 %10959, 4
  store i64 %10960, i64* %573, align 8
  %10961 = add i64 %10960, %10931
  store i64 %10961, i64* %RAX.i11582.pre-phi, align 8
  %10962 = icmp ult i64 %10961, %10931
  %10963 = icmp ult i64 %10961, %10960
  %10964 = or i1 %10962, %10963
  %10965 = zext i1 %10964 to i8
  store i8 %10965, i8* %14, align 1
  %10966 = trunc i64 %10961 to i32
  %10967 = and i32 %10966, 255
  %10968 = tail call i32 @llvm.ctpop.i32(i32 %10967)
  %10969 = trunc i32 %10968 to i8
  %10970 = and i8 %10969, 1
  %10971 = xor i8 %10970, 1
  store i8 %10971, i8* %21, align 1
  %10972 = xor i64 %10960, %10931
  %10973 = xor i64 %10972, %10961
  %10974 = lshr i64 %10973, 4
  %10975 = trunc i64 %10974 to i8
  %10976 = and i8 %10975, 1
  store i8 %10976, i8* %27, align 1
  %10977 = icmp eq i64 %10961, 0
  %10978 = zext i1 %10977 to i8
  store i8 %10978, i8* %30, align 1
  %10979 = lshr i64 %10961, 63
  %10980 = trunc i64 %10979 to i8
  store i8 %10980, i8* %33, align 1
  %10981 = xor i64 %10979, %10947
  %10982 = add nuw nsw i64 %10981, %10979
  %10983 = icmp eq i64 %10982, 2
  %10984 = zext i1 %10983 to i8
  store i8 %10984, i8* %39, align 1
  %10985 = add i64 %10961, 8
  %10986 = add i64 %10927, 29
  store i64 %10986, i64* %3, align 8
  %10987 = inttoptr i64 %10985 to i32*
  %10988 = load i32, i32* %10987, align 4
  %10989 = zext i32 %10988 to i64
  store i64 %10989, i64* %RCX.i11580, align 8
  %10990 = load i64, i64* %RBP.i, align 8
  %10991 = add i64 %10990, -148
  %10992 = add i64 %10927, 35
  store i64 %10992, i64* %3, align 8
  %10993 = inttoptr i64 %10991 to i32*
  %10994 = load i32, i32* %10993, align 4
  %10995 = add i32 %10994, %10988
  %10996 = zext i32 %10995 to i64
  store i64 %10996, i64* %RCX.i11580, align 8
  %10997 = icmp ult i32 %10995, %10988
  %10998 = icmp ult i32 %10995, %10994
  %10999 = or i1 %10997, %10998
  %11000 = zext i1 %10999 to i8
  store i8 %11000, i8* %14, align 1
  %11001 = and i32 %10995, 255
  %11002 = tail call i32 @llvm.ctpop.i32(i32 %11001)
  %11003 = trunc i32 %11002 to i8
  %11004 = and i8 %11003, 1
  %11005 = xor i8 %11004, 1
  store i8 %11005, i8* %21, align 1
  %11006 = xor i32 %10994, %10988
  %11007 = xor i32 %11006, %10995
  %11008 = lshr i32 %11007, 4
  %11009 = trunc i32 %11008 to i8
  %11010 = and i8 %11009, 1
  store i8 %11010, i8* %27, align 1
  %11011 = icmp eq i32 %10995, 0
  %11012 = zext i1 %11011 to i8
  store i8 %11012, i8* %30, align 1
  %11013 = lshr i32 %10995, 31
  %11014 = trunc i32 %11013 to i8
  store i8 %11014, i8* %33, align 1
  %11015 = lshr i32 %10988, 31
  %11016 = lshr i32 %10994, 31
  %11017 = xor i32 %11013, %11015
  %11018 = xor i32 %11013, %11016
  %11019 = add nuw nsw i32 %11017, %11018
  %11020 = icmp eq i32 %11019, 2
  %11021 = zext i1 %11020 to i8
  store i8 %11021, i8* %39, align 1
  %11022 = add i64 %10927, 41
  store i64 %11022, i64* %3, align 8
  store i32 %10995, i32* %10993, align 4
  %11023 = load i64, i64* %RBP.i, align 8
  %11024 = add i64 %11023, -120
  %11025 = load i64, i64* %3, align 8
  %11026 = add i64 %11025, 4
  store i64 %11026, i64* %3, align 8
  %11027 = inttoptr i64 %11024 to i64*
  %11028 = load i64, i64* %11027, align 8
  store i64 %11028, i64* %RAX.i11582.pre-phi, align 8
  %11029 = add i64 %11023, -28
  %11030 = add i64 %11025, 7
  store i64 %11030, i64* %3, align 8
  %11031 = inttoptr i64 %11029 to i32*
  %11032 = load i32, i32* %11031, align 4
  %11033 = add i32 %11032, 25
  %11034 = zext i32 %11033 to i64
  store i64 %11034, i64* %RCX.i11580, align 8
  %11035 = icmp ugt i32 %11032, -26
  %11036 = zext i1 %11035 to i8
  store i8 %11036, i8* %14, align 1
  %11037 = and i32 %11033, 255
  %11038 = tail call i32 @llvm.ctpop.i32(i32 %11037)
  %11039 = trunc i32 %11038 to i8
  %11040 = and i8 %11039, 1
  %11041 = xor i8 %11040, 1
  store i8 %11041, i8* %21, align 1
  %11042 = xor i32 %11032, 16
  %11043 = xor i32 %11042, %11033
  %11044 = lshr i32 %11043, 4
  %11045 = trunc i32 %11044 to i8
  %11046 = and i8 %11045, 1
  store i8 %11046, i8* %27, align 1
  %11047 = icmp eq i32 %11033, 0
  %11048 = zext i1 %11047 to i8
  store i8 %11048, i8* %30, align 1
  %11049 = lshr i32 %11033, 31
  %11050 = trunc i32 %11049 to i8
  store i8 %11050, i8* %33, align 1
  %11051 = lshr i32 %11032, 31
  %11052 = xor i32 %11049, %11051
  %11053 = add nuw nsw i32 %11052, %11049
  %11054 = icmp eq i32 %11053, 2
  %11055 = zext i1 %11054 to i8
  store i8 %11055, i8* %39, align 1
  %11056 = sext i32 %11033 to i64
  store i64 %11056, i64* %573, align 8
  %11057 = shl nsw i64 %11056, 1
  %11058 = add i64 %11028, %11057
  %11059 = add i64 %11025, 17
  store i64 %11059, i64* %3, align 8
  %11060 = inttoptr i64 %11058 to i16*
  %11061 = load i16, i16* %11060, align 2
  store i16 %11061, i16* %SI.i, align 2
  %11062 = add i64 %11023, -150
  %11063 = add i64 %11025, 24
  store i64 %11063, i64* %3, align 8
  %11064 = inttoptr i64 %11062 to i16*
  store i16 %11061, i16* %11064, align 2
  %11065 = load i64, i64* %RBP.i, align 8
  %11066 = add i64 %11065, -8
  %11067 = load i64, i64* %3, align 8
  %11068 = add i64 %11067, 4
  store i64 %11068, i64* %3, align 8
  %11069 = inttoptr i64 %11066 to i64*
  %11070 = load i64, i64* %11069, align 8
  %11071 = add i64 %11070, 51640
  store i64 %11071, i64* %RAX.i11582.pre-phi, align 8
  %11072 = icmp ugt i64 %11070, -51641
  %11073 = zext i1 %11072 to i8
  store i8 %11073, i8* %14, align 1
  %11074 = trunc i64 %11071 to i32
  %11075 = and i32 %11074, 255
  %11076 = tail call i32 @llvm.ctpop.i32(i32 %11075)
  %11077 = trunc i32 %11076 to i8
  %11078 = and i8 %11077, 1
  %11079 = xor i8 %11078, 1
  store i8 %11079, i8* %21, align 1
  %11080 = xor i64 %11070, 16
  %11081 = xor i64 %11080, %11071
  %11082 = lshr i64 %11081, 4
  %11083 = trunc i64 %11082 to i8
  %11084 = and i8 %11083, 1
  store i8 %11084, i8* %27, align 1
  %11085 = icmp eq i64 %11071, 0
  %11086 = zext i1 %11085 to i8
  store i8 %11086, i8* %30, align 1
  %11087 = lshr i64 %11071, 63
  %11088 = trunc i64 %11087 to i8
  store i8 %11088, i8* %33, align 1
  %11089 = lshr i64 %11070, 63
  %11090 = xor i64 %11087, %11089
  %11091 = add nuw nsw i64 %11090, %11087
  %11092 = icmp eq i64 %11091, 2
  %11093 = zext i1 %11092 to i8
  store i8 %11093, i8* %39, align 1
  %11094 = add i64 %11065, -150
  %11095 = add i64 %11067, 17
  store i64 %11095, i64* %3, align 8
  %11096 = inttoptr i64 %11094 to i16*
  %11097 = load i16, i16* %11096, align 2
  %11098 = zext i16 %11097 to i64
  store i64 %11098, i64* %RCX.i11580, align 8
  %11099 = zext i16 %11097 to i64
  %11100 = shl nuw nsw i64 %11099, 4
  store i64 %11100, i64* %573, align 8
  %11101 = add i64 %11100, %11071
  store i64 %11101, i64* %RAX.i11582.pre-phi, align 8
  %11102 = icmp ult i64 %11101, %11071
  %11103 = icmp ult i64 %11101, %11100
  %11104 = or i1 %11102, %11103
  %11105 = zext i1 %11104 to i8
  store i8 %11105, i8* %14, align 1
  %11106 = trunc i64 %11101 to i32
  %11107 = and i32 %11106, 255
  %11108 = tail call i32 @llvm.ctpop.i32(i32 %11107)
  %11109 = trunc i32 %11108 to i8
  %11110 = and i8 %11109, 1
  %11111 = xor i8 %11110, 1
  store i8 %11111, i8* %21, align 1
  %11112 = xor i64 %11100, %11071
  %11113 = xor i64 %11112, %11101
  %11114 = lshr i64 %11113, 4
  %11115 = trunc i64 %11114 to i8
  %11116 = and i8 %11115, 1
  store i8 %11116, i8* %27, align 1
  %11117 = icmp eq i64 %11101, 0
  %11118 = zext i1 %11117 to i8
  store i8 %11118, i8* %30, align 1
  %11119 = lshr i64 %11101, 63
  %11120 = trunc i64 %11119 to i8
  store i8 %11120, i8* %33, align 1
  %11121 = xor i64 %11119, %11087
  %11122 = add nuw nsw i64 %11121, %11119
  %11123 = icmp eq i64 %11122, 2
  %11124 = zext i1 %11123 to i8
  store i8 %11124, i8* %39, align 1
  %11125 = inttoptr i64 %11101 to i32*
  %11126 = add i64 %11067, 28
  store i64 %11126, i64* %3, align 8
  %11127 = load i32, i32* %11125, align 4
  %11128 = zext i32 %11127 to i64
  store i64 %11128, i64* %RCX.i11580, align 8
  %11129 = load i64, i64* %RBP.i, align 8
  %11130 = add i64 %11129, -140
  %11131 = add i64 %11067, 34
  store i64 %11131, i64* %3, align 8
  %11132 = inttoptr i64 %11130 to i32*
  %11133 = load i32, i32* %11132, align 4
  %11134 = add i32 %11133, %11127
  %11135 = zext i32 %11134 to i64
  store i64 %11135, i64* %RCX.i11580, align 8
  %11136 = icmp ult i32 %11134, %11127
  %11137 = icmp ult i32 %11134, %11133
  %11138 = or i1 %11136, %11137
  %11139 = zext i1 %11138 to i8
  store i8 %11139, i8* %14, align 1
  %11140 = and i32 %11134, 255
  %11141 = tail call i32 @llvm.ctpop.i32(i32 %11140)
  %11142 = trunc i32 %11141 to i8
  %11143 = and i8 %11142, 1
  %11144 = xor i8 %11143, 1
  store i8 %11144, i8* %21, align 1
  %11145 = xor i32 %11133, %11127
  %11146 = xor i32 %11145, %11134
  %11147 = lshr i32 %11146, 4
  %11148 = trunc i32 %11147 to i8
  %11149 = and i8 %11148, 1
  store i8 %11149, i8* %27, align 1
  %11150 = icmp eq i32 %11134, 0
  %11151 = zext i1 %11150 to i8
  store i8 %11151, i8* %30, align 1
  %11152 = lshr i32 %11134, 31
  %11153 = trunc i32 %11152 to i8
  store i8 %11153, i8* %33, align 1
  %11154 = lshr i32 %11127, 31
  %11155 = lshr i32 %11133, 31
  %11156 = xor i32 %11152, %11154
  %11157 = xor i32 %11152, %11155
  %11158 = add nuw nsw i32 %11156, %11157
  %11159 = icmp eq i32 %11158, 2
  %11160 = zext i1 %11159 to i8
  store i8 %11160, i8* %39, align 1
  %11161 = add i64 %11067, 40
  store i64 %11161, i64* %3, align 8
  store i32 %11134, i32* %11132, align 4
  %11162 = load i64, i64* %RBP.i, align 8
  %11163 = add i64 %11162, -8
  %11164 = load i64, i64* %3, align 8
  %11165 = add i64 %11164, 4
  store i64 %11165, i64* %3, align 8
  %11166 = inttoptr i64 %11163 to i64*
  %11167 = load i64, i64* %11166, align 8
  %11168 = add i64 %11167, 51640
  store i64 %11168, i64* %RAX.i11582.pre-phi, align 8
  %11169 = icmp ugt i64 %11167, -51641
  %11170 = zext i1 %11169 to i8
  store i8 %11170, i8* %14, align 1
  %11171 = trunc i64 %11168 to i32
  %11172 = and i32 %11171, 255
  %11173 = tail call i32 @llvm.ctpop.i32(i32 %11172)
  %11174 = trunc i32 %11173 to i8
  %11175 = and i8 %11174, 1
  %11176 = xor i8 %11175, 1
  store i8 %11176, i8* %21, align 1
  %11177 = xor i64 %11167, 16
  %11178 = xor i64 %11177, %11168
  %11179 = lshr i64 %11178, 4
  %11180 = trunc i64 %11179 to i8
  %11181 = and i8 %11180, 1
  store i8 %11181, i8* %27, align 1
  %11182 = icmp eq i64 %11168, 0
  %11183 = zext i1 %11182 to i8
  store i8 %11183, i8* %30, align 1
  %11184 = lshr i64 %11168, 63
  %11185 = trunc i64 %11184 to i8
  store i8 %11185, i8* %33, align 1
  %11186 = lshr i64 %11167, 63
  %11187 = xor i64 %11184, %11186
  %11188 = add nuw nsw i64 %11187, %11184
  %11189 = icmp eq i64 %11188, 2
  %11190 = zext i1 %11189 to i8
  store i8 %11190, i8* %39, align 1
  %11191 = add i64 %11162, -150
  %11192 = add i64 %11164, 17
  store i64 %11192, i64* %3, align 8
  %11193 = inttoptr i64 %11191 to i16*
  %11194 = load i16, i16* %11193, align 2
  %11195 = zext i16 %11194 to i64
  store i64 %11195, i64* %RCX.i11580, align 8
  %11196 = zext i16 %11194 to i64
  %11197 = shl nuw nsw i64 %11196, 4
  store i64 %11197, i64* %573, align 8
  %11198 = add i64 %11197, %11168
  store i64 %11198, i64* %RAX.i11582.pre-phi, align 8
  %11199 = icmp ult i64 %11198, %11168
  %11200 = icmp ult i64 %11198, %11197
  %11201 = or i1 %11199, %11200
  %11202 = zext i1 %11201 to i8
  store i8 %11202, i8* %14, align 1
  %11203 = trunc i64 %11198 to i32
  %11204 = and i32 %11203, 255
  %11205 = tail call i32 @llvm.ctpop.i32(i32 %11204)
  %11206 = trunc i32 %11205 to i8
  %11207 = and i8 %11206, 1
  %11208 = xor i8 %11207, 1
  store i8 %11208, i8* %21, align 1
  %11209 = xor i64 %11197, %11168
  %11210 = xor i64 %11209, %11198
  %11211 = lshr i64 %11210, 4
  %11212 = trunc i64 %11211 to i8
  %11213 = and i8 %11212, 1
  store i8 %11213, i8* %27, align 1
  %11214 = icmp eq i64 %11198, 0
  %11215 = zext i1 %11214 to i8
  store i8 %11215, i8* %30, align 1
  %11216 = lshr i64 %11198, 63
  %11217 = trunc i64 %11216 to i8
  store i8 %11217, i8* %33, align 1
  %11218 = xor i64 %11216, %11184
  %11219 = add nuw nsw i64 %11218, %11216
  %11220 = icmp eq i64 %11219, 2
  %11221 = zext i1 %11220 to i8
  store i8 %11221, i8* %39, align 1
  %11222 = add i64 %11198, 4
  %11223 = add i64 %11164, 29
  store i64 %11223, i64* %3, align 8
  %11224 = inttoptr i64 %11222 to i32*
  %11225 = load i32, i32* %11224, align 4
  %11226 = zext i32 %11225 to i64
  store i64 %11226, i64* %RCX.i11580, align 8
  %11227 = load i64, i64* %RBP.i, align 8
  %11228 = add i64 %11227, -144
  %11229 = add i64 %11164, 35
  store i64 %11229, i64* %3, align 8
  %11230 = inttoptr i64 %11228 to i32*
  %11231 = load i32, i32* %11230, align 4
  %11232 = add i32 %11231, %11225
  %11233 = zext i32 %11232 to i64
  store i64 %11233, i64* %RCX.i11580, align 8
  %11234 = icmp ult i32 %11232, %11225
  %11235 = icmp ult i32 %11232, %11231
  %11236 = or i1 %11234, %11235
  %11237 = zext i1 %11236 to i8
  store i8 %11237, i8* %14, align 1
  %11238 = and i32 %11232, 255
  %11239 = tail call i32 @llvm.ctpop.i32(i32 %11238)
  %11240 = trunc i32 %11239 to i8
  %11241 = and i8 %11240, 1
  %11242 = xor i8 %11241, 1
  store i8 %11242, i8* %21, align 1
  %11243 = xor i32 %11231, %11225
  %11244 = xor i32 %11243, %11232
  %11245 = lshr i32 %11244, 4
  %11246 = trunc i32 %11245 to i8
  %11247 = and i8 %11246, 1
  store i8 %11247, i8* %27, align 1
  %11248 = icmp eq i32 %11232, 0
  %11249 = zext i1 %11248 to i8
  store i8 %11249, i8* %30, align 1
  %11250 = lshr i32 %11232, 31
  %11251 = trunc i32 %11250 to i8
  store i8 %11251, i8* %33, align 1
  %11252 = lshr i32 %11225, 31
  %11253 = lshr i32 %11231, 31
  %11254 = xor i32 %11250, %11252
  %11255 = xor i32 %11250, %11253
  %11256 = add nuw nsw i32 %11254, %11255
  %11257 = icmp eq i32 %11256, 2
  %11258 = zext i1 %11257 to i8
  store i8 %11258, i8* %39, align 1
  %11259 = add i64 %11164, 41
  store i64 %11259, i64* %3, align 8
  store i32 %11232, i32* %11230, align 4
  %11260 = load i64, i64* %RBP.i, align 8
  %11261 = add i64 %11260, -8
  %11262 = load i64, i64* %3, align 8
  %11263 = add i64 %11262, 4
  store i64 %11263, i64* %3, align 8
  %11264 = inttoptr i64 %11261 to i64*
  %11265 = load i64, i64* %11264, align 8
  %11266 = add i64 %11265, 51640
  store i64 %11266, i64* %RAX.i11582.pre-phi, align 8
  %11267 = icmp ugt i64 %11265, -51641
  %11268 = zext i1 %11267 to i8
  store i8 %11268, i8* %14, align 1
  %11269 = trunc i64 %11266 to i32
  %11270 = and i32 %11269, 255
  %11271 = tail call i32 @llvm.ctpop.i32(i32 %11270)
  %11272 = trunc i32 %11271 to i8
  %11273 = and i8 %11272, 1
  %11274 = xor i8 %11273, 1
  store i8 %11274, i8* %21, align 1
  %11275 = xor i64 %11265, 16
  %11276 = xor i64 %11275, %11266
  %11277 = lshr i64 %11276, 4
  %11278 = trunc i64 %11277 to i8
  %11279 = and i8 %11278, 1
  store i8 %11279, i8* %27, align 1
  %11280 = icmp eq i64 %11266, 0
  %11281 = zext i1 %11280 to i8
  store i8 %11281, i8* %30, align 1
  %11282 = lshr i64 %11266, 63
  %11283 = trunc i64 %11282 to i8
  store i8 %11283, i8* %33, align 1
  %11284 = lshr i64 %11265, 63
  %11285 = xor i64 %11282, %11284
  %11286 = add nuw nsw i64 %11285, %11282
  %11287 = icmp eq i64 %11286, 2
  %11288 = zext i1 %11287 to i8
  store i8 %11288, i8* %39, align 1
  %11289 = add i64 %11260, -150
  %11290 = add i64 %11262, 17
  store i64 %11290, i64* %3, align 8
  %11291 = inttoptr i64 %11289 to i16*
  %11292 = load i16, i16* %11291, align 2
  %11293 = zext i16 %11292 to i64
  store i64 %11293, i64* %RCX.i11580, align 8
  %11294 = zext i16 %11292 to i64
  %11295 = shl nuw nsw i64 %11294, 4
  store i64 %11295, i64* %573, align 8
  %11296 = add i64 %11295, %11266
  store i64 %11296, i64* %RAX.i11582.pre-phi, align 8
  %11297 = icmp ult i64 %11296, %11266
  %11298 = icmp ult i64 %11296, %11295
  %11299 = or i1 %11297, %11298
  %11300 = zext i1 %11299 to i8
  store i8 %11300, i8* %14, align 1
  %11301 = trunc i64 %11296 to i32
  %11302 = and i32 %11301, 255
  %11303 = tail call i32 @llvm.ctpop.i32(i32 %11302)
  %11304 = trunc i32 %11303 to i8
  %11305 = and i8 %11304, 1
  %11306 = xor i8 %11305, 1
  store i8 %11306, i8* %21, align 1
  %11307 = xor i64 %11295, %11266
  %11308 = xor i64 %11307, %11296
  %11309 = lshr i64 %11308, 4
  %11310 = trunc i64 %11309 to i8
  %11311 = and i8 %11310, 1
  store i8 %11311, i8* %27, align 1
  %11312 = icmp eq i64 %11296, 0
  %11313 = zext i1 %11312 to i8
  store i8 %11313, i8* %30, align 1
  %11314 = lshr i64 %11296, 63
  %11315 = trunc i64 %11314 to i8
  store i8 %11315, i8* %33, align 1
  %11316 = xor i64 %11314, %11282
  %11317 = add nuw nsw i64 %11316, %11314
  %11318 = icmp eq i64 %11317, 2
  %11319 = zext i1 %11318 to i8
  store i8 %11319, i8* %39, align 1
  %11320 = add i64 %11296, 8
  %11321 = add i64 %11262, 29
  store i64 %11321, i64* %3, align 8
  %11322 = inttoptr i64 %11320 to i32*
  %11323 = load i32, i32* %11322, align 4
  %11324 = zext i32 %11323 to i64
  store i64 %11324, i64* %RCX.i11580, align 8
  %11325 = load i64, i64* %RBP.i, align 8
  %11326 = add i64 %11325, -148
  %11327 = add i64 %11262, 35
  store i64 %11327, i64* %3, align 8
  %11328 = inttoptr i64 %11326 to i32*
  %11329 = load i32, i32* %11328, align 4
  %11330 = add i32 %11329, %11323
  %11331 = zext i32 %11330 to i64
  store i64 %11331, i64* %RCX.i11580, align 8
  %11332 = icmp ult i32 %11330, %11323
  %11333 = icmp ult i32 %11330, %11329
  %11334 = or i1 %11332, %11333
  %11335 = zext i1 %11334 to i8
  store i8 %11335, i8* %14, align 1
  %11336 = and i32 %11330, 255
  %11337 = tail call i32 @llvm.ctpop.i32(i32 %11336)
  %11338 = trunc i32 %11337 to i8
  %11339 = and i8 %11338, 1
  %11340 = xor i8 %11339, 1
  store i8 %11340, i8* %21, align 1
  %11341 = xor i32 %11329, %11323
  %11342 = xor i32 %11341, %11330
  %11343 = lshr i32 %11342, 4
  %11344 = trunc i32 %11343 to i8
  %11345 = and i8 %11344, 1
  store i8 %11345, i8* %27, align 1
  %11346 = icmp eq i32 %11330, 0
  %11347 = zext i1 %11346 to i8
  store i8 %11347, i8* %30, align 1
  %11348 = lshr i32 %11330, 31
  %11349 = trunc i32 %11348 to i8
  store i8 %11349, i8* %33, align 1
  %11350 = lshr i32 %11323, 31
  %11351 = lshr i32 %11329, 31
  %11352 = xor i32 %11348, %11350
  %11353 = xor i32 %11348, %11351
  %11354 = add nuw nsw i32 %11352, %11353
  %11355 = icmp eq i32 %11354, 2
  %11356 = zext i1 %11355 to i8
  store i8 %11356, i8* %39, align 1
  %11357 = add i64 %11262, 41
  store i64 %11357, i64* %3, align 8
  store i32 %11330, i32* %11328, align 4
  %11358 = load i64, i64* %RBP.i, align 8
  %11359 = add i64 %11358, -120
  %11360 = load i64, i64* %3, align 8
  %11361 = add i64 %11360, 4
  store i64 %11361, i64* %3, align 8
  %11362 = inttoptr i64 %11359 to i64*
  %11363 = load i64, i64* %11362, align 8
  store i64 %11363, i64* %RAX.i11582.pre-phi, align 8
  %11364 = add i64 %11358, -28
  %11365 = add i64 %11360, 7
  store i64 %11365, i64* %3, align 8
  %11366 = inttoptr i64 %11364 to i32*
  %11367 = load i32, i32* %11366, align 4
  %11368 = add i32 %11367, 26
  %11369 = zext i32 %11368 to i64
  store i64 %11369, i64* %RCX.i11580, align 8
  %11370 = icmp ugt i32 %11367, -27
  %11371 = zext i1 %11370 to i8
  store i8 %11371, i8* %14, align 1
  %11372 = and i32 %11368, 255
  %11373 = tail call i32 @llvm.ctpop.i32(i32 %11372)
  %11374 = trunc i32 %11373 to i8
  %11375 = and i8 %11374, 1
  %11376 = xor i8 %11375, 1
  store i8 %11376, i8* %21, align 1
  %11377 = xor i32 %11367, 16
  %11378 = xor i32 %11377, %11368
  %11379 = lshr i32 %11378, 4
  %11380 = trunc i32 %11379 to i8
  %11381 = and i8 %11380, 1
  store i8 %11381, i8* %27, align 1
  %11382 = icmp eq i32 %11368, 0
  %11383 = zext i1 %11382 to i8
  store i8 %11383, i8* %30, align 1
  %11384 = lshr i32 %11368, 31
  %11385 = trunc i32 %11384 to i8
  store i8 %11385, i8* %33, align 1
  %11386 = lshr i32 %11367, 31
  %11387 = xor i32 %11384, %11386
  %11388 = add nuw nsw i32 %11387, %11384
  %11389 = icmp eq i32 %11388, 2
  %11390 = zext i1 %11389 to i8
  store i8 %11390, i8* %39, align 1
  %11391 = sext i32 %11368 to i64
  store i64 %11391, i64* %573, align 8
  %11392 = shl nsw i64 %11391, 1
  %11393 = add i64 %11363, %11392
  %11394 = add i64 %11360, 17
  store i64 %11394, i64* %3, align 8
  %11395 = inttoptr i64 %11393 to i16*
  %11396 = load i16, i16* %11395, align 2
  store i16 %11396, i16* %SI.i, align 2
  %11397 = add i64 %11358, -150
  %11398 = add i64 %11360, 24
  store i64 %11398, i64* %3, align 8
  %11399 = inttoptr i64 %11397 to i16*
  store i16 %11396, i16* %11399, align 2
  %11400 = load i64, i64* %RBP.i, align 8
  %11401 = add i64 %11400, -8
  %11402 = load i64, i64* %3, align 8
  %11403 = add i64 %11402, 4
  store i64 %11403, i64* %3, align 8
  %11404 = inttoptr i64 %11401 to i64*
  %11405 = load i64, i64* %11404, align 8
  %11406 = add i64 %11405, 51640
  store i64 %11406, i64* %RAX.i11582.pre-phi, align 8
  %11407 = icmp ugt i64 %11405, -51641
  %11408 = zext i1 %11407 to i8
  store i8 %11408, i8* %14, align 1
  %11409 = trunc i64 %11406 to i32
  %11410 = and i32 %11409, 255
  %11411 = tail call i32 @llvm.ctpop.i32(i32 %11410)
  %11412 = trunc i32 %11411 to i8
  %11413 = and i8 %11412, 1
  %11414 = xor i8 %11413, 1
  store i8 %11414, i8* %21, align 1
  %11415 = xor i64 %11405, 16
  %11416 = xor i64 %11415, %11406
  %11417 = lshr i64 %11416, 4
  %11418 = trunc i64 %11417 to i8
  %11419 = and i8 %11418, 1
  store i8 %11419, i8* %27, align 1
  %11420 = icmp eq i64 %11406, 0
  %11421 = zext i1 %11420 to i8
  store i8 %11421, i8* %30, align 1
  %11422 = lshr i64 %11406, 63
  %11423 = trunc i64 %11422 to i8
  store i8 %11423, i8* %33, align 1
  %11424 = lshr i64 %11405, 63
  %11425 = xor i64 %11422, %11424
  %11426 = add nuw nsw i64 %11425, %11422
  %11427 = icmp eq i64 %11426, 2
  %11428 = zext i1 %11427 to i8
  store i8 %11428, i8* %39, align 1
  %11429 = add i64 %11400, -150
  %11430 = add i64 %11402, 17
  store i64 %11430, i64* %3, align 8
  %11431 = inttoptr i64 %11429 to i16*
  %11432 = load i16, i16* %11431, align 2
  %11433 = zext i16 %11432 to i64
  store i64 %11433, i64* %RCX.i11580, align 8
  %11434 = zext i16 %11432 to i64
  %11435 = shl nuw nsw i64 %11434, 4
  store i64 %11435, i64* %573, align 8
  %11436 = add i64 %11435, %11406
  store i64 %11436, i64* %RAX.i11582.pre-phi, align 8
  %11437 = icmp ult i64 %11436, %11406
  %11438 = icmp ult i64 %11436, %11435
  %11439 = or i1 %11437, %11438
  %11440 = zext i1 %11439 to i8
  store i8 %11440, i8* %14, align 1
  %11441 = trunc i64 %11436 to i32
  %11442 = and i32 %11441, 255
  %11443 = tail call i32 @llvm.ctpop.i32(i32 %11442)
  %11444 = trunc i32 %11443 to i8
  %11445 = and i8 %11444, 1
  %11446 = xor i8 %11445, 1
  store i8 %11446, i8* %21, align 1
  %11447 = xor i64 %11435, %11406
  %11448 = xor i64 %11447, %11436
  %11449 = lshr i64 %11448, 4
  %11450 = trunc i64 %11449 to i8
  %11451 = and i8 %11450, 1
  store i8 %11451, i8* %27, align 1
  %11452 = icmp eq i64 %11436, 0
  %11453 = zext i1 %11452 to i8
  store i8 %11453, i8* %30, align 1
  %11454 = lshr i64 %11436, 63
  %11455 = trunc i64 %11454 to i8
  store i8 %11455, i8* %33, align 1
  %11456 = xor i64 %11454, %11422
  %11457 = add nuw nsw i64 %11456, %11454
  %11458 = icmp eq i64 %11457, 2
  %11459 = zext i1 %11458 to i8
  store i8 %11459, i8* %39, align 1
  %11460 = inttoptr i64 %11436 to i32*
  %11461 = add i64 %11402, 28
  store i64 %11461, i64* %3, align 8
  %11462 = load i32, i32* %11460, align 4
  %11463 = zext i32 %11462 to i64
  store i64 %11463, i64* %RCX.i11580, align 8
  %11464 = load i64, i64* %RBP.i, align 8
  %11465 = add i64 %11464, -140
  %11466 = add i64 %11402, 34
  store i64 %11466, i64* %3, align 8
  %11467 = inttoptr i64 %11465 to i32*
  %11468 = load i32, i32* %11467, align 4
  %11469 = add i32 %11468, %11462
  %11470 = zext i32 %11469 to i64
  store i64 %11470, i64* %RCX.i11580, align 8
  %11471 = icmp ult i32 %11469, %11462
  %11472 = icmp ult i32 %11469, %11468
  %11473 = or i1 %11471, %11472
  %11474 = zext i1 %11473 to i8
  store i8 %11474, i8* %14, align 1
  %11475 = and i32 %11469, 255
  %11476 = tail call i32 @llvm.ctpop.i32(i32 %11475)
  %11477 = trunc i32 %11476 to i8
  %11478 = and i8 %11477, 1
  %11479 = xor i8 %11478, 1
  store i8 %11479, i8* %21, align 1
  %11480 = xor i32 %11468, %11462
  %11481 = xor i32 %11480, %11469
  %11482 = lshr i32 %11481, 4
  %11483 = trunc i32 %11482 to i8
  %11484 = and i8 %11483, 1
  store i8 %11484, i8* %27, align 1
  %11485 = icmp eq i32 %11469, 0
  %11486 = zext i1 %11485 to i8
  store i8 %11486, i8* %30, align 1
  %11487 = lshr i32 %11469, 31
  %11488 = trunc i32 %11487 to i8
  store i8 %11488, i8* %33, align 1
  %11489 = lshr i32 %11462, 31
  %11490 = lshr i32 %11468, 31
  %11491 = xor i32 %11487, %11489
  %11492 = xor i32 %11487, %11490
  %11493 = add nuw nsw i32 %11491, %11492
  %11494 = icmp eq i32 %11493, 2
  %11495 = zext i1 %11494 to i8
  store i8 %11495, i8* %39, align 1
  %11496 = add i64 %11402, 40
  store i64 %11496, i64* %3, align 8
  store i32 %11469, i32* %11467, align 4
  %11497 = load i64, i64* %RBP.i, align 8
  %11498 = add i64 %11497, -8
  %11499 = load i64, i64* %3, align 8
  %11500 = add i64 %11499, 4
  store i64 %11500, i64* %3, align 8
  %11501 = inttoptr i64 %11498 to i64*
  %11502 = load i64, i64* %11501, align 8
  %11503 = add i64 %11502, 51640
  store i64 %11503, i64* %RAX.i11582.pre-phi, align 8
  %11504 = icmp ugt i64 %11502, -51641
  %11505 = zext i1 %11504 to i8
  store i8 %11505, i8* %14, align 1
  %11506 = trunc i64 %11503 to i32
  %11507 = and i32 %11506, 255
  %11508 = tail call i32 @llvm.ctpop.i32(i32 %11507)
  %11509 = trunc i32 %11508 to i8
  %11510 = and i8 %11509, 1
  %11511 = xor i8 %11510, 1
  store i8 %11511, i8* %21, align 1
  %11512 = xor i64 %11502, 16
  %11513 = xor i64 %11512, %11503
  %11514 = lshr i64 %11513, 4
  %11515 = trunc i64 %11514 to i8
  %11516 = and i8 %11515, 1
  store i8 %11516, i8* %27, align 1
  %11517 = icmp eq i64 %11503, 0
  %11518 = zext i1 %11517 to i8
  store i8 %11518, i8* %30, align 1
  %11519 = lshr i64 %11503, 63
  %11520 = trunc i64 %11519 to i8
  store i8 %11520, i8* %33, align 1
  %11521 = lshr i64 %11502, 63
  %11522 = xor i64 %11519, %11521
  %11523 = add nuw nsw i64 %11522, %11519
  %11524 = icmp eq i64 %11523, 2
  %11525 = zext i1 %11524 to i8
  store i8 %11525, i8* %39, align 1
  %11526 = add i64 %11497, -150
  %11527 = add i64 %11499, 17
  store i64 %11527, i64* %3, align 8
  %11528 = inttoptr i64 %11526 to i16*
  %11529 = load i16, i16* %11528, align 2
  %11530 = zext i16 %11529 to i64
  store i64 %11530, i64* %RCX.i11580, align 8
  %11531 = zext i16 %11529 to i64
  %11532 = shl nuw nsw i64 %11531, 4
  store i64 %11532, i64* %573, align 8
  %11533 = add i64 %11532, %11503
  store i64 %11533, i64* %RAX.i11582.pre-phi, align 8
  %11534 = icmp ult i64 %11533, %11503
  %11535 = icmp ult i64 %11533, %11532
  %11536 = or i1 %11534, %11535
  %11537 = zext i1 %11536 to i8
  store i8 %11537, i8* %14, align 1
  %11538 = trunc i64 %11533 to i32
  %11539 = and i32 %11538, 255
  %11540 = tail call i32 @llvm.ctpop.i32(i32 %11539)
  %11541 = trunc i32 %11540 to i8
  %11542 = and i8 %11541, 1
  %11543 = xor i8 %11542, 1
  store i8 %11543, i8* %21, align 1
  %11544 = xor i64 %11532, %11503
  %11545 = xor i64 %11544, %11533
  %11546 = lshr i64 %11545, 4
  %11547 = trunc i64 %11546 to i8
  %11548 = and i8 %11547, 1
  store i8 %11548, i8* %27, align 1
  %11549 = icmp eq i64 %11533, 0
  %11550 = zext i1 %11549 to i8
  store i8 %11550, i8* %30, align 1
  %11551 = lshr i64 %11533, 63
  %11552 = trunc i64 %11551 to i8
  store i8 %11552, i8* %33, align 1
  %11553 = xor i64 %11551, %11519
  %11554 = add nuw nsw i64 %11553, %11551
  %11555 = icmp eq i64 %11554, 2
  %11556 = zext i1 %11555 to i8
  store i8 %11556, i8* %39, align 1
  %11557 = add i64 %11533, 4
  %11558 = add i64 %11499, 29
  store i64 %11558, i64* %3, align 8
  %11559 = inttoptr i64 %11557 to i32*
  %11560 = load i32, i32* %11559, align 4
  %11561 = zext i32 %11560 to i64
  store i64 %11561, i64* %RCX.i11580, align 8
  %11562 = load i64, i64* %RBP.i, align 8
  %11563 = add i64 %11562, -144
  %11564 = add i64 %11499, 35
  store i64 %11564, i64* %3, align 8
  %11565 = inttoptr i64 %11563 to i32*
  %11566 = load i32, i32* %11565, align 4
  %11567 = add i32 %11566, %11560
  %11568 = zext i32 %11567 to i64
  store i64 %11568, i64* %RCX.i11580, align 8
  %11569 = icmp ult i32 %11567, %11560
  %11570 = icmp ult i32 %11567, %11566
  %11571 = or i1 %11569, %11570
  %11572 = zext i1 %11571 to i8
  store i8 %11572, i8* %14, align 1
  %11573 = and i32 %11567, 255
  %11574 = tail call i32 @llvm.ctpop.i32(i32 %11573)
  %11575 = trunc i32 %11574 to i8
  %11576 = and i8 %11575, 1
  %11577 = xor i8 %11576, 1
  store i8 %11577, i8* %21, align 1
  %11578 = xor i32 %11566, %11560
  %11579 = xor i32 %11578, %11567
  %11580 = lshr i32 %11579, 4
  %11581 = trunc i32 %11580 to i8
  %11582 = and i8 %11581, 1
  store i8 %11582, i8* %27, align 1
  %11583 = icmp eq i32 %11567, 0
  %11584 = zext i1 %11583 to i8
  store i8 %11584, i8* %30, align 1
  %11585 = lshr i32 %11567, 31
  %11586 = trunc i32 %11585 to i8
  store i8 %11586, i8* %33, align 1
  %11587 = lshr i32 %11560, 31
  %11588 = lshr i32 %11566, 31
  %11589 = xor i32 %11585, %11587
  %11590 = xor i32 %11585, %11588
  %11591 = add nuw nsw i32 %11589, %11590
  %11592 = icmp eq i32 %11591, 2
  %11593 = zext i1 %11592 to i8
  store i8 %11593, i8* %39, align 1
  %11594 = add i64 %11499, 41
  store i64 %11594, i64* %3, align 8
  store i32 %11567, i32* %11565, align 4
  %11595 = load i64, i64* %RBP.i, align 8
  %11596 = add i64 %11595, -8
  %11597 = load i64, i64* %3, align 8
  %11598 = add i64 %11597, 4
  store i64 %11598, i64* %3, align 8
  %11599 = inttoptr i64 %11596 to i64*
  %11600 = load i64, i64* %11599, align 8
  %11601 = add i64 %11600, 51640
  store i64 %11601, i64* %RAX.i11582.pre-phi, align 8
  %11602 = icmp ugt i64 %11600, -51641
  %11603 = zext i1 %11602 to i8
  store i8 %11603, i8* %14, align 1
  %11604 = trunc i64 %11601 to i32
  %11605 = and i32 %11604, 255
  %11606 = tail call i32 @llvm.ctpop.i32(i32 %11605)
  %11607 = trunc i32 %11606 to i8
  %11608 = and i8 %11607, 1
  %11609 = xor i8 %11608, 1
  store i8 %11609, i8* %21, align 1
  %11610 = xor i64 %11600, 16
  %11611 = xor i64 %11610, %11601
  %11612 = lshr i64 %11611, 4
  %11613 = trunc i64 %11612 to i8
  %11614 = and i8 %11613, 1
  store i8 %11614, i8* %27, align 1
  %11615 = icmp eq i64 %11601, 0
  %11616 = zext i1 %11615 to i8
  store i8 %11616, i8* %30, align 1
  %11617 = lshr i64 %11601, 63
  %11618 = trunc i64 %11617 to i8
  store i8 %11618, i8* %33, align 1
  %11619 = lshr i64 %11600, 63
  %11620 = xor i64 %11617, %11619
  %11621 = add nuw nsw i64 %11620, %11617
  %11622 = icmp eq i64 %11621, 2
  %11623 = zext i1 %11622 to i8
  store i8 %11623, i8* %39, align 1
  %11624 = add i64 %11595, -150
  %11625 = add i64 %11597, 17
  store i64 %11625, i64* %3, align 8
  %11626 = inttoptr i64 %11624 to i16*
  %11627 = load i16, i16* %11626, align 2
  %11628 = zext i16 %11627 to i64
  store i64 %11628, i64* %RCX.i11580, align 8
  %11629 = zext i16 %11627 to i64
  %11630 = shl nuw nsw i64 %11629, 4
  store i64 %11630, i64* %573, align 8
  %11631 = add i64 %11630, %11601
  store i64 %11631, i64* %RAX.i11582.pre-phi, align 8
  %11632 = icmp ult i64 %11631, %11601
  %11633 = icmp ult i64 %11631, %11630
  %11634 = or i1 %11632, %11633
  %11635 = zext i1 %11634 to i8
  store i8 %11635, i8* %14, align 1
  %11636 = trunc i64 %11631 to i32
  %11637 = and i32 %11636, 255
  %11638 = tail call i32 @llvm.ctpop.i32(i32 %11637)
  %11639 = trunc i32 %11638 to i8
  %11640 = and i8 %11639, 1
  %11641 = xor i8 %11640, 1
  store i8 %11641, i8* %21, align 1
  %11642 = xor i64 %11630, %11601
  %11643 = xor i64 %11642, %11631
  %11644 = lshr i64 %11643, 4
  %11645 = trunc i64 %11644 to i8
  %11646 = and i8 %11645, 1
  store i8 %11646, i8* %27, align 1
  %11647 = icmp eq i64 %11631, 0
  %11648 = zext i1 %11647 to i8
  store i8 %11648, i8* %30, align 1
  %11649 = lshr i64 %11631, 63
  %11650 = trunc i64 %11649 to i8
  store i8 %11650, i8* %33, align 1
  %11651 = xor i64 %11649, %11617
  %11652 = add nuw nsw i64 %11651, %11649
  %11653 = icmp eq i64 %11652, 2
  %11654 = zext i1 %11653 to i8
  store i8 %11654, i8* %39, align 1
  %11655 = add i64 %11631, 8
  %11656 = add i64 %11597, 29
  store i64 %11656, i64* %3, align 8
  %11657 = inttoptr i64 %11655 to i32*
  %11658 = load i32, i32* %11657, align 4
  %11659 = zext i32 %11658 to i64
  store i64 %11659, i64* %RCX.i11580, align 8
  %11660 = load i64, i64* %RBP.i, align 8
  %11661 = add i64 %11660, -148
  %11662 = add i64 %11597, 35
  store i64 %11662, i64* %3, align 8
  %11663 = inttoptr i64 %11661 to i32*
  %11664 = load i32, i32* %11663, align 4
  %11665 = add i32 %11664, %11658
  %11666 = zext i32 %11665 to i64
  store i64 %11666, i64* %RCX.i11580, align 8
  %11667 = icmp ult i32 %11665, %11658
  %11668 = icmp ult i32 %11665, %11664
  %11669 = or i1 %11667, %11668
  %11670 = zext i1 %11669 to i8
  store i8 %11670, i8* %14, align 1
  %11671 = and i32 %11665, 255
  %11672 = tail call i32 @llvm.ctpop.i32(i32 %11671)
  %11673 = trunc i32 %11672 to i8
  %11674 = and i8 %11673, 1
  %11675 = xor i8 %11674, 1
  store i8 %11675, i8* %21, align 1
  %11676 = xor i32 %11664, %11658
  %11677 = xor i32 %11676, %11665
  %11678 = lshr i32 %11677, 4
  %11679 = trunc i32 %11678 to i8
  %11680 = and i8 %11679, 1
  store i8 %11680, i8* %27, align 1
  %11681 = icmp eq i32 %11665, 0
  %11682 = zext i1 %11681 to i8
  store i8 %11682, i8* %30, align 1
  %11683 = lshr i32 %11665, 31
  %11684 = trunc i32 %11683 to i8
  store i8 %11684, i8* %33, align 1
  %11685 = lshr i32 %11658, 31
  %11686 = lshr i32 %11664, 31
  %11687 = xor i32 %11683, %11685
  %11688 = xor i32 %11683, %11686
  %11689 = add nuw nsw i32 %11687, %11688
  %11690 = icmp eq i32 %11689, 2
  %11691 = zext i1 %11690 to i8
  store i8 %11691, i8* %39, align 1
  %11692 = add i64 %11597, 41
  store i64 %11692, i64* %3, align 8
  store i32 %11665, i32* %11663, align 4
  %11693 = load i64, i64* %RBP.i, align 8
  %11694 = add i64 %11693, -120
  %11695 = load i64, i64* %3, align 8
  %11696 = add i64 %11695, 4
  store i64 %11696, i64* %3, align 8
  %11697 = inttoptr i64 %11694 to i64*
  %11698 = load i64, i64* %11697, align 8
  store i64 %11698, i64* %RAX.i11582.pre-phi, align 8
  %11699 = add i64 %11693, -28
  %11700 = add i64 %11695, 7
  store i64 %11700, i64* %3, align 8
  %11701 = inttoptr i64 %11699 to i32*
  %11702 = load i32, i32* %11701, align 4
  %11703 = add i32 %11702, 27
  %11704 = zext i32 %11703 to i64
  store i64 %11704, i64* %RCX.i11580, align 8
  %11705 = icmp ugt i32 %11702, -28
  %11706 = zext i1 %11705 to i8
  store i8 %11706, i8* %14, align 1
  %11707 = and i32 %11703, 255
  %11708 = tail call i32 @llvm.ctpop.i32(i32 %11707)
  %11709 = trunc i32 %11708 to i8
  %11710 = and i8 %11709, 1
  %11711 = xor i8 %11710, 1
  store i8 %11711, i8* %21, align 1
  %11712 = xor i32 %11702, 16
  %11713 = xor i32 %11712, %11703
  %11714 = lshr i32 %11713, 4
  %11715 = trunc i32 %11714 to i8
  %11716 = and i8 %11715, 1
  store i8 %11716, i8* %27, align 1
  %11717 = icmp eq i32 %11703, 0
  %11718 = zext i1 %11717 to i8
  store i8 %11718, i8* %30, align 1
  %11719 = lshr i32 %11703, 31
  %11720 = trunc i32 %11719 to i8
  store i8 %11720, i8* %33, align 1
  %11721 = lshr i32 %11702, 31
  %11722 = xor i32 %11719, %11721
  %11723 = add nuw nsw i32 %11722, %11719
  %11724 = icmp eq i32 %11723, 2
  %11725 = zext i1 %11724 to i8
  store i8 %11725, i8* %39, align 1
  %11726 = sext i32 %11703 to i64
  store i64 %11726, i64* %573, align 8
  %11727 = shl nsw i64 %11726, 1
  %11728 = add i64 %11698, %11727
  %11729 = add i64 %11695, 17
  store i64 %11729, i64* %3, align 8
  %11730 = inttoptr i64 %11728 to i16*
  %11731 = load i16, i16* %11730, align 2
  store i16 %11731, i16* %SI.i, align 2
  %11732 = add i64 %11693, -150
  %11733 = add i64 %11695, 24
  store i64 %11733, i64* %3, align 8
  %11734 = inttoptr i64 %11732 to i16*
  store i16 %11731, i16* %11734, align 2
  %11735 = load i64, i64* %RBP.i, align 8
  %11736 = add i64 %11735, -8
  %11737 = load i64, i64* %3, align 8
  %11738 = add i64 %11737, 4
  store i64 %11738, i64* %3, align 8
  %11739 = inttoptr i64 %11736 to i64*
  %11740 = load i64, i64* %11739, align 8
  %11741 = add i64 %11740, 51640
  store i64 %11741, i64* %RAX.i11582.pre-phi, align 8
  %11742 = icmp ugt i64 %11740, -51641
  %11743 = zext i1 %11742 to i8
  store i8 %11743, i8* %14, align 1
  %11744 = trunc i64 %11741 to i32
  %11745 = and i32 %11744, 255
  %11746 = tail call i32 @llvm.ctpop.i32(i32 %11745)
  %11747 = trunc i32 %11746 to i8
  %11748 = and i8 %11747, 1
  %11749 = xor i8 %11748, 1
  store i8 %11749, i8* %21, align 1
  %11750 = xor i64 %11740, 16
  %11751 = xor i64 %11750, %11741
  %11752 = lshr i64 %11751, 4
  %11753 = trunc i64 %11752 to i8
  %11754 = and i8 %11753, 1
  store i8 %11754, i8* %27, align 1
  %11755 = icmp eq i64 %11741, 0
  %11756 = zext i1 %11755 to i8
  store i8 %11756, i8* %30, align 1
  %11757 = lshr i64 %11741, 63
  %11758 = trunc i64 %11757 to i8
  store i8 %11758, i8* %33, align 1
  %11759 = lshr i64 %11740, 63
  %11760 = xor i64 %11757, %11759
  %11761 = add nuw nsw i64 %11760, %11757
  %11762 = icmp eq i64 %11761, 2
  %11763 = zext i1 %11762 to i8
  store i8 %11763, i8* %39, align 1
  %11764 = add i64 %11735, -150
  %11765 = add i64 %11737, 17
  store i64 %11765, i64* %3, align 8
  %11766 = inttoptr i64 %11764 to i16*
  %11767 = load i16, i16* %11766, align 2
  %11768 = zext i16 %11767 to i64
  store i64 %11768, i64* %RCX.i11580, align 8
  %11769 = zext i16 %11767 to i64
  %11770 = shl nuw nsw i64 %11769, 4
  store i64 %11770, i64* %573, align 8
  %11771 = add i64 %11770, %11741
  store i64 %11771, i64* %RAX.i11582.pre-phi, align 8
  %11772 = icmp ult i64 %11771, %11741
  %11773 = icmp ult i64 %11771, %11770
  %11774 = or i1 %11772, %11773
  %11775 = zext i1 %11774 to i8
  store i8 %11775, i8* %14, align 1
  %11776 = trunc i64 %11771 to i32
  %11777 = and i32 %11776, 255
  %11778 = tail call i32 @llvm.ctpop.i32(i32 %11777)
  %11779 = trunc i32 %11778 to i8
  %11780 = and i8 %11779, 1
  %11781 = xor i8 %11780, 1
  store i8 %11781, i8* %21, align 1
  %11782 = xor i64 %11770, %11741
  %11783 = xor i64 %11782, %11771
  %11784 = lshr i64 %11783, 4
  %11785 = trunc i64 %11784 to i8
  %11786 = and i8 %11785, 1
  store i8 %11786, i8* %27, align 1
  %11787 = icmp eq i64 %11771, 0
  %11788 = zext i1 %11787 to i8
  store i8 %11788, i8* %30, align 1
  %11789 = lshr i64 %11771, 63
  %11790 = trunc i64 %11789 to i8
  store i8 %11790, i8* %33, align 1
  %11791 = xor i64 %11789, %11757
  %11792 = add nuw nsw i64 %11791, %11789
  %11793 = icmp eq i64 %11792, 2
  %11794 = zext i1 %11793 to i8
  store i8 %11794, i8* %39, align 1
  %11795 = inttoptr i64 %11771 to i32*
  %11796 = add i64 %11737, 28
  store i64 %11796, i64* %3, align 8
  %11797 = load i32, i32* %11795, align 4
  %11798 = zext i32 %11797 to i64
  store i64 %11798, i64* %RCX.i11580, align 8
  %11799 = load i64, i64* %RBP.i, align 8
  %11800 = add i64 %11799, -140
  %11801 = add i64 %11737, 34
  store i64 %11801, i64* %3, align 8
  %11802 = inttoptr i64 %11800 to i32*
  %11803 = load i32, i32* %11802, align 4
  %11804 = add i32 %11803, %11797
  %11805 = zext i32 %11804 to i64
  store i64 %11805, i64* %RCX.i11580, align 8
  %11806 = icmp ult i32 %11804, %11797
  %11807 = icmp ult i32 %11804, %11803
  %11808 = or i1 %11806, %11807
  %11809 = zext i1 %11808 to i8
  store i8 %11809, i8* %14, align 1
  %11810 = and i32 %11804, 255
  %11811 = tail call i32 @llvm.ctpop.i32(i32 %11810)
  %11812 = trunc i32 %11811 to i8
  %11813 = and i8 %11812, 1
  %11814 = xor i8 %11813, 1
  store i8 %11814, i8* %21, align 1
  %11815 = xor i32 %11803, %11797
  %11816 = xor i32 %11815, %11804
  %11817 = lshr i32 %11816, 4
  %11818 = trunc i32 %11817 to i8
  %11819 = and i8 %11818, 1
  store i8 %11819, i8* %27, align 1
  %11820 = icmp eq i32 %11804, 0
  %11821 = zext i1 %11820 to i8
  store i8 %11821, i8* %30, align 1
  %11822 = lshr i32 %11804, 31
  %11823 = trunc i32 %11822 to i8
  store i8 %11823, i8* %33, align 1
  %11824 = lshr i32 %11797, 31
  %11825 = lshr i32 %11803, 31
  %11826 = xor i32 %11822, %11824
  %11827 = xor i32 %11822, %11825
  %11828 = add nuw nsw i32 %11826, %11827
  %11829 = icmp eq i32 %11828, 2
  %11830 = zext i1 %11829 to i8
  store i8 %11830, i8* %39, align 1
  %11831 = add i64 %11737, 40
  store i64 %11831, i64* %3, align 8
  store i32 %11804, i32* %11802, align 4
  %11832 = load i64, i64* %RBP.i, align 8
  %11833 = add i64 %11832, -8
  %11834 = load i64, i64* %3, align 8
  %11835 = add i64 %11834, 4
  store i64 %11835, i64* %3, align 8
  %11836 = inttoptr i64 %11833 to i64*
  %11837 = load i64, i64* %11836, align 8
  %11838 = add i64 %11837, 51640
  store i64 %11838, i64* %RAX.i11582.pre-phi, align 8
  %11839 = icmp ugt i64 %11837, -51641
  %11840 = zext i1 %11839 to i8
  store i8 %11840, i8* %14, align 1
  %11841 = trunc i64 %11838 to i32
  %11842 = and i32 %11841, 255
  %11843 = tail call i32 @llvm.ctpop.i32(i32 %11842)
  %11844 = trunc i32 %11843 to i8
  %11845 = and i8 %11844, 1
  %11846 = xor i8 %11845, 1
  store i8 %11846, i8* %21, align 1
  %11847 = xor i64 %11837, 16
  %11848 = xor i64 %11847, %11838
  %11849 = lshr i64 %11848, 4
  %11850 = trunc i64 %11849 to i8
  %11851 = and i8 %11850, 1
  store i8 %11851, i8* %27, align 1
  %11852 = icmp eq i64 %11838, 0
  %11853 = zext i1 %11852 to i8
  store i8 %11853, i8* %30, align 1
  %11854 = lshr i64 %11838, 63
  %11855 = trunc i64 %11854 to i8
  store i8 %11855, i8* %33, align 1
  %11856 = lshr i64 %11837, 63
  %11857 = xor i64 %11854, %11856
  %11858 = add nuw nsw i64 %11857, %11854
  %11859 = icmp eq i64 %11858, 2
  %11860 = zext i1 %11859 to i8
  store i8 %11860, i8* %39, align 1
  %11861 = add i64 %11832, -150
  %11862 = add i64 %11834, 17
  store i64 %11862, i64* %3, align 8
  %11863 = inttoptr i64 %11861 to i16*
  %11864 = load i16, i16* %11863, align 2
  %11865 = zext i16 %11864 to i64
  store i64 %11865, i64* %RCX.i11580, align 8
  %11866 = zext i16 %11864 to i64
  %11867 = shl nuw nsw i64 %11866, 4
  store i64 %11867, i64* %573, align 8
  %11868 = add i64 %11867, %11838
  store i64 %11868, i64* %RAX.i11582.pre-phi, align 8
  %11869 = icmp ult i64 %11868, %11838
  %11870 = icmp ult i64 %11868, %11867
  %11871 = or i1 %11869, %11870
  %11872 = zext i1 %11871 to i8
  store i8 %11872, i8* %14, align 1
  %11873 = trunc i64 %11868 to i32
  %11874 = and i32 %11873, 255
  %11875 = tail call i32 @llvm.ctpop.i32(i32 %11874)
  %11876 = trunc i32 %11875 to i8
  %11877 = and i8 %11876, 1
  %11878 = xor i8 %11877, 1
  store i8 %11878, i8* %21, align 1
  %11879 = xor i64 %11867, %11838
  %11880 = xor i64 %11879, %11868
  %11881 = lshr i64 %11880, 4
  %11882 = trunc i64 %11881 to i8
  %11883 = and i8 %11882, 1
  store i8 %11883, i8* %27, align 1
  %11884 = icmp eq i64 %11868, 0
  %11885 = zext i1 %11884 to i8
  store i8 %11885, i8* %30, align 1
  %11886 = lshr i64 %11868, 63
  %11887 = trunc i64 %11886 to i8
  store i8 %11887, i8* %33, align 1
  %11888 = xor i64 %11886, %11854
  %11889 = add nuw nsw i64 %11888, %11886
  %11890 = icmp eq i64 %11889, 2
  %11891 = zext i1 %11890 to i8
  store i8 %11891, i8* %39, align 1
  %11892 = add i64 %11868, 4
  %11893 = add i64 %11834, 29
  store i64 %11893, i64* %3, align 8
  %11894 = inttoptr i64 %11892 to i32*
  %11895 = load i32, i32* %11894, align 4
  %11896 = zext i32 %11895 to i64
  store i64 %11896, i64* %RCX.i11580, align 8
  %11897 = load i64, i64* %RBP.i, align 8
  %11898 = add i64 %11897, -144
  %11899 = add i64 %11834, 35
  store i64 %11899, i64* %3, align 8
  %11900 = inttoptr i64 %11898 to i32*
  %11901 = load i32, i32* %11900, align 4
  %11902 = add i32 %11901, %11895
  %11903 = zext i32 %11902 to i64
  store i64 %11903, i64* %RCX.i11580, align 8
  %11904 = icmp ult i32 %11902, %11895
  %11905 = icmp ult i32 %11902, %11901
  %11906 = or i1 %11904, %11905
  %11907 = zext i1 %11906 to i8
  store i8 %11907, i8* %14, align 1
  %11908 = and i32 %11902, 255
  %11909 = tail call i32 @llvm.ctpop.i32(i32 %11908)
  %11910 = trunc i32 %11909 to i8
  %11911 = and i8 %11910, 1
  %11912 = xor i8 %11911, 1
  store i8 %11912, i8* %21, align 1
  %11913 = xor i32 %11901, %11895
  %11914 = xor i32 %11913, %11902
  %11915 = lshr i32 %11914, 4
  %11916 = trunc i32 %11915 to i8
  %11917 = and i8 %11916, 1
  store i8 %11917, i8* %27, align 1
  %11918 = icmp eq i32 %11902, 0
  %11919 = zext i1 %11918 to i8
  store i8 %11919, i8* %30, align 1
  %11920 = lshr i32 %11902, 31
  %11921 = trunc i32 %11920 to i8
  store i8 %11921, i8* %33, align 1
  %11922 = lshr i32 %11895, 31
  %11923 = lshr i32 %11901, 31
  %11924 = xor i32 %11920, %11922
  %11925 = xor i32 %11920, %11923
  %11926 = add nuw nsw i32 %11924, %11925
  %11927 = icmp eq i32 %11926, 2
  %11928 = zext i1 %11927 to i8
  store i8 %11928, i8* %39, align 1
  %11929 = add i64 %11834, 41
  store i64 %11929, i64* %3, align 8
  store i32 %11902, i32* %11900, align 4
  %11930 = load i64, i64* %RBP.i, align 8
  %11931 = add i64 %11930, -8
  %11932 = load i64, i64* %3, align 8
  %11933 = add i64 %11932, 4
  store i64 %11933, i64* %3, align 8
  %11934 = inttoptr i64 %11931 to i64*
  %11935 = load i64, i64* %11934, align 8
  %11936 = add i64 %11935, 51640
  store i64 %11936, i64* %RAX.i11582.pre-phi, align 8
  %11937 = icmp ugt i64 %11935, -51641
  %11938 = zext i1 %11937 to i8
  store i8 %11938, i8* %14, align 1
  %11939 = trunc i64 %11936 to i32
  %11940 = and i32 %11939, 255
  %11941 = tail call i32 @llvm.ctpop.i32(i32 %11940)
  %11942 = trunc i32 %11941 to i8
  %11943 = and i8 %11942, 1
  %11944 = xor i8 %11943, 1
  store i8 %11944, i8* %21, align 1
  %11945 = xor i64 %11935, 16
  %11946 = xor i64 %11945, %11936
  %11947 = lshr i64 %11946, 4
  %11948 = trunc i64 %11947 to i8
  %11949 = and i8 %11948, 1
  store i8 %11949, i8* %27, align 1
  %11950 = icmp eq i64 %11936, 0
  %11951 = zext i1 %11950 to i8
  store i8 %11951, i8* %30, align 1
  %11952 = lshr i64 %11936, 63
  %11953 = trunc i64 %11952 to i8
  store i8 %11953, i8* %33, align 1
  %11954 = lshr i64 %11935, 63
  %11955 = xor i64 %11952, %11954
  %11956 = add nuw nsw i64 %11955, %11952
  %11957 = icmp eq i64 %11956, 2
  %11958 = zext i1 %11957 to i8
  store i8 %11958, i8* %39, align 1
  %11959 = add i64 %11930, -150
  %11960 = add i64 %11932, 17
  store i64 %11960, i64* %3, align 8
  %11961 = inttoptr i64 %11959 to i16*
  %11962 = load i16, i16* %11961, align 2
  %11963 = zext i16 %11962 to i64
  store i64 %11963, i64* %RCX.i11580, align 8
  %11964 = zext i16 %11962 to i64
  %11965 = shl nuw nsw i64 %11964, 4
  store i64 %11965, i64* %573, align 8
  %11966 = add i64 %11965, %11936
  store i64 %11966, i64* %RAX.i11582.pre-phi, align 8
  %11967 = icmp ult i64 %11966, %11936
  %11968 = icmp ult i64 %11966, %11965
  %11969 = or i1 %11967, %11968
  %11970 = zext i1 %11969 to i8
  store i8 %11970, i8* %14, align 1
  %11971 = trunc i64 %11966 to i32
  %11972 = and i32 %11971, 255
  %11973 = tail call i32 @llvm.ctpop.i32(i32 %11972)
  %11974 = trunc i32 %11973 to i8
  %11975 = and i8 %11974, 1
  %11976 = xor i8 %11975, 1
  store i8 %11976, i8* %21, align 1
  %11977 = xor i64 %11965, %11936
  %11978 = xor i64 %11977, %11966
  %11979 = lshr i64 %11978, 4
  %11980 = trunc i64 %11979 to i8
  %11981 = and i8 %11980, 1
  store i8 %11981, i8* %27, align 1
  %11982 = icmp eq i64 %11966, 0
  %11983 = zext i1 %11982 to i8
  store i8 %11983, i8* %30, align 1
  %11984 = lshr i64 %11966, 63
  %11985 = trunc i64 %11984 to i8
  store i8 %11985, i8* %33, align 1
  %11986 = xor i64 %11984, %11952
  %11987 = add nuw nsw i64 %11986, %11984
  %11988 = icmp eq i64 %11987, 2
  %11989 = zext i1 %11988 to i8
  store i8 %11989, i8* %39, align 1
  %11990 = add i64 %11966, 8
  %11991 = add i64 %11932, 29
  store i64 %11991, i64* %3, align 8
  %11992 = inttoptr i64 %11990 to i32*
  %11993 = load i32, i32* %11992, align 4
  %11994 = zext i32 %11993 to i64
  store i64 %11994, i64* %RCX.i11580, align 8
  %11995 = load i64, i64* %RBP.i, align 8
  %11996 = add i64 %11995, -148
  %11997 = add i64 %11932, 35
  store i64 %11997, i64* %3, align 8
  %11998 = inttoptr i64 %11996 to i32*
  %11999 = load i32, i32* %11998, align 4
  %12000 = add i32 %11999, %11993
  %12001 = zext i32 %12000 to i64
  store i64 %12001, i64* %RCX.i11580, align 8
  %12002 = icmp ult i32 %12000, %11993
  %12003 = icmp ult i32 %12000, %11999
  %12004 = or i1 %12002, %12003
  %12005 = zext i1 %12004 to i8
  store i8 %12005, i8* %14, align 1
  %12006 = and i32 %12000, 255
  %12007 = tail call i32 @llvm.ctpop.i32(i32 %12006)
  %12008 = trunc i32 %12007 to i8
  %12009 = and i8 %12008, 1
  %12010 = xor i8 %12009, 1
  store i8 %12010, i8* %21, align 1
  %12011 = xor i32 %11999, %11993
  %12012 = xor i32 %12011, %12000
  %12013 = lshr i32 %12012, 4
  %12014 = trunc i32 %12013 to i8
  %12015 = and i8 %12014, 1
  store i8 %12015, i8* %27, align 1
  %12016 = icmp eq i32 %12000, 0
  %12017 = zext i1 %12016 to i8
  store i8 %12017, i8* %30, align 1
  %12018 = lshr i32 %12000, 31
  %12019 = trunc i32 %12018 to i8
  store i8 %12019, i8* %33, align 1
  %12020 = lshr i32 %11993, 31
  %12021 = lshr i32 %11999, 31
  %12022 = xor i32 %12018, %12020
  %12023 = xor i32 %12018, %12021
  %12024 = add nuw nsw i32 %12022, %12023
  %12025 = icmp eq i32 %12024, 2
  %12026 = zext i1 %12025 to i8
  store i8 %12026, i8* %39, align 1
  %12027 = add i64 %11932, 41
  store i64 %12027, i64* %3, align 8
  store i32 %12000, i32* %11998, align 4
  %12028 = load i64, i64* %RBP.i, align 8
  %12029 = add i64 %12028, -120
  %12030 = load i64, i64* %3, align 8
  %12031 = add i64 %12030, 4
  store i64 %12031, i64* %3, align 8
  %12032 = inttoptr i64 %12029 to i64*
  %12033 = load i64, i64* %12032, align 8
  store i64 %12033, i64* %RAX.i11582.pre-phi, align 8
  %12034 = add i64 %12028, -28
  %12035 = add i64 %12030, 7
  store i64 %12035, i64* %3, align 8
  %12036 = inttoptr i64 %12034 to i32*
  %12037 = load i32, i32* %12036, align 4
  %12038 = add i32 %12037, 28
  %12039 = zext i32 %12038 to i64
  store i64 %12039, i64* %RCX.i11580, align 8
  %12040 = icmp ugt i32 %12037, -29
  %12041 = zext i1 %12040 to i8
  store i8 %12041, i8* %14, align 1
  %12042 = and i32 %12038, 255
  %12043 = tail call i32 @llvm.ctpop.i32(i32 %12042)
  %12044 = trunc i32 %12043 to i8
  %12045 = and i8 %12044, 1
  %12046 = xor i8 %12045, 1
  store i8 %12046, i8* %21, align 1
  %12047 = xor i32 %12037, 16
  %12048 = xor i32 %12047, %12038
  %12049 = lshr i32 %12048, 4
  %12050 = trunc i32 %12049 to i8
  %12051 = and i8 %12050, 1
  store i8 %12051, i8* %27, align 1
  %12052 = icmp eq i32 %12038, 0
  %12053 = zext i1 %12052 to i8
  store i8 %12053, i8* %30, align 1
  %12054 = lshr i32 %12038, 31
  %12055 = trunc i32 %12054 to i8
  store i8 %12055, i8* %33, align 1
  %12056 = lshr i32 %12037, 31
  %12057 = xor i32 %12054, %12056
  %12058 = add nuw nsw i32 %12057, %12054
  %12059 = icmp eq i32 %12058, 2
  %12060 = zext i1 %12059 to i8
  store i8 %12060, i8* %39, align 1
  %12061 = sext i32 %12038 to i64
  store i64 %12061, i64* %573, align 8
  %12062 = shl nsw i64 %12061, 1
  %12063 = add i64 %12033, %12062
  %12064 = add i64 %12030, 17
  store i64 %12064, i64* %3, align 8
  %12065 = inttoptr i64 %12063 to i16*
  %12066 = load i16, i16* %12065, align 2
  store i16 %12066, i16* %SI.i, align 2
  %12067 = add i64 %12028, -150
  %12068 = add i64 %12030, 24
  store i64 %12068, i64* %3, align 8
  %12069 = inttoptr i64 %12067 to i16*
  store i16 %12066, i16* %12069, align 2
  %12070 = load i64, i64* %RBP.i, align 8
  %12071 = add i64 %12070, -8
  %12072 = load i64, i64* %3, align 8
  %12073 = add i64 %12072, 4
  store i64 %12073, i64* %3, align 8
  %12074 = inttoptr i64 %12071 to i64*
  %12075 = load i64, i64* %12074, align 8
  %12076 = add i64 %12075, 51640
  store i64 %12076, i64* %RAX.i11582.pre-phi, align 8
  %12077 = icmp ugt i64 %12075, -51641
  %12078 = zext i1 %12077 to i8
  store i8 %12078, i8* %14, align 1
  %12079 = trunc i64 %12076 to i32
  %12080 = and i32 %12079, 255
  %12081 = tail call i32 @llvm.ctpop.i32(i32 %12080)
  %12082 = trunc i32 %12081 to i8
  %12083 = and i8 %12082, 1
  %12084 = xor i8 %12083, 1
  store i8 %12084, i8* %21, align 1
  %12085 = xor i64 %12075, 16
  %12086 = xor i64 %12085, %12076
  %12087 = lshr i64 %12086, 4
  %12088 = trunc i64 %12087 to i8
  %12089 = and i8 %12088, 1
  store i8 %12089, i8* %27, align 1
  %12090 = icmp eq i64 %12076, 0
  %12091 = zext i1 %12090 to i8
  store i8 %12091, i8* %30, align 1
  %12092 = lshr i64 %12076, 63
  %12093 = trunc i64 %12092 to i8
  store i8 %12093, i8* %33, align 1
  %12094 = lshr i64 %12075, 63
  %12095 = xor i64 %12092, %12094
  %12096 = add nuw nsw i64 %12095, %12092
  %12097 = icmp eq i64 %12096, 2
  %12098 = zext i1 %12097 to i8
  store i8 %12098, i8* %39, align 1
  %12099 = add i64 %12070, -150
  %12100 = add i64 %12072, 17
  store i64 %12100, i64* %3, align 8
  %12101 = inttoptr i64 %12099 to i16*
  %12102 = load i16, i16* %12101, align 2
  %12103 = zext i16 %12102 to i64
  store i64 %12103, i64* %RCX.i11580, align 8
  %12104 = zext i16 %12102 to i64
  %12105 = shl nuw nsw i64 %12104, 4
  store i64 %12105, i64* %573, align 8
  %12106 = add i64 %12105, %12076
  store i64 %12106, i64* %RAX.i11582.pre-phi, align 8
  %12107 = icmp ult i64 %12106, %12076
  %12108 = icmp ult i64 %12106, %12105
  %12109 = or i1 %12107, %12108
  %12110 = zext i1 %12109 to i8
  store i8 %12110, i8* %14, align 1
  %12111 = trunc i64 %12106 to i32
  %12112 = and i32 %12111, 255
  %12113 = tail call i32 @llvm.ctpop.i32(i32 %12112)
  %12114 = trunc i32 %12113 to i8
  %12115 = and i8 %12114, 1
  %12116 = xor i8 %12115, 1
  store i8 %12116, i8* %21, align 1
  %12117 = xor i64 %12105, %12076
  %12118 = xor i64 %12117, %12106
  %12119 = lshr i64 %12118, 4
  %12120 = trunc i64 %12119 to i8
  %12121 = and i8 %12120, 1
  store i8 %12121, i8* %27, align 1
  %12122 = icmp eq i64 %12106, 0
  %12123 = zext i1 %12122 to i8
  store i8 %12123, i8* %30, align 1
  %12124 = lshr i64 %12106, 63
  %12125 = trunc i64 %12124 to i8
  store i8 %12125, i8* %33, align 1
  %12126 = xor i64 %12124, %12092
  %12127 = add nuw nsw i64 %12126, %12124
  %12128 = icmp eq i64 %12127, 2
  %12129 = zext i1 %12128 to i8
  store i8 %12129, i8* %39, align 1
  %12130 = inttoptr i64 %12106 to i32*
  %12131 = add i64 %12072, 28
  store i64 %12131, i64* %3, align 8
  %12132 = load i32, i32* %12130, align 4
  %12133 = zext i32 %12132 to i64
  store i64 %12133, i64* %RCX.i11580, align 8
  %12134 = load i64, i64* %RBP.i, align 8
  %12135 = add i64 %12134, -140
  %12136 = add i64 %12072, 34
  store i64 %12136, i64* %3, align 8
  %12137 = inttoptr i64 %12135 to i32*
  %12138 = load i32, i32* %12137, align 4
  %12139 = add i32 %12138, %12132
  %12140 = zext i32 %12139 to i64
  store i64 %12140, i64* %RCX.i11580, align 8
  %12141 = icmp ult i32 %12139, %12132
  %12142 = icmp ult i32 %12139, %12138
  %12143 = or i1 %12141, %12142
  %12144 = zext i1 %12143 to i8
  store i8 %12144, i8* %14, align 1
  %12145 = and i32 %12139, 255
  %12146 = tail call i32 @llvm.ctpop.i32(i32 %12145)
  %12147 = trunc i32 %12146 to i8
  %12148 = and i8 %12147, 1
  %12149 = xor i8 %12148, 1
  store i8 %12149, i8* %21, align 1
  %12150 = xor i32 %12138, %12132
  %12151 = xor i32 %12150, %12139
  %12152 = lshr i32 %12151, 4
  %12153 = trunc i32 %12152 to i8
  %12154 = and i8 %12153, 1
  store i8 %12154, i8* %27, align 1
  %12155 = icmp eq i32 %12139, 0
  %12156 = zext i1 %12155 to i8
  store i8 %12156, i8* %30, align 1
  %12157 = lshr i32 %12139, 31
  %12158 = trunc i32 %12157 to i8
  store i8 %12158, i8* %33, align 1
  %12159 = lshr i32 %12132, 31
  %12160 = lshr i32 %12138, 31
  %12161 = xor i32 %12157, %12159
  %12162 = xor i32 %12157, %12160
  %12163 = add nuw nsw i32 %12161, %12162
  %12164 = icmp eq i32 %12163, 2
  %12165 = zext i1 %12164 to i8
  store i8 %12165, i8* %39, align 1
  %12166 = add i64 %12072, 40
  store i64 %12166, i64* %3, align 8
  store i32 %12139, i32* %12137, align 4
  %12167 = load i64, i64* %RBP.i, align 8
  %12168 = add i64 %12167, -8
  %12169 = load i64, i64* %3, align 8
  %12170 = add i64 %12169, 4
  store i64 %12170, i64* %3, align 8
  %12171 = inttoptr i64 %12168 to i64*
  %12172 = load i64, i64* %12171, align 8
  %12173 = add i64 %12172, 51640
  store i64 %12173, i64* %RAX.i11582.pre-phi, align 8
  %12174 = icmp ugt i64 %12172, -51641
  %12175 = zext i1 %12174 to i8
  store i8 %12175, i8* %14, align 1
  %12176 = trunc i64 %12173 to i32
  %12177 = and i32 %12176, 255
  %12178 = tail call i32 @llvm.ctpop.i32(i32 %12177)
  %12179 = trunc i32 %12178 to i8
  %12180 = and i8 %12179, 1
  %12181 = xor i8 %12180, 1
  store i8 %12181, i8* %21, align 1
  %12182 = xor i64 %12172, 16
  %12183 = xor i64 %12182, %12173
  %12184 = lshr i64 %12183, 4
  %12185 = trunc i64 %12184 to i8
  %12186 = and i8 %12185, 1
  store i8 %12186, i8* %27, align 1
  %12187 = icmp eq i64 %12173, 0
  %12188 = zext i1 %12187 to i8
  store i8 %12188, i8* %30, align 1
  %12189 = lshr i64 %12173, 63
  %12190 = trunc i64 %12189 to i8
  store i8 %12190, i8* %33, align 1
  %12191 = lshr i64 %12172, 63
  %12192 = xor i64 %12189, %12191
  %12193 = add nuw nsw i64 %12192, %12189
  %12194 = icmp eq i64 %12193, 2
  %12195 = zext i1 %12194 to i8
  store i8 %12195, i8* %39, align 1
  %12196 = add i64 %12167, -150
  %12197 = add i64 %12169, 17
  store i64 %12197, i64* %3, align 8
  %12198 = inttoptr i64 %12196 to i16*
  %12199 = load i16, i16* %12198, align 2
  %12200 = zext i16 %12199 to i64
  store i64 %12200, i64* %RCX.i11580, align 8
  %12201 = zext i16 %12199 to i64
  %12202 = shl nuw nsw i64 %12201, 4
  store i64 %12202, i64* %573, align 8
  %12203 = add i64 %12202, %12173
  store i64 %12203, i64* %RAX.i11582.pre-phi, align 8
  %12204 = icmp ult i64 %12203, %12173
  %12205 = icmp ult i64 %12203, %12202
  %12206 = or i1 %12204, %12205
  %12207 = zext i1 %12206 to i8
  store i8 %12207, i8* %14, align 1
  %12208 = trunc i64 %12203 to i32
  %12209 = and i32 %12208, 255
  %12210 = tail call i32 @llvm.ctpop.i32(i32 %12209)
  %12211 = trunc i32 %12210 to i8
  %12212 = and i8 %12211, 1
  %12213 = xor i8 %12212, 1
  store i8 %12213, i8* %21, align 1
  %12214 = xor i64 %12202, %12173
  %12215 = xor i64 %12214, %12203
  %12216 = lshr i64 %12215, 4
  %12217 = trunc i64 %12216 to i8
  %12218 = and i8 %12217, 1
  store i8 %12218, i8* %27, align 1
  %12219 = icmp eq i64 %12203, 0
  %12220 = zext i1 %12219 to i8
  store i8 %12220, i8* %30, align 1
  %12221 = lshr i64 %12203, 63
  %12222 = trunc i64 %12221 to i8
  store i8 %12222, i8* %33, align 1
  %12223 = xor i64 %12221, %12189
  %12224 = add nuw nsw i64 %12223, %12221
  %12225 = icmp eq i64 %12224, 2
  %12226 = zext i1 %12225 to i8
  store i8 %12226, i8* %39, align 1
  %12227 = add i64 %12203, 4
  %12228 = add i64 %12169, 29
  store i64 %12228, i64* %3, align 8
  %12229 = inttoptr i64 %12227 to i32*
  %12230 = load i32, i32* %12229, align 4
  %12231 = zext i32 %12230 to i64
  store i64 %12231, i64* %RCX.i11580, align 8
  %12232 = load i64, i64* %RBP.i, align 8
  %12233 = add i64 %12232, -144
  %12234 = add i64 %12169, 35
  store i64 %12234, i64* %3, align 8
  %12235 = inttoptr i64 %12233 to i32*
  %12236 = load i32, i32* %12235, align 4
  %12237 = add i32 %12236, %12230
  %12238 = zext i32 %12237 to i64
  store i64 %12238, i64* %RCX.i11580, align 8
  %12239 = icmp ult i32 %12237, %12230
  %12240 = icmp ult i32 %12237, %12236
  %12241 = or i1 %12239, %12240
  %12242 = zext i1 %12241 to i8
  store i8 %12242, i8* %14, align 1
  %12243 = and i32 %12237, 255
  %12244 = tail call i32 @llvm.ctpop.i32(i32 %12243)
  %12245 = trunc i32 %12244 to i8
  %12246 = and i8 %12245, 1
  %12247 = xor i8 %12246, 1
  store i8 %12247, i8* %21, align 1
  %12248 = xor i32 %12236, %12230
  %12249 = xor i32 %12248, %12237
  %12250 = lshr i32 %12249, 4
  %12251 = trunc i32 %12250 to i8
  %12252 = and i8 %12251, 1
  store i8 %12252, i8* %27, align 1
  %12253 = icmp eq i32 %12237, 0
  %12254 = zext i1 %12253 to i8
  store i8 %12254, i8* %30, align 1
  %12255 = lshr i32 %12237, 31
  %12256 = trunc i32 %12255 to i8
  store i8 %12256, i8* %33, align 1
  %12257 = lshr i32 %12230, 31
  %12258 = lshr i32 %12236, 31
  %12259 = xor i32 %12255, %12257
  %12260 = xor i32 %12255, %12258
  %12261 = add nuw nsw i32 %12259, %12260
  %12262 = icmp eq i32 %12261, 2
  %12263 = zext i1 %12262 to i8
  store i8 %12263, i8* %39, align 1
  %12264 = add i64 %12169, 41
  store i64 %12264, i64* %3, align 8
  store i32 %12237, i32* %12235, align 4
  %12265 = load i64, i64* %RBP.i, align 8
  %12266 = add i64 %12265, -8
  %12267 = load i64, i64* %3, align 8
  %12268 = add i64 %12267, 4
  store i64 %12268, i64* %3, align 8
  %12269 = inttoptr i64 %12266 to i64*
  %12270 = load i64, i64* %12269, align 8
  %12271 = add i64 %12270, 51640
  store i64 %12271, i64* %RAX.i11582.pre-phi, align 8
  %12272 = icmp ugt i64 %12270, -51641
  %12273 = zext i1 %12272 to i8
  store i8 %12273, i8* %14, align 1
  %12274 = trunc i64 %12271 to i32
  %12275 = and i32 %12274, 255
  %12276 = tail call i32 @llvm.ctpop.i32(i32 %12275)
  %12277 = trunc i32 %12276 to i8
  %12278 = and i8 %12277, 1
  %12279 = xor i8 %12278, 1
  store i8 %12279, i8* %21, align 1
  %12280 = xor i64 %12270, 16
  %12281 = xor i64 %12280, %12271
  %12282 = lshr i64 %12281, 4
  %12283 = trunc i64 %12282 to i8
  %12284 = and i8 %12283, 1
  store i8 %12284, i8* %27, align 1
  %12285 = icmp eq i64 %12271, 0
  %12286 = zext i1 %12285 to i8
  store i8 %12286, i8* %30, align 1
  %12287 = lshr i64 %12271, 63
  %12288 = trunc i64 %12287 to i8
  store i8 %12288, i8* %33, align 1
  %12289 = lshr i64 %12270, 63
  %12290 = xor i64 %12287, %12289
  %12291 = add nuw nsw i64 %12290, %12287
  %12292 = icmp eq i64 %12291, 2
  %12293 = zext i1 %12292 to i8
  store i8 %12293, i8* %39, align 1
  %12294 = add i64 %12265, -150
  %12295 = add i64 %12267, 17
  store i64 %12295, i64* %3, align 8
  %12296 = inttoptr i64 %12294 to i16*
  %12297 = load i16, i16* %12296, align 2
  %12298 = zext i16 %12297 to i64
  store i64 %12298, i64* %RCX.i11580, align 8
  %12299 = zext i16 %12297 to i64
  %12300 = shl nuw nsw i64 %12299, 4
  store i64 %12300, i64* %573, align 8
  %12301 = add i64 %12300, %12271
  store i64 %12301, i64* %RAX.i11582.pre-phi, align 8
  %12302 = icmp ult i64 %12301, %12271
  %12303 = icmp ult i64 %12301, %12300
  %12304 = or i1 %12302, %12303
  %12305 = zext i1 %12304 to i8
  store i8 %12305, i8* %14, align 1
  %12306 = trunc i64 %12301 to i32
  %12307 = and i32 %12306, 255
  %12308 = tail call i32 @llvm.ctpop.i32(i32 %12307)
  %12309 = trunc i32 %12308 to i8
  %12310 = and i8 %12309, 1
  %12311 = xor i8 %12310, 1
  store i8 %12311, i8* %21, align 1
  %12312 = xor i64 %12300, %12271
  %12313 = xor i64 %12312, %12301
  %12314 = lshr i64 %12313, 4
  %12315 = trunc i64 %12314 to i8
  %12316 = and i8 %12315, 1
  store i8 %12316, i8* %27, align 1
  %12317 = icmp eq i64 %12301, 0
  %12318 = zext i1 %12317 to i8
  store i8 %12318, i8* %30, align 1
  %12319 = lshr i64 %12301, 63
  %12320 = trunc i64 %12319 to i8
  store i8 %12320, i8* %33, align 1
  %12321 = xor i64 %12319, %12287
  %12322 = add nuw nsw i64 %12321, %12319
  %12323 = icmp eq i64 %12322, 2
  %12324 = zext i1 %12323 to i8
  store i8 %12324, i8* %39, align 1
  %12325 = add i64 %12301, 8
  %12326 = add i64 %12267, 29
  store i64 %12326, i64* %3, align 8
  %12327 = inttoptr i64 %12325 to i32*
  %12328 = load i32, i32* %12327, align 4
  %12329 = zext i32 %12328 to i64
  store i64 %12329, i64* %RCX.i11580, align 8
  %12330 = load i64, i64* %RBP.i, align 8
  %12331 = add i64 %12330, -148
  %12332 = add i64 %12267, 35
  store i64 %12332, i64* %3, align 8
  %12333 = inttoptr i64 %12331 to i32*
  %12334 = load i32, i32* %12333, align 4
  %12335 = add i32 %12334, %12328
  %12336 = zext i32 %12335 to i64
  store i64 %12336, i64* %RCX.i11580, align 8
  %12337 = icmp ult i32 %12335, %12328
  %12338 = icmp ult i32 %12335, %12334
  %12339 = or i1 %12337, %12338
  %12340 = zext i1 %12339 to i8
  store i8 %12340, i8* %14, align 1
  %12341 = and i32 %12335, 255
  %12342 = tail call i32 @llvm.ctpop.i32(i32 %12341)
  %12343 = trunc i32 %12342 to i8
  %12344 = and i8 %12343, 1
  %12345 = xor i8 %12344, 1
  store i8 %12345, i8* %21, align 1
  %12346 = xor i32 %12334, %12328
  %12347 = xor i32 %12346, %12335
  %12348 = lshr i32 %12347, 4
  %12349 = trunc i32 %12348 to i8
  %12350 = and i8 %12349, 1
  store i8 %12350, i8* %27, align 1
  %12351 = icmp eq i32 %12335, 0
  %12352 = zext i1 %12351 to i8
  store i8 %12352, i8* %30, align 1
  %12353 = lshr i32 %12335, 31
  %12354 = trunc i32 %12353 to i8
  store i8 %12354, i8* %33, align 1
  %12355 = lshr i32 %12328, 31
  %12356 = lshr i32 %12334, 31
  %12357 = xor i32 %12353, %12355
  %12358 = xor i32 %12353, %12356
  %12359 = add nuw nsw i32 %12357, %12358
  %12360 = icmp eq i32 %12359, 2
  %12361 = zext i1 %12360 to i8
  store i8 %12361, i8* %39, align 1
  %12362 = add i64 %12267, 41
  store i64 %12362, i64* %3, align 8
  store i32 %12335, i32* %12333, align 4
  %12363 = load i64, i64* %RBP.i, align 8
  %12364 = add i64 %12363, -120
  %12365 = load i64, i64* %3, align 8
  %12366 = add i64 %12365, 4
  store i64 %12366, i64* %3, align 8
  %12367 = inttoptr i64 %12364 to i64*
  %12368 = load i64, i64* %12367, align 8
  store i64 %12368, i64* %RAX.i11582.pre-phi, align 8
  %12369 = add i64 %12363, -28
  %12370 = add i64 %12365, 7
  store i64 %12370, i64* %3, align 8
  %12371 = inttoptr i64 %12369 to i32*
  %12372 = load i32, i32* %12371, align 4
  %12373 = add i32 %12372, 29
  %12374 = zext i32 %12373 to i64
  store i64 %12374, i64* %RCX.i11580, align 8
  %12375 = icmp ugt i32 %12372, -30
  %12376 = zext i1 %12375 to i8
  store i8 %12376, i8* %14, align 1
  %12377 = and i32 %12373, 255
  %12378 = tail call i32 @llvm.ctpop.i32(i32 %12377)
  %12379 = trunc i32 %12378 to i8
  %12380 = and i8 %12379, 1
  %12381 = xor i8 %12380, 1
  store i8 %12381, i8* %21, align 1
  %12382 = xor i32 %12372, 16
  %12383 = xor i32 %12382, %12373
  %12384 = lshr i32 %12383, 4
  %12385 = trunc i32 %12384 to i8
  %12386 = and i8 %12385, 1
  store i8 %12386, i8* %27, align 1
  %12387 = icmp eq i32 %12373, 0
  %12388 = zext i1 %12387 to i8
  store i8 %12388, i8* %30, align 1
  %12389 = lshr i32 %12373, 31
  %12390 = trunc i32 %12389 to i8
  store i8 %12390, i8* %33, align 1
  %12391 = lshr i32 %12372, 31
  %12392 = xor i32 %12389, %12391
  %12393 = add nuw nsw i32 %12392, %12389
  %12394 = icmp eq i32 %12393, 2
  %12395 = zext i1 %12394 to i8
  store i8 %12395, i8* %39, align 1
  %12396 = sext i32 %12373 to i64
  store i64 %12396, i64* %573, align 8
  %12397 = shl nsw i64 %12396, 1
  %12398 = add i64 %12368, %12397
  %12399 = add i64 %12365, 17
  store i64 %12399, i64* %3, align 8
  %12400 = inttoptr i64 %12398 to i16*
  %12401 = load i16, i16* %12400, align 2
  store i16 %12401, i16* %SI.i, align 2
  %12402 = add i64 %12363, -150
  %12403 = add i64 %12365, 24
  store i64 %12403, i64* %3, align 8
  %12404 = inttoptr i64 %12402 to i16*
  store i16 %12401, i16* %12404, align 2
  %12405 = load i64, i64* %RBP.i, align 8
  %12406 = add i64 %12405, -8
  %12407 = load i64, i64* %3, align 8
  %12408 = add i64 %12407, 4
  store i64 %12408, i64* %3, align 8
  %12409 = inttoptr i64 %12406 to i64*
  %12410 = load i64, i64* %12409, align 8
  %12411 = add i64 %12410, 51640
  store i64 %12411, i64* %RAX.i11582.pre-phi, align 8
  %12412 = icmp ugt i64 %12410, -51641
  %12413 = zext i1 %12412 to i8
  store i8 %12413, i8* %14, align 1
  %12414 = trunc i64 %12411 to i32
  %12415 = and i32 %12414, 255
  %12416 = tail call i32 @llvm.ctpop.i32(i32 %12415)
  %12417 = trunc i32 %12416 to i8
  %12418 = and i8 %12417, 1
  %12419 = xor i8 %12418, 1
  store i8 %12419, i8* %21, align 1
  %12420 = xor i64 %12410, 16
  %12421 = xor i64 %12420, %12411
  %12422 = lshr i64 %12421, 4
  %12423 = trunc i64 %12422 to i8
  %12424 = and i8 %12423, 1
  store i8 %12424, i8* %27, align 1
  %12425 = icmp eq i64 %12411, 0
  %12426 = zext i1 %12425 to i8
  store i8 %12426, i8* %30, align 1
  %12427 = lshr i64 %12411, 63
  %12428 = trunc i64 %12427 to i8
  store i8 %12428, i8* %33, align 1
  %12429 = lshr i64 %12410, 63
  %12430 = xor i64 %12427, %12429
  %12431 = add nuw nsw i64 %12430, %12427
  %12432 = icmp eq i64 %12431, 2
  %12433 = zext i1 %12432 to i8
  store i8 %12433, i8* %39, align 1
  %12434 = add i64 %12405, -150
  %12435 = add i64 %12407, 17
  store i64 %12435, i64* %3, align 8
  %12436 = inttoptr i64 %12434 to i16*
  %12437 = load i16, i16* %12436, align 2
  %12438 = zext i16 %12437 to i64
  store i64 %12438, i64* %RCX.i11580, align 8
  %12439 = zext i16 %12437 to i64
  %12440 = shl nuw nsw i64 %12439, 4
  store i64 %12440, i64* %573, align 8
  %12441 = add i64 %12440, %12411
  store i64 %12441, i64* %RAX.i11582.pre-phi, align 8
  %12442 = icmp ult i64 %12441, %12411
  %12443 = icmp ult i64 %12441, %12440
  %12444 = or i1 %12442, %12443
  %12445 = zext i1 %12444 to i8
  store i8 %12445, i8* %14, align 1
  %12446 = trunc i64 %12441 to i32
  %12447 = and i32 %12446, 255
  %12448 = tail call i32 @llvm.ctpop.i32(i32 %12447)
  %12449 = trunc i32 %12448 to i8
  %12450 = and i8 %12449, 1
  %12451 = xor i8 %12450, 1
  store i8 %12451, i8* %21, align 1
  %12452 = xor i64 %12440, %12411
  %12453 = xor i64 %12452, %12441
  %12454 = lshr i64 %12453, 4
  %12455 = trunc i64 %12454 to i8
  %12456 = and i8 %12455, 1
  store i8 %12456, i8* %27, align 1
  %12457 = icmp eq i64 %12441, 0
  %12458 = zext i1 %12457 to i8
  store i8 %12458, i8* %30, align 1
  %12459 = lshr i64 %12441, 63
  %12460 = trunc i64 %12459 to i8
  store i8 %12460, i8* %33, align 1
  %12461 = xor i64 %12459, %12427
  %12462 = add nuw nsw i64 %12461, %12459
  %12463 = icmp eq i64 %12462, 2
  %12464 = zext i1 %12463 to i8
  store i8 %12464, i8* %39, align 1
  %12465 = inttoptr i64 %12441 to i32*
  %12466 = add i64 %12407, 28
  store i64 %12466, i64* %3, align 8
  %12467 = load i32, i32* %12465, align 4
  %12468 = zext i32 %12467 to i64
  store i64 %12468, i64* %RCX.i11580, align 8
  %12469 = load i64, i64* %RBP.i, align 8
  %12470 = add i64 %12469, -140
  %12471 = add i64 %12407, 34
  store i64 %12471, i64* %3, align 8
  %12472 = inttoptr i64 %12470 to i32*
  %12473 = load i32, i32* %12472, align 4
  %12474 = add i32 %12473, %12467
  %12475 = zext i32 %12474 to i64
  store i64 %12475, i64* %RCX.i11580, align 8
  %12476 = icmp ult i32 %12474, %12467
  %12477 = icmp ult i32 %12474, %12473
  %12478 = or i1 %12476, %12477
  %12479 = zext i1 %12478 to i8
  store i8 %12479, i8* %14, align 1
  %12480 = and i32 %12474, 255
  %12481 = tail call i32 @llvm.ctpop.i32(i32 %12480)
  %12482 = trunc i32 %12481 to i8
  %12483 = and i8 %12482, 1
  %12484 = xor i8 %12483, 1
  store i8 %12484, i8* %21, align 1
  %12485 = xor i32 %12473, %12467
  %12486 = xor i32 %12485, %12474
  %12487 = lshr i32 %12486, 4
  %12488 = trunc i32 %12487 to i8
  %12489 = and i8 %12488, 1
  store i8 %12489, i8* %27, align 1
  %12490 = icmp eq i32 %12474, 0
  %12491 = zext i1 %12490 to i8
  store i8 %12491, i8* %30, align 1
  %12492 = lshr i32 %12474, 31
  %12493 = trunc i32 %12492 to i8
  store i8 %12493, i8* %33, align 1
  %12494 = lshr i32 %12467, 31
  %12495 = lshr i32 %12473, 31
  %12496 = xor i32 %12492, %12494
  %12497 = xor i32 %12492, %12495
  %12498 = add nuw nsw i32 %12496, %12497
  %12499 = icmp eq i32 %12498, 2
  %12500 = zext i1 %12499 to i8
  store i8 %12500, i8* %39, align 1
  %12501 = add i64 %12407, 40
  store i64 %12501, i64* %3, align 8
  store i32 %12474, i32* %12472, align 4
  %12502 = load i64, i64* %RBP.i, align 8
  %12503 = add i64 %12502, -8
  %12504 = load i64, i64* %3, align 8
  %12505 = add i64 %12504, 4
  store i64 %12505, i64* %3, align 8
  %12506 = inttoptr i64 %12503 to i64*
  %12507 = load i64, i64* %12506, align 8
  %12508 = add i64 %12507, 51640
  store i64 %12508, i64* %RAX.i11582.pre-phi, align 8
  %12509 = icmp ugt i64 %12507, -51641
  %12510 = zext i1 %12509 to i8
  store i8 %12510, i8* %14, align 1
  %12511 = trunc i64 %12508 to i32
  %12512 = and i32 %12511, 255
  %12513 = tail call i32 @llvm.ctpop.i32(i32 %12512)
  %12514 = trunc i32 %12513 to i8
  %12515 = and i8 %12514, 1
  %12516 = xor i8 %12515, 1
  store i8 %12516, i8* %21, align 1
  %12517 = xor i64 %12507, 16
  %12518 = xor i64 %12517, %12508
  %12519 = lshr i64 %12518, 4
  %12520 = trunc i64 %12519 to i8
  %12521 = and i8 %12520, 1
  store i8 %12521, i8* %27, align 1
  %12522 = icmp eq i64 %12508, 0
  %12523 = zext i1 %12522 to i8
  store i8 %12523, i8* %30, align 1
  %12524 = lshr i64 %12508, 63
  %12525 = trunc i64 %12524 to i8
  store i8 %12525, i8* %33, align 1
  %12526 = lshr i64 %12507, 63
  %12527 = xor i64 %12524, %12526
  %12528 = add nuw nsw i64 %12527, %12524
  %12529 = icmp eq i64 %12528, 2
  %12530 = zext i1 %12529 to i8
  store i8 %12530, i8* %39, align 1
  %12531 = add i64 %12502, -150
  %12532 = add i64 %12504, 17
  store i64 %12532, i64* %3, align 8
  %12533 = inttoptr i64 %12531 to i16*
  %12534 = load i16, i16* %12533, align 2
  %12535 = zext i16 %12534 to i64
  store i64 %12535, i64* %RCX.i11580, align 8
  %12536 = zext i16 %12534 to i64
  %12537 = shl nuw nsw i64 %12536, 4
  store i64 %12537, i64* %573, align 8
  %12538 = add i64 %12537, %12508
  store i64 %12538, i64* %RAX.i11582.pre-phi, align 8
  %12539 = icmp ult i64 %12538, %12508
  %12540 = icmp ult i64 %12538, %12537
  %12541 = or i1 %12539, %12540
  %12542 = zext i1 %12541 to i8
  store i8 %12542, i8* %14, align 1
  %12543 = trunc i64 %12538 to i32
  %12544 = and i32 %12543, 255
  %12545 = tail call i32 @llvm.ctpop.i32(i32 %12544)
  %12546 = trunc i32 %12545 to i8
  %12547 = and i8 %12546, 1
  %12548 = xor i8 %12547, 1
  store i8 %12548, i8* %21, align 1
  %12549 = xor i64 %12537, %12508
  %12550 = xor i64 %12549, %12538
  %12551 = lshr i64 %12550, 4
  %12552 = trunc i64 %12551 to i8
  %12553 = and i8 %12552, 1
  store i8 %12553, i8* %27, align 1
  %12554 = icmp eq i64 %12538, 0
  %12555 = zext i1 %12554 to i8
  store i8 %12555, i8* %30, align 1
  %12556 = lshr i64 %12538, 63
  %12557 = trunc i64 %12556 to i8
  store i8 %12557, i8* %33, align 1
  %12558 = xor i64 %12556, %12524
  %12559 = add nuw nsw i64 %12558, %12556
  %12560 = icmp eq i64 %12559, 2
  %12561 = zext i1 %12560 to i8
  store i8 %12561, i8* %39, align 1
  %12562 = add i64 %12538, 4
  %12563 = add i64 %12504, 29
  store i64 %12563, i64* %3, align 8
  %12564 = inttoptr i64 %12562 to i32*
  %12565 = load i32, i32* %12564, align 4
  %12566 = zext i32 %12565 to i64
  store i64 %12566, i64* %RCX.i11580, align 8
  %12567 = load i64, i64* %RBP.i, align 8
  %12568 = add i64 %12567, -144
  %12569 = add i64 %12504, 35
  store i64 %12569, i64* %3, align 8
  %12570 = inttoptr i64 %12568 to i32*
  %12571 = load i32, i32* %12570, align 4
  %12572 = add i32 %12571, %12565
  %12573 = zext i32 %12572 to i64
  store i64 %12573, i64* %RCX.i11580, align 8
  %12574 = icmp ult i32 %12572, %12565
  %12575 = icmp ult i32 %12572, %12571
  %12576 = or i1 %12574, %12575
  %12577 = zext i1 %12576 to i8
  store i8 %12577, i8* %14, align 1
  %12578 = and i32 %12572, 255
  %12579 = tail call i32 @llvm.ctpop.i32(i32 %12578)
  %12580 = trunc i32 %12579 to i8
  %12581 = and i8 %12580, 1
  %12582 = xor i8 %12581, 1
  store i8 %12582, i8* %21, align 1
  %12583 = xor i32 %12571, %12565
  %12584 = xor i32 %12583, %12572
  %12585 = lshr i32 %12584, 4
  %12586 = trunc i32 %12585 to i8
  %12587 = and i8 %12586, 1
  store i8 %12587, i8* %27, align 1
  %12588 = icmp eq i32 %12572, 0
  %12589 = zext i1 %12588 to i8
  store i8 %12589, i8* %30, align 1
  %12590 = lshr i32 %12572, 31
  %12591 = trunc i32 %12590 to i8
  store i8 %12591, i8* %33, align 1
  %12592 = lshr i32 %12565, 31
  %12593 = lshr i32 %12571, 31
  %12594 = xor i32 %12590, %12592
  %12595 = xor i32 %12590, %12593
  %12596 = add nuw nsw i32 %12594, %12595
  %12597 = icmp eq i32 %12596, 2
  %12598 = zext i1 %12597 to i8
  store i8 %12598, i8* %39, align 1
  %12599 = add i64 %12504, 41
  store i64 %12599, i64* %3, align 8
  store i32 %12572, i32* %12570, align 4
  %12600 = load i64, i64* %RBP.i, align 8
  %12601 = add i64 %12600, -8
  %12602 = load i64, i64* %3, align 8
  %12603 = add i64 %12602, 4
  store i64 %12603, i64* %3, align 8
  %12604 = inttoptr i64 %12601 to i64*
  %12605 = load i64, i64* %12604, align 8
  %12606 = add i64 %12605, 51640
  store i64 %12606, i64* %RAX.i11582.pre-phi, align 8
  %12607 = icmp ugt i64 %12605, -51641
  %12608 = zext i1 %12607 to i8
  store i8 %12608, i8* %14, align 1
  %12609 = trunc i64 %12606 to i32
  %12610 = and i32 %12609, 255
  %12611 = tail call i32 @llvm.ctpop.i32(i32 %12610)
  %12612 = trunc i32 %12611 to i8
  %12613 = and i8 %12612, 1
  %12614 = xor i8 %12613, 1
  store i8 %12614, i8* %21, align 1
  %12615 = xor i64 %12605, 16
  %12616 = xor i64 %12615, %12606
  %12617 = lshr i64 %12616, 4
  %12618 = trunc i64 %12617 to i8
  %12619 = and i8 %12618, 1
  store i8 %12619, i8* %27, align 1
  %12620 = icmp eq i64 %12606, 0
  %12621 = zext i1 %12620 to i8
  store i8 %12621, i8* %30, align 1
  %12622 = lshr i64 %12606, 63
  %12623 = trunc i64 %12622 to i8
  store i8 %12623, i8* %33, align 1
  %12624 = lshr i64 %12605, 63
  %12625 = xor i64 %12622, %12624
  %12626 = add nuw nsw i64 %12625, %12622
  %12627 = icmp eq i64 %12626, 2
  %12628 = zext i1 %12627 to i8
  store i8 %12628, i8* %39, align 1
  %12629 = add i64 %12600, -150
  %12630 = add i64 %12602, 17
  store i64 %12630, i64* %3, align 8
  %12631 = inttoptr i64 %12629 to i16*
  %12632 = load i16, i16* %12631, align 2
  %12633 = zext i16 %12632 to i64
  store i64 %12633, i64* %RCX.i11580, align 8
  %12634 = zext i16 %12632 to i64
  %12635 = shl nuw nsw i64 %12634, 4
  store i64 %12635, i64* %573, align 8
  %12636 = add i64 %12635, %12606
  store i64 %12636, i64* %RAX.i11582.pre-phi, align 8
  %12637 = icmp ult i64 %12636, %12606
  %12638 = icmp ult i64 %12636, %12635
  %12639 = or i1 %12637, %12638
  %12640 = zext i1 %12639 to i8
  store i8 %12640, i8* %14, align 1
  %12641 = trunc i64 %12636 to i32
  %12642 = and i32 %12641, 255
  %12643 = tail call i32 @llvm.ctpop.i32(i32 %12642)
  %12644 = trunc i32 %12643 to i8
  %12645 = and i8 %12644, 1
  %12646 = xor i8 %12645, 1
  store i8 %12646, i8* %21, align 1
  %12647 = xor i64 %12635, %12606
  %12648 = xor i64 %12647, %12636
  %12649 = lshr i64 %12648, 4
  %12650 = trunc i64 %12649 to i8
  %12651 = and i8 %12650, 1
  store i8 %12651, i8* %27, align 1
  %12652 = icmp eq i64 %12636, 0
  %12653 = zext i1 %12652 to i8
  store i8 %12653, i8* %30, align 1
  %12654 = lshr i64 %12636, 63
  %12655 = trunc i64 %12654 to i8
  store i8 %12655, i8* %33, align 1
  %12656 = xor i64 %12654, %12622
  %12657 = add nuw nsw i64 %12656, %12654
  %12658 = icmp eq i64 %12657, 2
  %12659 = zext i1 %12658 to i8
  store i8 %12659, i8* %39, align 1
  %12660 = add i64 %12636, 8
  %12661 = add i64 %12602, 29
  store i64 %12661, i64* %3, align 8
  %12662 = inttoptr i64 %12660 to i32*
  %12663 = load i32, i32* %12662, align 4
  %12664 = zext i32 %12663 to i64
  store i64 %12664, i64* %RCX.i11580, align 8
  %12665 = load i64, i64* %RBP.i, align 8
  %12666 = add i64 %12665, -148
  %12667 = add i64 %12602, 35
  store i64 %12667, i64* %3, align 8
  %12668 = inttoptr i64 %12666 to i32*
  %12669 = load i32, i32* %12668, align 4
  %12670 = add i32 %12669, %12663
  %12671 = zext i32 %12670 to i64
  store i64 %12671, i64* %RCX.i11580, align 8
  %12672 = icmp ult i32 %12670, %12663
  %12673 = icmp ult i32 %12670, %12669
  %12674 = or i1 %12672, %12673
  %12675 = zext i1 %12674 to i8
  store i8 %12675, i8* %14, align 1
  %12676 = and i32 %12670, 255
  %12677 = tail call i32 @llvm.ctpop.i32(i32 %12676)
  %12678 = trunc i32 %12677 to i8
  %12679 = and i8 %12678, 1
  %12680 = xor i8 %12679, 1
  store i8 %12680, i8* %21, align 1
  %12681 = xor i32 %12669, %12663
  %12682 = xor i32 %12681, %12670
  %12683 = lshr i32 %12682, 4
  %12684 = trunc i32 %12683 to i8
  %12685 = and i8 %12684, 1
  store i8 %12685, i8* %27, align 1
  %12686 = icmp eq i32 %12670, 0
  %12687 = zext i1 %12686 to i8
  store i8 %12687, i8* %30, align 1
  %12688 = lshr i32 %12670, 31
  %12689 = trunc i32 %12688 to i8
  store i8 %12689, i8* %33, align 1
  %12690 = lshr i32 %12663, 31
  %12691 = lshr i32 %12669, 31
  %12692 = xor i32 %12688, %12690
  %12693 = xor i32 %12688, %12691
  %12694 = add nuw nsw i32 %12692, %12693
  %12695 = icmp eq i32 %12694, 2
  %12696 = zext i1 %12695 to i8
  store i8 %12696, i8* %39, align 1
  %12697 = add i64 %12602, 41
  store i64 %12697, i64* %3, align 8
  store i32 %12670, i32* %12668, align 4
  %12698 = load i64, i64* %RBP.i, align 8
  %12699 = add i64 %12698, -120
  %12700 = load i64, i64* %3, align 8
  %12701 = add i64 %12700, 4
  store i64 %12701, i64* %3, align 8
  %12702 = inttoptr i64 %12699 to i64*
  %12703 = load i64, i64* %12702, align 8
  store i64 %12703, i64* %RAX.i11582.pre-phi, align 8
  %12704 = add i64 %12698, -28
  %12705 = add i64 %12700, 7
  store i64 %12705, i64* %3, align 8
  %12706 = inttoptr i64 %12704 to i32*
  %12707 = load i32, i32* %12706, align 4
  %12708 = add i32 %12707, 30
  %12709 = zext i32 %12708 to i64
  store i64 %12709, i64* %RCX.i11580, align 8
  %12710 = icmp ugt i32 %12707, -31
  %12711 = zext i1 %12710 to i8
  store i8 %12711, i8* %14, align 1
  %12712 = and i32 %12708, 255
  %12713 = tail call i32 @llvm.ctpop.i32(i32 %12712)
  %12714 = trunc i32 %12713 to i8
  %12715 = and i8 %12714, 1
  %12716 = xor i8 %12715, 1
  store i8 %12716, i8* %21, align 1
  %12717 = xor i32 %12707, 16
  %12718 = xor i32 %12717, %12708
  %12719 = lshr i32 %12718, 4
  %12720 = trunc i32 %12719 to i8
  %12721 = and i8 %12720, 1
  store i8 %12721, i8* %27, align 1
  %12722 = icmp eq i32 %12708, 0
  %12723 = zext i1 %12722 to i8
  store i8 %12723, i8* %30, align 1
  %12724 = lshr i32 %12708, 31
  %12725 = trunc i32 %12724 to i8
  store i8 %12725, i8* %33, align 1
  %12726 = lshr i32 %12707, 31
  %12727 = xor i32 %12724, %12726
  %12728 = add nuw nsw i32 %12727, %12724
  %12729 = icmp eq i32 %12728, 2
  %12730 = zext i1 %12729 to i8
  store i8 %12730, i8* %39, align 1
  %12731 = sext i32 %12708 to i64
  store i64 %12731, i64* %573, align 8
  %12732 = shl nsw i64 %12731, 1
  %12733 = add i64 %12703, %12732
  %12734 = add i64 %12700, 17
  store i64 %12734, i64* %3, align 8
  %12735 = inttoptr i64 %12733 to i16*
  %12736 = load i16, i16* %12735, align 2
  store i16 %12736, i16* %SI.i, align 2
  %12737 = add i64 %12698, -150
  %12738 = add i64 %12700, 24
  store i64 %12738, i64* %3, align 8
  %12739 = inttoptr i64 %12737 to i16*
  store i16 %12736, i16* %12739, align 2
  %12740 = load i64, i64* %RBP.i, align 8
  %12741 = add i64 %12740, -8
  %12742 = load i64, i64* %3, align 8
  %12743 = add i64 %12742, 4
  store i64 %12743, i64* %3, align 8
  %12744 = inttoptr i64 %12741 to i64*
  %12745 = load i64, i64* %12744, align 8
  %12746 = add i64 %12745, 51640
  store i64 %12746, i64* %RAX.i11582.pre-phi, align 8
  %12747 = icmp ugt i64 %12745, -51641
  %12748 = zext i1 %12747 to i8
  store i8 %12748, i8* %14, align 1
  %12749 = trunc i64 %12746 to i32
  %12750 = and i32 %12749, 255
  %12751 = tail call i32 @llvm.ctpop.i32(i32 %12750)
  %12752 = trunc i32 %12751 to i8
  %12753 = and i8 %12752, 1
  %12754 = xor i8 %12753, 1
  store i8 %12754, i8* %21, align 1
  %12755 = xor i64 %12745, 16
  %12756 = xor i64 %12755, %12746
  %12757 = lshr i64 %12756, 4
  %12758 = trunc i64 %12757 to i8
  %12759 = and i8 %12758, 1
  store i8 %12759, i8* %27, align 1
  %12760 = icmp eq i64 %12746, 0
  %12761 = zext i1 %12760 to i8
  store i8 %12761, i8* %30, align 1
  %12762 = lshr i64 %12746, 63
  %12763 = trunc i64 %12762 to i8
  store i8 %12763, i8* %33, align 1
  %12764 = lshr i64 %12745, 63
  %12765 = xor i64 %12762, %12764
  %12766 = add nuw nsw i64 %12765, %12762
  %12767 = icmp eq i64 %12766, 2
  %12768 = zext i1 %12767 to i8
  store i8 %12768, i8* %39, align 1
  %12769 = add i64 %12740, -150
  %12770 = add i64 %12742, 17
  store i64 %12770, i64* %3, align 8
  %12771 = inttoptr i64 %12769 to i16*
  %12772 = load i16, i16* %12771, align 2
  %12773 = zext i16 %12772 to i64
  store i64 %12773, i64* %RCX.i11580, align 8
  %12774 = zext i16 %12772 to i64
  %12775 = shl nuw nsw i64 %12774, 4
  store i64 %12775, i64* %573, align 8
  %12776 = add i64 %12775, %12746
  store i64 %12776, i64* %RAX.i11582.pre-phi, align 8
  %12777 = icmp ult i64 %12776, %12746
  %12778 = icmp ult i64 %12776, %12775
  %12779 = or i1 %12777, %12778
  %12780 = zext i1 %12779 to i8
  store i8 %12780, i8* %14, align 1
  %12781 = trunc i64 %12776 to i32
  %12782 = and i32 %12781, 255
  %12783 = tail call i32 @llvm.ctpop.i32(i32 %12782)
  %12784 = trunc i32 %12783 to i8
  %12785 = and i8 %12784, 1
  %12786 = xor i8 %12785, 1
  store i8 %12786, i8* %21, align 1
  %12787 = xor i64 %12775, %12746
  %12788 = xor i64 %12787, %12776
  %12789 = lshr i64 %12788, 4
  %12790 = trunc i64 %12789 to i8
  %12791 = and i8 %12790, 1
  store i8 %12791, i8* %27, align 1
  %12792 = icmp eq i64 %12776, 0
  %12793 = zext i1 %12792 to i8
  store i8 %12793, i8* %30, align 1
  %12794 = lshr i64 %12776, 63
  %12795 = trunc i64 %12794 to i8
  store i8 %12795, i8* %33, align 1
  %12796 = xor i64 %12794, %12762
  %12797 = add nuw nsw i64 %12796, %12794
  %12798 = icmp eq i64 %12797, 2
  %12799 = zext i1 %12798 to i8
  store i8 %12799, i8* %39, align 1
  %12800 = inttoptr i64 %12776 to i32*
  %12801 = add i64 %12742, 28
  store i64 %12801, i64* %3, align 8
  %12802 = load i32, i32* %12800, align 4
  %12803 = zext i32 %12802 to i64
  store i64 %12803, i64* %RCX.i11580, align 8
  %12804 = load i64, i64* %RBP.i, align 8
  %12805 = add i64 %12804, -140
  %12806 = add i64 %12742, 34
  store i64 %12806, i64* %3, align 8
  %12807 = inttoptr i64 %12805 to i32*
  %12808 = load i32, i32* %12807, align 4
  %12809 = add i32 %12808, %12802
  %12810 = zext i32 %12809 to i64
  store i64 %12810, i64* %RCX.i11580, align 8
  %12811 = icmp ult i32 %12809, %12802
  %12812 = icmp ult i32 %12809, %12808
  %12813 = or i1 %12811, %12812
  %12814 = zext i1 %12813 to i8
  store i8 %12814, i8* %14, align 1
  %12815 = and i32 %12809, 255
  %12816 = tail call i32 @llvm.ctpop.i32(i32 %12815)
  %12817 = trunc i32 %12816 to i8
  %12818 = and i8 %12817, 1
  %12819 = xor i8 %12818, 1
  store i8 %12819, i8* %21, align 1
  %12820 = xor i32 %12808, %12802
  %12821 = xor i32 %12820, %12809
  %12822 = lshr i32 %12821, 4
  %12823 = trunc i32 %12822 to i8
  %12824 = and i8 %12823, 1
  store i8 %12824, i8* %27, align 1
  %12825 = icmp eq i32 %12809, 0
  %12826 = zext i1 %12825 to i8
  store i8 %12826, i8* %30, align 1
  %12827 = lshr i32 %12809, 31
  %12828 = trunc i32 %12827 to i8
  store i8 %12828, i8* %33, align 1
  %12829 = lshr i32 %12802, 31
  %12830 = lshr i32 %12808, 31
  %12831 = xor i32 %12827, %12829
  %12832 = xor i32 %12827, %12830
  %12833 = add nuw nsw i32 %12831, %12832
  %12834 = icmp eq i32 %12833, 2
  %12835 = zext i1 %12834 to i8
  store i8 %12835, i8* %39, align 1
  %12836 = add i64 %12742, 40
  store i64 %12836, i64* %3, align 8
  store i32 %12809, i32* %12807, align 4
  %12837 = load i64, i64* %RBP.i, align 8
  %12838 = add i64 %12837, -8
  %12839 = load i64, i64* %3, align 8
  %12840 = add i64 %12839, 4
  store i64 %12840, i64* %3, align 8
  %12841 = inttoptr i64 %12838 to i64*
  %12842 = load i64, i64* %12841, align 8
  %12843 = add i64 %12842, 51640
  store i64 %12843, i64* %RAX.i11582.pre-phi, align 8
  %12844 = icmp ugt i64 %12842, -51641
  %12845 = zext i1 %12844 to i8
  store i8 %12845, i8* %14, align 1
  %12846 = trunc i64 %12843 to i32
  %12847 = and i32 %12846, 255
  %12848 = tail call i32 @llvm.ctpop.i32(i32 %12847)
  %12849 = trunc i32 %12848 to i8
  %12850 = and i8 %12849, 1
  %12851 = xor i8 %12850, 1
  store i8 %12851, i8* %21, align 1
  %12852 = xor i64 %12842, 16
  %12853 = xor i64 %12852, %12843
  %12854 = lshr i64 %12853, 4
  %12855 = trunc i64 %12854 to i8
  %12856 = and i8 %12855, 1
  store i8 %12856, i8* %27, align 1
  %12857 = icmp eq i64 %12843, 0
  %12858 = zext i1 %12857 to i8
  store i8 %12858, i8* %30, align 1
  %12859 = lshr i64 %12843, 63
  %12860 = trunc i64 %12859 to i8
  store i8 %12860, i8* %33, align 1
  %12861 = lshr i64 %12842, 63
  %12862 = xor i64 %12859, %12861
  %12863 = add nuw nsw i64 %12862, %12859
  %12864 = icmp eq i64 %12863, 2
  %12865 = zext i1 %12864 to i8
  store i8 %12865, i8* %39, align 1
  %12866 = add i64 %12837, -150
  %12867 = add i64 %12839, 17
  store i64 %12867, i64* %3, align 8
  %12868 = inttoptr i64 %12866 to i16*
  %12869 = load i16, i16* %12868, align 2
  %12870 = zext i16 %12869 to i64
  store i64 %12870, i64* %RCX.i11580, align 8
  %12871 = zext i16 %12869 to i64
  %12872 = shl nuw nsw i64 %12871, 4
  store i64 %12872, i64* %573, align 8
  %12873 = add i64 %12872, %12843
  store i64 %12873, i64* %RAX.i11582.pre-phi, align 8
  %12874 = icmp ult i64 %12873, %12843
  %12875 = icmp ult i64 %12873, %12872
  %12876 = or i1 %12874, %12875
  %12877 = zext i1 %12876 to i8
  store i8 %12877, i8* %14, align 1
  %12878 = trunc i64 %12873 to i32
  %12879 = and i32 %12878, 255
  %12880 = tail call i32 @llvm.ctpop.i32(i32 %12879)
  %12881 = trunc i32 %12880 to i8
  %12882 = and i8 %12881, 1
  %12883 = xor i8 %12882, 1
  store i8 %12883, i8* %21, align 1
  %12884 = xor i64 %12872, %12843
  %12885 = xor i64 %12884, %12873
  %12886 = lshr i64 %12885, 4
  %12887 = trunc i64 %12886 to i8
  %12888 = and i8 %12887, 1
  store i8 %12888, i8* %27, align 1
  %12889 = icmp eq i64 %12873, 0
  %12890 = zext i1 %12889 to i8
  store i8 %12890, i8* %30, align 1
  %12891 = lshr i64 %12873, 63
  %12892 = trunc i64 %12891 to i8
  store i8 %12892, i8* %33, align 1
  %12893 = xor i64 %12891, %12859
  %12894 = add nuw nsw i64 %12893, %12891
  %12895 = icmp eq i64 %12894, 2
  %12896 = zext i1 %12895 to i8
  store i8 %12896, i8* %39, align 1
  %12897 = add i64 %12873, 4
  %12898 = add i64 %12839, 29
  store i64 %12898, i64* %3, align 8
  %12899 = inttoptr i64 %12897 to i32*
  %12900 = load i32, i32* %12899, align 4
  %12901 = zext i32 %12900 to i64
  store i64 %12901, i64* %RCX.i11580, align 8
  %12902 = load i64, i64* %RBP.i, align 8
  %12903 = add i64 %12902, -144
  %12904 = add i64 %12839, 35
  store i64 %12904, i64* %3, align 8
  %12905 = inttoptr i64 %12903 to i32*
  %12906 = load i32, i32* %12905, align 4
  %12907 = add i32 %12906, %12900
  %12908 = zext i32 %12907 to i64
  store i64 %12908, i64* %RCX.i11580, align 8
  %12909 = icmp ult i32 %12907, %12900
  %12910 = icmp ult i32 %12907, %12906
  %12911 = or i1 %12909, %12910
  %12912 = zext i1 %12911 to i8
  store i8 %12912, i8* %14, align 1
  %12913 = and i32 %12907, 255
  %12914 = tail call i32 @llvm.ctpop.i32(i32 %12913)
  %12915 = trunc i32 %12914 to i8
  %12916 = and i8 %12915, 1
  %12917 = xor i8 %12916, 1
  store i8 %12917, i8* %21, align 1
  %12918 = xor i32 %12906, %12900
  %12919 = xor i32 %12918, %12907
  %12920 = lshr i32 %12919, 4
  %12921 = trunc i32 %12920 to i8
  %12922 = and i8 %12921, 1
  store i8 %12922, i8* %27, align 1
  %12923 = icmp eq i32 %12907, 0
  %12924 = zext i1 %12923 to i8
  store i8 %12924, i8* %30, align 1
  %12925 = lshr i32 %12907, 31
  %12926 = trunc i32 %12925 to i8
  store i8 %12926, i8* %33, align 1
  %12927 = lshr i32 %12900, 31
  %12928 = lshr i32 %12906, 31
  %12929 = xor i32 %12925, %12927
  %12930 = xor i32 %12925, %12928
  %12931 = add nuw nsw i32 %12929, %12930
  %12932 = icmp eq i32 %12931, 2
  %12933 = zext i1 %12932 to i8
  store i8 %12933, i8* %39, align 1
  %12934 = add i64 %12839, 41
  store i64 %12934, i64* %3, align 8
  store i32 %12907, i32* %12905, align 4
  %12935 = load i64, i64* %RBP.i, align 8
  %12936 = add i64 %12935, -8
  %12937 = load i64, i64* %3, align 8
  %12938 = add i64 %12937, 4
  store i64 %12938, i64* %3, align 8
  %12939 = inttoptr i64 %12936 to i64*
  %12940 = load i64, i64* %12939, align 8
  %12941 = add i64 %12940, 51640
  store i64 %12941, i64* %RAX.i11582.pre-phi, align 8
  %12942 = icmp ugt i64 %12940, -51641
  %12943 = zext i1 %12942 to i8
  store i8 %12943, i8* %14, align 1
  %12944 = trunc i64 %12941 to i32
  %12945 = and i32 %12944, 255
  %12946 = tail call i32 @llvm.ctpop.i32(i32 %12945)
  %12947 = trunc i32 %12946 to i8
  %12948 = and i8 %12947, 1
  %12949 = xor i8 %12948, 1
  store i8 %12949, i8* %21, align 1
  %12950 = xor i64 %12940, 16
  %12951 = xor i64 %12950, %12941
  %12952 = lshr i64 %12951, 4
  %12953 = trunc i64 %12952 to i8
  %12954 = and i8 %12953, 1
  store i8 %12954, i8* %27, align 1
  %12955 = icmp eq i64 %12941, 0
  %12956 = zext i1 %12955 to i8
  store i8 %12956, i8* %30, align 1
  %12957 = lshr i64 %12941, 63
  %12958 = trunc i64 %12957 to i8
  store i8 %12958, i8* %33, align 1
  %12959 = lshr i64 %12940, 63
  %12960 = xor i64 %12957, %12959
  %12961 = add nuw nsw i64 %12960, %12957
  %12962 = icmp eq i64 %12961, 2
  %12963 = zext i1 %12962 to i8
  store i8 %12963, i8* %39, align 1
  %12964 = add i64 %12935, -150
  %12965 = add i64 %12937, 17
  store i64 %12965, i64* %3, align 8
  %12966 = inttoptr i64 %12964 to i16*
  %12967 = load i16, i16* %12966, align 2
  %12968 = zext i16 %12967 to i64
  store i64 %12968, i64* %RCX.i11580, align 8
  %12969 = zext i16 %12967 to i64
  %12970 = shl nuw nsw i64 %12969, 4
  store i64 %12970, i64* %573, align 8
  %12971 = add i64 %12970, %12941
  store i64 %12971, i64* %RAX.i11582.pre-phi, align 8
  %12972 = icmp ult i64 %12971, %12941
  %12973 = icmp ult i64 %12971, %12970
  %12974 = or i1 %12972, %12973
  %12975 = zext i1 %12974 to i8
  store i8 %12975, i8* %14, align 1
  %12976 = trunc i64 %12971 to i32
  %12977 = and i32 %12976, 255
  %12978 = tail call i32 @llvm.ctpop.i32(i32 %12977)
  %12979 = trunc i32 %12978 to i8
  %12980 = and i8 %12979, 1
  %12981 = xor i8 %12980, 1
  store i8 %12981, i8* %21, align 1
  %12982 = xor i64 %12970, %12941
  %12983 = xor i64 %12982, %12971
  %12984 = lshr i64 %12983, 4
  %12985 = trunc i64 %12984 to i8
  %12986 = and i8 %12985, 1
  store i8 %12986, i8* %27, align 1
  %12987 = icmp eq i64 %12971, 0
  %12988 = zext i1 %12987 to i8
  store i8 %12988, i8* %30, align 1
  %12989 = lshr i64 %12971, 63
  %12990 = trunc i64 %12989 to i8
  store i8 %12990, i8* %33, align 1
  %12991 = xor i64 %12989, %12957
  %12992 = add nuw nsw i64 %12991, %12989
  %12993 = icmp eq i64 %12992, 2
  %12994 = zext i1 %12993 to i8
  store i8 %12994, i8* %39, align 1
  %12995 = add i64 %12971, 8
  %12996 = add i64 %12937, 29
  store i64 %12996, i64* %3, align 8
  %12997 = inttoptr i64 %12995 to i32*
  %12998 = load i32, i32* %12997, align 4
  %12999 = zext i32 %12998 to i64
  store i64 %12999, i64* %RCX.i11580, align 8
  %13000 = load i64, i64* %RBP.i, align 8
  %13001 = add i64 %13000, -148
  %13002 = add i64 %12937, 35
  store i64 %13002, i64* %3, align 8
  %13003 = inttoptr i64 %13001 to i32*
  %13004 = load i32, i32* %13003, align 4
  %13005 = add i32 %13004, %12998
  %13006 = zext i32 %13005 to i64
  store i64 %13006, i64* %RCX.i11580, align 8
  %13007 = icmp ult i32 %13005, %12998
  %13008 = icmp ult i32 %13005, %13004
  %13009 = or i1 %13007, %13008
  %13010 = zext i1 %13009 to i8
  store i8 %13010, i8* %14, align 1
  %13011 = and i32 %13005, 255
  %13012 = tail call i32 @llvm.ctpop.i32(i32 %13011)
  %13013 = trunc i32 %13012 to i8
  %13014 = and i8 %13013, 1
  %13015 = xor i8 %13014, 1
  store i8 %13015, i8* %21, align 1
  %13016 = xor i32 %13004, %12998
  %13017 = xor i32 %13016, %13005
  %13018 = lshr i32 %13017, 4
  %13019 = trunc i32 %13018 to i8
  %13020 = and i8 %13019, 1
  store i8 %13020, i8* %27, align 1
  %13021 = icmp eq i32 %13005, 0
  %13022 = zext i1 %13021 to i8
  store i8 %13022, i8* %30, align 1
  %13023 = lshr i32 %13005, 31
  %13024 = trunc i32 %13023 to i8
  store i8 %13024, i8* %33, align 1
  %13025 = lshr i32 %12998, 31
  %13026 = lshr i32 %13004, 31
  %13027 = xor i32 %13023, %13025
  %13028 = xor i32 %13023, %13026
  %13029 = add nuw nsw i32 %13027, %13028
  %13030 = icmp eq i32 %13029, 2
  %13031 = zext i1 %13030 to i8
  store i8 %13031, i8* %39, align 1
  %13032 = add i64 %12937, 41
  store i64 %13032, i64* %3, align 8
  store i32 %13005, i32* %13003, align 4
  %13033 = load i64, i64* %RBP.i, align 8
  %13034 = add i64 %13033, -120
  %13035 = load i64, i64* %3, align 8
  %13036 = add i64 %13035, 4
  store i64 %13036, i64* %3, align 8
  %13037 = inttoptr i64 %13034 to i64*
  %13038 = load i64, i64* %13037, align 8
  store i64 %13038, i64* %RAX.i11582.pre-phi, align 8
  %13039 = add i64 %13033, -28
  %13040 = add i64 %13035, 7
  store i64 %13040, i64* %3, align 8
  %13041 = inttoptr i64 %13039 to i32*
  %13042 = load i32, i32* %13041, align 4
  %13043 = add i32 %13042, 31
  %13044 = zext i32 %13043 to i64
  store i64 %13044, i64* %RCX.i11580, align 8
  %13045 = icmp ugt i32 %13042, -32
  %13046 = zext i1 %13045 to i8
  store i8 %13046, i8* %14, align 1
  %13047 = and i32 %13043, 255
  %13048 = tail call i32 @llvm.ctpop.i32(i32 %13047)
  %13049 = trunc i32 %13048 to i8
  %13050 = and i8 %13049, 1
  %13051 = xor i8 %13050, 1
  store i8 %13051, i8* %21, align 1
  %13052 = xor i32 %13042, 16
  %13053 = xor i32 %13052, %13043
  %13054 = lshr i32 %13053, 4
  %13055 = trunc i32 %13054 to i8
  %13056 = and i8 %13055, 1
  store i8 %13056, i8* %27, align 1
  %13057 = icmp eq i32 %13043, 0
  %13058 = zext i1 %13057 to i8
  store i8 %13058, i8* %30, align 1
  %13059 = lshr i32 %13043, 31
  %13060 = trunc i32 %13059 to i8
  store i8 %13060, i8* %33, align 1
  %13061 = lshr i32 %13042, 31
  %13062 = xor i32 %13059, %13061
  %13063 = add nuw nsw i32 %13062, %13059
  %13064 = icmp eq i32 %13063, 2
  %13065 = zext i1 %13064 to i8
  store i8 %13065, i8* %39, align 1
  %13066 = sext i32 %13043 to i64
  store i64 %13066, i64* %573, align 8
  %13067 = shl nsw i64 %13066, 1
  %13068 = add i64 %13038, %13067
  %13069 = add i64 %13035, 17
  store i64 %13069, i64* %3, align 8
  %13070 = inttoptr i64 %13068 to i16*
  %13071 = load i16, i16* %13070, align 2
  store i16 %13071, i16* %SI.i, align 2
  %13072 = add i64 %13033, -150
  %13073 = add i64 %13035, 24
  store i64 %13073, i64* %3, align 8
  %13074 = inttoptr i64 %13072 to i16*
  store i16 %13071, i16* %13074, align 2
  %13075 = load i64, i64* %RBP.i, align 8
  %13076 = add i64 %13075, -8
  %13077 = load i64, i64* %3, align 8
  %13078 = add i64 %13077, 4
  store i64 %13078, i64* %3, align 8
  %13079 = inttoptr i64 %13076 to i64*
  %13080 = load i64, i64* %13079, align 8
  %13081 = add i64 %13080, 51640
  store i64 %13081, i64* %RAX.i11582.pre-phi, align 8
  %13082 = icmp ugt i64 %13080, -51641
  %13083 = zext i1 %13082 to i8
  store i8 %13083, i8* %14, align 1
  %13084 = trunc i64 %13081 to i32
  %13085 = and i32 %13084, 255
  %13086 = tail call i32 @llvm.ctpop.i32(i32 %13085)
  %13087 = trunc i32 %13086 to i8
  %13088 = and i8 %13087, 1
  %13089 = xor i8 %13088, 1
  store i8 %13089, i8* %21, align 1
  %13090 = xor i64 %13080, 16
  %13091 = xor i64 %13090, %13081
  %13092 = lshr i64 %13091, 4
  %13093 = trunc i64 %13092 to i8
  %13094 = and i8 %13093, 1
  store i8 %13094, i8* %27, align 1
  %13095 = icmp eq i64 %13081, 0
  %13096 = zext i1 %13095 to i8
  store i8 %13096, i8* %30, align 1
  %13097 = lshr i64 %13081, 63
  %13098 = trunc i64 %13097 to i8
  store i8 %13098, i8* %33, align 1
  %13099 = lshr i64 %13080, 63
  %13100 = xor i64 %13097, %13099
  %13101 = add nuw nsw i64 %13100, %13097
  %13102 = icmp eq i64 %13101, 2
  %13103 = zext i1 %13102 to i8
  store i8 %13103, i8* %39, align 1
  %13104 = add i64 %13075, -150
  %13105 = add i64 %13077, 17
  store i64 %13105, i64* %3, align 8
  %13106 = inttoptr i64 %13104 to i16*
  %13107 = load i16, i16* %13106, align 2
  %13108 = zext i16 %13107 to i64
  store i64 %13108, i64* %RCX.i11580, align 8
  %13109 = zext i16 %13107 to i64
  %13110 = shl nuw nsw i64 %13109, 4
  store i64 %13110, i64* %573, align 8
  %13111 = add i64 %13110, %13081
  store i64 %13111, i64* %RAX.i11582.pre-phi, align 8
  %13112 = icmp ult i64 %13111, %13081
  %13113 = icmp ult i64 %13111, %13110
  %13114 = or i1 %13112, %13113
  %13115 = zext i1 %13114 to i8
  store i8 %13115, i8* %14, align 1
  %13116 = trunc i64 %13111 to i32
  %13117 = and i32 %13116, 255
  %13118 = tail call i32 @llvm.ctpop.i32(i32 %13117)
  %13119 = trunc i32 %13118 to i8
  %13120 = and i8 %13119, 1
  %13121 = xor i8 %13120, 1
  store i8 %13121, i8* %21, align 1
  %13122 = xor i64 %13110, %13081
  %13123 = xor i64 %13122, %13111
  %13124 = lshr i64 %13123, 4
  %13125 = trunc i64 %13124 to i8
  %13126 = and i8 %13125, 1
  store i8 %13126, i8* %27, align 1
  %13127 = icmp eq i64 %13111, 0
  %13128 = zext i1 %13127 to i8
  store i8 %13128, i8* %30, align 1
  %13129 = lshr i64 %13111, 63
  %13130 = trunc i64 %13129 to i8
  store i8 %13130, i8* %33, align 1
  %13131 = xor i64 %13129, %13097
  %13132 = add nuw nsw i64 %13131, %13129
  %13133 = icmp eq i64 %13132, 2
  %13134 = zext i1 %13133 to i8
  store i8 %13134, i8* %39, align 1
  %13135 = inttoptr i64 %13111 to i32*
  %13136 = add i64 %13077, 28
  store i64 %13136, i64* %3, align 8
  %13137 = load i32, i32* %13135, align 4
  %13138 = zext i32 %13137 to i64
  store i64 %13138, i64* %RCX.i11580, align 8
  %13139 = load i64, i64* %RBP.i, align 8
  %13140 = add i64 %13139, -140
  %13141 = add i64 %13077, 34
  store i64 %13141, i64* %3, align 8
  %13142 = inttoptr i64 %13140 to i32*
  %13143 = load i32, i32* %13142, align 4
  %13144 = add i32 %13143, %13137
  %13145 = zext i32 %13144 to i64
  store i64 %13145, i64* %RCX.i11580, align 8
  %13146 = icmp ult i32 %13144, %13137
  %13147 = icmp ult i32 %13144, %13143
  %13148 = or i1 %13146, %13147
  %13149 = zext i1 %13148 to i8
  store i8 %13149, i8* %14, align 1
  %13150 = and i32 %13144, 255
  %13151 = tail call i32 @llvm.ctpop.i32(i32 %13150)
  %13152 = trunc i32 %13151 to i8
  %13153 = and i8 %13152, 1
  %13154 = xor i8 %13153, 1
  store i8 %13154, i8* %21, align 1
  %13155 = xor i32 %13143, %13137
  %13156 = xor i32 %13155, %13144
  %13157 = lshr i32 %13156, 4
  %13158 = trunc i32 %13157 to i8
  %13159 = and i8 %13158, 1
  store i8 %13159, i8* %27, align 1
  %13160 = icmp eq i32 %13144, 0
  %13161 = zext i1 %13160 to i8
  store i8 %13161, i8* %30, align 1
  %13162 = lshr i32 %13144, 31
  %13163 = trunc i32 %13162 to i8
  store i8 %13163, i8* %33, align 1
  %13164 = lshr i32 %13137, 31
  %13165 = lshr i32 %13143, 31
  %13166 = xor i32 %13162, %13164
  %13167 = xor i32 %13162, %13165
  %13168 = add nuw nsw i32 %13166, %13167
  %13169 = icmp eq i32 %13168, 2
  %13170 = zext i1 %13169 to i8
  store i8 %13170, i8* %39, align 1
  %13171 = add i64 %13077, 40
  store i64 %13171, i64* %3, align 8
  store i32 %13144, i32* %13142, align 4
  %13172 = load i64, i64* %RBP.i, align 8
  %13173 = add i64 %13172, -8
  %13174 = load i64, i64* %3, align 8
  %13175 = add i64 %13174, 4
  store i64 %13175, i64* %3, align 8
  %13176 = inttoptr i64 %13173 to i64*
  %13177 = load i64, i64* %13176, align 8
  %13178 = add i64 %13177, 51640
  store i64 %13178, i64* %RAX.i11582.pre-phi, align 8
  %13179 = icmp ugt i64 %13177, -51641
  %13180 = zext i1 %13179 to i8
  store i8 %13180, i8* %14, align 1
  %13181 = trunc i64 %13178 to i32
  %13182 = and i32 %13181, 255
  %13183 = tail call i32 @llvm.ctpop.i32(i32 %13182)
  %13184 = trunc i32 %13183 to i8
  %13185 = and i8 %13184, 1
  %13186 = xor i8 %13185, 1
  store i8 %13186, i8* %21, align 1
  %13187 = xor i64 %13177, 16
  %13188 = xor i64 %13187, %13178
  %13189 = lshr i64 %13188, 4
  %13190 = trunc i64 %13189 to i8
  %13191 = and i8 %13190, 1
  store i8 %13191, i8* %27, align 1
  %13192 = icmp eq i64 %13178, 0
  %13193 = zext i1 %13192 to i8
  store i8 %13193, i8* %30, align 1
  %13194 = lshr i64 %13178, 63
  %13195 = trunc i64 %13194 to i8
  store i8 %13195, i8* %33, align 1
  %13196 = lshr i64 %13177, 63
  %13197 = xor i64 %13194, %13196
  %13198 = add nuw nsw i64 %13197, %13194
  %13199 = icmp eq i64 %13198, 2
  %13200 = zext i1 %13199 to i8
  store i8 %13200, i8* %39, align 1
  %13201 = add i64 %13172, -150
  %13202 = add i64 %13174, 17
  store i64 %13202, i64* %3, align 8
  %13203 = inttoptr i64 %13201 to i16*
  %13204 = load i16, i16* %13203, align 2
  %13205 = zext i16 %13204 to i64
  store i64 %13205, i64* %RCX.i11580, align 8
  %13206 = zext i16 %13204 to i64
  %13207 = shl nuw nsw i64 %13206, 4
  store i64 %13207, i64* %573, align 8
  %13208 = add i64 %13207, %13178
  store i64 %13208, i64* %RAX.i11582.pre-phi, align 8
  %13209 = icmp ult i64 %13208, %13178
  %13210 = icmp ult i64 %13208, %13207
  %13211 = or i1 %13209, %13210
  %13212 = zext i1 %13211 to i8
  store i8 %13212, i8* %14, align 1
  %13213 = trunc i64 %13208 to i32
  %13214 = and i32 %13213, 255
  %13215 = tail call i32 @llvm.ctpop.i32(i32 %13214)
  %13216 = trunc i32 %13215 to i8
  %13217 = and i8 %13216, 1
  %13218 = xor i8 %13217, 1
  store i8 %13218, i8* %21, align 1
  %13219 = xor i64 %13207, %13178
  %13220 = xor i64 %13219, %13208
  %13221 = lshr i64 %13220, 4
  %13222 = trunc i64 %13221 to i8
  %13223 = and i8 %13222, 1
  store i8 %13223, i8* %27, align 1
  %13224 = icmp eq i64 %13208, 0
  %13225 = zext i1 %13224 to i8
  store i8 %13225, i8* %30, align 1
  %13226 = lshr i64 %13208, 63
  %13227 = trunc i64 %13226 to i8
  store i8 %13227, i8* %33, align 1
  %13228 = xor i64 %13226, %13194
  %13229 = add nuw nsw i64 %13228, %13226
  %13230 = icmp eq i64 %13229, 2
  %13231 = zext i1 %13230 to i8
  store i8 %13231, i8* %39, align 1
  %13232 = add i64 %13208, 4
  %13233 = add i64 %13174, 29
  store i64 %13233, i64* %3, align 8
  %13234 = inttoptr i64 %13232 to i32*
  %13235 = load i32, i32* %13234, align 4
  %13236 = zext i32 %13235 to i64
  store i64 %13236, i64* %RCX.i11580, align 8
  %13237 = load i64, i64* %RBP.i, align 8
  %13238 = add i64 %13237, -144
  %13239 = add i64 %13174, 35
  store i64 %13239, i64* %3, align 8
  %13240 = inttoptr i64 %13238 to i32*
  %13241 = load i32, i32* %13240, align 4
  %13242 = add i32 %13241, %13235
  %13243 = zext i32 %13242 to i64
  store i64 %13243, i64* %RCX.i11580, align 8
  %13244 = icmp ult i32 %13242, %13235
  %13245 = icmp ult i32 %13242, %13241
  %13246 = or i1 %13244, %13245
  %13247 = zext i1 %13246 to i8
  store i8 %13247, i8* %14, align 1
  %13248 = and i32 %13242, 255
  %13249 = tail call i32 @llvm.ctpop.i32(i32 %13248)
  %13250 = trunc i32 %13249 to i8
  %13251 = and i8 %13250, 1
  %13252 = xor i8 %13251, 1
  store i8 %13252, i8* %21, align 1
  %13253 = xor i32 %13241, %13235
  %13254 = xor i32 %13253, %13242
  %13255 = lshr i32 %13254, 4
  %13256 = trunc i32 %13255 to i8
  %13257 = and i8 %13256, 1
  store i8 %13257, i8* %27, align 1
  %13258 = icmp eq i32 %13242, 0
  %13259 = zext i1 %13258 to i8
  store i8 %13259, i8* %30, align 1
  %13260 = lshr i32 %13242, 31
  %13261 = trunc i32 %13260 to i8
  store i8 %13261, i8* %33, align 1
  %13262 = lshr i32 %13235, 31
  %13263 = lshr i32 %13241, 31
  %13264 = xor i32 %13260, %13262
  %13265 = xor i32 %13260, %13263
  %13266 = add nuw nsw i32 %13264, %13265
  %13267 = icmp eq i32 %13266, 2
  %13268 = zext i1 %13267 to i8
  store i8 %13268, i8* %39, align 1
  %13269 = add i64 %13174, 41
  store i64 %13269, i64* %3, align 8
  store i32 %13242, i32* %13240, align 4
  %13270 = load i64, i64* %RBP.i, align 8
  %13271 = add i64 %13270, -8
  %13272 = load i64, i64* %3, align 8
  %13273 = add i64 %13272, 4
  store i64 %13273, i64* %3, align 8
  %13274 = inttoptr i64 %13271 to i64*
  %13275 = load i64, i64* %13274, align 8
  %13276 = add i64 %13275, 51640
  store i64 %13276, i64* %RAX.i11582.pre-phi, align 8
  %13277 = icmp ugt i64 %13275, -51641
  %13278 = zext i1 %13277 to i8
  store i8 %13278, i8* %14, align 1
  %13279 = trunc i64 %13276 to i32
  %13280 = and i32 %13279, 255
  %13281 = tail call i32 @llvm.ctpop.i32(i32 %13280)
  %13282 = trunc i32 %13281 to i8
  %13283 = and i8 %13282, 1
  %13284 = xor i8 %13283, 1
  store i8 %13284, i8* %21, align 1
  %13285 = xor i64 %13275, 16
  %13286 = xor i64 %13285, %13276
  %13287 = lshr i64 %13286, 4
  %13288 = trunc i64 %13287 to i8
  %13289 = and i8 %13288, 1
  store i8 %13289, i8* %27, align 1
  %13290 = icmp eq i64 %13276, 0
  %13291 = zext i1 %13290 to i8
  store i8 %13291, i8* %30, align 1
  %13292 = lshr i64 %13276, 63
  %13293 = trunc i64 %13292 to i8
  store i8 %13293, i8* %33, align 1
  %13294 = lshr i64 %13275, 63
  %13295 = xor i64 %13292, %13294
  %13296 = add nuw nsw i64 %13295, %13292
  %13297 = icmp eq i64 %13296, 2
  %13298 = zext i1 %13297 to i8
  store i8 %13298, i8* %39, align 1
  %13299 = add i64 %13270, -150
  %13300 = add i64 %13272, 17
  store i64 %13300, i64* %3, align 8
  %13301 = inttoptr i64 %13299 to i16*
  %13302 = load i16, i16* %13301, align 2
  %13303 = zext i16 %13302 to i64
  store i64 %13303, i64* %RCX.i11580, align 8
  %13304 = zext i16 %13302 to i64
  %13305 = shl nuw nsw i64 %13304, 4
  store i64 %13305, i64* %573, align 8
  %13306 = add i64 %13305, %13276
  store i64 %13306, i64* %RAX.i11582.pre-phi, align 8
  %13307 = icmp ult i64 %13306, %13276
  %13308 = icmp ult i64 %13306, %13305
  %13309 = or i1 %13307, %13308
  %13310 = zext i1 %13309 to i8
  store i8 %13310, i8* %14, align 1
  %13311 = trunc i64 %13306 to i32
  %13312 = and i32 %13311, 255
  %13313 = tail call i32 @llvm.ctpop.i32(i32 %13312)
  %13314 = trunc i32 %13313 to i8
  %13315 = and i8 %13314, 1
  %13316 = xor i8 %13315, 1
  store i8 %13316, i8* %21, align 1
  %13317 = xor i64 %13305, %13276
  %13318 = xor i64 %13317, %13306
  %13319 = lshr i64 %13318, 4
  %13320 = trunc i64 %13319 to i8
  %13321 = and i8 %13320, 1
  store i8 %13321, i8* %27, align 1
  %13322 = icmp eq i64 %13306, 0
  %13323 = zext i1 %13322 to i8
  store i8 %13323, i8* %30, align 1
  %13324 = lshr i64 %13306, 63
  %13325 = trunc i64 %13324 to i8
  store i8 %13325, i8* %33, align 1
  %13326 = xor i64 %13324, %13292
  %13327 = add nuw nsw i64 %13326, %13324
  %13328 = icmp eq i64 %13327, 2
  %13329 = zext i1 %13328 to i8
  store i8 %13329, i8* %39, align 1
  %13330 = add i64 %13306, 8
  %13331 = add i64 %13272, 29
  store i64 %13331, i64* %3, align 8
  %13332 = inttoptr i64 %13330 to i32*
  %13333 = load i32, i32* %13332, align 4
  %13334 = zext i32 %13333 to i64
  store i64 %13334, i64* %RCX.i11580, align 8
  %13335 = load i64, i64* %RBP.i, align 8
  %13336 = add i64 %13335, -148
  %13337 = add i64 %13272, 35
  store i64 %13337, i64* %3, align 8
  %13338 = inttoptr i64 %13336 to i32*
  %13339 = load i32, i32* %13338, align 4
  %13340 = add i32 %13339, %13333
  %13341 = zext i32 %13340 to i64
  store i64 %13341, i64* %RCX.i11580, align 8
  %13342 = icmp ult i32 %13340, %13333
  %13343 = icmp ult i32 %13340, %13339
  %13344 = or i1 %13342, %13343
  %13345 = zext i1 %13344 to i8
  store i8 %13345, i8* %14, align 1
  %13346 = and i32 %13340, 255
  %13347 = tail call i32 @llvm.ctpop.i32(i32 %13346)
  %13348 = trunc i32 %13347 to i8
  %13349 = and i8 %13348, 1
  %13350 = xor i8 %13349, 1
  store i8 %13350, i8* %21, align 1
  %13351 = xor i32 %13339, %13333
  %13352 = xor i32 %13351, %13340
  %13353 = lshr i32 %13352, 4
  %13354 = trunc i32 %13353 to i8
  %13355 = and i8 %13354, 1
  store i8 %13355, i8* %27, align 1
  %13356 = icmp eq i32 %13340, 0
  %13357 = zext i1 %13356 to i8
  store i8 %13357, i8* %30, align 1
  %13358 = lshr i32 %13340, 31
  %13359 = trunc i32 %13358 to i8
  store i8 %13359, i8* %33, align 1
  %13360 = lshr i32 %13333, 31
  %13361 = lshr i32 %13339, 31
  %13362 = xor i32 %13358, %13360
  %13363 = xor i32 %13358, %13361
  %13364 = add nuw nsw i32 %13362, %13363
  %13365 = icmp eq i32 %13364, 2
  %13366 = zext i1 %13365 to i8
  store i8 %13366, i8* %39, align 1
  %13367 = add i64 %13272, 41
  store i64 %13367, i64* %3, align 8
  store i32 %13340, i32* %13338, align 4
  %13368 = load i64, i64* %RBP.i, align 8
  %13369 = add i64 %13368, -120
  %13370 = load i64, i64* %3, align 8
  %13371 = add i64 %13370, 4
  store i64 %13371, i64* %3, align 8
  %13372 = inttoptr i64 %13369 to i64*
  %13373 = load i64, i64* %13372, align 8
  store i64 %13373, i64* %RAX.i11582.pre-phi, align 8
  %13374 = add i64 %13368, -28
  %13375 = add i64 %13370, 7
  store i64 %13375, i64* %3, align 8
  %13376 = inttoptr i64 %13374 to i32*
  %13377 = load i32, i32* %13376, align 4
  %13378 = add i32 %13377, 32
  %13379 = zext i32 %13378 to i64
  store i64 %13379, i64* %RCX.i11580, align 8
  %13380 = icmp ugt i32 %13377, -33
  %13381 = zext i1 %13380 to i8
  store i8 %13381, i8* %14, align 1
  %13382 = and i32 %13378, 255
  %13383 = tail call i32 @llvm.ctpop.i32(i32 %13382)
  %13384 = trunc i32 %13383 to i8
  %13385 = and i8 %13384, 1
  %13386 = xor i8 %13385, 1
  store i8 %13386, i8* %21, align 1
  %13387 = xor i32 %13378, %13377
  %13388 = lshr i32 %13387, 4
  %13389 = trunc i32 %13388 to i8
  %13390 = and i8 %13389, 1
  store i8 %13390, i8* %27, align 1
  %13391 = icmp eq i32 %13378, 0
  %13392 = zext i1 %13391 to i8
  store i8 %13392, i8* %30, align 1
  %13393 = lshr i32 %13378, 31
  %13394 = trunc i32 %13393 to i8
  store i8 %13394, i8* %33, align 1
  %13395 = lshr i32 %13377, 31
  %13396 = xor i32 %13393, %13395
  %13397 = add nuw nsw i32 %13396, %13393
  %13398 = icmp eq i32 %13397, 2
  %13399 = zext i1 %13398 to i8
  store i8 %13399, i8* %39, align 1
  %13400 = sext i32 %13378 to i64
  store i64 %13400, i64* %573, align 8
  %13401 = shl nsw i64 %13400, 1
  %13402 = add i64 %13373, %13401
  %13403 = add i64 %13370, 17
  store i64 %13403, i64* %3, align 8
  %13404 = inttoptr i64 %13402 to i16*
  %13405 = load i16, i16* %13404, align 2
  store i16 %13405, i16* %SI.i, align 2
  %13406 = add i64 %13368, -150
  %13407 = add i64 %13370, 24
  store i64 %13407, i64* %3, align 8
  %13408 = inttoptr i64 %13406 to i16*
  store i16 %13405, i16* %13408, align 2
  %13409 = load i64, i64* %RBP.i, align 8
  %13410 = add i64 %13409, -8
  %13411 = load i64, i64* %3, align 8
  %13412 = add i64 %13411, 4
  store i64 %13412, i64* %3, align 8
  %13413 = inttoptr i64 %13410 to i64*
  %13414 = load i64, i64* %13413, align 8
  %13415 = add i64 %13414, 51640
  store i64 %13415, i64* %RAX.i11582.pre-phi, align 8
  %13416 = icmp ugt i64 %13414, -51641
  %13417 = zext i1 %13416 to i8
  store i8 %13417, i8* %14, align 1
  %13418 = trunc i64 %13415 to i32
  %13419 = and i32 %13418, 255
  %13420 = tail call i32 @llvm.ctpop.i32(i32 %13419)
  %13421 = trunc i32 %13420 to i8
  %13422 = and i8 %13421, 1
  %13423 = xor i8 %13422, 1
  store i8 %13423, i8* %21, align 1
  %13424 = xor i64 %13414, 16
  %13425 = xor i64 %13424, %13415
  %13426 = lshr i64 %13425, 4
  %13427 = trunc i64 %13426 to i8
  %13428 = and i8 %13427, 1
  store i8 %13428, i8* %27, align 1
  %13429 = icmp eq i64 %13415, 0
  %13430 = zext i1 %13429 to i8
  store i8 %13430, i8* %30, align 1
  %13431 = lshr i64 %13415, 63
  %13432 = trunc i64 %13431 to i8
  store i8 %13432, i8* %33, align 1
  %13433 = lshr i64 %13414, 63
  %13434 = xor i64 %13431, %13433
  %13435 = add nuw nsw i64 %13434, %13431
  %13436 = icmp eq i64 %13435, 2
  %13437 = zext i1 %13436 to i8
  store i8 %13437, i8* %39, align 1
  %13438 = add i64 %13409, -150
  %13439 = add i64 %13411, 17
  store i64 %13439, i64* %3, align 8
  %13440 = inttoptr i64 %13438 to i16*
  %13441 = load i16, i16* %13440, align 2
  %13442 = zext i16 %13441 to i64
  store i64 %13442, i64* %RCX.i11580, align 8
  %13443 = zext i16 %13441 to i64
  %13444 = shl nuw nsw i64 %13443, 4
  store i64 %13444, i64* %573, align 8
  %13445 = add i64 %13444, %13415
  store i64 %13445, i64* %RAX.i11582.pre-phi, align 8
  %13446 = icmp ult i64 %13445, %13415
  %13447 = icmp ult i64 %13445, %13444
  %13448 = or i1 %13446, %13447
  %13449 = zext i1 %13448 to i8
  store i8 %13449, i8* %14, align 1
  %13450 = trunc i64 %13445 to i32
  %13451 = and i32 %13450, 255
  %13452 = tail call i32 @llvm.ctpop.i32(i32 %13451)
  %13453 = trunc i32 %13452 to i8
  %13454 = and i8 %13453, 1
  %13455 = xor i8 %13454, 1
  store i8 %13455, i8* %21, align 1
  %13456 = xor i64 %13444, %13415
  %13457 = xor i64 %13456, %13445
  %13458 = lshr i64 %13457, 4
  %13459 = trunc i64 %13458 to i8
  %13460 = and i8 %13459, 1
  store i8 %13460, i8* %27, align 1
  %13461 = icmp eq i64 %13445, 0
  %13462 = zext i1 %13461 to i8
  store i8 %13462, i8* %30, align 1
  %13463 = lshr i64 %13445, 63
  %13464 = trunc i64 %13463 to i8
  store i8 %13464, i8* %33, align 1
  %13465 = xor i64 %13463, %13431
  %13466 = add nuw nsw i64 %13465, %13463
  %13467 = icmp eq i64 %13466, 2
  %13468 = zext i1 %13467 to i8
  store i8 %13468, i8* %39, align 1
  %13469 = inttoptr i64 %13445 to i32*
  %13470 = add i64 %13411, 28
  store i64 %13470, i64* %3, align 8
  %13471 = load i32, i32* %13469, align 4
  %13472 = zext i32 %13471 to i64
  store i64 %13472, i64* %RCX.i11580, align 8
  %13473 = load i64, i64* %RBP.i, align 8
  %13474 = add i64 %13473, -140
  %13475 = add i64 %13411, 34
  store i64 %13475, i64* %3, align 8
  %13476 = inttoptr i64 %13474 to i32*
  %13477 = load i32, i32* %13476, align 4
  %13478 = add i32 %13477, %13471
  %13479 = zext i32 %13478 to i64
  store i64 %13479, i64* %RCX.i11580, align 8
  %13480 = icmp ult i32 %13478, %13471
  %13481 = icmp ult i32 %13478, %13477
  %13482 = or i1 %13480, %13481
  %13483 = zext i1 %13482 to i8
  store i8 %13483, i8* %14, align 1
  %13484 = and i32 %13478, 255
  %13485 = tail call i32 @llvm.ctpop.i32(i32 %13484)
  %13486 = trunc i32 %13485 to i8
  %13487 = and i8 %13486, 1
  %13488 = xor i8 %13487, 1
  store i8 %13488, i8* %21, align 1
  %13489 = xor i32 %13477, %13471
  %13490 = xor i32 %13489, %13478
  %13491 = lshr i32 %13490, 4
  %13492 = trunc i32 %13491 to i8
  %13493 = and i8 %13492, 1
  store i8 %13493, i8* %27, align 1
  %13494 = icmp eq i32 %13478, 0
  %13495 = zext i1 %13494 to i8
  store i8 %13495, i8* %30, align 1
  %13496 = lshr i32 %13478, 31
  %13497 = trunc i32 %13496 to i8
  store i8 %13497, i8* %33, align 1
  %13498 = lshr i32 %13471, 31
  %13499 = lshr i32 %13477, 31
  %13500 = xor i32 %13496, %13498
  %13501 = xor i32 %13496, %13499
  %13502 = add nuw nsw i32 %13500, %13501
  %13503 = icmp eq i32 %13502, 2
  %13504 = zext i1 %13503 to i8
  store i8 %13504, i8* %39, align 1
  %13505 = add i64 %13411, 40
  store i64 %13505, i64* %3, align 8
  store i32 %13478, i32* %13476, align 4
  %13506 = load i64, i64* %RBP.i, align 8
  %13507 = add i64 %13506, -8
  %13508 = load i64, i64* %3, align 8
  %13509 = add i64 %13508, 4
  store i64 %13509, i64* %3, align 8
  %13510 = inttoptr i64 %13507 to i64*
  %13511 = load i64, i64* %13510, align 8
  %13512 = add i64 %13511, 51640
  store i64 %13512, i64* %RAX.i11582.pre-phi, align 8
  %13513 = icmp ugt i64 %13511, -51641
  %13514 = zext i1 %13513 to i8
  store i8 %13514, i8* %14, align 1
  %13515 = trunc i64 %13512 to i32
  %13516 = and i32 %13515, 255
  %13517 = tail call i32 @llvm.ctpop.i32(i32 %13516)
  %13518 = trunc i32 %13517 to i8
  %13519 = and i8 %13518, 1
  %13520 = xor i8 %13519, 1
  store i8 %13520, i8* %21, align 1
  %13521 = xor i64 %13511, 16
  %13522 = xor i64 %13521, %13512
  %13523 = lshr i64 %13522, 4
  %13524 = trunc i64 %13523 to i8
  %13525 = and i8 %13524, 1
  store i8 %13525, i8* %27, align 1
  %13526 = icmp eq i64 %13512, 0
  %13527 = zext i1 %13526 to i8
  store i8 %13527, i8* %30, align 1
  %13528 = lshr i64 %13512, 63
  %13529 = trunc i64 %13528 to i8
  store i8 %13529, i8* %33, align 1
  %13530 = lshr i64 %13511, 63
  %13531 = xor i64 %13528, %13530
  %13532 = add nuw nsw i64 %13531, %13528
  %13533 = icmp eq i64 %13532, 2
  %13534 = zext i1 %13533 to i8
  store i8 %13534, i8* %39, align 1
  %13535 = add i64 %13506, -150
  %13536 = add i64 %13508, 17
  store i64 %13536, i64* %3, align 8
  %13537 = inttoptr i64 %13535 to i16*
  %13538 = load i16, i16* %13537, align 2
  %13539 = zext i16 %13538 to i64
  store i64 %13539, i64* %RCX.i11580, align 8
  %13540 = zext i16 %13538 to i64
  %13541 = shl nuw nsw i64 %13540, 4
  store i64 %13541, i64* %573, align 8
  %13542 = add i64 %13541, %13512
  store i64 %13542, i64* %RAX.i11582.pre-phi, align 8
  %13543 = icmp ult i64 %13542, %13512
  %13544 = icmp ult i64 %13542, %13541
  %13545 = or i1 %13543, %13544
  %13546 = zext i1 %13545 to i8
  store i8 %13546, i8* %14, align 1
  %13547 = trunc i64 %13542 to i32
  %13548 = and i32 %13547, 255
  %13549 = tail call i32 @llvm.ctpop.i32(i32 %13548)
  %13550 = trunc i32 %13549 to i8
  %13551 = and i8 %13550, 1
  %13552 = xor i8 %13551, 1
  store i8 %13552, i8* %21, align 1
  %13553 = xor i64 %13541, %13512
  %13554 = xor i64 %13553, %13542
  %13555 = lshr i64 %13554, 4
  %13556 = trunc i64 %13555 to i8
  %13557 = and i8 %13556, 1
  store i8 %13557, i8* %27, align 1
  %13558 = icmp eq i64 %13542, 0
  %13559 = zext i1 %13558 to i8
  store i8 %13559, i8* %30, align 1
  %13560 = lshr i64 %13542, 63
  %13561 = trunc i64 %13560 to i8
  store i8 %13561, i8* %33, align 1
  %13562 = xor i64 %13560, %13528
  %13563 = add nuw nsw i64 %13562, %13560
  %13564 = icmp eq i64 %13563, 2
  %13565 = zext i1 %13564 to i8
  store i8 %13565, i8* %39, align 1
  %13566 = add i64 %13542, 4
  %13567 = add i64 %13508, 29
  store i64 %13567, i64* %3, align 8
  %13568 = inttoptr i64 %13566 to i32*
  %13569 = load i32, i32* %13568, align 4
  %13570 = zext i32 %13569 to i64
  store i64 %13570, i64* %RCX.i11580, align 8
  %13571 = load i64, i64* %RBP.i, align 8
  %13572 = add i64 %13571, -144
  %13573 = add i64 %13508, 35
  store i64 %13573, i64* %3, align 8
  %13574 = inttoptr i64 %13572 to i32*
  %13575 = load i32, i32* %13574, align 4
  %13576 = add i32 %13575, %13569
  %13577 = zext i32 %13576 to i64
  store i64 %13577, i64* %RCX.i11580, align 8
  %13578 = icmp ult i32 %13576, %13569
  %13579 = icmp ult i32 %13576, %13575
  %13580 = or i1 %13578, %13579
  %13581 = zext i1 %13580 to i8
  store i8 %13581, i8* %14, align 1
  %13582 = and i32 %13576, 255
  %13583 = tail call i32 @llvm.ctpop.i32(i32 %13582)
  %13584 = trunc i32 %13583 to i8
  %13585 = and i8 %13584, 1
  %13586 = xor i8 %13585, 1
  store i8 %13586, i8* %21, align 1
  %13587 = xor i32 %13575, %13569
  %13588 = xor i32 %13587, %13576
  %13589 = lshr i32 %13588, 4
  %13590 = trunc i32 %13589 to i8
  %13591 = and i8 %13590, 1
  store i8 %13591, i8* %27, align 1
  %13592 = icmp eq i32 %13576, 0
  %13593 = zext i1 %13592 to i8
  store i8 %13593, i8* %30, align 1
  %13594 = lshr i32 %13576, 31
  %13595 = trunc i32 %13594 to i8
  store i8 %13595, i8* %33, align 1
  %13596 = lshr i32 %13569, 31
  %13597 = lshr i32 %13575, 31
  %13598 = xor i32 %13594, %13596
  %13599 = xor i32 %13594, %13597
  %13600 = add nuw nsw i32 %13598, %13599
  %13601 = icmp eq i32 %13600, 2
  %13602 = zext i1 %13601 to i8
  store i8 %13602, i8* %39, align 1
  %13603 = add i64 %13508, 41
  store i64 %13603, i64* %3, align 8
  store i32 %13576, i32* %13574, align 4
  %13604 = load i64, i64* %RBP.i, align 8
  %13605 = add i64 %13604, -8
  %13606 = load i64, i64* %3, align 8
  %13607 = add i64 %13606, 4
  store i64 %13607, i64* %3, align 8
  %13608 = inttoptr i64 %13605 to i64*
  %13609 = load i64, i64* %13608, align 8
  %13610 = add i64 %13609, 51640
  store i64 %13610, i64* %RAX.i11582.pre-phi, align 8
  %13611 = icmp ugt i64 %13609, -51641
  %13612 = zext i1 %13611 to i8
  store i8 %13612, i8* %14, align 1
  %13613 = trunc i64 %13610 to i32
  %13614 = and i32 %13613, 255
  %13615 = tail call i32 @llvm.ctpop.i32(i32 %13614)
  %13616 = trunc i32 %13615 to i8
  %13617 = and i8 %13616, 1
  %13618 = xor i8 %13617, 1
  store i8 %13618, i8* %21, align 1
  %13619 = xor i64 %13609, 16
  %13620 = xor i64 %13619, %13610
  %13621 = lshr i64 %13620, 4
  %13622 = trunc i64 %13621 to i8
  %13623 = and i8 %13622, 1
  store i8 %13623, i8* %27, align 1
  %13624 = icmp eq i64 %13610, 0
  %13625 = zext i1 %13624 to i8
  store i8 %13625, i8* %30, align 1
  %13626 = lshr i64 %13610, 63
  %13627 = trunc i64 %13626 to i8
  store i8 %13627, i8* %33, align 1
  %13628 = lshr i64 %13609, 63
  %13629 = xor i64 %13626, %13628
  %13630 = add nuw nsw i64 %13629, %13626
  %13631 = icmp eq i64 %13630, 2
  %13632 = zext i1 %13631 to i8
  store i8 %13632, i8* %39, align 1
  %13633 = add i64 %13604, -150
  %13634 = add i64 %13606, 17
  store i64 %13634, i64* %3, align 8
  %13635 = inttoptr i64 %13633 to i16*
  %13636 = load i16, i16* %13635, align 2
  %13637 = zext i16 %13636 to i64
  store i64 %13637, i64* %RCX.i11580, align 8
  %13638 = zext i16 %13636 to i64
  %13639 = shl nuw nsw i64 %13638, 4
  store i64 %13639, i64* %573, align 8
  %13640 = add i64 %13639, %13610
  store i64 %13640, i64* %RAX.i11582.pre-phi, align 8
  %13641 = icmp ult i64 %13640, %13610
  %13642 = icmp ult i64 %13640, %13639
  %13643 = or i1 %13641, %13642
  %13644 = zext i1 %13643 to i8
  store i8 %13644, i8* %14, align 1
  %13645 = trunc i64 %13640 to i32
  %13646 = and i32 %13645, 255
  %13647 = tail call i32 @llvm.ctpop.i32(i32 %13646)
  %13648 = trunc i32 %13647 to i8
  %13649 = and i8 %13648, 1
  %13650 = xor i8 %13649, 1
  store i8 %13650, i8* %21, align 1
  %13651 = xor i64 %13639, %13610
  %13652 = xor i64 %13651, %13640
  %13653 = lshr i64 %13652, 4
  %13654 = trunc i64 %13653 to i8
  %13655 = and i8 %13654, 1
  store i8 %13655, i8* %27, align 1
  %13656 = icmp eq i64 %13640, 0
  %13657 = zext i1 %13656 to i8
  store i8 %13657, i8* %30, align 1
  %13658 = lshr i64 %13640, 63
  %13659 = trunc i64 %13658 to i8
  store i8 %13659, i8* %33, align 1
  %13660 = xor i64 %13658, %13626
  %13661 = add nuw nsw i64 %13660, %13658
  %13662 = icmp eq i64 %13661, 2
  %13663 = zext i1 %13662 to i8
  store i8 %13663, i8* %39, align 1
  %13664 = add i64 %13640, 8
  %13665 = add i64 %13606, 29
  store i64 %13665, i64* %3, align 8
  %13666 = inttoptr i64 %13664 to i32*
  %13667 = load i32, i32* %13666, align 4
  %13668 = zext i32 %13667 to i64
  store i64 %13668, i64* %RCX.i11580, align 8
  %13669 = load i64, i64* %RBP.i, align 8
  %13670 = add i64 %13669, -148
  %13671 = add i64 %13606, 35
  store i64 %13671, i64* %3, align 8
  %13672 = inttoptr i64 %13670 to i32*
  %13673 = load i32, i32* %13672, align 4
  %13674 = add i32 %13673, %13667
  %13675 = zext i32 %13674 to i64
  store i64 %13675, i64* %RCX.i11580, align 8
  %13676 = icmp ult i32 %13674, %13667
  %13677 = icmp ult i32 %13674, %13673
  %13678 = or i1 %13676, %13677
  %13679 = zext i1 %13678 to i8
  store i8 %13679, i8* %14, align 1
  %13680 = and i32 %13674, 255
  %13681 = tail call i32 @llvm.ctpop.i32(i32 %13680)
  %13682 = trunc i32 %13681 to i8
  %13683 = and i8 %13682, 1
  %13684 = xor i8 %13683, 1
  store i8 %13684, i8* %21, align 1
  %13685 = xor i32 %13673, %13667
  %13686 = xor i32 %13685, %13674
  %13687 = lshr i32 %13686, 4
  %13688 = trunc i32 %13687 to i8
  %13689 = and i8 %13688, 1
  store i8 %13689, i8* %27, align 1
  %13690 = icmp eq i32 %13674, 0
  %13691 = zext i1 %13690 to i8
  store i8 %13691, i8* %30, align 1
  %13692 = lshr i32 %13674, 31
  %13693 = trunc i32 %13692 to i8
  store i8 %13693, i8* %33, align 1
  %13694 = lshr i32 %13667, 31
  %13695 = lshr i32 %13673, 31
  %13696 = xor i32 %13692, %13694
  %13697 = xor i32 %13692, %13695
  %13698 = add nuw nsw i32 %13696, %13697
  %13699 = icmp eq i32 %13698, 2
  %13700 = zext i1 %13699 to i8
  store i8 %13700, i8* %39, align 1
  %13701 = add i64 %13606, 41
  store i64 %13701, i64* %3, align 8
  store i32 %13674, i32* %13672, align 4
  %13702 = load i64, i64* %RBP.i, align 8
  %13703 = add i64 %13702, -120
  %13704 = load i64, i64* %3, align 8
  %13705 = add i64 %13704, 4
  store i64 %13705, i64* %3, align 8
  %13706 = inttoptr i64 %13703 to i64*
  %13707 = load i64, i64* %13706, align 8
  store i64 %13707, i64* %RAX.i11582.pre-phi, align 8
  %13708 = add i64 %13702, -28
  %13709 = add i64 %13704, 7
  store i64 %13709, i64* %3, align 8
  %13710 = inttoptr i64 %13708 to i32*
  %13711 = load i32, i32* %13710, align 4
  %13712 = add i32 %13711, 33
  %13713 = zext i32 %13712 to i64
  store i64 %13713, i64* %RCX.i11580, align 8
  %13714 = icmp ugt i32 %13711, -34
  %13715 = zext i1 %13714 to i8
  store i8 %13715, i8* %14, align 1
  %13716 = and i32 %13712, 255
  %13717 = tail call i32 @llvm.ctpop.i32(i32 %13716)
  %13718 = trunc i32 %13717 to i8
  %13719 = and i8 %13718, 1
  %13720 = xor i8 %13719, 1
  store i8 %13720, i8* %21, align 1
  %13721 = xor i32 %13712, %13711
  %13722 = lshr i32 %13721, 4
  %13723 = trunc i32 %13722 to i8
  %13724 = and i8 %13723, 1
  store i8 %13724, i8* %27, align 1
  %13725 = icmp eq i32 %13712, 0
  %13726 = zext i1 %13725 to i8
  store i8 %13726, i8* %30, align 1
  %13727 = lshr i32 %13712, 31
  %13728 = trunc i32 %13727 to i8
  store i8 %13728, i8* %33, align 1
  %13729 = lshr i32 %13711, 31
  %13730 = xor i32 %13727, %13729
  %13731 = add nuw nsw i32 %13730, %13727
  %13732 = icmp eq i32 %13731, 2
  %13733 = zext i1 %13732 to i8
  store i8 %13733, i8* %39, align 1
  %13734 = sext i32 %13712 to i64
  store i64 %13734, i64* %573, align 8
  %13735 = shl nsw i64 %13734, 1
  %13736 = add i64 %13707, %13735
  %13737 = add i64 %13704, 17
  store i64 %13737, i64* %3, align 8
  %13738 = inttoptr i64 %13736 to i16*
  %13739 = load i16, i16* %13738, align 2
  store i16 %13739, i16* %SI.i, align 2
  %13740 = add i64 %13702, -150
  %13741 = add i64 %13704, 24
  store i64 %13741, i64* %3, align 8
  %13742 = inttoptr i64 %13740 to i16*
  store i16 %13739, i16* %13742, align 2
  %13743 = load i64, i64* %RBP.i, align 8
  %13744 = add i64 %13743, -8
  %13745 = load i64, i64* %3, align 8
  %13746 = add i64 %13745, 4
  store i64 %13746, i64* %3, align 8
  %13747 = inttoptr i64 %13744 to i64*
  %13748 = load i64, i64* %13747, align 8
  %13749 = add i64 %13748, 51640
  store i64 %13749, i64* %RAX.i11582.pre-phi, align 8
  %13750 = icmp ugt i64 %13748, -51641
  %13751 = zext i1 %13750 to i8
  store i8 %13751, i8* %14, align 1
  %13752 = trunc i64 %13749 to i32
  %13753 = and i32 %13752, 255
  %13754 = tail call i32 @llvm.ctpop.i32(i32 %13753)
  %13755 = trunc i32 %13754 to i8
  %13756 = and i8 %13755, 1
  %13757 = xor i8 %13756, 1
  store i8 %13757, i8* %21, align 1
  %13758 = xor i64 %13748, 16
  %13759 = xor i64 %13758, %13749
  %13760 = lshr i64 %13759, 4
  %13761 = trunc i64 %13760 to i8
  %13762 = and i8 %13761, 1
  store i8 %13762, i8* %27, align 1
  %13763 = icmp eq i64 %13749, 0
  %13764 = zext i1 %13763 to i8
  store i8 %13764, i8* %30, align 1
  %13765 = lshr i64 %13749, 63
  %13766 = trunc i64 %13765 to i8
  store i8 %13766, i8* %33, align 1
  %13767 = lshr i64 %13748, 63
  %13768 = xor i64 %13765, %13767
  %13769 = add nuw nsw i64 %13768, %13765
  %13770 = icmp eq i64 %13769, 2
  %13771 = zext i1 %13770 to i8
  store i8 %13771, i8* %39, align 1
  %13772 = add i64 %13743, -150
  %13773 = add i64 %13745, 17
  store i64 %13773, i64* %3, align 8
  %13774 = inttoptr i64 %13772 to i16*
  %13775 = load i16, i16* %13774, align 2
  %13776 = zext i16 %13775 to i64
  store i64 %13776, i64* %RCX.i11580, align 8
  %13777 = zext i16 %13775 to i64
  %13778 = shl nuw nsw i64 %13777, 4
  store i64 %13778, i64* %573, align 8
  %13779 = add i64 %13778, %13749
  store i64 %13779, i64* %RAX.i11582.pre-phi, align 8
  %13780 = icmp ult i64 %13779, %13749
  %13781 = icmp ult i64 %13779, %13778
  %13782 = or i1 %13780, %13781
  %13783 = zext i1 %13782 to i8
  store i8 %13783, i8* %14, align 1
  %13784 = trunc i64 %13779 to i32
  %13785 = and i32 %13784, 255
  %13786 = tail call i32 @llvm.ctpop.i32(i32 %13785)
  %13787 = trunc i32 %13786 to i8
  %13788 = and i8 %13787, 1
  %13789 = xor i8 %13788, 1
  store i8 %13789, i8* %21, align 1
  %13790 = xor i64 %13778, %13749
  %13791 = xor i64 %13790, %13779
  %13792 = lshr i64 %13791, 4
  %13793 = trunc i64 %13792 to i8
  %13794 = and i8 %13793, 1
  store i8 %13794, i8* %27, align 1
  %13795 = icmp eq i64 %13779, 0
  %13796 = zext i1 %13795 to i8
  store i8 %13796, i8* %30, align 1
  %13797 = lshr i64 %13779, 63
  %13798 = trunc i64 %13797 to i8
  store i8 %13798, i8* %33, align 1
  %13799 = xor i64 %13797, %13765
  %13800 = add nuw nsw i64 %13799, %13797
  %13801 = icmp eq i64 %13800, 2
  %13802 = zext i1 %13801 to i8
  store i8 %13802, i8* %39, align 1
  %13803 = inttoptr i64 %13779 to i32*
  %13804 = add i64 %13745, 28
  store i64 %13804, i64* %3, align 8
  %13805 = load i32, i32* %13803, align 4
  %13806 = zext i32 %13805 to i64
  store i64 %13806, i64* %RCX.i11580, align 8
  %13807 = load i64, i64* %RBP.i, align 8
  %13808 = add i64 %13807, -140
  %13809 = add i64 %13745, 34
  store i64 %13809, i64* %3, align 8
  %13810 = inttoptr i64 %13808 to i32*
  %13811 = load i32, i32* %13810, align 4
  %13812 = add i32 %13811, %13805
  %13813 = zext i32 %13812 to i64
  store i64 %13813, i64* %RCX.i11580, align 8
  %13814 = icmp ult i32 %13812, %13805
  %13815 = icmp ult i32 %13812, %13811
  %13816 = or i1 %13814, %13815
  %13817 = zext i1 %13816 to i8
  store i8 %13817, i8* %14, align 1
  %13818 = and i32 %13812, 255
  %13819 = tail call i32 @llvm.ctpop.i32(i32 %13818)
  %13820 = trunc i32 %13819 to i8
  %13821 = and i8 %13820, 1
  %13822 = xor i8 %13821, 1
  store i8 %13822, i8* %21, align 1
  %13823 = xor i32 %13811, %13805
  %13824 = xor i32 %13823, %13812
  %13825 = lshr i32 %13824, 4
  %13826 = trunc i32 %13825 to i8
  %13827 = and i8 %13826, 1
  store i8 %13827, i8* %27, align 1
  %13828 = icmp eq i32 %13812, 0
  %13829 = zext i1 %13828 to i8
  store i8 %13829, i8* %30, align 1
  %13830 = lshr i32 %13812, 31
  %13831 = trunc i32 %13830 to i8
  store i8 %13831, i8* %33, align 1
  %13832 = lshr i32 %13805, 31
  %13833 = lshr i32 %13811, 31
  %13834 = xor i32 %13830, %13832
  %13835 = xor i32 %13830, %13833
  %13836 = add nuw nsw i32 %13834, %13835
  %13837 = icmp eq i32 %13836, 2
  %13838 = zext i1 %13837 to i8
  store i8 %13838, i8* %39, align 1
  %13839 = add i64 %13745, 40
  store i64 %13839, i64* %3, align 8
  store i32 %13812, i32* %13810, align 4
  %13840 = load i64, i64* %RBP.i, align 8
  %13841 = add i64 %13840, -8
  %13842 = load i64, i64* %3, align 8
  %13843 = add i64 %13842, 4
  store i64 %13843, i64* %3, align 8
  %13844 = inttoptr i64 %13841 to i64*
  %13845 = load i64, i64* %13844, align 8
  %13846 = add i64 %13845, 51640
  store i64 %13846, i64* %RAX.i11582.pre-phi, align 8
  %13847 = icmp ugt i64 %13845, -51641
  %13848 = zext i1 %13847 to i8
  store i8 %13848, i8* %14, align 1
  %13849 = trunc i64 %13846 to i32
  %13850 = and i32 %13849, 255
  %13851 = tail call i32 @llvm.ctpop.i32(i32 %13850)
  %13852 = trunc i32 %13851 to i8
  %13853 = and i8 %13852, 1
  %13854 = xor i8 %13853, 1
  store i8 %13854, i8* %21, align 1
  %13855 = xor i64 %13845, 16
  %13856 = xor i64 %13855, %13846
  %13857 = lshr i64 %13856, 4
  %13858 = trunc i64 %13857 to i8
  %13859 = and i8 %13858, 1
  store i8 %13859, i8* %27, align 1
  %13860 = icmp eq i64 %13846, 0
  %13861 = zext i1 %13860 to i8
  store i8 %13861, i8* %30, align 1
  %13862 = lshr i64 %13846, 63
  %13863 = trunc i64 %13862 to i8
  store i8 %13863, i8* %33, align 1
  %13864 = lshr i64 %13845, 63
  %13865 = xor i64 %13862, %13864
  %13866 = add nuw nsw i64 %13865, %13862
  %13867 = icmp eq i64 %13866, 2
  %13868 = zext i1 %13867 to i8
  store i8 %13868, i8* %39, align 1
  %13869 = add i64 %13840, -150
  %13870 = add i64 %13842, 17
  store i64 %13870, i64* %3, align 8
  %13871 = inttoptr i64 %13869 to i16*
  %13872 = load i16, i16* %13871, align 2
  %13873 = zext i16 %13872 to i64
  store i64 %13873, i64* %RCX.i11580, align 8
  %13874 = zext i16 %13872 to i64
  %13875 = shl nuw nsw i64 %13874, 4
  store i64 %13875, i64* %573, align 8
  %13876 = add i64 %13875, %13846
  store i64 %13876, i64* %RAX.i11582.pre-phi, align 8
  %13877 = icmp ult i64 %13876, %13846
  %13878 = icmp ult i64 %13876, %13875
  %13879 = or i1 %13877, %13878
  %13880 = zext i1 %13879 to i8
  store i8 %13880, i8* %14, align 1
  %13881 = trunc i64 %13876 to i32
  %13882 = and i32 %13881, 255
  %13883 = tail call i32 @llvm.ctpop.i32(i32 %13882)
  %13884 = trunc i32 %13883 to i8
  %13885 = and i8 %13884, 1
  %13886 = xor i8 %13885, 1
  store i8 %13886, i8* %21, align 1
  %13887 = xor i64 %13875, %13846
  %13888 = xor i64 %13887, %13876
  %13889 = lshr i64 %13888, 4
  %13890 = trunc i64 %13889 to i8
  %13891 = and i8 %13890, 1
  store i8 %13891, i8* %27, align 1
  %13892 = icmp eq i64 %13876, 0
  %13893 = zext i1 %13892 to i8
  store i8 %13893, i8* %30, align 1
  %13894 = lshr i64 %13876, 63
  %13895 = trunc i64 %13894 to i8
  store i8 %13895, i8* %33, align 1
  %13896 = xor i64 %13894, %13862
  %13897 = add nuw nsw i64 %13896, %13894
  %13898 = icmp eq i64 %13897, 2
  %13899 = zext i1 %13898 to i8
  store i8 %13899, i8* %39, align 1
  %13900 = add i64 %13876, 4
  %13901 = add i64 %13842, 29
  store i64 %13901, i64* %3, align 8
  %13902 = inttoptr i64 %13900 to i32*
  %13903 = load i32, i32* %13902, align 4
  %13904 = zext i32 %13903 to i64
  store i64 %13904, i64* %RCX.i11580, align 8
  %13905 = load i64, i64* %RBP.i, align 8
  %13906 = add i64 %13905, -144
  %13907 = add i64 %13842, 35
  store i64 %13907, i64* %3, align 8
  %13908 = inttoptr i64 %13906 to i32*
  %13909 = load i32, i32* %13908, align 4
  %13910 = add i32 %13909, %13903
  %13911 = zext i32 %13910 to i64
  store i64 %13911, i64* %RCX.i11580, align 8
  %13912 = icmp ult i32 %13910, %13903
  %13913 = icmp ult i32 %13910, %13909
  %13914 = or i1 %13912, %13913
  %13915 = zext i1 %13914 to i8
  store i8 %13915, i8* %14, align 1
  %13916 = and i32 %13910, 255
  %13917 = tail call i32 @llvm.ctpop.i32(i32 %13916)
  %13918 = trunc i32 %13917 to i8
  %13919 = and i8 %13918, 1
  %13920 = xor i8 %13919, 1
  store i8 %13920, i8* %21, align 1
  %13921 = xor i32 %13909, %13903
  %13922 = xor i32 %13921, %13910
  %13923 = lshr i32 %13922, 4
  %13924 = trunc i32 %13923 to i8
  %13925 = and i8 %13924, 1
  store i8 %13925, i8* %27, align 1
  %13926 = icmp eq i32 %13910, 0
  %13927 = zext i1 %13926 to i8
  store i8 %13927, i8* %30, align 1
  %13928 = lshr i32 %13910, 31
  %13929 = trunc i32 %13928 to i8
  store i8 %13929, i8* %33, align 1
  %13930 = lshr i32 %13903, 31
  %13931 = lshr i32 %13909, 31
  %13932 = xor i32 %13928, %13930
  %13933 = xor i32 %13928, %13931
  %13934 = add nuw nsw i32 %13932, %13933
  %13935 = icmp eq i32 %13934, 2
  %13936 = zext i1 %13935 to i8
  store i8 %13936, i8* %39, align 1
  %13937 = add i64 %13842, 41
  store i64 %13937, i64* %3, align 8
  store i32 %13910, i32* %13908, align 4
  %13938 = load i64, i64* %RBP.i, align 8
  %13939 = add i64 %13938, -8
  %13940 = load i64, i64* %3, align 8
  %13941 = add i64 %13940, 4
  store i64 %13941, i64* %3, align 8
  %13942 = inttoptr i64 %13939 to i64*
  %13943 = load i64, i64* %13942, align 8
  %13944 = add i64 %13943, 51640
  store i64 %13944, i64* %RAX.i11582.pre-phi, align 8
  %13945 = icmp ugt i64 %13943, -51641
  %13946 = zext i1 %13945 to i8
  store i8 %13946, i8* %14, align 1
  %13947 = trunc i64 %13944 to i32
  %13948 = and i32 %13947, 255
  %13949 = tail call i32 @llvm.ctpop.i32(i32 %13948)
  %13950 = trunc i32 %13949 to i8
  %13951 = and i8 %13950, 1
  %13952 = xor i8 %13951, 1
  store i8 %13952, i8* %21, align 1
  %13953 = xor i64 %13943, 16
  %13954 = xor i64 %13953, %13944
  %13955 = lshr i64 %13954, 4
  %13956 = trunc i64 %13955 to i8
  %13957 = and i8 %13956, 1
  store i8 %13957, i8* %27, align 1
  %13958 = icmp eq i64 %13944, 0
  %13959 = zext i1 %13958 to i8
  store i8 %13959, i8* %30, align 1
  %13960 = lshr i64 %13944, 63
  %13961 = trunc i64 %13960 to i8
  store i8 %13961, i8* %33, align 1
  %13962 = lshr i64 %13943, 63
  %13963 = xor i64 %13960, %13962
  %13964 = add nuw nsw i64 %13963, %13960
  %13965 = icmp eq i64 %13964, 2
  %13966 = zext i1 %13965 to i8
  store i8 %13966, i8* %39, align 1
  %13967 = add i64 %13938, -150
  %13968 = add i64 %13940, 17
  store i64 %13968, i64* %3, align 8
  %13969 = inttoptr i64 %13967 to i16*
  %13970 = load i16, i16* %13969, align 2
  %13971 = zext i16 %13970 to i64
  store i64 %13971, i64* %RCX.i11580, align 8
  %13972 = zext i16 %13970 to i64
  %13973 = shl nuw nsw i64 %13972, 4
  store i64 %13973, i64* %573, align 8
  %13974 = add i64 %13973, %13944
  store i64 %13974, i64* %RAX.i11582.pre-phi, align 8
  %13975 = icmp ult i64 %13974, %13944
  %13976 = icmp ult i64 %13974, %13973
  %13977 = or i1 %13975, %13976
  %13978 = zext i1 %13977 to i8
  store i8 %13978, i8* %14, align 1
  %13979 = trunc i64 %13974 to i32
  %13980 = and i32 %13979, 255
  %13981 = tail call i32 @llvm.ctpop.i32(i32 %13980)
  %13982 = trunc i32 %13981 to i8
  %13983 = and i8 %13982, 1
  %13984 = xor i8 %13983, 1
  store i8 %13984, i8* %21, align 1
  %13985 = xor i64 %13973, %13944
  %13986 = xor i64 %13985, %13974
  %13987 = lshr i64 %13986, 4
  %13988 = trunc i64 %13987 to i8
  %13989 = and i8 %13988, 1
  store i8 %13989, i8* %27, align 1
  %13990 = icmp eq i64 %13974, 0
  %13991 = zext i1 %13990 to i8
  store i8 %13991, i8* %30, align 1
  %13992 = lshr i64 %13974, 63
  %13993 = trunc i64 %13992 to i8
  store i8 %13993, i8* %33, align 1
  %13994 = xor i64 %13992, %13960
  %13995 = add nuw nsw i64 %13994, %13992
  %13996 = icmp eq i64 %13995, 2
  %13997 = zext i1 %13996 to i8
  store i8 %13997, i8* %39, align 1
  %13998 = add i64 %13974, 8
  %13999 = add i64 %13940, 29
  store i64 %13999, i64* %3, align 8
  %14000 = inttoptr i64 %13998 to i32*
  %14001 = load i32, i32* %14000, align 4
  %14002 = zext i32 %14001 to i64
  store i64 %14002, i64* %RCX.i11580, align 8
  %14003 = load i64, i64* %RBP.i, align 8
  %14004 = add i64 %14003, -148
  %14005 = add i64 %13940, 35
  store i64 %14005, i64* %3, align 8
  %14006 = inttoptr i64 %14004 to i32*
  %14007 = load i32, i32* %14006, align 4
  %14008 = add i32 %14007, %14001
  %14009 = zext i32 %14008 to i64
  store i64 %14009, i64* %RCX.i11580, align 8
  %14010 = icmp ult i32 %14008, %14001
  %14011 = icmp ult i32 %14008, %14007
  %14012 = or i1 %14010, %14011
  %14013 = zext i1 %14012 to i8
  store i8 %14013, i8* %14, align 1
  %14014 = and i32 %14008, 255
  %14015 = tail call i32 @llvm.ctpop.i32(i32 %14014)
  %14016 = trunc i32 %14015 to i8
  %14017 = and i8 %14016, 1
  %14018 = xor i8 %14017, 1
  store i8 %14018, i8* %21, align 1
  %14019 = xor i32 %14007, %14001
  %14020 = xor i32 %14019, %14008
  %14021 = lshr i32 %14020, 4
  %14022 = trunc i32 %14021 to i8
  %14023 = and i8 %14022, 1
  store i8 %14023, i8* %27, align 1
  %14024 = icmp eq i32 %14008, 0
  %14025 = zext i1 %14024 to i8
  store i8 %14025, i8* %30, align 1
  %14026 = lshr i32 %14008, 31
  %14027 = trunc i32 %14026 to i8
  store i8 %14027, i8* %33, align 1
  %14028 = lshr i32 %14001, 31
  %14029 = lshr i32 %14007, 31
  %14030 = xor i32 %14026, %14028
  %14031 = xor i32 %14026, %14029
  %14032 = add nuw nsw i32 %14030, %14031
  %14033 = icmp eq i32 %14032, 2
  %14034 = zext i1 %14033 to i8
  store i8 %14034, i8* %39, align 1
  %14035 = add i64 %13940, 41
  store i64 %14035, i64* %3, align 8
  store i32 %14008, i32* %14006, align 4
  %14036 = load i64, i64* %RBP.i, align 8
  %14037 = add i64 %14036, -120
  %14038 = load i64, i64* %3, align 8
  %14039 = add i64 %14038, 4
  store i64 %14039, i64* %3, align 8
  %14040 = inttoptr i64 %14037 to i64*
  %14041 = load i64, i64* %14040, align 8
  store i64 %14041, i64* %RAX.i11582.pre-phi, align 8
  %14042 = add i64 %14036, -28
  %14043 = add i64 %14038, 7
  store i64 %14043, i64* %3, align 8
  %14044 = inttoptr i64 %14042 to i32*
  %14045 = load i32, i32* %14044, align 4
  %14046 = add i32 %14045, 34
  %14047 = zext i32 %14046 to i64
  store i64 %14047, i64* %RCX.i11580, align 8
  %14048 = icmp ugt i32 %14045, -35
  %14049 = zext i1 %14048 to i8
  store i8 %14049, i8* %14, align 1
  %14050 = and i32 %14046, 255
  %14051 = tail call i32 @llvm.ctpop.i32(i32 %14050)
  %14052 = trunc i32 %14051 to i8
  %14053 = and i8 %14052, 1
  %14054 = xor i8 %14053, 1
  store i8 %14054, i8* %21, align 1
  %14055 = xor i32 %14046, %14045
  %14056 = lshr i32 %14055, 4
  %14057 = trunc i32 %14056 to i8
  %14058 = and i8 %14057, 1
  store i8 %14058, i8* %27, align 1
  %14059 = icmp eq i32 %14046, 0
  %14060 = zext i1 %14059 to i8
  store i8 %14060, i8* %30, align 1
  %14061 = lshr i32 %14046, 31
  %14062 = trunc i32 %14061 to i8
  store i8 %14062, i8* %33, align 1
  %14063 = lshr i32 %14045, 31
  %14064 = xor i32 %14061, %14063
  %14065 = add nuw nsw i32 %14064, %14061
  %14066 = icmp eq i32 %14065, 2
  %14067 = zext i1 %14066 to i8
  store i8 %14067, i8* %39, align 1
  %14068 = sext i32 %14046 to i64
  store i64 %14068, i64* %573, align 8
  %14069 = shl nsw i64 %14068, 1
  %14070 = add i64 %14041, %14069
  %14071 = add i64 %14038, 17
  store i64 %14071, i64* %3, align 8
  %14072 = inttoptr i64 %14070 to i16*
  %14073 = load i16, i16* %14072, align 2
  store i16 %14073, i16* %SI.i, align 2
  %14074 = add i64 %14036, -150
  %14075 = add i64 %14038, 24
  store i64 %14075, i64* %3, align 8
  %14076 = inttoptr i64 %14074 to i16*
  store i16 %14073, i16* %14076, align 2
  %14077 = load i64, i64* %RBP.i, align 8
  %14078 = add i64 %14077, -8
  %14079 = load i64, i64* %3, align 8
  %14080 = add i64 %14079, 4
  store i64 %14080, i64* %3, align 8
  %14081 = inttoptr i64 %14078 to i64*
  %14082 = load i64, i64* %14081, align 8
  %14083 = add i64 %14082, 51640
  store i64 %14083, i64* %RAX.i11582.pre-phi, align 8
  %14084 = icmp ugt i64 %14082, -51641
  %14085 = zext i1 %14084 to i8
  store i8 %14085, i8* %14, align 1
  %14086 = trunc i64 %14083 to i32
  %14087 = and i32 %14086, 255
  %14088 = tail call i32 @llvm.ctpop.i32(i32 %14087)
  %14089 = trunc i32 %14088 to i8
  %14090 = and i8 %14089, 1
  %14091 = xor i8 %14090, 1
  store i8 %14091, i8* %21, align 1
  %14092 = xor i64 %14082, 16
  %14093 = xor i64 %14092, %14083
  %14094 = lshr i64 %14093, 4
  %14095 = trunc i64 %14094 to i8
  %14096 = and i8 %14095, 1
  store i8 %14096, i8* %27, align 1
  %14097 = icmp eq i64 %14083, 0
  %14098 = zext i1 %14097 to i8
  store i8 %14098, i8* %30, align 1
  %14099 = lshr i64 %14083, 63
  %14100 = trunc i64 %14099 to i8
  store i8 %14100, i8* %33, align 1
  %14101 = lshr i64 %14082, 63
  %14102 = xor i64 %14099, %14101
  %14103 = add nuw nsw i64 %14102, %14099
  %14104 = icmp eq i64 %14103, 2
  %14105 = zext i1 %14104 to i8
  store i8 %14105, i8* %39, align 1
  %14106 = add i64 %14077, -150
  %14107 = add i64 %14079, 17
  store i64 %14107, i64* %3, align 8
  %14108 = inttoptr i64 %14106 to i16*
  %14109 = load i16, i16* %14108, align 2
  %14110 = zext i16 %14109 to i64
  store i64 %14110, i64* %RCX.i11580, align 8
  %14111 = zext i16 %14109 to i64
  %14112 = shl nuw nsw i64 %14111, 4
  store i64 %14112, i64* %573, align 8
  %14113 = add i64 %14112, %14083
  store i64 %14113, i64* %RAX.i11582.pre-phi, align 8
  %14114 = icmp ult i64 %14113, %14083
  %14115 = icmp ult i64 %14113, %14112
  %14116 = or i1 %14114, %14115
  %14117 = zext i1 %14116 to i8
  store i8 %14117, i8* %14, align 1
  %14118 = trunc i64 %14113 to i32
  %14119 = and i32 %14118, 255
  %14120 = tail call i32 @llvm.ctpop.i32(i32 %14119)
  %14121 = trunc i32 %14120 to i8
  %14122 = and i8 %14121, 1
  %14123 = xor i8 %14122, 1
  store i8 %14123, i8* %21, align 1
  %14124 = xor i64 %14112, %14083
  %14125 = xor i64 %14124, %14113
  %14126 = lshr i64 %14125, 4
  %14127 = trunc i64 %14126 to i8
  %14128 = and i8 %14127, 1
  store i8 %14128, i8* %27, align 1
  %14129 = icmp eq i64 %14113, 0
  %14130 = zext i1 %14129 to i8
  store i8 %14130, i8* %30, align 1
  %14131 = lshr i64 %14113, 63
  %14132 = trunc i64 %14131 to i8
  store i8 %14132, i8* %33, align 1
  %14133 = xor i64 %14131, %14099
  %14134 = add nuw nsw i64 %14133, %14131
  %14135 = icmp eq i64 %14134, 2
  %14136 = zext i1 %14135 to i8
  store i8 %14136, i8* %39, align 1
  %14137 = inttoptr i64 %14113 to i32*
  %14138 = add i64 %14079, 28
  store i64 %14138, i64* %3, align 8
  %14139 = load i32, i32* %14137, align 4
  %14140 = zext i32 %14139 to i64
  store i64 %14140, i64* %RCX.i11580, align 8
  %14141 = load i64, i64* %RBP.i, align 8
  %14142 = add i64 %14141, -140
  %14143 = add i64 %14079, 34
  store i64 %14143, i64* %3, align 8
  %14144 = inttoptr i64 %14142 to i32*
  %14145 = load i32, i32* %14144, align 4
  %14146 = add i32 %14145, %14139
  %14147 = zext i32 %14146 to i64
  store i64 %14147, i64* %RCX.i11580, align 8
  %14148 = icmp ult i32 %14146, %14139
  %14149 = icmp ult i32 %14146, %14145
  %14150 = or i1 %14148, %14149
  %14151 = zext i1 %14150 to i8
  store i8 %14151, i8* %14, align 1
  %14152 = and i32 %14146, 255
  %14153 = tail call i32 @llvm.ctpop.i32(i32 %14152)
  %14154 = trunc i32 %14153 to i8
  %14155 = and i8 %14154, 1
  %14156 = xor i8 %14155, 1
  store i8 %14156, i8* %21, align 1
  %14157 = xor i32 %14145, %14139
  %14158 = xor i32 %14157, %14146
  %14159 = lshr i32 %14158, 4
  %14160 = trunc i32 %14159 to i8
  %14161 = and i8 %14160, 1
  store i8 %14161, i8* %27, align 1
  %14162 = icmp eq i32 %14146, 0
  %14163 = zext i1 %14162 to i8
  store i8 %14163, i8* %30, align 1
  %14164 = lshr i32 %14146, 31
  %14165 = trunc i32 %14164 to i8
  store i8 %14165, i8* %33, align 1
  %14166 = lshr i32 %14139, 31
  %14167 = lshr i32 %14145, 31
  %14168 = xor i32 %14164, %14166
  %14169 = xor i32 %14164, %14167
  %14170 = add nuw nsw i32 %14168, %14169
  %14171 = icmp eq i32 %14170, 2
  %14172 = zext i1 %14171 to i8
  store i8 %14172, i8* %39, align 1
  %14173 = add i64 %14079, 40
  store i64 %14173, i64* %3, align 8
  store i32 %14146, i32* %14144, align 4
  %14174 = load i64, i64* %RBP.i, align 8
  %14175 = add i64 %14174, -8
  %14176 = load i64, i64* %3, align 8
  %14177 = add i64 %14176, 4
  store i64 %14177, i64* %3, align 8
  %14178 = inttoptr i64 %14175 to i64*
  %14179 = load i64, i64* %14178, align 8
  %14180 = add i64 %14179, 51640
  store i64 %14180, i64* %RAX.i11582.pre-phi, align 8
  %14181 = icmp ugt i64 %14179, -51641
  %14182 = zext i1 %14181 to i8
  store i8 %14182, i8* %14, align 1
  %14183 = trunc i64 %14180 to i32
  %14184 = and i32 %14183, 255
  %14185 = tail call i32 @llvm.ctpop.i32(i32 %14184)
  %14186 = trunc i32 %14185 to i8
  %14187 = and i8 %14186, 1
  %14188 = xor i8 %14187, 1
  store i8 %14188, i8* %21, align 1
  %14189 = xor i64 %14179, 16
  %14190 = xor i64 %14189, %14180
  %14191 = lshr i64 %14190, 4
  %14192 = trunc i64 %14191 to i8
  %14193 = and i8 %14192, 1
  store i8 %14193, i8* %27, align 1
  %14194 = icmp eq i64 %14180, 0
  %14195 = zext i1 %14194 to i8
  store i8 %14195, i8* %30, align 1
  %14196 = lshr i64 %14180, 63
  %14197 = trunc i64 %14196 to i8
  store i8 %14197, i8* %33, align 1
  %14198 = lshr i64 %14179, 63
  %14199 = xor i64 %14196, %14198
  %14200 = add nuw nsw i64 %14199, %14196
  %14201 = icmp eq i64 %14200, 2
  %14202 = zext i1 %14201 to i8
  store i8 %14202, i8* %39, align 1
  %14203 = add i64 %14174, -150
  %14204 = add i64 %14176, 17
  store i64 %14204, i64* %3, align 8
  %14205 = inttoptr i64 %14203 to i16*
  %14206 = load i16, i16* %14205, align 2
  %14207 = zext i16 %14206 to i64
  store i64 %14207, i64* %RCX.i11580, align 8
  %14208 = zext i16 %14206 to i64
  %14209 = shl nuw nsw i64 %14208, 4
  store i64 %14209, i64* %573, align 8
  %14210 = add i64 %14209, %14180
  store i64 %14210, i64* %RAX.i11582.pre-phi, align 8
  %14211 = icmp ult i64 %14210, %14180
  %14212 = icmp ult i64 %14210, %14209
  %14213 = or i1 %14211, %14212
  %14214 = zext i1 %14213 to i8
  store i8 %14214, i8* %14, align 1
  %14215 = trunc i64 %14210 to i32
  %14216 = and i32 %14215, 255
  %14217 = tail call i32 @llvm.ctpop.i32(i32 %14216)
  %14218 = trunc i32 %14217 to i8
  %14219 = and i8 %14218, 1
  %14220 = xor i8 %14219, 1
  store i8 %14220, i8* %21, align 1
  %14221 = xor i64 %14209, %14180
  %14222 = xor i64 %14221, %14210
  %14223 = lshr i64 %14222, 4
  %14224 = trunc i64 %14223 to i8
  %14225 = and i8 %14224, 1
  store i8 %14225, i8* %27, align 1
  %14226 = icmp eq i64 %14210, 0
  %14227 = zext i1 %14226 to i8
  store i8 %14227, i8* %30, align 1
  %14228 = lshr i64 %14210, 63
  %14229 = trunc i64 %14228 to i8
  store i8 %14229, i8* %33, align 1
  %14230 = xor i64 %14228, %14196
  %14231 = add nuw nsw i64 %14230, %14228
  %14232 = icmp eq i64 %14231, 2
  %14233 = zext i1 %14232 to i8
  store i8 %14233, i8* %39, align 1
  %14234 = add i64 %14210, 4
  %14235 = add i64 %14176, 29
  store i64 %14235, i64* %3, align 8
  %14236 = inttoptr i64 %14234 to i32*
  %14237 = load i32, i32* %14236, align 4
  %14238 = zext i32 %14237 to i64
  store i64 %14238, i64* %RCX.i11580, align 8
  %14239 = load i64, i64* %RBP.i, align 8
  %14240 = add i64 %14239, -144
  %14241 = add i64 %14176, 35
  store i64 %14241, i64* %3, align 8
  %14242 = inttoptr i64 %14240 to i32*
  %14243 = load i32, i32* %14242, align 4
  %14244 = add i32 %14243, %14237
  %14245 = zext i32 %14244 to i64
  store i64 %14245, i64* %RCX.i11580, align 8
  %14246 = icmp ult i32 %14244, %14237
  %14247 = icmp ult i32 %14244, %14243
  %14248 = or i1 %14246, %14247
  %14249 = zext i1 %14248 to i8
  store i8 %14249, i8* %14, align 1
  %14250 = and i32 %14244, 255
  %14251 = tail call i32 @llvm.ctpop.i32(i32 %14250)
  %14252 = trunc i32 %14251 to i8
  %14253 = and i8 %14252, 1
  %14254 = xor i8 %14253, 1
  store i8 %14254, i8* %21, align 1
  %14255 = xor i32 %14243, %14237
  %14256 = xor i32 %14255, %14244
  %14257 = lshr i32 %14256, 4
  %14258 = trunc i32 %14257 to i8
  %14259 = and i8 %14258, 1
  store i8 %14259, i8* %27, align 1
  %14260 = icmp eq i32 %14244, 0
  %14261 = zext i1 %14260 to i8
  store i8 %14261, i8* %30, align 1
  %14262 = lshr i32 %14244, 31
  %14263 = trunc i32 %14262 to i8
  store i8 %14263, i8* %33, align 1
  %14264 = lshr i32 %14237, 31
  %14265 = lshr i32 %14243, 31
  %14266 = xor i32 %14262, %14264
  %14267 = xor i32 %14262, %14265
  %14268 = add nuw nsw i32 %14266, %14267
  %14269 = icmp eq i32 %14268, 2
  %14270 = zext i1 %14269 to i8
  store i8 %14270, i8* %39, align 1
  %14271 = add i64 %14176, 41
  store i64 %14271, i64* %3, align 8
  store i32 %14244, i32* %14242, align 4
  %14272 = load i64, i64* %RBP.i, align 8
  %14273 = add i64 %14272, -8
  %14274 = load i64, i64* %3, align 8
  %14275 = add i64 %14274, 4
  store i64 %14275, i64* %3, align 8
  %14276 = inttoptr i64 %14273 to i64*
  %14277 = load i64, i64* %14276, align 8
  %14278 = add i64 %14277, 51640
  store i64 %14278, i64* %RAX.i11582.pre-phi, align 8
  %14279 = icmp ugt i64 %14277, -51641
  %14280 = zext i1 %14279 to i8
  store i8 %14280, i8* %14, align 1
  %14281 = trunc i64 %14278 to i32
  %14282 = and i32 %14281, 255
  %14283 = tail call i32 @llvm.ctpop.i32(i32 %14282)
  %14284 = trunc i32 %14283 to i8
  %14285 = and i8 %14284, 1
  %14286 = xor i8 %14285, 1
  store i8 %14286, i8* %21, align 1
  %14287 = xor i64 %14277, 16
  %14288 = xor i64 %14287, %14278
  %14289 = lshr i64 %14288, 4
  %14290 = trunc i64 %14289 to i8
  %14291 = and i8 %14290, 1
  store i8 %14291, i8* %27, align 1
  %14292 = icmp eq i64 %14278, 0
  %14293 = zext i1 %14292 to i8
  store i8 %14293, i8* %30, align 1
  %14294 = lshr i64 %14278, 63
  %14295 = trunc i64 %14294 to i8
  store i8 %14295, i8* %33, align 1
  %14296 = lshr i64 %14277, 63
  %14297 = xor i64 %14294, %14296
  %14298 = add nuw nsw i64 %14297, %14294
  %14299 = icmp eq i64 %14298, 2
  %14300 = zext i1 %14299 to i8
  store i8 %14300, i8* %39, align 1
  %14301 = add i64 %14272, -150
  %14302 = add i64 %14274, 17
  store i64 %14302, i64* %3, align 8
  %14303 = inttoptr i64 %14301 to i16*
  %14304 = load i16, i16* %14303, align 2
  %14305 = zext i16 %14304 to i64
  store i64 %14305, i64* %RCX.i11580, align 8
  %14306 = zext i16 %14304 to i64
  %14307 = shl nuw nsw i64 %14306, 4
  store i64 %14307, i64* %573, align 8
  %14308 = add i64 %14307, %14278
  store i64 %14308, i64* %RAX.i11582.pre-phi, align 8
  %14309 = icmp ult i64 %14308, %14278
  %14310 = icmp ult i64 %14308, %14307
  %14311 = or i1 %14309, %14310
  %14312 = zext i1 %14311 to i8
  store i8 %14312, i8* %14, align 1
  %14313 = trunc i64 %14308 to i32
  %14314 = and i32 %14313, 255
  %14315 = tail call i32 @llvm.ctpop.i32(i32 %14314)
  %14316 = trunc i32 %14315 to i8
  %14317 = and i8 %14316, 1
  %14318 = xor i8 %14317, 1
  store i8 %14318, i8* %21, align 1
  %14319 = xor i64 %14307, %14278
  %14320 = xor i64 %14319, %14308
  %14321 = lshr i64 %14320, 4
  %14322 = trunc i64 %14321 to i8
  %14323 = and i8 %14322, 1
  store i8 %14323, i8* %27, align 1
  %14324 = icmp eq i64 %14308, 0
  %14325 = zext i1 %14324 to i8
  store i8 %14325, i8* %30, align 1
  %14326 = lshr i64 %14308, 63
  %14327 = trunc i64 %14326 to i8
  store i8 %14327, i8* %33, align 1
  %14328 = xor i64 %14326, %14294
  %14329 = add nuw nsw i64 %14328, %14326
  %14330 = icmp eq i64 %14329, 2
  %14331 = zext i1 %14330 to i8
  store i8 %14331, i8* %39, align 1
  %14332 = add i64 %14308, 8
  %14333 = add i64 %14274, 29
  store i64 %14333, i64* %3, align 8
  %14334 = inttoptr i64 %14332 to i32*
  %14335 = load i32, i32* %14334, align 4
  %14336 = zext i32 %14335 to i64
  store i64 %14336, i64* %RCX.i11580, align 8
  %14337 = load i64, i64* %RBP.i, align 8
  %14338 = add i64 %14337, -148
  %14339 = add i64 %14274, 35
  store i64 %14339, i64* %3, align 8
  %14340 = inttoptr i64 %14338 to i32*
  %14341 = load i32, i32* %14340, align 4
  %14342 = add i32 %14341, %14335
  %14343 = zext i32 %14342 to i64
  store i64 %14343, i64* %RCX.i11580, align 8
  %14344 = icmp ult i32 %14342, %14335
  %14345 = icmp ult i32 %14342, %14341
  %14346 = or i1 %14344, %14345
  %14347 = zext i1 %14346 to i8
  store i8 %14347, i8* %14, align 1
  %14348 = and i32 %14342, 255
  %14349 = tail call i32 @llvm.ctpop.i32(i32 %14348)
  %14350 = trunc i32 %14349 to i8
  %14351 = and i8 %14350, 1
  %14352 = xor i8 %14351, 1
  store i8 %14352, i8* %21, align 1
  %14353 = xor i32 %14341, %14335
  %14354 = xor i32 %14353, %14342
  %14355 = lshr i32 %14354, 4
  %14356 = trunc i32 %14355 to i8
  %14357 = and i8 %14356, 1
  store i8 %14357, i8* %27, align 1
  %14358 = icmp eq i32 %14342, 0
  %14359 = zext i1 %14358 to i8
  store i8 %14359, i8* %30, align 1
  %14360 = lshr i32 %14342, 31
  %14361 = trunc i32 %14360 to i8
  store i8 %14361, i8* %33, align 1
  %14362 = lshr i32 %14335, 31
  %14363 = lshr i32 %14341, 31
  %14364 = xor i32 %14360, %14362
  %14365 = xor i32 %14360, %14363
  %14366 = add nuw nsw i32 %14364, %14365
  %14367 = icmp eq i32 %14366, 2
  %14368 = zext i1 %14367 to i8
  store i8 %14368, i8* %39, align 1
  %14369 = add i64 %14274, 41
  store i64 %14369, i64* %3, align 8
  store i32 %14342, i32* %14340, align 4
  %14370 = load i64, i64* %RBP.i, align 8
  %14371 = add i64 %14370, -120
  %14372 = load i64, i64* %3, align 8
  %14373 = add i64 %14372, 4
  store i64 %14373, i64* %3, align 8
  %14374 = inttoptr i64 %14371 to i64*
  %14375 = load i64, i64* %14374, align 8
  store i64 %14375, i64* %RAX.i11582.pre-phi, align 8
  %14376 = add i64 %14370, -28
  %14377 = add i64 %14372, 7
  store i64 %14377, i64* %3, align 8
  %14378 = inttoptr i64 %14376 to i32*
  %14379 = load i32, i32* %14378, align 4
  %14380 = add i32 %14379, 35
  %14381 = zext i32 %14380 to i64
  store i64 %14381, i64* %RCX.i11580, align 8
  %14382 = icmp ugt i32 %14379, -36
  %14383 = zext i1 %14382 to i8
  store i8 %14383, i8* %14, align 1
  %14384 = and i32 %14380, 255
  %14385 = tail call i32 @llvm.ctpop.i32(i32 %14384)
  %14386 = trunc i32 %14385 to i8
  %14387 = and i8 %14386, 1
  %14388 = xor i8 %14387, 1
  store i8 %14388, i8* %21, align 1
  %14389 = xor i32 %14380, %14379
  %14390 = lshr i32 %14389, 4
  %14391 = trunc i32 %14390 to i8
  %14392 = and i8 %14391, 1
  store i8 %14392, i8* %27, align 1
  %14393 = icmp eq i32 %14380, 0
  %14394 = zext i1 %14393 to i8
  store i8 %14394, i8* %30, align 1
  %14395 = lshr i32 %14380, 31
  %14396 = trunc i32 %14395 to i8
  store i8 %14396, i8* %33, align 1
  %14397 = lshr i32 %14379, 31
  %14398 = xor i32 %14395, %14397
  %14399 = add nuw nsw i32 %14398, %14395
  %14400 = icmp eq i32 %14399, 2
  %14401 = zext i1 %14400 to i8
  store i8 %14401, i8* %39, align 1
  %14402 = sext i32 %14380 to i64
  store i64 %14402, i64* %573, align 8
  %14403 = shl nsw i64 %14402, 1
  %14404 = add i64 %14375, %14403
  %14405 = add i64 %14372, 17
  store i64 %14405, i64* %3, align 8
  %14406 = inttoptr i64 %14404 to i16*
  %14407 = load i16, i16* %14406, align 2
  store i16 %14407, i16* %SI.i, align 2
  %14408 = add i64 %14370, -150
  %14409 = add i64 %14372, 24
  store i64 %14409, i64* %3, align 8
  %14410 = inttoptr i64 %14408 to i16*
  store i16 %14407, i16* %14410, align 2
  %14411 = load i64, i64* %RBP.i, align 8
  %14412 = add i64 %14411, -8
  %14413 = load i64, i64* %3, align 8
  %14414 = add i64 %14413, 4
  store i64 %14414, i64* %3, align 8
  %14415 = inttoptr i64 %14412 to i64*
  %14416 = load i64, i64* %14415, align 8
  %14417 = add i64 %14416, 51640
  store i64 %14417, i64* %RAX.i11582.pre-phi, align 8
  %14418 = icmp ugt i64 %14416, -51641
  %14419 = zext i1 %14418 to i8
  store i8 %14419, i8* %14, align 1
  %14420 = trunc i64 %14417 to i32
  %14421 = and i32 %14420, 255
  %14422 = tail call i32 @llvm.ctpop.i32(i32 %14421)
  %14423 = trunc i32 %14422 to i8
  %14424 = and i8 %14423, 1
  %14425 = xor i8 %14424, 1
  store i8 %14425, i8* %21, align 1
  %14426 = xor i64 %14416, 16
  %14427 = xor i64 %14426, %14417
  %14428 = lshr i64 %14427, 4
  %14429 = trunc i64 %14428 to i8
  %14430 = and i8 %14429, 1
  store i8 %14430, i8* %27, align 1
  %14431 = icmp eq i64 %14417, 0
  %14432 = zext i1 %14431 to i8
  store i8 %14432, i8* %30, align 1
  %14433 = lshr i64 %14417, 63
  %14434 = trunc i64 %14433 to i8
  store i8 %14434, i8* %33, align 1
  %14435 = lshr i64 %14416, 63
  %14436 = xor i64 %14433, %14435
  %14437 = add nuw nsw i64 %14436, %14433
  %14438 = icmp eq i64 %14437, 2
  %14439 = zext i1 %14438 to i8
  store i8 %14439, i8* %39, align 1
  %14440 = add i64 %14411, -150
  %14441 = add i64 %14413, 17
  store i64 %14441, i64* %3, align 8
  %14442 = inttoptr i64 %14440 to i16*
  %14443 = load i16, i16* %14442, align 2
  %14444 = zext i16 %14443 to i64
  store i64 %14444, i64* %RCX.i11580, align 8
  %14445 = zext i16 %14443 to i64
  %14446 = shl nuw nsw i64 %14445, 4
  store i64 %14446, i64* %573, align 8
  %14447 = add i64 %14446, %14417
  store i64 %14447, i64* %RAX.i11582.pre-phi, align 8
  %14448 = icmp ult i64 %14447, %14417
  %14449 = icmp ult i64 %14447, %14446
  %14450 = or i1 %14448, %14449
  %14451 = zext i1 %14450 to i8
  store i8 %14451, i8* %14, align 1
  %14452 = trunc i64 %14447 to i32
  %14453 = and i32 %14452, 255
  %14454 = tail call i32 @llvm.ctpop.i32(i32 %14453)
  %14455 = trunc i32 %14454 to i8
  %14456 = and i8 %14455, 1
  %14457 = xor i8 %14456, 1
  store i8 %14457, i8* %21, align 1
  %14458 = xor i64 %14446, %14417
  %14459 = xor i64 %14458, %14447
  %14460 = lshr i64 %14459, 4
  %14461 = trunc i64 %14460 to i8
  %14462 = and i8 %14461, 1
  store i8 %14462, i8* %27, align 1
  %14463 = icmp eq i64 %14447, 0
  %14464 = zext i1 %14463 to i8
  store i8 %14464, i8* %30, align 1
  %14465 = lshr i64 %14447, 63
  %14466 = trunc i64 %14465 to i8
  store i8 %14466, i8* %33, align 1
  %14467 = xor i64 %14465, %14433
  %14468 = add nuw nsw i64 %14467, %14465
  %14469 = icmp eq i64 %14468, 2
  %14470 = zext i1 %14469 to i8
  store i8 %14470, i8* %39, align 1
  %14471 = inttoptr i64 %14447 to i32*
  %14472 = add i64 %14413, 28
  store i64 %14472, i64* %3, align 8
  %14473 = load i32, i32* %14471, align 4
  %14474 = zext i32 %14473 to i64
  store i64 %14474, i64* %RCX.i11580, align 8
  %14475 = load i64, i64* %RBP.i, align 8
  %14476 = add i64 %14475, -140
  %14477 = add i64 %14413, 34
  store i64 %14477, i64* %3, align 8
  %14478 = inttoptr i64 %14476 to i32*
  %14479 = load i32, i32* %14478, align 4
  %14480 = add i32 %14479, %14473
  %14481 = zext i32 %14480 to i64
  store i64 %14481, i64* %RCX.i11580, align 8
  %14482 = icmp ult i32 %14480, %14473
  %14483 = icmp ult i32 %14480, %14479
  %14484 = or i1 %14482, %14483
  %14485 = zext i1 %14484 to i8
  store i8 %14485, i8* %14, align 1
  %14486 = and i32 %14480, 255
  %14487 = tail call i32 @llvm.ctpop.i32(i32 %14486)
  %14488 = trunc i32 %14487 to i8
  %14489 = and i8 %14488, 1
  %14490 = xor i8 %14489, 1
  store i8 %14490, i8* %21, align 1
  %14491 = xor i32 %14479, %14473
  %14492 = xor i32 %14491, %14480
  %14493 = lshr i32 %14492, 4
  %14494 = trunc i32 %14493 to i8
  %14495 = and i8 %14494, 1
  store i8 %14495, i8* %27, align 1
  %14496 = icmp eq i32 %14480, 0
  %14497 = zext i1 %14496 to i8
  store i8 %14497, i8* %30, align 1
  %14498 = lshr i32 %14480, 31
  %14499 = trunc i32 %14498 to i8
  store i8 %14499, i8* %33, align 1
  %14500 = lshr i32 %14473, 31
  %14501 = lshr i32 %14479, 31
  %14502 = xor i32 %14498, %14500
  %14503 = xor i32 %14498, %14501
  %14504 = add nuw nsw i32 %14502, %14503
  %14505 = icmp eq i32 %14504, 2
  %14506 = zext i1 %14505 to i8
  store i8 %14506, i8* %39, align 1
  %14507 = add i64 %14413, 40
  store i64 %14507, i64* %3, align 8
  store i32 %14480, i32* %14478, align 4
  %14508 = load i64, i64* %RBP.i, align 8
  %14509 = add i64 %14508, -8
  %14510 = load i64, i64* %3, align 8
  %14511 = add i64 %14510, 4
  store i64 %14511, i64* %3, align 8
  %14512 = inttoptr i64 %14509 to i64*
  %14513 = load i64, i64* %14512, align 8
  %14514 = add i64 %14513, 51640
  store i64 %14514, i64* %RAX.i11582.pre-phi, align 8
  %14515 = icmp ugt i64 %14513, -51641
  %14516 = zext i1 %14515 to i8
  store i8 %14516, i8* %14, align 1
  %14517 = trunc i64 %14514 to i32
  %14518 = and i32 %14517, 255
  %14519 = tail call i32 @llvm.ctpop.i32(i32 %14518)
  %14520 = trunc i32 %14519 to i8
  %14521 = and i8 %14520, 1
  %14522 = xor i8 %14521, 1
  store i8 %14522, i8* %21, align 1
  %14523 = xor i64 %14513, 16
  %14524 = xor i64 %14523, %14514
  %14525 = lshr i64 %14524, 4
  %14526 = trunc i64 %14525 to i8
  %14527 = and i8 %14526, 1
  store i8 %14527, i8* %27, align 1
  %14528 = icmp eq i64 %14514, 0
  %14529 = zext i1 %14528 to i8
  store i8 %14529, i8* %30, align 1
  %14530 = lshr i64 %14514, 63
  %14531 = trunc i64 %14530 to i8
  store i8 %14531, i8* %33, align 1
  %14532 = lshr i64 %14513, 63
  %14533 = xor i64 %14530, %14532
  %14534 = add nuw nsw i64 %14533, %14530
  %14535 = icmp eq i64 %14534, 2
  %14536 = zext i1 %14535 to i8
  store i8 %14536, i8* %39, align 1
  %14537 = add i64 %14508, -150
  %14538 = add i64 %14510, 17
  store i64 %14538, i64* %3, align 8
  %14539 = inttoptr i64 %14537 to i16*
  %14540 = load i16, i16* %14539, align 2
  %14541 = zext i16 %14540 to i64
  store i64 %14541, i64* %RCX.i11580, align 8
  %14542 = zext i16 %14540 to i64
  %14543 = shl nuw nsw i64 %14542, 4
  store i64 %14543, i64* %573, align 8
  %14544 = add i64 %14543, %14514
  store i64 %14544, i64* %RAX.i11582.pre-phi, align 8
  %14545 = icmp ult i64 %14544, %14514
  %14546 = icmp ult i64 %14544, %14543
  %14547 = or i1 %14545, %14546
  %14548 = zext i1 %14547 to i8
  store i8 %14548, i8* %14, align 1
  %14549 = trunc i64 %14544 to i32
  %14550 = and i32 %14549, 255
  %14551 = tail call i32 @llvm.ctpop.i32(i32 %14550)
  %14552 = trunc i32 %14551 to i8
  %14553 = and i8 %14552, 1
  %14554 = xor i8 %14553, 1
  store i8 %14554, i8* %21, align 1
  %14555 = xor i64 %14543, %14514
  %14556 = xor i64 %14555, %14544
  %14557 = lshr i64 %14556, 4
  %14558 = trunc i64 %14557 to i8
  %14559 = and i8 %14558, 1
  store i8 %14559, i8* %27, align 1
  %14560 = icmp eq i64 %14544, 0
  %14561 = zext i1 %14560 to i8
  store i8 %14561, i8* %30, align 1
  %14562 = lshr i64 %14544, 63
  %14563 = trunc i64 %14562 to i8
  store i8 %14563, i8* %33, align 1
  %14564 = xor i64 %14562, %14530
  %14565 = add nuw nsw i64 %14564, %14562
  %14566 = icmp eq i64 %14565, 2
  %14567 = zext i1 %14566 to i8
  store i8 %14567, i8* %39, align 1
  %14568 = add i64 %14544, 4
  %14569 = add i64 %14510, 29
  store i64 %14569, i64* %3, align 8
  %14570 = inttoptr i64 %14568 to i32*
  %14571 = load i32, i32* %14570, align 4
  %14572 = zext i32 %14571 to i64
  store i64 %14572, i64* %RCX.i11580, align 8
  %14573 = load i64, i64* %RBP.i, align 8
  %14574 = add i64 %14573, -144
  %14575 = add i64 %14510, 35
  store i64 %14575, i64* %3, align 8
  %14576 = inttoptr i64 %14574 to i32*
  %14577 = load i32, i32* %14576, align 4
  %14578 = add i32 %14577, %14571
  %14579 = zext i32 %14578 to i64
  store i64 %14579, i64* %RCX.i11580, align 8
  %14580 = icmp ult i32 %14578, %14571
  %14581 = icmp ult i32 %14578, %14577
  %14582 = or i1 %14580, %14581
  %14583 = zext i1 %14582 to i8
  store i8 %14583, i8* %14, align 1
  %14584 = and i32 %14578, 255
  %14585 = tail call i32 @llvm.ctpop.i32(i32 %14584)
  %14586 = trunc i32 %14585 to i8
  %14587 = and i8 %14586, 1
  %14588 = xor i8 %14587, 1
  store i8 %14588, i8* %21, align 1
  %14589 = xor i32 %14577, %14571
  %14590 = xor i32 %14589, %14578
  %14591 = lshr i32 %14590, 4
  %14592 = trunc i32 %14591 to i8
  %14593 = and i8 %14592, 1
  store i8 %14593, i8* %27, align 1
  %14594 = icmp eq i32 %14578, 0
  %14595 = zext i1 %14594 to i8
  store i8 %14595, i8* %30, align 1
  %14596 = lshr i32 %14578, 31
  %14597 = trunc i32 %14596 to i8
  store i8 %14597, i8* %33, align 1
  %14598 = lshr i32 %14571, 31
  %14599 = lshr i32 %14577, 31
  %14600 = xor i32 %14596, %14598
  %14601 = xor i32 %14596, %14599
  %14602 = add nuw nsw i32 %14600, %14601
  %14603 = icmp eq i32 %14602, 2
  %14604 = zext i1 %14603 to i8
  store i8 %14604, i8* %39, align 1
  %14605 = add i64 %14510, 41
  store i64 %14605, i64* %3, align 8
  store i32 %14578, i32* %14576, align 4
  %14606 = load i64, i64* %RBP.i, align 8
  %14607 = add i64 %14606, -8
  %14608 = load i64, i64* %3, align 8
  %14609 = add i64 %14608, 4
  store i64 %14609, i64* %3, align 8
  %14610 = inttoptr i64 %14607 to i64*
  %14611 = load i64, i64* %14610, align 8
  %14612 = add i64 %14611, 51640
  store i64 %14612, i64* %RAX.i11582.pre-phi, align 8
  %14613 = icmp ugt i64 %14611, -51641
  %14614 = zext i1 %14613 to i8
  store i8 %14614, i8* %14, align 1
  %14615 = trunc i64 %14612 to i32
  %14616 = and i32 %14615, 255
  %14617 = tail call i32 @llvm.ctpop.i32(i32 %14616)
  %14618 = trunc i32 %14617 to i8
  %14619 = and i8 %14618, 1
  %14620 = xor i8 %14619, 1
  store i8 %14620, i8* %21, align 1
  %14621 = xor i64 %14611, 16
  %14622 = xor i64 %14621, %14612
  %14623 = lshr i64 %14622, 4
  %14624 = trunc i64 %14623 to i8
  %14625 = and i8 %14624, 1
  store i8 %14625, i8* %27, align 1
  %14626 = icmp eq i64 %14612, 0
  %14627 = zext i1 %14626 to i8
  store i8 %14627, i8* %30, align 1
  %14628 = lshr i64 %14612, 63
  %14629 = trunc i64 %14628 to i8
  store i8 %14629, i8* %33, align 1
  %14630 = lshr i64 %14611, 63
  %14631 = xor i64 %14628, %14630
  %14632 = add nuw nsw i64 %14631, %14628
  %14633 = icmp eq i64 %14632, 2
  %14634 = zext i1 %14633 to i8
  store i8 %14634, i8* %39, align 1
  %14635 = add i64 %14606, -150
  %14636 = add i64 %14608, 17
  store i64 %14636, i64* %3, align 8
  %14637 = inttoptr i64 %14635 to i16*
  %14638 = load i16, i16* %14637, align 2
  %14639 = zext i16 %14638 to i64
  store i64 %14639, i64* %RCX.i11580, align 8
  %14640 = zext i16 %14638 to i64
  %14641 = shl nuw nsw i64 %14640, 4
  store i64 %14641, i64* %573, align 8
  %14642 = add i64 %14641, %14612
  store i64 %14642, i64* %RAX.i11582.pre-phi, align 8
  %14643 = icmp ult i64 %14642, %14612
  %14644 = icmp ult i64 %14642, %14641
  %14645 = or i1 %14643, %14644
  %14646 = zext i1 %14645 to i8
  store i8 %14646, i8* %14, align 1
  %14647 = trunc i64 %14642 to i32
  %14648 = and i32 %14647, 255
  %14649 = tail call i32 @llvm.ctpop.i32(i32 %14648)
  %14650 = trunc i32 %14649 to i8
  %14651 = and i8 %14650, 1
  %14652 = xor i8 %14651, 1
  store i8 %14652, i8* %21, align 1
  %14653 = xor i64 %14641, %14612
  %14654 = xor i64 %14653, %14642
  %14655 = lshr i64 %14654, 4
  %14656 = trunc i64 %14655 to i8
  %14657 = and i8 %14656, 1
  store i8 %14657, i8* %27, align 1
  %14658 = icmp eq i64 %14642, 0
  %14659 = zext i1 %14658 to i8
  store i8 %14659, i8* %30, align 1
  %14660 = lshr i64 %14642, 63
  %14661 = trunc i64 %14660 to i8
  store i8 %14661, i8* %33, align 1
  %14662 = xor i64 %14660, %14628
  %14663 = add nuw nsw i64 %14662, %14660
  %14664 = icmp eq i64 %14663, 2
  %14665 = zext i1 %14664 to i8
  store i8 %14665, i8* %39, align 1
  %14666 = add i64 %14642, 8
  %14667 = add i64 %14608, 29
  store i64 %14667, i64* %3, align 8
  %14668 = inttoptr i64 %14666 to i32*
  %14669 = load i32, i32* %14668, align 4
  %14670 = zext i32 %14669 to i64
  store i64 %14670, i64* %RCX.i11580, align 8
  %14671 = load i64, i64* %RBP.i, align 8
  %14672 = add i64 %14671, -148
  %14673 = add i64 %14608, 35
  store i64 %14673, i64* %3, align 8
  %14674 = inttoptr i64 %14672 to i32*
  %14675 = load i32, i32* %14674, align 4
  %14676 = add i32 %14675, %14669
  %14677 = zext i32 %14676 to i64
  store i64 %14677, i64* %RCX.i11580, align 8
  %14678 = icmp ult i32 %14676, %14669
  %14679 = icmp ult i32 %14676, %14675
  %14680 = or i1 %14678, %14679
  %14681 = zext i1 %14680 to i8
  store i8 %14681, i8* %14, align 1
  %14682 = and i32 %14676, 255
  %14683 = tail call i32 @llvm.ctpop.i32(i32 %14682)
  %14684 = trunc i32 %14683 to i8
  %14685 = and i8 %14684, 1
  %14686 = xor i8 %14685, 1
  store i8 %14686, i8* %21, align 1
  %14687 = xor i32 %14675, %14669
  %14688 = xor i32 %14687, %14676
  %14689 = lshr i32 %14688, 4
  %14690 = trunc i32 %14689 to i8
  %14691 = and i8 %14690, 1
  store i8 %14691, i8* %27, align 1
  %14692 = icmp eq i32 %14676, 0
  %14693 = zext i1 %14692 to i8
  store i8 %14693, i8* %30, align 1
  %14694 = lshr i32 %14676, 31
  %14695 = trunc i32 %14694 to i8
  store i8 %14695, i8* %33, align 1
  %14696 = lshr i32 %14669, 31
  %14697 = lshr i32 %14675, 31
  %14698 = xor i32 %14694, %14696
  %14699 = xor i32 %14694, %14697
  %14700 = add nuw nsw i32 %14698, %14699
  %14701 = icmp eq i32 %14700, 2
  %14702 = zext i1 %14701 to i8
  store i8 %14702, i8* %39, align 1
  %14703 = add i64 %14608, 41
  store i64 %14703, i64* %3, align 8
  store i32 %14676, i32* %14674, align 4
  %14704 = load i64, i64* %RBP.i, align 8
  %14705 = add i64 %14704, -120
  %14706 = load i64, i64* %3, align 8
  %14707 = add i64 %14706, 4
  store i64 %14707, i64* %3, align 8
  %14708 = inttoptr i64 %14705 to i64*
  %14709 = load i64, i64* %14708, align 8
  store i64 %14709, i64* %RAX.i11582.pre-phi, align 8
  %14710 = add i64 %14704, -28
  %14711 = add i64 %14706, 7
  store i64 %14711, i64* %3, align 8
  %14712 = inttoptr i64 %14710 to i32*
  %14713 = load i32, i32* %14712, align 4
  %14714 = add i32 %14713, 36
  %14715 = zext i32 %14714 to i64
  store i64 %14715, i64* %RCX.i11580, align 8
  %14716 = icmp ugt i32 %14713, -37
  %14717 = zext i1 %14716 to i8
  store i8 %14717, i8* %14, align 1
  %14718 = and i32 %14714, 255
  %14719 = tail call i32 @llvm.ctpop.i32(i32 %14718)
  %14720 = trunc i32 %14719 to i8
  %14721 = and i8 %14720, 1
  %14722 = xor i8 %14721, 1
  store i8 %14722, i8* %21, align 1
  %14723 = xor i32 %14714, %14713
  %14724 = lshr i32 %14723, 4
  %14725 = trunc i32 %14724 to i8
  %14726 = and i8 %14725, 1
  store i8 %14726, i8* %27, align 1
  %14727 = icmp eq i32 %14714, 0
  %14728 = zext i1 %14727 to i8
  store i8 %14728, i8* %30, align 1
  %14729 = lshr i32 %14714, 31
  %14730 = trunc i32 %14729 to i8
  store i8 %14730, i8* %33, align 1
  %14731 = lshr i32 %14713, 31
  %14732 = xor i32 %14729, %14731
  %14733 = add nuw nsw i32 %14732, %14729
  %14734 = icmp eq i32 %14733, 2
  %14735 = zext i1 %14734 to i8
  store i8 %14735, i8* %39, align 1
  %14736 = sext i32 %14714 to i64
  store i64 %14736, i64* %573, align 8
  %14737 = shl nsw i64 %14736, 1
  %14738 = add i64 %14709, %14737
  %14739 = add i64 %14706, 17
  store i64 %14739, i64* %3, align 8
  %14740 = inttoptr i64 %14738 to i16*
  %14741 = load i16, i16* %14740, align 2
  store i16 %14741, i16* %SI.i, align 2
  %14742 = add i64 %14704, -150
  %14743 = add i64 %14706, 24
  store i64 %14743, i64* %3, align 8
  %14744 = inttoptr i64 %14742 to i16*
  store i16 %14741, i16* %14744, align 2
  %14745 = load i64, i64* %RBP.i, align 8
  %14746 = add i64 %14745, -8
  %14747 = load i64, i64* %3, align 8
  %14748 = add i64 %14747, 4
  store i64 %14748, i64* %3, align 8
  %14749 = inttoptr i64 %14746 to i64*
  %14750 = load i64, i64* %14749, align 8
  %14751 = add i64 %14750, 51640
  store i64 %14751, i64* %RAX.i11582.pre-phi, align 8
  %14752 = icmp ugt i64 %14750, -51641
  %14753 = zext i1 %14752 to i8
  store i8 %14753, i8* %14, align 1
  %14754 = trunc i64 %14751 to i32
  %14755 = and i32 %14754, 255
  %14756 = tail call i32 @llvm.ctpop.i32(i32 %14755)
  %14757 = trunc i32 %14756 to i8
  %14758 = and i8 %14757, 1
  %14759 = xor i8 %14758, 1
  store i8 %14759, i8* %21, align 1
  %14760 = xor i64 %14750, 16
  %14761 = xor i64 %14760, %14751
  %14762 = lshr i64 %14761, 4
  %14763 = trunc i64 %14762 to i8
  %14764 = and i8 %14763, 1
  store i8 %14764, i8* %27, align 1
  %14765 = icmp eq i64 %14751, 0
  %14766 = zext i1 %14765 to i8
  store i8 %14766, i8* %30, align 1
  %14767 = lshr i64 %14751, 63
  %14768 = trunc i64 %14767 to i8
  store i8 %14768, i8* %33, align 1
  %14769 = lshr i64 %14750, 63
  %14770 = xor i64 %14767, %14769
  %14771 = add nuw nsw i64 %14770, %14767
  %14772 = icmp eq i64 %14771, 2
  %14773 = zext i1 %14772 to i8
  store i8 %14773, i8* %39, align 1
  %14774 = add i64 %14745, -150
  %14775 = add i64 %14747, 17
  store i64 %14775, i64* %3, align 8
  %14776 = inttoptr i64 %14774 to i16*
  %14777 = load i16, i16* %14776, align 2
  %14778 = zext i16 %14777 to i64
  store i64 %14778, i64* %RCX.i11580, align 8
  %14779 = zext i16 %14777 to i64
  %14780 = shl nuw nsw i64 %14779, 4
  store i64 %14780, i64* %573, align 8
  %14781 = add i64 %14780, %14751
  store i64 %14781, i64* %RAX.i11582.pre-phi, align 8
  %14782 = icmp ult i64 %14781, %14751
  %14783 = icmp ult i64 %14781, %14780
  %14784 = or i1 %14782, %14783
  %14785 = zext i1 %14784 to i8
  store i8 %14785, i8* %14, align 1
  %14786 = trunc i64 %14781 to i32
  %14787 = and i32 %14786, 255
  %14788 = tail call i32 @llvm.ctpop.i32(i32 %14787)
  %14789 = trunc i32 %14788 to i8
  %14790 = and i8 %14789, 1
  %14791 = xor i8 %14790, 1
  store i8 %14791, i8* %21, align 1
  %14792 = xor i64 %14780, %14751
  %14793 = xor i64 %14792, %14781
  %14794 = lshr i64 %14793, 4
  %14795 = trunc i64 %14794 to i8
  %14796 = and i8 %14795, 1
  store i8 %14796, i8* %27, align 1
  %14797 = icmp eq i64 %14781, 0
  %14798 = zext i1 %14797 to i8
  store i8 %14798, i8* %30, align 1
  %14799 = lshr i64 %14781, 63
  %14800 = trunc i64 %14799 to i8
  store i8 %14800, i8* %33, align 1
  %14801 = xor i64 %14799, %14767
  %14802 = add nuw nsw i64 %14801, %14799
  %14803 = icmp eq i64 %14802, 2
  %14804 = zext i1 %14803 to i8
  store i8 %14804, i8* %39, align 1
  %14805 = inttoptr i64 %14781 to i32*
  %14806 = add i64 %14747, 28
  store i64 %14806, i64* %3, align 8
  %14807 = load i32, i32* %14805, align 4
  %14808 = zext i32 %14807 to i64
  store i64 %14808, i64* %RCX.i11580, align 8
  %14809 = load i64, i64* %RBP.i, align 8
  %14810 = add i64 %14809, -140
  %14811 = add i64 %14747, 34
  store i64 %14811, i64* %3, align 8
  %14812 = inttoptr i64 %14810 to i32*
  %14813 = load i32, i32* %14812, align 4
  %14814 = add i32 %14813, %14807
  %14815 = zext i32 %14814 to i64
  store i64 %14815, i64* %RCX.i11580, align 8
  %14816 = icmp ult i32 %14814, %14807
  %14817 = icmp ult i32 %14814, %14813
  %14818 = or i1 %14816, %14817
  %14819 = zext i1 %14818 to i8
  store i8 %14819, i8* %14, align 1
  %14820 = and i32 %14814, 255
  %14821 = tail call i32 @llvm.ctpop.i32(i32 %14820)
  %14822 = trunc i32 %14821 to i8
  %14823 = and i8 %14822, 1
  %14824 = xor i8 %14823, 1
  store i8 %14824, i8* %21, align 1
  %14825 = xor i32 %14813, %14807
  %14826 = xor i32 %14825, %14814
  %14827 = lshr i32 %14826, 4
  %14828 = trunc i32 %14827 to i8
  %14829 = and i8 %14828, 1
  store i8 %14829, i8* %27, align 1
  %14830 = icmp eq i32 %14814, 0
  %14831 = zext i1 %14830 to i8
  store i8 %14831, i8* %30, align 1
  %14832 = lshr i32 %14814, 31
  %14833 = trunc i32 %14832 to i8
  store i8 %14833, i8* %33, align 1
  %14834 = lshr i32 %14807, 31
  %14835 = lshr i32 %14813, 31
  %14836 = xor i32 %14832, %14834
  %14837 = xor i32 %14832, %14835
  %14838 = add nuw nsw i32 %14836, %14837
  %14839 = icmp eq i32 %14838, 2
  %14840 = zext i1 %14839 to i8
  store i8 %14840, i8* %39, align 1
  %14841 = add i64 %14747, 40
  store i64 %14841, i64* %3, align 8
  store i32 %14814, i32* %14812, align 4
  %14842 = load i64, i64* %RBP.i, align 8
  %14843 = add i64 %14842, -8
  %14844 = load i64, i64* %3, align 8
  %14845 = add i64 %14844, 4
  store i64 %14845, i64* %3, align 8
  %14846 = inttoptr i64 %14843 to i64*
  %14847 = load i64, i64* %14846, align 8
  %14848 = add i64 %14847, 51640
  store i64 %14848, i64* %RAX.i11582.pre-phi, align 8
  %14849 = icmp ugt i64 %14847, -51641
  %14850 = zext i1 %14849 to i8
  store i8 %14850, i8* %14, align 1
  %14851 = trunc i64 %14848 to i32
  %14852 = and i32 %14851, 255
  %14853 = tail call i32 @llvm.ctpop.i32(i32 %14852)
  %14854 = trunc i32 %14853 to i8
  %14855 = and i8 %14854, 1
  %14856 = xor i8 %14855, 1
  store i8 %14856, i8* %21, align 1
  %14857 = xor i64 %14847, 16
  %14858 = xor i64 %14857, %14848
  %14859 = lshr i64 %14858, 4
  %14860 = trunc i64 %14859 to i8
  %14861 = and i8 %14860, 1
  store i8 %14861, i8* %27, align 1
  %14862 = icmp eq i64 %14848, 0
  %14863 = zext i1 %14862 to i8
  store i8 %14863, i8* %30, align 1
  %14864 = lshr i64 %14848, 63
  %14865 = trunc i64 %14864 to i8
  store i8 %14865, i8* %33, align 1
  %14866 = lshr i64 %14847, 63
  %14867 = xor i64 %14864, %14866
  %14868 = add nuw nsw i64 %14867, %14864
  %14869 = icmp eq i64 %14868, 2
  %14870 = zext i1 %14869 to i8
  store i8 %14870, i8* %39, align 1
  %14871 = add i64 %14842, -150
  %14872 = add i64 %14844, 17
  store i64 %14872, i64* %3, align 8
  %14873 = inttoptr i64 %14871 to i16*
  %14874 = load i16, i16* %14873, align 2
  %14875 = zext i16 %14874 to i64
  store i64 %14875, i64* %RCX.i11580, align 8
  %14876 = zext i16 %14874 to i64
  %14877 = shl nuw nsw i64 %14876, 4
  store i64 %14877, i64* %573, align 8
  %14878 = add i64 %14877, %14848
  store i64 %14878, i64* %RAX.i11582.pre-phi, align 8
  %14879 = icmp ult i64 %14878, %14848
  %14880 = icmp ult i64 %14878, %14877
  %14881 = or i1 %14879, %14880
  %14882 = zext i1 %14881 to i8
  store i8 %14882, i8* %14, align 1
  %14883 = trunc i64 %14878 to i32
  %14884 = and i32 %14883, 255
  %14885 = tail call i32 @llvm.ctpop.i32(i32 %14884)
  %14886 = trunc i32 %14885 to i8
  %14887 = and i8 %14886, 1
  %14888 = xor i8 %14887, 1
  store i8 %14888, i8* %21, align 1
  %14889 = xor i64 %14877, %14848
  %14890 = xor i64 %14889, %14878
  %14891 = lshr i64 %14890, 4
  %14892 = trunc i64 %14891 to i8
  %14893 = and i8 %14892, 1
  store i8 %14893, i8* %27, align 1
  %14894 = icmp eq i64 %14878, 0
  %14895 = zext i1 %14894 to i8
  store i8 %14895, i8* %30, align 1
  %14896 = lshr i64 %14878, 63
  %14897 = trunc i64 %14896 to i8
  store i8 %14897, i8* %33, align 1
  %14898 = xor i64 %14896, %14864
  %14899 = add nuw nsw i64 %14898, %14896
  %14900 = icmp eq i64 %14899, 2
  %14901 = zext i1 %14900 to i8
  store i8 %14901, i8* %39, align 1
  %14902 = add i64 %14878, 4
  %14903 = add i64 %14844, 29
  store i64 %14903, i64* %3, align 8
  %14904 = inttoptr i64 %14902 to i32*
  %14905 = load i32, i32* %14904, align 4
  %14906 = zext i32 %14905 to i64
  store i64 %14906, i64* %RCX.i11580, align 8
  %14907 = load i64, i64* %RBP.i, align 8
  %14908 = add i64 %14907, -144
  %14909 = add i64 %14844, 35
  store i64 %14909, i64* %3, align 8
  %14910 = inttoptr i64 %14908 to i32*
  %14911 = load i32, i32* %14910, align 4
  %14912 = add i32 %14911, %14905
  %14913 = zext i32 %14912 to i64
  store i64 %14913, i64* %RCX.i11580, align 8
  %14914 = icmp ult i32 %14912, %14905
  %14915 = icmp ult i32 %14912, %14911
  %14916 = or i1 %14914, %14915
  %14917 = zext i1 %14916 to i8
  store i8 %14917, i8* %14, align 1
  %14918 = and i32 %14912, 255
  %14919 = tail call i32 @llvm.ctpop.i32(i32 %14918)
  %14920 = trunc i32 %14919 to i8
  %14921 = and i8 %14920, 1
  %14922 = xor i8 %14921, 1
  store i8 %14922, i8* %21, align 1
  %14923 = xor i32 %14911, %14905
  %14924 = xor i32 %14923, %14912
  %14925 = lshr i32 %14924, 4
  %14926 = trunc i32 %14925 to i8
  %14927 = and i8 %14926, 1
  store i8 %14927, i8* %27, align 1
  %14928 = icmp eq i32 %14912, 0
  %14929 = zext i1 %14928 to i8
  store i8 %14929, i8* %30, align 1
  %14930 = lshr i32 %14912, 31
  %14931 = trunc i32 %14930 to i8
  store i8 %14931, i8* %33, align 1
  %14932 = lshr i32 %14905, 31
  %14933 = lshr i32 %14911, 31
  %14934 = xor i32 %14930, %14932
  %14935 = xor i32 %14930, %14933
  %14936 = add nuw nsw i32 %14934, %14935
  %14937 = icmp eq i32 %14936, 2
  %14938 = zext i1 %14937 to i8
  store i8 %14938, i8* %39, align 1
  %14939 = add i64 %14844, 41
  store i64 %14939, i64* %3, align 8
  store i32 %14912, i32* %14910, align 4
  %14940 = load i64, i64* %RBP.i, align 8
  %14941 = add i64 %14940, -8
  %14942 = load i64, i64* %3, align 8
  %14943 = add i64 %14942, 4
  store i64 %14943, i64* %3, align 8
  %14944 = inttoptr i64 %14941 to i64*
  %14945 = load i64, i64* %14944, align 8
  %14946 = add i64 %14945, 51640
  store i64 %14946, i64* %RAX.i11582.pre-phi, align 8
  %14947 = icmp ugt i64 %14945, -51641
  %14948 = zext i1 %14947 to i8
  store i8 %14948, i8* %14, align 1
  %14949 = trunc i64 %14946 to i32
  %14950 = and i32 %14949, 255
  %14951 = tail call i32 @llvm.ctpop.i32(i32 %14950)
  %14952 = trunc i32 %14951 to i8
  %14953 = and i8 %14952, 1
  %14954 = xor i8 %14953, 1
  store i8 %14954, i8* %21, align 1
  %14955 = xor i64 %14945, 16
  %14956 = xor i64 %14955, %14946
  %14957 = lshr i64 %14956, 4
  %14958 = trunc i64 %14957 to i8
  %14959 = and i8 %14958, 1
  store i8 %14959, i8* %27, align 1
  %14960 = icmp eq i64 %14946, 0
  %14961 = zext i1 %14960 to i8
  store i8 %14961, i8* %30, align 1
  %14962 = lshr i64 %14946, 63
  %14963 = trunc i64 %14962 to i8
  store i8 %14963, i8* %33, align 1
  %14964 = lshr i64 %14945, 63
  %14965 = xor i64 %14962, %14964
  %14966 = add nuw nsw i64 %14965, %14962
  %14967 = icmp eq i64 %14966, 2
  %14968 = zext i1 %14967 to i8
  store i8 %14968, i8* %39, align 1
  %14969 = add i64 %14940, -150
  %14970 = add i64 %14942, 17
  store i64 %14970, i64* %3, align 8
  %14971 = inttoptr i64 %14969 to i16*
  %14972 = load i16, i16* %14971, align 2
  %14973 = zext i16 %14972 to i64
  store i64 %14973, i64* %RCX.i11580, align 8
  %14974 = zext i16 %14972 to i64
  %14975 = shl nuw nsw i64 %14974, 4
  store i64 %14975, i64* %573, align 8
  %14976 = add i64 %14975, %14946
  store i64 %14976, i64* %RAX.i11582.pre-phi, align 8
  %14977 = icmp ult i64 %14976, %14946
  %14978 = icmp ult i64 %14976, %14975
  %14979 = or i1 %14977, %14978
  %14980 = zext i1 %14979 to i8
  store i8 %14980, i8* %14, align 1
  %14981 = trunc i64 %14976 to i32
  %14982 = and i32 %14981, 255
  %14983 = tail call i32 @llvm.ctpop.i32(i32 %14982)
  %14984 = trunc i32 %14983 to i8
  %14985 = and i8 %14984, 1
  %14986 = xor i8 %14985, 1
  store i8 %14986, i8* %21, align 1
  %14987 = xor i64 %14975, %14946
  %14988 = xor i64 %14987, %14976
  %14989 = lshr i64 %14988, 4
  %14990 = trunc i64 %14989 to i8
  %14991 = and i8 %14990, 1
  store i8 %14991, i8* %27, align 1
  %14992 = icmp eq i64 %14976, 0
  %14993 = zext i1 %14992 to i8
  store i8 %14993, i8* %30, align 1
  %14994 = lshr i64 %14976, 63
  %14995 = trunc i64 %14994 to i8
  store i8 %14995, i8* %33, align 1
  %14996 = xor i64 %14994, %14962
  %14997 = add nuw nsw i64 %14996, %14994
  %14998 = icmp eq i64 %14997, 2
  %14999 = zext i1 %14998 to i8
  store i8 %14999, i8* %39, align 1
  %15000 = add i64 %14976, 8
  %15001 = add i64 %14942, 29
  store i64 %15001, i64* %3, align 8
  %15002 = inttoptr i64 %15000 to i32*
  %15003 = load i32, i32* %15002, align 4
  %15004 = zext i32 %15003 to i64
  store i64 %15004, i64* %RCX.i11580, align 8
  %15005 = load i64, i64* %RBP.i, align 8
  %15006 = add i64 %15005, -148
  %15007 = add i64 %14942, 35
  store i64 %15007, i64* %3, align 8
  %15008 = inttoptr i64 %15006 to i32*
  %15009 = load i32, i32* %15008, align 4
  %15010 = add i32 %15009, %15003
  %15011 = zext i32 %15010 to i64
  store i64 %15011, i64* %RCX.i11580, align 8
  %15012 = icmp ult i32 %15010, %15003
  %15013 = icmp ult i32 %15010, %15009
  %15014 = or i1 %15012, %15013
  %15015 = zext i1 %15014 to i8
  store i8 %15015, i8* %14, align 1
  %15016 = and i32 %15010, 255
  %15017 = tail call i32 @llvm.ctpop.i32(i32 %15016)
  %15018 = trunc i32 %15017 to i8
  %15019 = and i8 %15018, 1
  %15020 = xor i8 %15019, 1
  store i8 %15020, i8* %21, align 1
  %15021 = xor i32 %15009, %15003
  %15022 = xor i32 %15021, %15010
  %15023 = lshr i32 %15022, 4
  %15024 = trunc i32 %15023 to i8
  %15025 = and i8 %15024, 1
  store i8 %15025, i8* %27, align 1
  %15026 = icmp eq i32 %15010, 0
  %15027 = zext i1 %15026 to i8
  store i8 %15027, i8* %30, align 1
  %15028 = lshr i32 %15010, 31
  %15029 = trunc i32 %15028 to i8
  store i8 %15029, i8* %33, align 1
  %15030 = lshr i32 %15003, 31
  %15031 = lshr i32 %15009, 31
  %15032 = xor i32 %15028, %15030
  %15033 = xor i32 %15028, %15031
  %15034 = add nuw nsw i32 %15032, %15033
  %15035 = icmp eq i32 %15034, 2
  %15036 = zext i1 %15035 to i8
  store i8 %15036, i8* %39, align 1
  %15037 = add i64 %14942, 41
  store i64 %15037, i64* %3, align 8
  store i32 %15010, i32* %15008, align 4
  %15038 = load i64, i64* %RBP.i, align 8
  %15039 = add i64 %15038, -120
  %15040 = load i64, i64* %3, align 8
  %15041 = add i64 %15040, 4
  store i64 %15041, i64* %3, align 8
  %15042 = inttoptr i64 %15039 to i64*
  %15043 = load i64, i64* %15042, align 8
  store i64 %15043, i64* %RAX.i11582.pre-phi, align 8
  %15044 = add i64 %15038, -28
  %15045 = add i64 %15040, 7
  store i64 %15045, i64* %3, align 8
  %15046 = inttoptr i64 %15044 to i32*
  %15047 = load i32, i32* %15046, align 4
  %15048 = add i32 %15047, 37
  %15049 = zext i32 %15048 to i64
  store i64 %15049, i64* %RCX.i11580, align 8
  %15050 = icmp ugt i32 %15047, -38
  %15051 = zext i1 %15050 to i8
  store i8 %15051, i8* %14, align 1
  %15052 = and i32 %15048, 255
  %15053 = tail call i32 @llvm.ctpop.i32(i32 %15052)
  %15054 = trunc i32 %15053 to i8
  %15055 = and i8 %15054, 1
  %15056 = xor i8 %15055, 1
  store i8 %15056, i8* %21, align 1
  %15057 = xor i32 %15048, %15047
  %15058 = lshr i32 %15057, 4
  %15059 = trunc i32 %15058 to i8
  %15060 = and i8 %15059, 1
  store i8 %15060, i8* %27, align 1
  %15061 = icmp eq i32 %15048, 0
  %15062 = zext i1 %15061 to i8
  store i8 %15062, i8* %30, align 1
  %15063 = lshr i32 %15048, 31
  %15064 = trunc i32 %15063 to i8
  store i8 %15064, i8* %33, align 1
  %15065 = lshr i32 %15047, 31
  %15066 = xor i32 %15063, %15065
  %15067 = add nuw nsw i32 %15066, %15063
  %15068 = icmp eq i32 %15067, 2
  %15069 = zext i1 %15068 to i8
  store i8 %15069, i8* %39, align 1
  %15070 = sext i32 %15048 to i64
  store i64 %15070, i64* %573, align 8
  %15071 = shl nsw i64 %15070, 1
  %15072 = add i64 %15043, %15071
  %15073 = add i64 %15040, 17
  store i64 %15073, i64* %3, align 8
  %15074 = inttoptr i64 %15072 to i16*
  %15075 = load i16, i16* %15074, align 2
  store i16 %15075, i16* %SI.i, align 2
  %15076 = add i64 %15038, -150
  %15077 = add i64 %15040, 24
  store i64 %15077, i64* %3, align 8
  %15078 = inttoptr i64 %15076 to i16*
  store i16 %15075, i16* %15078, align 2
  %15079 = load i64, i64* %RBP.i, align 8
  %15080 = add i64 %15079, -8
  %15081 = load i64, i64* %3, align 8
  %15082 = add i64 %15081, 4
  store i64 %15082, i64* %3, align 8
  %15083 = inttoptr i64 %15080 to i64*
  %15084 = load i64, i64* %15083, align 8
  %15085 = add i64 %15084, 51640
  store i64 %15085, i64* %RAX.i11582.pre-phi, align 8
  %15086 = icmp ugt i64 %15084, -51641
  %15087 = zext i1 %15086 to i8
  store i8 %15087, i8* %14, align 1
  %15088 = trunc i64 %15085 to i32
  %15089 = and i32 %15088, 255
  %15090 = tail call i32 @llvm.ctpop.i32(i32 %15089)
  %15091 = trunc i32 %15090 to i8
  %15092 = and i8 %15091, 1
  %15093 = xor i8 %15092, 1
  store i8 %15093, i8* %21, align 1
  %15094 = xor i64 %15084, 16
  %15095 = xor i64 %15094, %15085
  %15096 = lshr i64 %15095, 4
  %15097 = trunc i64 %15096 to i8
  %15098 = and i8 %15097, 1
  store i8 %15098, i8* %27, align 1
  %15099 = icmp eq i64 %15085, 0
  %15100 = zext i1 %15099 to i8
  store i8 %15100, i8* %30, align 1
  %15101 = lshr i64 %15085, 63
  %15102 = trunc i64 %15101 to i8
  store i8 %15102, i8* %33, align 1
  %15103 = lshr i64 %15084, 63
  %15104 = xor i64 %15101, %15103
  %15105 = add nuw nsw i64 %15104, %15101
  %15106 = icmp eq i64 %15105, 2
  %15107 = zext i1 %15106 to i8
  store i8 %15107, i8* %39, align 1
  %15108 = add i64 %15079, -150
  %15109 = add i64 %15081, 17
  store i64 %15109, i64* %3, align 8
  %15110 = inttoptr i64 %15108 to i16*
  %15111 = load i16, i16* %15110, align 2
  %15112 = zext i16 %15111 to i64
  store i64 %15112, i64* %RCX.i11580, align 8
  %15113 = zext i16 %15111 to i64
  %15114 = shl nuw nsw i64 %15113, 4
  store i64 %15114, i64* %573, align 8
  %15115 = add i64 %15114, %15085
  store i64 %15115, i64* %RAX.i11582.pre-phi, align 8
  %15116 = icmp ult i64 %15115, %15085
  %15117 = icmp ult i64 %15115, %15114
  %15118 = or i1 %15116, %15117
  %15119 = zext i1 %15118 to i8
  store i8 %15119, i8* %14, align 1
  %15120 = trunc i64 %15115 to i32
  %15121 = and i32 %15120, 255
  %15122 = tail call i32 @llvm.ctpop.i32(i32 %15121)
  %15123 = trunc i32 %15122 to i8
  %15124 = and i8 %15123, 1
  %15125 = xor i8 %15124, 1
  store i8 %15125, i8* %21, align 1
  %15126 = xor i64 %15114, %15085
  %15127 = xor i64 %15126, %15115
  %15128 = lshr i64 %15127, 4
  %15129 = trunc i64 %15128 to i8
  %15130 = and i8 %15129, 1
  store i8 %15130, i8* %27, align 1
  %15131 = icmp eq i64 %15115, 0
  %15132 = zext i1 %15131 to i8
  store i8 %15132, i8* %30, align 1
  %15133 = lshr i64 %15115, 63
  %15134 = trunc i64 %15133 to i8
  store i8 %15134, i8* %33, align 1
  %15135 = xor i64 %15133, %15101
  %15136 = add nuw nsw i64 %15135, %15133
  %15137 = icmp eq i64 %15136, 2
  %15138 = zext i1 %15137 to i8
  store i8 %15138, i8* %39, align 1
  %15139 = inttoptr i64 %15115 to i32*
  %15140 = add i64 %15081, 28
  store i64 %15140, i64* %3, align 8
  %15141 = load i32, i32* %15139, align 4
  %15142 = zext i32 %15141 to i64
  store i64 %15142, i64* %RCX.i11580, align 8
  %15143 = load i64, i64* %RBP.i, align 8
  %15144 = add i64 %15143, -140
  %15145 = add i64 %15081, 34
  store i64 %15145, i64* %3, align 8
  %15146 = inttoptr i64 %15144 to i32*
  %15147 = load i32, i32* %15146, align 4
  %15148 = add i32 %15147, %15141
  %15149 = zext i32 %15148 to i64
  store i64 %15149, i64* %RCX.i11580, align 8
  %15150 = icmp ult i32 %15148, %15141
  %15151 = icmp ult i32 %15148, %15147
  %15152 = or i1 %15150, %15151
  %15153 = zext i1 %15152 to i8
  store i8 %15153, i8* %14, align 1
  %15154 = and i32 %15148, 255
  %15155 = tail call i32 @llvm.ctpop.i32(i32 %15154)
  %15156 = trunc i32 %15155 to i8
  %15157 = and i8 %15156, 1
  %15158 = xor i8 %15157, 1
  store i8 %15158, i8* %21, align 1
  %15159 = xor i32 %15147, %15141
  %15160 = xor i32 %15159, %15148
  %15161 = lshr i32 %15160, 4
  %15162 = trunc i32 %15161 to i8
  %15163 = and i8 %15162, 1
  store i8 %15163, i8* %27, align 1
  %15164 = icmp eq i32 %15148, 0
  %15165 = zext i1 %15164 to i8
  store i8 %15165, i8* %30, align 1
  %15166 = lshr i32 %15148, 31
  %15167 = trunc i32 %15166 to i8
  store i8 %15167, i8* %33, align 1
  %15168 = lshr i32 %15141, 31
  %15169 = lshr i32 %15147, 31
  %15170 = xor i32 %15166, %15168
  %15171 = xor i32 %15166, %15169
  %15172 = add nuw nsw i32 %15170, %15171
  %15173 = icmp eq i32 %15172, 2
  %15174 = zext i1 %15173 to i8
  store i8 %15174, i8* %39, align 1
  %15175 = add i64 %15081, 40
  store i64 %15175, i64* %3, align 8
  store i32 %15148, i32* %15146, align 4
  %15176 = load i64, i64* %RBP.i, align 8
  %15177 = add i64 %15176, -8
  %15178 = load i64, i64* %3, align 8
  %15179 = add i64 %15178, 4
  store i64 %15179, i64* %3, align 8
  %15180 = inttoptr i64 %15177 to i64*
  %15181 = load i64, i64* %15180, align 8
  %15182 = add i64 %15181, 51640
  store i64 %15182, i64* %RAX.i11582.pre-phi, align 8
  %15183 = icmp ugt i64 %15181, -51641
  %15184 = zext i1 %15183 to i8
  store i8 %15184, i8* %14, align 1
  %15185 = trunc i64 %15182 to i32
  %15186 = and i32 %15185, 255
  %15187 = tail call i32 @llvm.ctpop.i32(i32 %15186)
  %15188 = trunc i32 %15187 to i8
  %15189 = and i8 %15188, 1
  %15190 = xor i8 %15189, 1
  store i8 %15190, i8* %21, align 1
  %15191 = xor i64 %15181, 16
  %15192 = xor i64 %15191, %15182
  %15193 = lshr i64 %15192, 4
  %15194 = trunc i64 %15193 to i8
  %15195 = and i8 %15194, 1
  store i8 %15195, i8* %27, align 1
  %15196 = icmp eq i64 %15182, 0
  %15197 = zext i1 %15196 to i8
  store i8 %15197, i8* %30, align 1
  %15198 = lshr i64 %15182, 63
  %15199 = trunc i64 %15198 to i8
  store i8 %15199, i8* %33, align 1
  %15200 = lshr i64 %15181, 63
  %15201 = xor i64 %15198, %15200
  %15202 = add nuw nsw i64 %15201, %15198
  %15203 = icmp eq i64 %15202, 2
  %15204 = zext i1 %15203 to i8
  store i8 %15204, i8* %39, align 1
  %15205 = add i64 %15176, -150
  %15206 = add i64 %15178, 17
  store i64 %15206, i64* %3, align 8
  %15207 = inttoptr i64 %15205 to i16*
  %15208 = load i16, i16* %15207, align 2
  %15209 = zext i16 %15208 to i64
  store i64 %15209, i64* %RCX.i11580, align 8
  %15210 = zext i16 %15208 to i64
  %15211 = shl nuw nsw i64 %15210, 4
  store i64 %15211, i64* %573, align 8
  %15212 = add i64 %15211, %15182
  store i64 %15212, i64* %RAX.i11582.pre-phi, align 8
  %15213 = icmp ult i64 %15212, %15182
  %15214 = icmp ult i64 %15212, %15211
  %15215 = or i1 %15213, %15214
  %15216 = zext i1 %15215 to i8
  store i8 %15216, i8* %14, align 1
  %15217 = trunc i64 %15212 to i32
  %15218 = and i32 %15217, 255
  %15219 = tail call i32 @llvm.ctpop.i32(i32 %15218)
  %15220 = trunc i32 %15219 to i8
  %15221 = and i8 %15220, 1
  %15222 = xor i8 %15221, 1
  store i8 %15222, i8* %21, align 1
  %15223 = xor i64 %15211, %15182
  %15224 = xor i64 %15223, %15212
  %15225 = lshr i64 %15224, 4
  %15226 = trunc i64 %15225 to i8
  %15227 = and i8 %15226, 1
  store i8 %15227, i8* %27, align 1
  %15228 = icmp eq i64 %15212, 0
  %15229 = zext i1 %15228 to i8
  store i8 %15229, i8* %30, align 1
  %15230 = lshr i64 %15212, 63
  %15231 = trunc i64 %15230 to i8
  store i8 %15231, i8* %33, align 1
  %15232 = xor i64 %15230, %15198
  %15233 = add nuw nsw i64 %15232, %15230
  %15234 = icmp eq i64 %15233, 2
  %15235 = zext i1 %15234 to i8
  store i8 %15235, i8* %39, align 1
  %15236 = add i64 %15212, 4
  %15237 = add i64 %15178, 29
  store i64 %15237, i64* %3, align 8
  %15238 = inttoptr i64 %15236 to i32*
  %15239 = load i32, i32* %15238, align 4
  %15240 = zext i32 %15239 to i64
  store i64 %15240, i64* %RCX.i11580, align 8
  %15241 = load i64, i64* %RBP.i, align 8
  %15242 = add i64 %15241, -144
  %15243 = add i64 %15178, 35
  store i64 %15243, i64* %3, align 8
  %15244 = inttoptr i64 %15242 to i32*
  %15245 = load i32, i32* %15244, align 4
  %15246 = add i32 %15245, %15239
  %15247 = zext i32 %15246 to i64
  store i64 %15247, i64* %RCX.i11580, align 8
  %15248 = icmp ult i32 %15246, %15239
  %15249 = icmp ult i32 %15246, %15245
  %15250 = or i1 %15248, %15249
  %15251 = zext i1 %15250 to i8
  store i8 %15251, i8* %14, align 1
  %15252 = and i32 %15246, 255
  %15253 = tail call i32 @llvm.ctpop.i32(i32 %15252)
  %15254 = trunc i32 %15253 to i8
  %15255 = and i8 %15254, 1
  %15256 = xor i8 %15255, 1
  store i8 %15256, i8* %21, align 1
  %15257 = xor i32 %15245, %15239
  %15258 = xor i32 %15257, %15246
  %15259 = lshr i32 %15258, 4
  %15260 = trunc i32 %15259 to i8
  %15261 = and i8 %15260, 1
  store i8 %15261, i8* %27, align 1
  %15262 = icmp eq i32 %15246, 0
  %15263 = zext i1 %15262 to i8
  store i8 %15263, i8* %30, align 1
  %15264 = lshr i32 %15246, 31
  %15265 = trunc i32 %15264 to i8
  store i8 %15265, i8* %33, align 1
  %15266 = lshr i32 %15239, 31
  %15267 = lshr i32 %15245, 31
  %15268 = xor i32 %15264, %15266
  %15269 = xor i32 %15264, %15267
  %15270 = add nuw nsw i32 %15268, %15269
  %15271 = icmp eq i32 %15270, 2
  %15272 = zext i1 %15271 to i8
  store i8 %15272, i8* %39, align 1
  %15273 = add i64 %15178, 41
  store i64 %15273, i64* %3, align 8
  store i32 %15246, i32* %15244, align 4
  %15274 = load i64, i64* %RBP.i, align 8
  %15275 = add i64 %15274, -8
  %15276 = load i64, i64* %3, align 8
  %15277 = add i64 %15276, 4
  store i64 %15277, i64* %3, align 8
  %15278 = inttoptr i64 %15275 to i64*
  %15279 = load i64, i64* %15278, align 8
  %15280 = add i64 %15279, 51640
  store i64 %15280, i64* %RAX.i11582.pre-phi, align 8
  %15281 = icmp ugt i64 %15279, -51641
  %15282 = zext i1 %15281 to i8
  store i8 %15282, i8* %14, align 1
  %15283 = trunc i64 %15280 to i32
  %15284 = and i32 %15283, 255
  %15285 = tail call i32 @llvm.ctpop.i32(i32 %15284)
  %15286 = trunc i32 %15285 to i8
  %15287 = and i8 %15286, 1
  %15288 = xor i8 %15287, 1
  store i8 %15288, i8* %21, align 1
  %15289 = xor i64 %15279, 16
  %15290 = xor i64 %15289, %15280
  %15291 = lshr i64 %15290, 4
  %15292 = trunc i64 %15291 to i8
  %15293 = and i8 %15292, 1
  store i8 %15293, i8* %27, align 1
  %15294 = icmp eq i64 %15280, 0
  %15295 = zext i1 %15294 to i8
  store i8 %15295, i8* %30, align 1
  %15296 = lshr i64 %15280, 63
  %15297 = trunc i64 %15296 to i8
  store i8 %15297, i8* %33, align 1
  %15298 = lshr i64 %15279, 63
  %15299 = xor i64 %15296, %15298
  %15300 = add nuw nsw i64 %15299, %15296
  %15301 = icmp eq i64 %15300, 2
  %15302 = zext i1 %15301 to i8
  store i8 %15302, i8* %39, align 1
  %15303 = add i64 %15274, -150
  %15304 = add i64 %15276, 17
  store i64 %15304, i64* %3, align 8
  %15305 = inttoptr i64 %15303 to i16*
  %15306 = load i16, i16* %15305, align 2
  %15307 = zext i16 %15306 to i64
  store i64 %15307, i64* %RCX.i11580, align 8
  %15308 = zext i16 %15306 to i64
  %15309 = shl nuw nsw i64 %15308, 4
  store i64 %15309, i64* %573, align 8
  %15310 = add i64 %15309, %15280
  store i64 %15310, i64* %RAX.i11582.pre-phi, align 8
  %15311 = icmp ult i64 %15310, %15280
  %15312 = icmp ult i64 %15310, %15309
  %15313 = or i1 %15311, %15312
  %15314 = zext i1 %15313 to i8
  store i8 %15314, i8* %14, align 1
  %15315 = trunc i64 %15310 to i32
  %15316 = and i32 %15315, 255
  %15317 = tail call i32 @llvm.ctpop.i32(i32 %15316)
  %15318 = trunc i32 %15317 to i8
  %15319 = and i8 %15318, 1
  %15320 = xor i8 %15319, 1
  store i8 %15320, i8* %21, align 1
  %15321 = xor i64 %15309, %15280
  %15322 = xor i64 %15321, %15310
  %15323 = lshr i64 %15322, 4
  %15324 = trunc i64 %15323 to i8
  %15325 = and i8 %15324, 1
  store i8 %15325, i8* %27, align 1
  %15326 = icmp eq i64 %15310, 0
  %15327 = zext i1 %15326 to i8
  store i8 %15327, i8* %30, align 1
  %15328 = lshr i64 %15310, 63
  %15329 = trunc i64 %15328 to i8
  store i8 %15329, i8* %33, align 1
  %15330 = xor i64 %15328, %15296
  %15331 = add nuw nsw i64 %15330, %15328
  %15332 = icmp eq i64 %15331, 2
  %15333 = zext i1 %15332 to i8
  store i8 %15333, i8* %39, align 1
  %15334 = add i64 %15310, 8
  %15335 = add i64 %15276, 29
  store i64 %15335, i64* %3, align 8
  %15336 = inttoptr i64 %15334 to i32*
  %15337 = load i32, i32* %15336, align 4
  %15338 = zext i32 %15337 to i64
  store i64 %15338, i64* %RCX.i11580, align 8
  %15339 = load i64, i64* %RBP.i, align 8
  %15340 = add i64 %15339, -148
  %15341 = add i64 %15276, 35
  store i64 %15341, i64* %3, align 8
  %15342 = inttoptr i64 %15340 to i32*
  %15343 = load i32, i32* %15342, align 4
  %15344 = add i32 %15343, %15337
  %15345 = zext i32 %15344 to i64
  store i64 %15345, i64* %RCX.i11580, align 8
  %15346 = icmp ult i32 %15344, %15337
  %15347 = icmp ult i32 %15344, %15343
  %15348 = or i1 %15346, %15347
  %15349 = zext i1 %15348 to i8
  store i8 %15349, i8* %14, align 1
  %15350 = and i32 %15344, 255
  %15351 = tail call i32 @llvm.ctpop.i32(i32 %15350)
  %15352 = trunc i32 %15351 to i8
  %15353 = and i8 %15352, 1
  %15354 = xor i8 %15353, 1
  store i8 %15354, i8* %21, align 1
  %15355 = xor i32 %15343, %15337
  %15356 = xor i32 %15355, %15344
  %15357 = lshr i32 %15356, 4
  %15358 = trunc i32 %15357 to i8
  %15359 = and i8 %15358, 1
  store i8 %15359, i8* %27, align 1
  %15360 = icmp eq i32 %15344, 0
  %15361 = zext i1 %15360 to i8
  store i8 %15361, i8* %30, align 1
  %15362 = lshr i32 %15344, 31
  %15363 = trunc i32 %15362 to i8
  store i8 %15363, i8* %33, align 1
  %15364 = lshr i32 %15337, 31
  %15365 = lshr i32 %15343, 31
  %15366 = xor i32 %15362, %15364
  %15367 = xor i32 %15362, %15365
  %15368 = add nuw nsw i32 %15366, %15367
  %15369 = icmp eq i32 %15368, 2
  %15370 = zext i1 %15369 to i8
  store i8 %15370, i8* %39, align 1
  %15371 = add i64 %15276, 41
  store i64 %15371, i64* %3, align 8
  store i32 %15344, i32* %15342, align 4
  %15372 = load i64, i64* %RBP.i, align 8
  %15373 = add i64 %15372, -120
  %15374 = load i64, i64* %3, align 8
  %15375 = add i64 %15374, 4
  store i64 %15375, i64* %3, align 8
  %15376 = inttoptr i64 %15373 to i64*
  %15377 = load i64, i64* %15376, align 8
  store i64 %15377, i64* %RAX.i11582.pre-phi, align 8
  %15378 = add i64 %15372, -28
  %15379 = add i64 %15374, 7
  store i64 %15379, i64* %3, align 8
  %15380 = inttoptr i64 %15378 to i32*
  %15381 = load i32, i32* %15380, align 4
  %15382 = add i32 %15381, 38
  %15383 = zext i32 %15382 to i64
  store i64 %15383, i64* %RCX.i11580, align 8
  %15384 = icmp ugt i32 %15381, -39
  %15385 = zext i1 %15384 to i8
  store i8 %15385, i8* %14, align 1
  %15386 = and i32 %15382, 255
  %15387 = tail call i32 @llvm.ctpop.i32(i32 %15386)
  %15388 = trunc i32 %15387 to i8
  %15389 = and i8 %15388, 1
  %15390 = xor i8 %15389, 1
  store i8 %15390, i8* %21, align 1
  %15391 = xor i32 %15382, %15381
  %15392 = lshr i32 %15391, 4
  %15393 = trunc i32 %15392 to i8
  %15394 = and i8 %15393, 1
  store i8 %15394, i8* %27, align 1
  %15395 = icmp eq i32 %15382, 0
  %15396 = zext i1 %15395 to i8
  store i8 %15396, i8* %30, align 1
  %15397 = lshr i32 %15382, 31
  %15398 = trunc i32 %15397 to i8
  store i8 %15398, i8* %33, align 1
  %15399 = lshr i32 %15381, 31
  %15400 = xor i32 %15397, %15399
  %15401 = add nuw nsw i32 %15400, %15397
  %15402 = icmp eq i32 %15401, 2
  %15403 = zext i1 %15402 to i8
  store i8 %15403, i8* %39, align 1
  %15404 = sext i32 %15382 to i64
  store i64 %15404, i64* %573, align 8
  %15405 = shl nsw i64 %15404, 1
  %15406 = add i64 %15377, %15405
  %15407 = add i64 %15374, 17
  store i64 %15407, i64* %3, align 8
  %15408 = inttoptr i64 %15406 to i16*
  %15409 = load i16, i16* %15408, align 2
  store i16 %15409, i16* %SI.i, align 2
  %15410 = add i64 %15372, -150
  %15411 = add i64 %15374, 24
  store i64 %15411, i64* %3, align 8
  %15412 = inttoptr i64 %15410 to i16*
  store i16 %15409, i16* %15412, align 2
  %15413 = load i64, i64* %RBP.i, align 8
  %15414 = add i64 %15413, -8
  %15415 = load i64, i64* %3, align 8
  %15416 = add i64 %15415, 4
  store i64 %15416, i64* %3, align 8
  %15417 = inttoptr i64 %15414 to i64*
  %15418 = load i64, i64* %15417, align 8
  %15419 = add i64 %15418, 51640
  store i64 %15419, i64* %RAX.i11582.pre-phi, align 8
  %15420 = icmp ugt i64 %15418, -51641
  %15421 = zext i1 %15420 to i8
  store i8 %15421, i8* %14, align 1
  %15422 = trunc i64 %15419 to i32
  %15423 = and i32 %15422, 255
  %15424 = tail call i32 @llvm.ctpop.i32(i32 %15423)
  %15425 = trunc i32 %15424 to i8
  %15426 = and i8 %15425, 1
  %15427 = xor i8 %15426, 1
  store i8 %15427, i8* %21, align 1
  %15428 = xor i64 %15418, 16
  %15429 = xor i64 %15428, %15419
  %15430 = lshr i64 %15429, 4
  %15431 = trunc i64 %15430 to i8
  %15432 = and i8 %15431, 1
  store i8 %15432, i8* %27, align 1
  %15433 = icmp eq i64 %15419, 0
  %15434 = zext i1 %15433 to i8
  store i8 %15434, i8* %30, align 1
  %15435 = lshr i64 %15419, 63
  %15436 = trunc i64 %15435 to i8
  store i8 %15436, i8* %33, align 1
  %15437 = lshr i64 %15418, 63
  %15438 = xor i64 %15435, %15437
  %15439 = add nuw nsw i64 %15438, %15435
  %15440 = icmp eq i64 %15439, 2
  %15441 = zext i1 %15440 to i8
  store i8 %15441, i8* %39, align 1
  %15442 = add i64 %15413, -150
  %15443 = add i64 %15415, 17
  store i64 %15443, i64* %3, align 8
  %15444 = inttoptr i64 %15442 to i16*
  %15445 = load i16, i16* %15444, align 2
  %15446 = zext i16 %15445 to i64
  store i64 %15446, i64* %RCX.i11580, align 8
  %15447 = zext i16 %15445 to i64
  %15448 = shl nuw nsw i64 %15447, 4
  store i64 %15448, i64* %573, align 8
  %15449 = add i64 %15448, %15419
  store i64 %15449, i64* %RAX.i11582.pre-phi, align 8
  %15450 = icmp ult i64 %15449, %15419
  %15451 = icmp ult i64 %15449, %15448
  %15452 = or i1 %15450, %15451
  %15453 = zext i1 %15452 to i8
  store i8 %15453, i8* %14, align 1
  %15454 = trunc i64 %15449 to i32
  %15455 = and i32 %15454, 255
  %15456 = tail call i32 @llvm.ctpop.i32(i32 %15455)
  %15457 = trunc i32 %15456 to i8
  %15458 = and i8 %15457, 1
  %15459 = xor i8 %15458, 1
  store i8 %15459, i8* %21, align 1
  %15460 = xor i64 %15448, %15419
  %15461 = xor i64 %15460, %15449
  %15462 = lshr i64 %15461, 4
  %15463 = trunc i64 %15462 to i8
  %15464 = and i8 %15463, 1
  store i8 %15464, i8* %27, align 1
  %15465 = icmp eq i64 %15449, 0
  %15466 = zext i1 %15465 to i8
  store i8 %15466, i8* %30, align 1
  %15467 = lshr i64 %15449, 63
  %15468 = trunc i64 %15467 to i8
  store i8 %15468, i8* %33, align 1
  %15469 = xor i64 %15467, %15435
  %15470 = add nuw nsw i64 %15469, %15467
  %15471 = icmp eq i64 %15470, 2
  %15472 = zext i1 %15471 to i8
  store i8 %15472, i8* %39, align 1
  %15473 = inttoptr i64 %15449 to i32*
  %15474 = add i64 %15415, 28
  store i64 %15474, i64* %3, align 8
  %15475 = load i32, i32* %15473, align 4
  %15476 = zext i32 %15475 to i64
  store i64 %15476, i64* %RCX.i11580, align 8
  %15477 = load i64, i64* %RBP.i, align 8
  %15478 = add i64 %15477, -140
  %15479 = add i64 %15415, 34
  store i64 %15479, i64* %3, align 8
  %15480 = inttoptr i64 %15478 to i32*
  %15481 = load i32, i32* %15480, align 4
  %15482 = add i32 %15481, %15475
  %15483 = zext i32 %15482 to i64
  store i64 %15483, i64* %RCX.i11580, align 8
  %15484 = icmp ult i32 %15482, %15475
  %15485 = icmp ult i32 %15482, %15481
  %15486 = or i1 %15484, %15485
  %15487 = zext i1 %15486 to i8
  store i8 %15487, i8* %14, align 1
  %15488 = and i32 %15482, 255
  %15489 = tail call i32 @llvm.ctpop.i32(i32 %15488)
  %15490 = trunc i32 %15489 to i8
  %15491 = and i8 %15490, 1
  %15492 = xor i8 %15491, 1
  store i8 %15492, i8* %21, align 1
  %15493 = xor i32 %15481, %15475
  %15494 = xor i32 %15493, %15482
  %15495 = lshr i32 %15494, 4
  %15496 = trunc i32 %15495 to i8
  %15497 = and i8 %15496, 1
  store i8 %15497, i8* %27, align 1
  %15498 = icmp eq i32 %15482, 0
  %15499 = zext i1 %15498 to i8
  store i8 %15499, i8* %30, align 1
  %15500 = lshr i32 %15482, 31
  %15501 = trunc i32 %15500 to i8
  store i8 %15501, i8* %33, align 1
  %15502 = lshr i32 %15475, 31
  %15503 = lshr i32 %15481, 31
  %15504 = xor i32 %15500, %15502
  %15505 = xor i32 %15500, %15503
  %15506 = add nuw nsw i32 %15504, %15505
  %15507 = icmp eq i32 %15506, 2
  %15508 = zext i1 %15507 to i8
  store i8 %15508, i8* %39, align 1
  %15509 = add i64 %15415, 40
  store i64 %15509, i64* %3, align 8
  store i32 %15482, i32* %15480, align 4
  %15510 = load i64, i64* %RBP.i, align 8
  %15511 = add i64 %15510, -8
  %15512 = load i64, i64* %3, align 8
  %15513 = add i64 %15512, 4
  store i64 %15513, i64* %3, align 8
  %15514 = inttoptr i64 %15511 to i64*
  %15515 = load i64, i64* %15514, align 8
  %15516 = add i64 %15515, 51640
  store i64 %15516, i64* %RAX.i11582.pre-phi, align 8
  %15517 = icmp ugt i64 %15515, -51641
  %15518 = zext i1 %15517 to i8
  store i8 %15518, i8* %14, align 1
  %15519 = trunc i64 %15516 to i32
  %15520 = and i32 %15519, 255
  %15521 = tail call i32 @llvm.ctpop.i32(i32 %15520)
  %15522 = trunc i32 %15521 to i8
  %15523 = and i8 %15522, 1
  %15524 = xor i8 %15523, 1
  store i8 %15524, i8* %21, align 1
  %15525 = xor i64 %15515, 16
  %15526 = xor i64 %15525, %15516
  %15527 = lshr i64 %15526, 4
  %15528 = trunc i64 %15527 to i8
  %15529 = and i8 %15528, 1
  store i8 %15529, i8* %27, align 1
  %15530 = icmp eq i64 %15516, 0
  %15531 = zext i1 %15530 to i8
  store i8 %15531, i8* %30, align 1
  %15532 = lshr i64 %15516, 63
  %15533 = trunc i64 %15532 to i8
  store i8 %15533, i8* %33, align 1
  %15534 = lshr i64 %15515, 63
  %15535 = xor i64 %15532, %15534
  %15536 = add nuw nsw i64 %15535, %15532
  %15537 = icmp eq i64 %15536, 2
  %15538 = zext i1 %15537 to i8
  store i8 %15538, i8* %39, align 1
  %15539 = add i64 %15510, -150
  %15540 = add i64 %15512, 17
  store i64 %15540, i64* %3, align 8
  %15541 = inttoptr i64 %15539 to i16*
  %15542 = load i16, i16* %15541, align 2
  %15543 = zext i16 %15542 to i64
  store i64 %15543, i64* %RCX.i11580, align 8
  %15544 = zext i16 %15542 to i64
  %15545 = shl nuw nsw i64 %15544, 4
  store i64 %15545, i64* %573, align 8
  %15546 = add i64 %15545, %15516
  store i64 %15546, i64* %RAX.i11582.pre-phi, align 8
  %15547 = icmp ult i64 %15546, %15516
  %15548 = icmp ult i64 %15546, %15545
  %15549 = or i1 %15547, %15548
  %15550 = zext i1 %15549 to i8
  store i8 %15550, i8* %14, align 1
  %15551 = trunc i64 %15546 to i32
  %15552 = and i32 %15551, 255
  %15553 = tail call i32 @llvm.ctpop.i32(i32 %15552)
  %15554 = trunc i32 %15553 to i8
  %15555 = and i8 %15554, 1
  %15556 = xor i8 %15555, 1
  store i8 %15556, i8* %21, align 1
  %15557 = xor i64 %15545, %15516
  %15558 = xor i64 %15557, %15546
  %15559 = lshr i64 %15558, 4
  %15560 = trunc i64 %15559 to i8
  %15561 = and i8 %15560, 1
  store i8 %15561, i8* %27, align 1
  %15562 = icmp eq i64 %15546, 0
  %15563 = zext i1 %15562 to i8
  store i8 %15563, i8* %30, align 1
  %15564 = lshr i64 %15546, 63
  %15565 = trunc i64 %15564 to i8
  store i8 %15565, i8* %33, align 1
  %15566 = xor i64 %15564, %15532
  %15567 = add nuw nsw i64 %15566, %15564
  %15568 = icmp eq i64 %15567, 2
  %15569 = zext i1 %15568 to i8
  store i8 %15569, i8* %39, align 1
  %15570 = add i64 %15546, 4
  %15571 = add i64 %15512, 29
  store i64 %15571, i64* %3, align 8
  %15572 = inttoptr i64 %15570 to i32*
  %15573 = load i32, i32* %15572, align 4
  %15574 = zext i32 %15573 to i64
  store i64 %15574, i64* %RCX.i11580, align 8
  %15575 = load i64, i64* %RBP.i, align 8
  %15576 = add i64 %15575, -144
  %15577 = add i64 %15512, 35
  store i64 %15577, i64* %3, align 8
  %15578 = inttoptr i64 %15576 to i32*
  %15579 = load i32, i32* %15578, align 4
  %15580 = add i32 %15579, %15573
  %15581 = zext i32 %15580 to i64
  store i64 %15581, i64* %RCX.i11580, align 8
  %15582 = icmp ult i32 %15580, %15573
  %15583 = icmp ult i32 %15580, %15579
  %15584 = or i1 %15582, %15583
  %15585 = zext i1 %15584 to i8
  store i8 %15585, i8* %14, align 1
  %15586 = and i32 %15580, 255
  %15587 = tail call i32 @llvm.ctpop.i32(i32 %15586)
  %15588 = trunc i32 %15587 to i8
  %15589 = and i8 %15588, 1
  %15590 = xor i8 %15589, 1
  store i8 %15590, i8* %21, align 1
  %15591 = xor i32 %15579, %15573
  %15592 = xor i32 %15591, %15580
  %15593 = lshr i32 %15592, 4
  %15594 = trunc i32 %15593 to i8
  %15595 = and i8 %15594, 1
  store i8 %15595, i8* %27, align 1
  %15596 = icmp eq i32 %15580, 0
  %15597 = zext i1 %15596 to i8
  store i8 %15597, i8* %30, align 1
  %15598 = lshr i32 %15580, 31
  %15599 = trunc i32 %15598 to i8
  store i8 %15599, i8* %33, align 1
  %15600 = lshr i32 %15573, 31
  %15601 = lshr i32 %15579, 31
  %15602 = xor i32 %15598, %15600
  %15603 = xor i32 %15598, %15601
  %15604 = add nuw nsw i32 %15602, %15603
  %15605 = icmp eq i32 %15604, 2
  %15606 = zext i1 %15605 to i8
  store i8 %15606, i8* %39, align 1
  %15607 = add i64 %15512, 41
  store i64 %15607, i64* %3, align 8
  store i32 %15580, i32* %15578, align 4
  %15608 = load i64, i64* %RBP.i, align 8
  %15609 = add i64 %15608, -8
  %15610 = load i64, i64* %3, align 8
  %15611 = add i64 %15610, 4
  store i64 %15611, i64* %3, align 8
  %15612 = inttoptr i64 %15609 to i64*
  %15613 = load i64, i64* %15612, align 8
  %15614 = add i64 %15613, 51640
  store i64 %15614, i64* %RAX.i11582.pre-phi, align 8
  %15615 = icmp ugt i64 %15613, -51641
  %15616 = zext i1 %15615 to i8
  store i8 %15616, i8* %14, align 1
  %15617 = trunc i64 %15614 to i32
  %15618 = and i32 %15617, 255
  %15619 = tail call i32 @llvm.ctpop.i32(i32 %15618)
  %15620 = trunc i32 %15619 to i8
  %15621 = and i8 %15620, 1
  %15622 = xor i8 %15621, 1
  store i8 %15622, i8* %21, align 1
  %15623 = xor i64 %15613, 16
  %15624 = xor i64 %15623, %15614
  %15625 = lshr i64 %15624, 4
  %15626 = trunc i64 %15625 to i8
  %15627 = and i8 %15626, 1
  store i8 %15627, i8* %27, align 1
  %15628 = icmp eq i64 %15614, 0
  %15629 = zext i1 %15628 to i8
  store i8 %15629, i8* %30, align 1
  %15630 = lshr i64 %15614, 63
  %15631 = trunc i64 %15630 to i8
  store i8 %15631, i8* %33, align 1
  %15632 = lshr i64 %15613, 63
  %15633 = xor i64 %15630, %15632
  %15634 = add nuw nsw i64 %15633, %15630
  %15635 = icmp eq i64 %15634, 2
  %15636 = zext i1 %15635 to i8
  store i8 %15636, i8* %39, align 1
  %15637 = add i64 %15608, -150
  %15638 = add i64 %15610, 17
  store i64 %15638, i64* %3, align 8
  %15639 = inttoptr i64 %15637 to i16*
  %15640 = load i16, i16* %15639, align 2
  %15641 = zext i16 %15640 to i64
  store i64 %15641, i64* %RCX.i11580, align 8
  %15642 = zext i16 %15640 to i64
  %15643 = shl nuw nsw i64 %15642, 4
  store i64 %15643, i64* %573, align 8
  %15644 = add i64 %15643, %15614
  store i64 %15644, i64* %RAX.i11582.pre-phi, align 8
  %15645 = icmp ult i64 %15644, %15614
  %15646 = icmp ult i64 %15644, %15643
  %15647 = or i1 %15645, %15646
  %15648 = zext i1 %15647 to i8
  store i8 %15648, i8* %14, align 1
  %15649 = trunc i64 %15644 to i32
  %15650 = and i32 %15649, 255
  %15651 = tail call i32 @llvm.ctpop.i32(i32 %15650)
  %15652 = trunc i32 %15651 to i8
  %15653 = and i8 %15652, 1
  %15654 = xor i8 %15653, 1
  store i8 %15654, i8* %21, align 1
  %15655 = xor i64 %15643, %15614
  %15656 = xor i64 %15655, %15644
  %15657 = lshr i64 %15656, 4
  %15658 = trunc i64 %15657 to i8
  %15659 = and i8 %15658, 1
  store i8 %15659, i8* %27, align 1
  %15660 = icmp eq i64 %15644, 0
  %15661 = zext i1 %15660 to i8
  store i8 %15661, i8* %30, align 1
  %15662 = lshr i64 %15644, 63
  %15663 = trunc i64 %15662 to i8
  store i8 %15663, i8* %33, align 1
  %15664 = xor i64 %15662, %15630
  %15665 = add nuw nsw i64 %15664, %15662
  %15666 = icmp eq i64 %15665, 2
  %15667 = zext i1 %15666 to i8
  store i8 %15667, i8* %39, align 1
  %15668 = add i64 %15644, 8
  %15669 = add i64 %15610, 29
  store i64 %15669, i64* %3, align 8
  %15670 = inttoptr i64 %15668 to i32*
  %15671 = load i32, i32* %15670, align 4
  %15672 = zext i32 %15671 to i64
  store i64 %15672, i64* %RCX.i11580, align 8
  %15673 = load i64, i64* %RBP.i, align 8
  %15674 = add i64 %15673, -148
  %15675 = add i64 %15610, 35
  store i64 %15675, i64* %3, align 8
  %15676 = inttoptr i64 %15674 to i32*
  %15677 = load i32, i32* %15676, align 4
  %15678 = add i32 %15677, %15671
  %15679 = zext i32 %15678 to i64
  store i64 %15679, i64* %RCX.i11580, align 8
  %15680 = icmp ult i32 %15678, %15671
  %15681 = icmp ult i32 %15678, %15677
  %15682 = or i1 %15680, %15681
  %15683 = zext i1 %15682 to i8
  store i8 %15683, i8* %14, align 1
  %15684 = and i32 %15678, 255
  %15685 = tail call i32 @llvm.ctpop.i32(i32 %15684)
  %15686 = trunc i32 %15685 to i8
  %15687 = and i8 %15686, 1
  %15688 = xor i8 %15687, 1
  store i8 %15688, i8* %21, align 1
  %15689 = xor i32 %15677, %15671
  %15690 = xor i32 %15689, %15678
  %15691 = lshr i32 %15690, 4
  %15692 = trunc i32 %15691 to i8
  %15693 = and i8 %15692, 1
  store i8 %15693, i8* %27, align 1
  %15694 = icmp eq i32 %15678, 0
  %15695 = zext i1 %15694 to i8
  store i8 %15695, i8* %30, align 1
  %15696 = lshr i32 %15678, 31
  %15697 = trunc i32 %15696 to i8
  store i8 %15697, i8* %33, align 1
  %15698 = lshr i32 %15671, 31
  %15699 = lshr i32 %15677, 31
  %15700 = xor i32 %15696, %15698
  %15701 = xor i32 %15696, %15699
  %15702 = add nuw nsw i32 %15700, %15701
  %15703 = icmp eq i32 %15702, 2
  %15704 = zext i1 %15703 to i8
  store i8 %15704, i8* %39, align 1
  %15705 = add i64 %15610, 41
  store i64 %15705, i64* %3, align 8
  store i32 %15678, i32* %15676, align 4
  %15706 = load i64, i64* %RBP.i, align 8
  %15707 = add i64 %15706, -120
  %15708 = load i64, i64* %3, align 8
  %15709 = add i64 %15708, 4
  store i64 %15709, i64* %3, align 8
  %15710 = inttoptr i64 %15707 to i64*
  %15711 = load i64, i64* %15710, align 8
  store i64 %15711, i64* %RAX.i11582.pre-phi, align 8
  %15712 = add i64 %15706, -28
  %15713 = add i64 %15708, 7
  store i64 %15713, i64* %3, align 8
  %15714 = inttoptr i64 %15712 to i32*
  %15715 = load i32, i32* %15714, align 4
  %15716 = add i32 %15715, 39
  %15717 = zext i32 %15716 to i64
  store i64 %15717, i64* %RCX.i11580, align 8
  %15718 = icmp ugt i32 %15715, -40
  %15719 = zext i1 %15718 to i8
  store i8 %15719, i8* %14, align 1
  %15720 = and i32 %15716, 255
  %15721 = tail call i32 @llvm.ctpop.i32(i32 %15720)
  %15722 = trunc i32 %15721 to i8
  %15723 = and i8 %15722, 1
  %15724 = xor i8 %15723, 1
  store i8 %15724, i8* %21, align 1
  %15725 = xor i32 %15716, %15715
  %15726 = lshr i32 %15725, 4
  %15727 = trunc i32 %15726 to i8
  %15728 = and i8 %15727, 1
  store i8 %15728, i8* %27, align 1
  %15729 = icmp eq i32 %15716, 0
  %15730 = zext i1 %15729 to i8
  store i8 %15730, i8* %30, align 1
  %15731 = lshr i32 %15716, 31
  %15732 = trunc i32 %15731 to i8
  store i8 %15732, i8* %33, align 1
  %15733 = lshr i32 %15715, 31
  %15734 = xor i32 %15731, %15733
  %15735 = add nuw nsw i32 %15734, %15731
  %15736 = icmp eq i32 %15735, 2
  %15737 = zext i1 %15736 to i8
  store i8 %15737, i8* %39, align 1
  %15738 = sext i32 %15716 to i64
  store i64 %15738, i64* %573, align 8
  %15739 = shl nsw i64 %15738, 1
  %15740 = add i64 %15711, %15739
  %15741 = add i64 %15708, 17
  store i64 %15741, i64* %3, align 8
  %15742 = inttoptr i64 %15740 to i16*
  %15743 = load i16, i16* %15742, align 2
  store i16 %15743, i16* %SI.i, align 2
  %15744 = add i64 %15706, -150
  %15745 = add i64 %15708, 24
  store i64 %15745, i64* %3, align 8
  %15746 = inttoptr i64 %15744 to i16*
  store i16 %15743, i16* %15746, align 2
  %15747 = load i64, i64* %RBP.i, align 8
  %15748 = add i64 %15747, -8
  %15749 = load i64, i64* %3, align 8
  %15750 = add i64 %15749, 4
  store i64 %15750, i64* %3, align 8
  %15751 = inttoptr i64 %15748 to i64*
  %15752 = load i64, i64* %15751, align 8
  %15753 = add i64 %15752, 51640
  store i64 %15753, i64* %RAX.i11582.pre-phi, align 8
  %15754 = icmp ugt i64 %15752, -51641
  %15755 = zext i1 %15754 to i8
  store i8 %15755, i8* %14, align 1
  %15756 = trunc i64 %15753 to i32
  %15757 = and i32 %15756, 255
  %15758 = tail call i32 @llvm.ctpop.i32(i32 %15757)
  %15759 = trunc i32 %15758 to i8
  %15760 = and i8 %15759, 1
  %15761 = xor i8 %15760, 1
  store i8 %15761, i8* %21, align 1
  %15762 = xor i64 %15752, 16
  %15763 = xor i64 %15762, %15753
  %15764 = lshr i64 %15763, 4
  %15765 = trunc i64 %15764 to i8
  %15766 = and i8 %15765, 1
  store i8 %15766, i8* %27, align 1
  %15767 = icmp eq i64 %15753, 0
  %15768 = zext i1 %15767 to i8
  store i8 %15768, i8* %30, align 1
  %15769 = lshr i64 %15753, 63
  %15770 = trunc i64 %15769 to i8
  store i8 %15770, i8* %33, align 1
  %15771 = lshr i64 %15752, 63
  %15772 = xor i64 %15769, %15771
  %15773 = add nuw nsw i64 %15772, %15769
  %15774 = icmp eq i64 %15773, 2
  %15775 = zext i1 %15774 to i8
  store i8 %15775, i8* %39, align 1
  %15776 = add i64 %15747, -150
  %15777 = add i64 %15749, 17
  store i64 %15777, i64* %3, align 8
  %15778 = inttoptr i64 %15776 to i16*
  %15779 = load i16, i16* %15778, align 2
  %15780 = zext i16 %15779 to i64
  store i64 %15780, i64* %RCX.i11580, align 8
  %15781 = zext i16 %15779 to i64
  %15782 = shl nuw nsw i64 %15781, 4
  store i64 %15782, i64* %573, align 8
  %15783 = add i64 %15782, %15753
  store i64 %15783, i64* %RAX.i11582.pre-phi, align 8
  %15784 = icmp ult i64 %15783, %15753
  %15785 = icmp ult i64 %15783, %15782
  %15786 = or i1 %15784, %15785
  %15787 = zext i1 %15786 to i8
  store i8 %15787, i8* %14, align 1
  %15788 = trunc i64 %15783 to i32
  %15789 = and i32 %15788, 255
  %15790 = tail call i32 @llvm.ctpop.i32(i32 %15789)
  %15791 = trunc i32 %15790 to i8
  %15792 = and i8 %15791, 1
  %15793 = xor i8 %15792, 1
  store i8 %15793, i8* %21, align 1
  %15794 = xor i64 %15782, %15753
  %15795 = xor i64 %15794, %15783
  %15796 = lshr i64 %15795, 4
  %15797 = trunc i64 %15796 to i8
  %15798 = and i8 %15797, 1
  store i8 %15798, i8* %27, align 1
  %15799 = icmp eq i64 %15783, 0
  %15800 = zext i1 %15799 to i8
  store i8 %15800, i8* %30, align 1
  %15801 = lshr i64 %15783, 63
  %15802 = trunc i64 %15801 to i8
  store i8 %15802, i8* %33, align 1
  %15803 = xor i64 %15801, %15769
  %15804 = add nuw nsw i64 %15803, %15801
  %15805 = icmp eq i64 %15804, 2
  %15806 = zext i1 %15805 to i8
  store i8 %15806, i8* %39, align 1
  %15807 = inttoptr i64 %15783 to i32*
  %15808 = add i64 %15749, 28
  store i64 %15808, i64* %3, align 8
  %15809 = load i32, i32* %15807, align 4
  %15810 = zext i32 %15809 to i64
  store i64 %15810, i64* %RCX.i11580, align 8
  %15811 = load i64, i64* %RBP.i, align 8
  %15812 = add i64 %15811, -140
  %15813 = add i64 %15749, 34
  store i64 %15813, i64* %3, align 8
  %15814 = inttoptr i64 %15812 to i32*
  %15815 = load i32, i32* %15814, align 4
  %15816 = add i32 %15815, %15809
  %15817 = zext i32 %15816 to i64
  store i64 %15817, i64* %RCX.i11580, align 8
  %15818 = icmp ult i32 %15816, %15809
  %15819 = icmp ult i32 %15816, %15815
  %15820 = or i1 %15818, %15819
  %15821 = zext i1 %15820 to i8
  store i8 %15821, i8* %14, align 1
  %15822 = and i32 %15816, 255
  %15823 = tail call i32 @llvm.ctpop.i32(i32 %15822)
  %15824 = trunc i32 %15823 to i8
  %15825 = and i8 %15824, 1
  %15826 = xor i8 %15825, 1
  store i8 %15826, i8* %21, align 1
  %15827 = xor i32 %15815, %15809
  %15828 = xor i32 %15827, %15816
  %15829 = lshr i32 %15828, 4
  %15830 = trunc i32 %15829 to i8
  %15831 = and i8 %15830, 1
  store i8 %15831, i8* %27, align 1
  %15832 = icmp eq i32 %15816, 0
  %15833 = zext i1 %15832 to i8
  store i8 %15833, i8* %30, align 1
  %15834 = lshr i32 %15816, 31
  %15835 = trunc i32 %15834 to i8
  store i8 %15835, i8* %33, align 1
  %15836 = lshr i32 %15809, 31
  %15837 = lshr i32 %15815, 31
  %15838 = xor i32 %15834, %15836
  %15839 = xor i32 %15834, %15837
  %15840 = add nuw nsw i32 %15838, %15839
  %15841 = icmp eq i32 %15840, 2
  %15842 = zext i1 %15841 to i8
  store i8 %15842, i8* %39, align 1
  %15843 = add i64 %15749, 40
  store i64 %15843, i64* %3, align 8
  store i32 %15816, i32* %15814, align 4
  %15844 = load i64, i64* %RBP.i, align 8
  %15845 = add i64 %15844, -8
  %15846 = load i64, i64* %3, align 8
  %15847 = add i64 %15846, 4
  store i64 %15847, i64* %3, align 8
  %15848 = inttoptr i64 %15845 to i64*
  %15849 = load i64, i64* %15848, align 8
  %15850 = add i64 %15849, 51640
  store i64 %15850, i64* %RAX.i11582.pre-phi, align 8
  %15851 = icmp ugt i64 %15849, -51641
  %15852 = zext i1 %15851 to i8
  store i8 %15852, i8* %14, align 1
  %15853 = trunc i64 %15850 to i32
  %15854 = and i32 %15853, 255
  %15855 = tail call i32 @llvm.ctpop.i32(i32 %15854)
  %15856 = trunc i32 %15855 to i8
  %15857 = and i8 %15856, 1
  %15858 = xor i8 %15857, 1
  store i8 %15858, i8* %21, align 1
  %15859 = xor i64 %15849, 16
  %15860 = xor i64 %15859, %15850
  %15861 = lshr i64 %15860, 4
  %15862 = trunc i64 %15861 to i8
  %15863 = and i8 %15862, 1
  store i8 %15863, i8* %27, align 1
  %15864 = icmp eq i64 %15850, 0
  %15865 = zext i1 %15864 to i8
  store i8 %15865, i8* %30, align 1
  %15866 = lshr i64 %15850, 63
  %15867 = trunc i64 %15866 to i8
  store i8 %15867, i8* %33, align 1
  %15868 = lshr i64 %15849, 63
  %15869 = xor i64 %15866, %15868
  %15870 = add nuw nsw i64 %15869, %15866
  %15871 = icmp eq i64 %15870, 2
  %15872 = zext i1 %15871 to i8
  store i8 %15872, i8* %39, align 1
  %15873 = add i64 %15844, -150
  %15874 = add i64 %15846, 17
  store i64 %15874, i64* %3, align 8
  %15875 = inttoptr i64 %15873 to i16*
  %15876 = load i16, i16* %15875, align 2
  %15877 = zext i16 %15876 to i64
  store i64 %15877, i64* %RCX.i11580, align 8
  %15878 = zext i16 %15876 to i64
  %15879 = shl nuw nsw i64 %15878, 4
  store i64 %15879, i64* %573, align 8
  %15880 = add i64 %15879, %15850
  store i64 %15880, i64* %RAX.i11582.pre-phi, align 8
  %15881 = icmp ult i64 %15880, %15850
  %15882 = icmp ult i64 %15880, %15879
  %15883 = or i1 %15881, %15882
  %15884 = zext i1 %15883 to i8
  store i8 %15884, i8* %14, align 1
  %15885 = trunc i64 %15880 to i32
  %15886 = and i32 %15885, 255
  %15887 = tail call i32 @llvm.ctpop.i32(i32 %15886)
  %15888 = trunc i32 %15887 to i8
  %15889 = and i8 %15888, 1
  %15890 = xor i8 %15889, 1
  store i8 %15890, i8* %21, align 1
  %15891 = xor i64 %15879, %15850
  %15892 = xor i64 %15891, %15880
  %15893 = lshr i64 %15892, 4
  %15894 = trunc i64 %15893 to i8
  %15895 = and i8 %15894, 1
  store i8 %15895, i8* %27, align 1
  %15896 = icmp eq i64 %15880, 0
  %15897 = zext i1 %15896 to i8
  store i8 %15897, i8* %30, align 1
  %15898 = lshr i64 %15880, 63
  %15899 = trunc i64 %15898 to i8
  store i8 %15899, i8* %33, align 1
  %15900 = xor i64 %15898, %15866
  %15901 = add nuw nsw i64 %15900, %15898
  %15902 = icmp eq i64 %15901, 2
  %15903 = zext i1 %15902 to i8
  store i8 %15903, i8* %39, align 1
  %15904 = add i64 %15880, 4
  %15905 = add i64 %15846, 29
  store i64 %15905, i64* %3, align 8
  %15906 = inttoptr i64 %15904 to i32*
  %15907 = load i32, i32* %15906, align 4
  %15908 = zext i32 %15907 to i64
  store i64 %15908, i64* %RCX.i11580, align 8
  %15909 = load i64, i64* %RBP.i, align 8
  %15910 = add i64 %15909, -144
  %15911 = add i64 %15846, 35
  store i64 %15911, i64* %3, align 8
  %15912 = inttoptr i64 %15910 to i32*
  %15913 = load i32, i32* %15912, align 4
  %15914 = add i32 %15913, %15907
  %15915 = zext i32 %15914 to i64
  store i64 %15915, i64* %RCX.i11580, align 8
  %15916 = icmp ult i32 %15914, %15907
  %15917 = icmp ult i32 %15914, %15913
  %15918 = or i1 %15916, %15917
  %15919 = zext i1 %15918 to i8
  store i8 %15919, i8* %14, align 1
  %15920 = and i32 %15914, 255
  %15921 = tail call i32 @llvm.ctpop.i32(i32 %15920)
  %15922 = trunc i32 %15921 to i8
  %15923 = and i8 %15922, 1
  %15924 = xor i8 %15923, 1
  store i8 %15924, i8* %21, align 1
  %15925 = xor i32 %15913, %15907
  %15926 = xor i32 %15925, %15914
  %15927 = lshr i32 %15926, 4
  %15928 = trunc i32 %15927 to i8
  %15929 = and i8 %15928, 1
  store i8 %15929, i8* %27, align 1
  %15930 = icmp eq i32 %15914, 0
  %15931 = zext i1 %15930 to i8
  store i8 %15931, i8* %30, align 1
  %15932 = lshr i32 %15914, 31
  %15933 = trunc i32 %15932 to i8
  store i8 %15933, i8* %33, align 1
  %15934 = lshr i32 %15907, 31
  %15935 = lshr i32 %15913, 31
  %15936 = xor i32 %15932, %15934
  %15937 = xor i32 %15932, %15935
  %15938 = add nuw nsw i32 %15936, %15937
  %15939 = icmp eq i32 %15938, 2
  %15940 = zext i1 %15939 to i8
  store i8 %15940, i8* %39, align 1
  %15941 = add i64 %15846, 41
  store i64 %15941, i64* %3, align 8
  store i32 %15914, i32* %15912, align 4
  %15942 = load i64, i64* %RBP.i, align 8
  %15943 = add i64 %15942, -8
  %15944 = load i64, i64* %3, align 8
  %15945 = add i64 %15944, 4
  store i64 %15945, i64* %3, align 8
  %15946 = inttoptr i64 %15943 to i64*
  %15947 = load i64, i64* %15946, align 8
  %15948 = add i64 %15947, 51640
  store i64 %15948, i64* %RAX.i11582.pre-phi, align 8
  %15949 = icmp ugt i64 %15947, -51641
  %15950 = zext i1 %15949 to i8
  store i8 %15950, i8* %14, align 1
  %15951 = trunc i64 %15948 to i32
  %15952 = and i32 %15951, 255
  %15953 = tail call i32 @llvm.ctpop.i32(i32 %15952)
  %15954 = trunc i32 %15953 to i8
  %15955 = and i8 %15954, 1
  %15956 = xor i8 %15955, 1
  store i8 %15956, i8* %21, align 1
  %15957 = xor i64 %15947, 16
  %15958 = xor i64 %15957, %15948
  %15959 = lshr i64 %15958, 4
  %15960 = trunc i64 %15959 to i8
  %15961 = and i8 %15960, 1
  store i8 %15961, i8* %27, align 1
  %15962 = icmp eq i64 %15948, 0
  %15963 = zext i1 %15962 to i8
  store i8 %15963, i8* %30, align 1
  %15964 = lshr i64 %15948, 63
  %15965 = trunc i64 %15964 to i8
  store i8 %15965, i8* %33, align 1
  %15966 = lshr i64 %15947, 63
  %15967 = xor i64 %15964, %15966
  %15968 = add nuw nsw i64 %15967, %15964
  %15969 = icmp eq i64 %15968, 2
  %15970 = zext i1 %15969 to i8
  store i8 %15970, i8* %39, align 1
  %15971 = add i64 %15942, -150
  %15972 = add i64 %15944, 17
  store i64 %15972, i64* %3, align 8
  %15973 = inttoptr i64 %15971 to i16*
  %15974 = load i16, i16* %15973, align 2
  %15975 = zext i16 %15974 to i64
  store i64 %15975, i64* %RCX.i11580, align 8
  %15976 = zext i16 %15974 to i64
  %15977 = shl nuw nsw i64 %15976, 4
  store i64 %15977, i64* %573, align 8
  %15978 = add i64 %15977, %15948
  store i64 %15978, i64* %RAX.i11582.pre-phi, align 8
  %15979 = icmp ult i64 %15978, %15948
  %15980 = icmp ult i64 %15978, %15977
  %15981 = or i1 %15979, %15980
  %15982 = zext i1 %15981 to i8
  store i8 %15982, i8* %14, align 1
  %15983 = trunc i64 %15978 to i32
  %15984 = and i32 %15983, 255
  %15985 = tail call i32 @llvm.ctpop.i32(i32 %15984)
  %15986 = trunc i32 %15985 to i8
  %15987 = and i8 %15986, 1
  %15988 = xor i8 %15987, 1
  store i8 %15988, i8* %21, align 1
  %15989 = xor i64 %15977, %15948
  %15990 = xor i64 %15989, %15978
  %15991 = lshr i64 %15990, 4
  %15992 = trunc i64 %15991 to i8
  %15993 = and i8 %15992, 1
  store i8 %15993, i8* %27, align 1
  %15994 = icmp eq i64 %15978, 0
  %15995 = zext i1 %15994 to i8
  store i8 %15995, i8* %30, align 1
  %15996 = lshr i64 %15978, 63
  %15997 = trunc i64 %15996 to i8
  store i8 %15997, i8* %33, align 1
  %15998 = xor i64 %15996, %15964
  %15999 = add nuw nsw i64 %15998, %15996
  %16000 = icmp eq i64 %15999, 2
  %16001 = zext i1 %16000 to i8
  store i8 %16001, i8* %39, align 1
  %16002 = add i64 %15978, 8
  %16003 = add i64 %15944, 29
  store i64 %16003, i64* %3, align 8
  %16004 = inttoptr i64 %16002 to i32*
  %16005 = load i32, i32* %16004, align 4
  %16006 = zext i32 %16005 to i64
  store i64 %16006, i64* %RCX.i11580, align 8
  %16007 = load i64, i64* %RBP.i, align 8
  %16008 = add i64 %16007, -148
  %16009 = add i64 %15944, 35
  store i64 %16009, i64* %3, align 8
  %16010 = inttoptr i64 %16008 to i32*
  %16011 = load i32, i32* %16010, align 4
  %16012 = add i32 %16011, %16005
  %16013 = zext i32 %16012 to i64
  store i64 %16013, i64* %RCX.i11580, align 8
  %16014 = icmp ult i32 %16012, %16005
  %16015 = icmp ult i32 %16012, %16011
  %16016 = or i1 %16014, %16015
  %16017 = zext i1 %16016 to i8
  store i8 %16017, i8* %14, align 1
  %16018 = and i32 %16012, 255
  %16019 = tail call i32 @llvm.ctpop.i32(i32 %16018)
  %16020 = trunc i32 %16019 to i8
  %16021 = and i8 %16020, 1
  %16022 = xor i8 %16021, 1
  store i8 %16022, i8* %21, align 1
  %16023 = xor i32 %16011, %16005
  %16024 = xor i32 %16023, %16012
  %16025 = lshr i32 %16024, 4
  %16026 = trunc i32 %16025 to i8
  %16027 = and i8 %16026, 1
  store i8 %16027, i8* %27, align 1
  %16028 = icmp eq i32 %16012, 0
  %16029 = zext i1 %16028 to i8
  store i8 %16029, i8* %30, align 1
  %16030 = lshr i32 %16012, 31
  %16031 = trunc i32 %16030 to i8
  store i8 %16031, i8* %33, align 1
  %16032 = lshr i32 %16005, 31
  %16033 = lshr i32 %16011, 31
  %16034 = xor i32 %16030, %16032
  %16035 = xor i32 %16030, %16033
  %16036 = add nuw nsw i32 %16034, %16035
  %16037 = icmp eq i32 %16036, 2
  %16038 = zext i1 %16037 to i8
  store i8 %16038, i8* %39, align 1
  %16039 = add i64 %15944, 41
  store i64 %16039, i64* %3, align 8
  store i32 %16012, i32* %16010, align 4
  %16040 = load i64, i64* %RBP.i, align 8
  %16041 = add i64 %16040, -120
  %16042 = load i64, i64* %3, align 8
  %16043 = add i64 %16042, 4
  store i64 %16043, i64* %3, align 8
  %16044 = inttoptr i64 %16041 to i64*
  %16045 = load i64, i64* %16044, align 8
  store i64 %16045, i64* %RAX.i11582.pre-phi, align 8
  %16046 = add i64 %16040, -28
  %16047 = add i64 %16042, 7
  store i64 %16047, i64* %3, align 8
  %16048 = inttoptr i64 %16046 to i32*
  %16049 = load i32, i32* %16048, align 4
  %16050 = add i32 %16049, 40
  %16051 = zext i32 %16050 to i64
  store i64 %16051, i64* %RCX.i11580, align 8
  %16052 = icmp ugt i32 %16049, -41
  %16053 = zext i1 %16052 to i8
  store i8 %16053, i8* %14, align 1
  %16054 = and i32 %16050, 255
  %16055 = tail call i32 @llvm.ctpop.i32(i32 %16054)
  %16056 = trunc i32 %16055 to i8
  %16057 = and i8 %16056, 1
  %16058 = xor i8 %16057, 1
  store i8 %16058, i8* %21, align 1
  %16059 = xor i32 %16050, %16049
  %16060 = lshr i32 %16059, 4
  %16061 = trunc i32 %16060 to i8
  %16062 = and i8 %16061, 1
  store i8 %16062, i8* %27, align 1
  %16063 = icmp eq i32 %16050, 0
  %16064 = zext i1 %16063 to i8
  store i8 %16064, i8* %30, align 1
  %16065 = lshr i32 %16050, 31
  %16066 = trunc i32 %16065 to i8
  store i8 %16066, i8* %33, align 1
  %16067 = lshr i32 %16049, 31
  %16068 = xor i32 %16065, %16067
  %16069 = add nuw nsw i32 %16068, %16065
  %16070 = icmp eq i32 %16069, 2
  %16071 = zext i1 %16070 to i8
  store i8 %16071, i8* %39, align 1
  %16072 = sext i32 %16050 to i64
  store i64 %16072, i64* %573, align 8
  %16073 = shl nsw i64 %16072, 1
  %16074 = add i64 %16045, %16073
  %16075 = add i64 %16042, 17
  store i64 %16075, i64* %3, align 8
  %16076 = inttoptr i64 %16074 to i16*
  %16077 = load i16, i16* %16076, align 2
  store i16 %16077, i16* %SI.i, align 2
  %16078 = add i64 %16040, -150
  %16079 = add i64 %16042, 24
  store i64 %16079, i64* %3, align 8
  %16080 = inttoptr i64 %16078 to i16*
  store i16 %16077, i16* %16080, align 2
  %16081 = load i64, i64* %RBP.i, align 8
  %16082 = add i64 %16081, -8
  %16083 = load i64, i64* %3, align 8
  %16084 = add i64 %16083, 4
  store i64 %16084, i64* %3, align 8
  %16085 = inttoptr i64 %16082 to i64*
  %16086 = load i64, i64* %16085, align 8
  %16087 = add i64 %16086, 51640
  store i64 %16087, i64* %RAX.i11582.pre-phi, align 8
  %16088 = icmp ugt i64 %16086, -51641
  %16089 = zext i1 %16088 to i8
  store i8 %16089, i8* %14, align 1
  %16090 = trunc i64 %16087 to i32
  %16091 = and i32 %16090, 255
  %16092 = tail call i32 @llvm.ctpop.i32(i32 %16091)
  %16093 = trunc i32 %16092 to i8
  %16094 = and i8 %16093, 1
  %16095 = xor i8 %16094, 1
  store i8 %16095, i8* %21, align 1
  %16096 = xor i64 %16086, 16
  %16097 = xor i64 %16096, %16087
  %16098 = lshr i64 %16097, 4
  %16099 = trunc i64 %16098 to i8
  %16100 = and i8 %16099, 1
  store i8 %16100, i8* %27, align 1
  %16101 = icmp eq i64 %16087, 0
  %16102 = zext i1 %16101 to i8
  store i8 %16102, i8* %30, align 1
  %16103 = lshr i64 %16087, 63
  %16104 = trunc i64 %16103 to i8
  store i8 %16104, i8* %33, align 1
  %16105 = lshr i64 %16086, 63
  %16106 = xor i64 %16103, %16105
  %16107 = add nuw nsw i64 %16106, %16103
  %16108 = icmp eq i64 %16107, 2
  %16109 = zext i1 %16108 to i8
  store i8 %16109, i8* %39, align 1
  %16110 = add i64 %16081, -150
  %16111 = add i64 %16083, 17
  store i64 %16111, i64* %3, align 8
  %16112 = inttoptr i64 %16110 to i16*
  %16113 = load i16, i16* %16112, align 2
  %16114 = zext i16 %16113 to i64
  store i64 %16114, i64* %RCX.i11580, align 8
  %16115 = zext i16 %16113 to i64
  %16116 = shl nuw nsw i64 %16115, 4
  store i64 %16116, i64* %573, align 8
  %16117 = add i64 %16116, %16087
  store i64 %16117, i64* %RAX.i11582.pre-phi, align 8
  %16118 = icmp ult i64 %16117, %16087
  %16119 = icmp ult i64 %16117, %16116
  %16120 = or i1 %16118, %16119
  %16121 = zext i1 %16120 to i8
  store i8 %16121, i8* %14, align 1
  %16122 = trunc i64 %16117 to i32
  %16123 = and i32 %16122, 255
  %16124 = tail call i32 @llvm.ctpop.i32(i32 %16123)
  %16125 = trunc i32 %16124 to i8
  %16126 = and i8 %16125, 1
  %16127 = xor i8 %16126, 1
  store i8 %16127, i8* %21, align 1
  %16128 = xor i64 %16116, %16087
  %16129 = xor i64 %16128, %16117
  %16130 = lshr i64 %16129, 4
  %16131 = trunc i64 %16130 to i8
  %16132 = and i8 %16131, 1
  store i8 %16132, i8* %27, align 1
  %16133 = icmp eq i64 %16117, 0
  %16134 = zext i1 %16133 to i8
  store i8 %16134, i8* %30, align 1
  %16135 = lshr i64 %16117, 63
  %16136 = trunc i64 %16135 to i8
  store i8 %16136, i8* %33, align 1
  %16137 = xor i64 %16135, %16103
  %16138 = add nuw nsw i64 %16137, %16135
  %16139 = icmp eq i64 %16138, 2
  %16140 = zext i1 %16139 to i8
  store i8 %16140, i8* %39, align 1
  %16141 = inttoptr i64 %16117 to i32*
  %16142 = add i64 %16083, 28
  store i64 %16142, i64* %3, align 8
  %16143 = load i32, i32* %16141, align 4
  %16144 = zext i32 %16143 to i64
  store i64 %16144, i64* %RCX.i11580, align 8
  %16145 = load i64, i64* %RBP.i, align 8
  %16146 = add i64 %16145, -140
  %16147 = add i64 %16083, 34
  store i64 %16147, i64* %3, align 8
  %16148 = inttoptr i64 %16146 to i32*
  %16149 = load i32, i32* %16148, align 4
  %16150 = add i32 %16149, %16143
  %16151 = zext i32 %16150 to i64
  store i64 %16151, i64* %RCX.i11580, align 8
  %16152 = icmp ult i32 %16150, %16143
  %16153 = icmp ult i32 %16150, %16149
  %16154 = or i1 %16152, %16153
  %16155 = zext i1 %16154 to i8
  store i8 %16155, i8* %14, align 1
  %16156 = and i32 %16150, 255
  %16157 = tail call i32 @llvm.ctpop.i32(i32 %16156)
  %16158 = trunc i32 %16157 to i8
  %16159 = and i8 %16158, 1
  %16160 = xor i8 %16159, 1
  store i8 %16160, i8* %21, align 1
  %16161 = xor i32 %16149, %16143
  %16162 = xor i32 %16161, %16150
  %16163 = lshr i32 %16162, 4
  %16164 = trunc i32 %16163 to i8
  %16165 = and i8 %16164, 1
  store i8 %16165, i8* %27, align 1
  %16166 = icmp eq i32 %16150, 0
  %16167 = zext i1 %16166 to i8
  store i8 %16167, i8* %30, align 1
  %16168 = lshr i32 %16150, 31
  %16169 = trunc i32 %16168 to i8
  store i8 %16169, i8* %33, align 1
  %16170 = lshr i32 %16143, 31
  %16171 = lshr i32 %16149, 31
  %16172 = xor i32 %16168, %16170
  %16173 = xor i32 %16168, %16171
  %16174 = add nuw nsw i32 %16172, %16173
  %16175 = icmp eq i32 %16174, 2
  %16176 = zext i1 %16175 to i8
  store i8 %16176, i8* %39, align 1
  %16177 = add i64 %16083, 40
  store i64 %16177, i64* %3, align 8
  store i32 %16150, i32* %16148, align 4
  %16178 = load i64, i64* %RBP.i, align 8
  %16179 = add i64 %16178, -8
  %16180 = load i64, i64* %3, align 8
  %16181 = add i64 %16180, 4
  store i64 %16181, i64* %3, align 8
  %16182 = inttoptr i64 %16179 to i64*
  %16183 = load i64, i64* %16182, align 8
  %16184 = add i64 %16183, 51640
  store i64 %16184, i64* %RAX.i11582.pre-phi, align 8
  %16185 = icmp ugt i64 %16183, -51641
  %16186 = zext i1 %16185 to i8
  store i8 %16186, i8* %14, align 1
  %16187 = trunc i64 %16184 to i32
  %16188 = and i32 %16187, 255
  %16189 = tail call i32 @llvm.ctpop.i32(i32 %16188)
  %16190 = trunc i32 %16189 to i8
  %16191 = and i8 %16190, 1
  %16192 = xor i8 %16191, 1
  store i8 %16192, i8* %21, align 1
  %16193 = xor i64 %16183, 16
  %16194 = xor i64 %16193, %16184
  %16195 = lshr i64 %16194, 4
  %16196 = trunc i64 %16195 to i8
  %16197 = and i8 %16196, 1
  store i8 %16197, i8* %27, align 1
  %16198 = icmp eq i64 %16184, 0
  %16199 = zext i1 %16198 to i8
  store i8 %16199, i8* %30, align 1
  %16200 = lshr i64 %16184, 63
  %16201 = trunc i64 %16200 to i8
  store i8 %16201, i8* %33, align 1
  %16202 = lshr i64 %16183, 63
  %16203 = xor i64 %16200, %16202
  %16204 = add nuw nsw i64 %16203, %16200
  %16205 = icmp eq i64 %16204, 2
  %16206 = zext i1 %16205 to i8
  store i8 %16206, i8* %39, align 1
  %16207 = add i64 %16178, -150
  %16208 = add i64 %16180, 17
  store i64 %16208, i64* %3, align 8
  %16209 = inttoptr i64 %16207 to i16*
  %16210 = load i16, i16* %16209, align 2
  %16211 = zext i16 %16210 to i64
  store i64 %16211, i64* %RCX.i11580, align 8
  %16212 = zext i16 %16210 to i64
  %16213 = shl nuw nsw i64 %16212, 4
  store i64 %16213, i64* %573, align 8
  %16214 = add i64 %16213, %16184
  store i64 %16214, i64* %RAX.i11582.pre-phi, align 8
  %16215 = icmp ult i64 %16214, %16184
  %16216 = icmp ult i64 %16214, %16213
  %16217 = or i1 %16215, %16216
  %16218 = zext i1 %16217 to i8
  store i8 %16218, i8* %14, align 1
  %16219 = trunc i64 %16214 to i32
  %16220 = and i32 %16219, 255
  %16221 = tail call i32 @llvm.ctpop.i32(i32 %16220)
  %16222 = trunc i32 %16221 to i8
  %16223 = and i8 %16222, 1
  %16224 = xor i8 %16223, 1
  store i8 %16224, i8* %21, align 1
  %16225 = xor i64 %16213, %16184
  %16226 = xor i64 %16225, %16214
  %16227 = lshr i64 %16226, 4
  %16228 = trunc i64 %16227 to i8
  %16229 = and i8 %16228, 1
  store i8 %16229, i8* %27, align 1
  %16230 = icmp eq i64 %16214, 0
  %16231 = zext i1 %16230 to i8
  store i8 %16231, i8* %30, align 1
  %16232 = lshr i64 %16214, 63
  %16233 = trunc i64 %16232 to i8
  store i8 %16233, i8* %33, align 1
  %16234 = xor i64 %16232, %16200
  %16235 = add nuw nsw i64 %16234, %16232
  %16236 = icmp eq i64 %16235, 2
  %16237 = zext i1 %16236 to i8
  store i8 %16237, i8* %39, align 1
  %16238 = add i64 %16214, 4
  %16239 = add i64 %16180, 29
  store i64 %16239, i64* %3, align 8
  %16240 = inttoptr i64 %16238 to i32*
  %16241 = load i32, i32* %16240, align 4
  %16242 = zext i32 %16241 to i64
  store i64 %16242, i64* %RCX.i11580, align 8
  %16243 = load i64, i64* %RBP.i, align 8
  %16244 = add i64 %16243, -144
  %16245 = add i64 %16180, 35
  store i64 %16245, i64* %3, align 8
  %16246 = inttoptr i64 %16244 to i32*
  %16247 = load i32, i32* %16246, align 4
  %16248 = add i32 %16247, %16241
  %16249 = zext i32 %16248 to i64
  store i64 %16249, i64* %RCX.i11580, align 8
  %16250 = icmp ult i32 %16248, %16241
  %16251 = icmp ult i32 %16248, %16247
  %16252 = or i1 %16250, %16251
  %16253 = zext i1 %16252 to i8
  store i8 %16253, i8* %14, align 1
  %16254 = and i32 %16248, 255
  %16255 = tail call i32 @llvm.ctpop.i32(i32 %16254)
  %16256 = trunc i32 %16255 to i8
  %16257 = and i8 %16256, 1
  %16258 = xor i8 %16257, 1
  store i8 %16258, i8* %21, align 1
  %16259 = xor i32 %16247, %16241
  %16260 = xor i32 %16259, %16248
  %16261 = lshr i32 %16260, 4
  %16262 = trunc i32 %16261 to i8
  %16263 = and i8 %16262, 1
  store i8 %16263, i8* %27, align 1
  %16264 = icmp eq i32 %16248, 0
  %16265 = zext i1 %16264 to i8
  store i8 %16265, i8* %30, align 1
  %16266 = lshr i32 %16248, 31
  %16267 = trunc i32 %16266 to i8
  store i8 %16267, i8* %33, align 1
  %16268 = lshr i32 %16241, 31
  %16269 = lshr i32 %16247, 31
  %16270 = xor i32 %16266, %16268
  %16271 = xor i32 %16266, %16269
  %16272 = add nuw nsw i32 %16270, %16271
  %16273 = icmp eq i32 %16272, 2
  %16274 = zext i1 %16273 to i8
  store i8 %16274, i8* %39, align 1
  %16275 = add i64 %16180, 41
  store i64 %16275, i64* %3, align 8
  store i32 %16248, i32* %16246, align 4
  %16276 = load i64, i64* %RBP.i, align 8
  %16277 = add i64 %16276, -8
  %16278 = load i64, i64* %3, align 8
  %16279 = add i64 %16278, 4
  store i64 %16279, i64* %3, align 8
  %16280 = inttoptr i64 %16277 to i64*
  %16281 = load i64, i64* %16280, align 8
  %16282 = add i64 %16281, 51640
  store i64 %16282, i64* %RAX.i11582.pre-phi, align 8
  %16283 = icmp ugt i64 %16281, -51641
  %16284 = zext i1 %16283 to i8
  store i8 %16284, i8* %14, align 1
  %16285 = trunc i64 %16282 to i32
  %16286 = and i32 %16285, 255
  %16287 = tail call i32 @llvm.ctpop.i32(i32 %16286)
  %16288 = trunc i32 %16287 to i8
  %16289 = and i8 %16288, 1
  %16290 = xor i8 %16289, 1
  store i8 %16290, i8* %21, align 1
  %16291 = xor i64 %16281, 16
  %16292 = xor i64 %16291, %16282
  %16293 = lshr i64 %16292, 4
  %16294 = trunc i64 %16293 to i8
  %16295 = and i8 %16294, 1
  store i8 %16295, i8* %27, align 1
  %16296 = icmp eq i64 %16282, 0
  %16297 = zext i1 %16296 to i8
  store i8 %16297, i8* %30, align 1
  %16298 = lshr i64 %16282, 63
  %16299 = trunc i64 %16298 to i8
  store i8 %16299, i8* %33, align 1
  %16300 = lshr i64 %16281, 63
  %16301 = xor i64 %16298, %16300
  %16302 = add nuw nsw i64 %16301, %16298
  %16303 = icmp eq i64 %16302, 2
  %16304 = zext i1 %16303 to i8
  store i8 %16304, i8* %39, align 1
  %16305 = add i64 %16276, -150
  %16306 = add i64 %16278, 17
  store i64 %16306, i64* %3, align 8
  %16307 = inttoptr i64 %16305 to i16*
  %16308 = load i16, i16* %16307, align 2
  %16309 = zext i16 %16308 to i64
  store i64 %16309, i64* %RCX.i11580, align 8
  %16310 = zext i16 %16308 to i64
  %16311 = shl nuw nsw i64 %16310, 4
  store i64 %16311, i64* %573, align 8
  %16312 = add i64 %16311, %16282
  store i64 %16312, i64* %RAX.i11582.pre-phi, align 8
  %16313 = icmp ult i64 %16312, %16282
  %16314 = icmp ult i64 %16312, %16311
  %16315 = or i1 %16313, %16314
  %16316 = zext i1 %16315 to i8
  store i8 %16316, i8* %14, align 1
  %16317 = trunc i64 %16312 to i32
  %16318 = and i32 %16317, 255
  %16319 = tail call i32 @llvm.ctpop.i32(i32 %16318)
  %16320 = trunc i32 %16319 to i8
  %16321 = and i8 %16320, 1
  %16322 = xor i8 %16321, 1
  store i8 %16322, i8* %21, align 1
  %16323 = xor i64 %16311, %16282
  %16324 = xor i64 %16323, %16312
  %16325 = lshr i64 %16324, 4
  %16326 = trunc i64 %16325 to i8
  %16327 = and i8 %16326, 1
  store i8 %16327, i8* %27, align 1
  %16328 = icmp eq i64 %16312, 0
  %16329 = zext i1 %16328 to i8
  store i8 %16329, i8* %30, align 1
  %16330 = lshr i64 %16312, 63
  %16331 = trunc i64 %16330 to i8
  store i8 %16331, i8* %33, align 1
  %16332 = xor i64 %16330, %16298
  %16333 = add nuw nsw i64 %16332, %16330
  %16334 = icmp eq i64 %16333, 2
  %16335 = zext i1 %16334 to i8
  store i8 %16335, i8* %39, align 1
  %16336 = add i64 %16312, 8
  %16337 = add i64 %16278, 29
  store i64 %16337, i64* %3, align 8
  %16338 = inttoptr i64 %16336 to i32*
  %16339 = load i32, i32* %16338, align 4
  %16340 = zext i32 %16339 to i64
  store i64 %16340, i64* %RCX.i11580, align 8
  %16341 = load i64, i64* %RBP.i, align 8
  %16342 = add i64 %16341, -148
  %16343 = add i64 %16278, 35
  store i64 %16343, i64* %3, align 8
  %16344 = inttoptr i64 %16342 to i32*
  %16345 = load i32, i32* %16344, align 4
  %16346 = add i32 %16345, %16339
  %16347 = zext i32 %16346 to i64
  store i64 %16347, i64* %RCX.i11580, align 8
  %16348 = icmp ult i32 %16346, %16339
  %16349 = icmp ult i32 %16346, %16345
  %16350 = or i1 %16348, %16349
  %16351 = zext i1 %16350 to i8
  store i8 %16351, i8* %14, align 1
  %16352 = and i32 %16346, 255
  %16353 = tail call i32 @llvm.ctpop.i32(i32 %16352)
  %16354 = trunc i32 %16353 to i8
  %16355 = and i8 %16354, 1
  %16356 = xor i8 %16355, 1
  store i8 %16356, i8* %21, align 1
  %16357 = xor i32 %16345, %16339
  %16358 = xor i32 %16357, %16346
  %16359 = lshr i32 %16358, 4
  %16360 = trunc i32 %16359 to i8
  %16361 = and i8 %16360, 1
  store i8 %16361, i8* %27, align 1
  %16362 = icmp eq i32 %16346, 0
  %16363 = zext i1 %16362 to i8
  store i8 %16363, i8* %30, align 1
  %16364 = lshr i32 %16346, 31
  %16365 = trunc i32 %16364 to i8
  store i8 %16365, i8* %33, align 1
  %16366 = lshr i32 %16339, 31
  %16367 = lshr i32 %16345, 31
  %16368 = xor i32 %16364, %16366
  %16369 = xor i32 %16364, %16367
  %16370 = add nuw nsw i32 %16368, %16369
  %16371 = icmp eq i32 %16370, 2
  %16372 = zext i1 %16371 to i8
  store i8 %16372, i8* %39, align 1
  %16373 = add i64 %16278, 41
  store i64 %16373, i64* %3, align 8
  store i32 %16346, i32* %16344, align 4
  %16374 = load i64, i64* %RBP.i, align 8
  %16375 = add i64 %16374, -120
  %16376 = load i64, i64* %3, align 8
  %16377 = add i64 %16376, 4
  store i64 %16377, i64* %3, align 8
  %16378 = inttoptr i64 %16375 to i64*
  %16379 = load i64, i64* %16378, align 8
  store i64 %16379, i64* %RAX.i11582.pre-phi, align 8
  %16380 = add i64 %16374, -28
  %16381 = add i64 %16376, 7
  store i64 %16381, i64* %3, align 8
  %16382 = inttoptr i64 %16380 to i32*
  %16383 = load i32, i32* %16382, align 4
  %16384 = add i32 %16383, 41
  %16385 = zext i32 %16384 to i64
  store i64 %16385, i64* %RCX.i11580, align 8
  %16386 = icmp ugt i32 %16383, -42
  %16387 = zext i1 %16386 to i8
  store i8 %16387, i8* %14, align 1
  %16388 = and i32 %16384, 255
  %16389 = tail call i32 @llvm.ctpop.i32(i32 %16388)
  %16390 = trunc i32 %16389 to i8
  %16391 = and i8 %16390, 1
  %16392 = xor i8 %16391, 1
  store i8 %16392, i8* %21, align 1
  %16393 = xor i32 %16384, %16383
  %16394 = lshr i32 %16393, 4
  %16395 = trunc i32 %16394 to i8
  %16396 = and i8 %16395, 1
  store i8 %16396, i8* %27, align 1
  %16397 = icmp eq i32 %16384, 0
  %16398 = zext i1 %16397 to i8
  store i8 %16398, i8* %30, align 1
  %16399 = lshr i32 %16384, 31
  %16400 = trunc i32 %16399 to i8
  store i8 %16400, i8* %33, align 1
  %16401 = lshr i32 %16383, 31
  %16402 = xor i32 %16399, %16401
  %16403 = add nuw nsw i32 %16402, %16399
  %16404 = icmp eq i32 %16403, 2
  %16405 = zext i1 %16404 to i8
  store i8 %16405, i8* %39, align 1
  %16406 = sext i32 %16384 to i64
  store i64 %16406, i64* %573, align 8
  %16407 = shl nsw i64 %16406, 1
  %16408 = add i64 %16379, %16407
  %16409 = add i64 %16376, 17
  store i64 %16409, i64* %3, align 8
  %16410 = inttoptr i64 %16408 to i16*
  %16411 = load i16, i16* %16410, align 2
  store i16 %16411, i16* %SI.i, align 2
  %16412 = add i64 %16374, -150
  %16413 = add i64 %16376, 24
  store i64 %16413, i64* %3, align 8
  %16414 = inttoptr i64 %16412 to i16*
  store i16 %16411, i16* %16414, align 2
  %16415 = load i64, i64* %RBP.i, align 8
  %16416 = add i64 %16415, -8
  %16417 = load i64, i64* %3, align 8
  %16418 = add i64 %16417, 4
  store i64 %16418, i64* %3, align 8
  %16419 = inttoptr i64 %16416 to i64*
  %16420 = load i64, i64* %16419, align 8
  %16421 = add i64 %16420, 51640
  store i64 %16421, i64* %RAX.i11582.pre-phi, align 8
  %16422 = icmp ugt i64 %16420, -51641
  %16423 = zext i1 %16422 to i8
  store i8 %16423, i8* %14, align 1
  %16424 = trunc i64 %16421 to i32
  %16425 = and i32 %16424, 255
  %16426 = tail call i32 @llvm.ctpop.i32(i32 %16425)
  %16427 = trunc i32 %16426 to i8
  %16428 = and i8 %16427, 1
  %16429 = xor i8 %16428, 1
  store i8 %16429, i8* %21, align 1
  %16430 = xor i64 %16420, 16
  %16431 = xor i64 %16430, %16421
  %16432 = lshr i64 %16431, 4
  %16433 = trunc i64 %16432 to i8
  %16434 = and i8 %16433, 1
  store i8 %16434, i8* %27, align 1
  %16435 = icmp eq i64 %16421, 0
  %16436 = zext i1 %16435 to i8
  store i8 %16436, i8* %30, align 1
  %16437 = lshr i64 %16421, 63
  %16438 = trunc i64 %16437 to i8
  store i8 %16438, i8* %33, align 1
  %16439 = lshr i64 %16420, 63
  %16440 = xor i64 %16437, %16439
  %16441 = add nuw nsw i64 %16440, %16437
  %16442 = icmp eq i64 %16441, 2
  %16443 = zext i1 %16442 to i8
  store i8 %16443, i8* %39, align 1
  %16444 = add i64 %16415, -150
  %16445 = add i64 %16417, 17
  store i64 %16445, i64* %3, align 8
  %16446 = inttoptr i64 %16444 to i16*
  %16447 = load i16, i16* %16446, align 2
  %16448 = zext i16 %16447 to i64
  store i64 %16448, i64* %RCX.i11580, align 8
  %16449 = zext i16 %16447 to i64
  %16450 = shl nuw nsw i64 %16449, 4
  store i64 %16450, i64* %573, align 8
  %16451 = add i64 %16450, %16421
  store i64 %16451, i64* %RAX.i11582.pre-phi, align 8
  %16452 = icmp ult i64 %16451, %16421
  %16453 = icmp ult i64 %16451, %16450
  %16454 = or i1 %16452, %16453
  %16455 = zext i1 %16454 to i8
  store i8 %16455, i8* %14, align 1
  %16456 = trunc i64 %16451 to i32
  %16457 = and i32 %16456, 255
  %16458 = tail call i32 @llvm.ctpop.i32(i32 %16457)
  %16459 = trunc i32 %16458 to i8
  %16460 = and i8 %16459, 1
  %16461 = xor i8 %16460, 1
  store i8 %16461, i8* %21, align 1
  %16462 = xor i64 %16450, %16421
  %16463 = xor i64 %16462, %16451
  %16464 = lshr i64 %16463, 4
  %16465 = trunc i64 %16464 to i8
  %16466 = and i8 %16465, 1
  store i8 %16466, i8* %27, align 1
  %16467 = icmp eq i64 %16451, 0
  %16468 = zext i1 %16467 to i8
  store i8 %16468, i8* %30, align 1
  %16469 = lshr i64 %16451, 63
  %16470 = trunc i64 %16469 to i8
  store i8 %16470, i8* %33, align 1
  %16471 = xor i64 %16469, %16437
  %16472 = add nuw nsw i64 %16471, %16469
  %16473 = icmp eq i64 %16472, 2
  %16474 = zext i1 %16473 to i8
  store i8 %16474, i8* %39, align 1
  %16475 = inttoptr i64 %16451 to i32*
  %16476 = add i64 %16417, 28
  store i64 %16476, i64* %3, align 8
  %16477 = load i32, i32* %16475, align 4
  %16478 = zext i32 %16477 to i64
  store i64 %16478, i64* %RCX.i11580, align 8
  %16479 = load i64, i64* %RBP.i, align 8
  %16480 = add i64 %16479, -140
  %16481 = add i64 %16417, 34
  store i64 %16481, i64* %3, align 8
  %16482 = inttoptr i64 %16480 to i32*
  %16483 = load i32, i32* %16482, align 4
  %16484 = add i32 %16483, %16477
  %16485 = zext i32 %16484 to i64
  store i64 %16485, i64* %RCX.i11580, align 8
  %16486 = icmp ult i32 %16484, %16477
  %16487 = icmp ult i32 %16484, %16483
  %16488 = or i1 %16486, %16487
  %16489 = zext i1 %16488 to i8
  store i8 %16489, i8* %14, align 1
  %16490 = and i32 %16484, 255
  %16491 = tail call i32 @llvm.ctpop.i32(i32 %16490)
  %16492 = trunc i32 %16491 to i8
  %16493 = and i8 %16492, 1
  %16494 = xor i8 %16493, 1
  store i8 %16494, i8* %21, align 1
  %16495 = xor i32 %16483, %16477
  %16496 = xor i32 %16495, %16484
  %16497 = lshr i32 %16496, 4
  %16498 = trunc i32 %16497 to i8
  %16499 = and i8 %16498, 1
  store i8 %16499, i8* %27, align 1
  %16500 = icmp eq i32 %16484, 0
  %16501 = zext i1 %16500 to i8
  store i8 %16501, i8* %30, align 1
  %16502 = lshr i32 %16484, 31
  %16503 = trunc i32 %16502 to i8
  store i8 %16503, i8* %33, align 1
  %16504 = lshr i32 %16477, 31
  %16505 = lshr i32 %16483, 31
  %16506 = xor i32 %16502, %16504
  %16507 = xor i32 %16502, %16505
  %16508 = add nuw nsw i32 %16506, %16507
  %16509 = icmp eq i32 %16508, 2
  %16510 = zext i1 %16509 to i8
  store i8 %16510, i8* %39, align 1
  %16511 = add i64 %16417, 40
  store i64 %16511, i64* %3, align 8
  store i32 %16484, i32* %16482, align 4
  %16512 = load i64, i64* %RBP.i, align 8
  %16513 = add i64 %16512, -8
  %16514 = load i64, i64* %3, align 8
  %16515 = add i64 %16514, 4
  store i64 %16515, i64* %3, align 8
  %16516 = inttoptr i64 %16513 to i64*
  %16517 = load i64, i64* %16516, align 8
  %16518 = add i64 %16517, 51640
  store i64 %16518, i64* %RAX.i11582.pre-phi, align 8
  %16519 = icmp ugt i64 %16517, -51641
  %16520 = zext i1 %16519 to i8
  store i8 %16520, i8* %14, align 1
  %16521 = trunc i64 %16518 to i32
  %16522 = and i32 %16521, 255
  %16523 = tail call i32 @llvm.ctpop.i32(i32 %16522)
  %16524 = trunc i32 %16523 to i8
  %16525 = and i8 %16524, 1
  %16526 = xor i8 %16525, 1
  store i8 %16526, i8* %21, align 1
  %16527 = xor i64 %16517, 16
  %16528 = xor i64 %16527, %16518
  %16529 = lshr i64 %16528, 4
  %16530 = trunc i64 %16529 to i8
  %16531 = and i8 %16530, 1
  store i8 %16531, i8* %27, align 1
  %16532 = icmp eq i64 %16518, 0
  %16533 = zext i1 %16532 to i8
  store i8 %16533, i8* %30, align 1
  %16534 = lshr i64 %16518, 63
  %16535 = trunc i64 %16534 to i8
  store i8 %16535, i8* %33, align 1
  %16536 = lshr i64 %16517, 63
  %16537 = xor i64 %16534, %16536
  %16538 = add nuw nsw i64 %16537, %16534
  %16539 = icmp eq i64 %16538, 2
  %16540 = zext i1 %16539 to i8
  store i8 %16540, i8* %39, align 1
  %16541 = add i64 %16512, -150
  %16542 = add i64 %16514, 17
  store i64 %16542, i64* %3, align 8
  %16543 = inttoptr i64 %16541 to i16*
  %16544 = load i16, i16* %16543, align 2
  %16545 = zext i16 %16544 to i64
  store i64 %16545, i64* %RCX.i11580, align 8
  %16546 = zext i16 %16544 to i64
  %16547 = shl nuw nsw i64 %16546, 4
  store i64 %16547, i64* %573, align 8
  %16548 = add i64 %16547, %16518
  store i64 %16548, i64* %RAX.i11582.pre-phi, align 8
  %16549 = icmp ult i64 %16548, %16518
  %16550 = icmp ult i64 %16548, %16547
  %16551 = or i1 %16549, %16550
  %16552 = zext i1 %16551 to i8
  store i8 %16552, i8* %14, align 1
  %16553 = trunc i64 %16548 to i32
  %16554 = and i32 %16553, 255
  %16555 = tail call i32 @llvm.ctpop.i32(i32 %16554)
  %16556 = trunc i32 %16555 to i8
  %16557 = and i8 %16556, 1
  %16558 = xor i8 %16557, 1
  store i8 %16558, i8* %21, align 1
  %16559 = xor i64 %16547, %16518
  %16560 = xor i64 %16559, %16548
  %16561 = lshr i64 %16560, 4
  %16562 = trunc i64 %16561 to i8
  %16563 = and i8 %16562, 1
  store i8 %16563, i8* %27, align 1
  %16564 = icmp eq i64 %16548, 0
  %16565 = zext i1 %16564 to i8
  store i8 %16565, i8* %30, align 1
  %16566 = lshr i64 %16548, 63
  %16567 = trunc i64 %16566 to i8
  store i8 %16567, i8* %33, align 1
  %16568 = xor i64 %16566, %16534
  %16569 = add nuw nsw i64 %16568, %16566
  %16570 = icmp eq i64 %16569, 2
  %16571 = zext i1 %16570 to i8
  store i8 %16571, i8* %39, align 1
  %16572 = add i64 %16548, 4
  %16573 = add i64 %16514, 29
  store i64 %16573, i64* %3, align 8
  %16574 = inttoptr i64 %16572 to i32*
  %16575 = load i32, i32* %16574, align 4
  %16576 = zext i32 %16575 to i64
  store i64 %16576, i64* %RCX.i11580, align 8
  %16577 = load i64, i64* %RBP.i, align 8
  %16578 = add i64 %16577, -144
  %16579 = add i64 %16514, 35
  store i64 %16579, i64* %3, align 8
  %16580 = inttoptr i64 %16578 to i32*
  %16581 = load i32, i32* %16580, align 4
  %16582 = add i32 %16581, %16575
  %16583 = zext i32 %16582 to i64
  store i64 %16583, i64* %RCX.i11580, align 8
  %16584 = icmp ult i32 %16582, %16575
  %16585 = icmp ult i32 %16582, %16581
  %16586 = or i1 %16584, %16585
  %16587 = zext i1 %16586 to i8
  store i8 %16587, i8* %14, align 1
  %16588 = and i32 %16582, 255
  %16589 = tail call i32 @llvm.ctpop.i32(i32 %16588)
  %16590 = trunc i32 %16589 to i8
  %16591 = and i8 %16590, 1
  %16592 = xor i8 %16591, 1
  store i8 %16592, i8* %21, align 1
  %16593 = xor i32 %16581, %16575
  %16594 = xor i32 %16593, %16582
  %16595 = lshr i32 %16594, 4
  %16596 = trunc i32 %16595 to i8
  %16597 = and i8 %16596, 1
  store i8 %16597, i8* %27, align 1
  %16598 = icmp eq i32 %16582, 0
  %16599 = zext i1 %16598 to i8
  store i8 %16599, i8* %30, align 1
  %16600 = lshr i32 %16582, 31
  %16601 = trunc i32 %16600 to i8
  store i8 %16601, i8* %33, align 1
  %16602 = lshr i32 %16575, 31
  %16603 = lshr i32 %16581, 31
  %16604 = xor i32 %16600, %16602
  %16605 = xor i32 %16600, %16603
  %16606 = add nuw nsw i32 %16604, %16605
  %16607 = icmp eq i32 %16606, 2
  %16608 = zext i1 %16607 to i8
  store i8 %16608, i8* %39, align 1
  %16609 = add i64 %16514, 41
  store i64 %16609, i64* %3, align 8
  store i32 %16582, i32* %16580, align 4
  %16610 = load i64, i64* %RBP.i, align 8
  %16611 = add i64 %16610, -8
  %16612 = load i64, i64* %3, align 8
  %16613 = add i64 %16612, 4
  store i64 %16613, i64* %3, align 8
  %16614 = inttoptr i64 %16611 to i64*
  %16615 = load i64, i64* %16614, align 8
  %16616 = add i64 %16615, 51640
  store i64 %16616, i64* %RAX.i11582.pre-phi, align 8
  %16617 = icmp ugt i64 %16615, -51641
  %16618 = zext i1 %16617 to i8
  store i8 %16618, i8* %14, align 1
  %16619 = trunc i64 %16616 to i32
  %16620 = and i32 %16619, 255
  %16621 = tail call i32 @llvm.ctpop.i32(i32 %16620)
  %16622 = trunc i32 %16621 to i8
  %16623 = and i8 %16622, 1
  %16624 = xor i8 %16623, 1
  store i8 %16624, i8* %21, align 1
  %16625 = xor i64 %16615, 16
  %16626 = xor i64 %16625, %16616
  %16627 = lshr i64 %16626, 4
  %16628 = trunc i64 %16627 to i8
  %16629 = and i8 %16628, 1
  store i8 %16629, i8* %27, align 1
  %16630 = icmp eq i64 %16616, 0
  %16631 = zext i1 %16630 to i8
  store i8 %16631, i8* %30, align 1
  %16632 = lshr i64 %16616, 63
  %16633 = trunc i64 %16632 to i8
  store i8 %16633, i8* %33, align 1
  %16634 = lshr i64 %16615, 63
  %16635 = xor i64 %16632, %16634
  %16636 = add nuw nsw i64 %16635, %16632
  %16637 = icmp eq i64 %16636, 2
  %16638 = zext i1 %16637 to i8
  store i8 %16638, i8* %39, align 1
  %16639 = add i64 %16610, -150
  %16640 = add i64 %16612, 17
  store i64 %16640, i64* %3, align 8
  %16641 = inttoptr i64 %16639 to i16*
  %16642 = load i16, i16* %16641, align 2
  %16643 = zext i16 %16642 to i64
  store i64 %16643, i64* %RCX.i11580, align 8
  %16644 = zext i16 %16642 to i64
  %16645 = shl nuw nsw i64 %16644, 4
  store i64 %16645, i64* %573, align 8
  %16646 = add i64 %16645, %16616
  store i64 %16646, i64* %RAX.i11582.pre-phi, align 8
  %16647 = icmp ult i64 %16646, %16616
  %16648 = icmp ult i64 %16646, %16645
  %16649 = or i1 %16647, %16648
  %16650 = zext i1 %16649 to i8
  store i8 %16650, i8* %14, align 1
  %16651 = trunc i64 %16646 to i32
  %16652 = and i32 %16651, 255
  %16653 = tail call i32 @llvm.ctpop.i32(i32 %16652)
  %16654 = trunc i32 %16653 to i8
  %16655 = and i8 %16654, 1
  %16656 = xor i8 %16655, 1
  store i8 %16656, i8* %21, align 1
  %16657 = xor i64 %16645, %16616
  %16658 = xor i64 %16657, %16646
  %16659 = lshr i64 %16658, 4
  %16660 = trunc i64 %16659 to i8
  %16661 = and i8 %16660, 1
  store i8 %16661, i8* %27, align 1
  %16662 = icmp eq i64 %16646, 0
  %16663 = zext i1 %16662 to i8
  store i8 %16663, i8* %30, align 1
  %16664 = lshr i64 %16646, 63
  %16665 = trunc i64 %16664 to i8
  store i8 %16665, i8* %33, align 1
  %16666 = xor i64 %16664, %16632
  %16667 = add nuw nsw i64 %16666, %16664
  %16668 = icmp eq i64 %16667, 2
  %16669 = zext i1 %16668 to i8
  store i8 %16669, i8* %39, align 1
  %16670 = add i64 %16646, 8
  %16671 = add i64 %16612, 29
  store i64 %16671, i64* %3, align 8
  %16672 = inttoptr i64 %16670 to i32*
  %16673 = load i32, i32* %16672, align 4
  %16674 = zext i32 %16673 to i64
  store i64 %16674, i64* %RCX.i11580, align 8
  %16675 = load i64, i64* %RBP.i, align 8
  %16676 = add i64 %16675, -148
  %16677 = add i64 %16612, 35
  store i64 %16677, i64* %3, align 8
  %16678 = inttoptr i64 %16676 to i32*
  %16679 = load i32, i32* %16678, align 4
  %16680 = add i32 %16679, %16673
  %16681 = zext i32 %16680 to i64
  store i64 %16681, i64* %RCX.i11580, align 8
  %16682 = icmp ult i32 %16680, %16673
  %16683 = icmp ult i32 %16680, %16679
  %16684 = or i1 %16682, %16683
  %16685 = zext i1 %16684 to i8
  store i8 %16685, i8* %14, align 1
  %16686 = and i32 %16680, 255
  %16687 = tail call i32 @llvm.ctpop.i32(i32 %16686)
  %16688 = trunc i32 %16687 to i8
  %16689 = and i8 %16688, 1
  %16690 = xor i8 %16689, 1
  store i8 %16690, i8* %21, align 1
  %16691 = xor i32 %16679, %16673
  %16692 = xor i32 %16691, %16680
  %16693 = lshr i32 %16692, 4
  %16694 = trunc i32 %16693 to i8
  %16695 = and i8 %16694, 1
  store i8 %16695, i8* %27, align 1
  %16696 = icmp eq i32 %16680, 0
  %16697 = zext i1 %16696 to i8
  store i8 %16697, i8* %30, align 1
  %16698 = lshr i32 %16680, 31
  %16699 = trunc i32 %16698 to i8
  store i8 %16699, i8* %33, align 1
  %16700 = lshr i32 %16673, 31
  %16701 = lshr i32 %16679, 31
  %16702 = xor i32 %16698, %16700
  %16703 = xor i32 %16698, %16701
  %16704 = add nuw nsw i32 %16702, %16703
  %16705 = icmp eq i32 %16704, 2
  %16706 = zext i1 %16705 to i8
  store i8 %16706, i8* %39, align 1
  %16707 = add i64 %16612, 41
  store i64 %16707, i64* %3, align 8
  store i32 %16680, i32* %16678, align 4
  %16708 = load i64, i64* %RBP.i, align 8
  %16709 = add i64 %16708, -120
  %16710 = load i64, i64* %3, align 8
  %16711 = add i64 %16710, 4
  store i64 %16711, i64* %3, align 8
  %16712 = inttoptr i64 %16709 to i64*
  %16713 = load i64, i64* %16712, align 8
  store i64 %16713, i64* %RAX.i11582.pre-phi, align 8
  %16714 = add i64 %16708, -28
  %16715 = add i64 %16710, 7
  store i64 %16715, i64* %3, align 8
  %16716 = inttoptr i64 %16714 to i32*
  %16717 = load i32, i32* %16716, align 4
  %16718 = add i32 %16717, 42
  %16719 = zext i32 %16718 to i64
  store i64 %16719, i64* %RCX.i11580, align 8
  %16720 = icmp ugt i32 %16717, -43
  %16721 = zext i1 %16720 to i8
  store i8 %16721, i8* %14, align 1
  %16722 = and i32 %16718, 255
  %16723 = tail call i32 @llvm.ctpop.i32(i32 %16722)
  %16724 = trunc i32 %16723 to i8
  %16725 = and i8 %16724, 1
  %16726 = xor i8 %16725, 1
  store i8 %16726, i8* %21, align 1
  %16727 = xor i32 %16718, %16717
  %16728 = lshr i32 %16727, 4
  %16729 = trunc i32 %16728 to i8
  %16730 = and i8 %16729, 1
  store i8 %16730, i8* %27, align 1
  %16731 = icmp eq i32 %16718, 0
  %16732 = zext i1 %16731 to i8
  store i8 %16732, i8* %30, align 1
  %16733 = lshr i32 %16718, 31
  %16734 = trunc i32 %16733 to i8
  store i8 %16734, i8* %33, align 1
  %16735 = lshr i32 %16717, 31
  %16736 = xor i32 %16733, %16735
  %16737 = add nuw nsw i32 %16736, %16733
  %16738 = icmp eq i32 %16737, 2
  %16739 = zext i1 %16738 to i8
  store i8 %16739, i8* %39, align 1
  %16740 = sext i32 %16718 to i64
  store i64 %16740, i64* %573, align 8
  %16741 = shl nsw i64 %16740, 1
  %16742 = add i64 %16713, %16741
  %16743 = add i64 %16710, 17
  store i64 %16743, i64* %3, align 8
  %16744 = inttoptr i64 %16742 to i16*
  %16745 = load i16, i16* %16744, align 2
  store i16 %16745, i16* %SI.i, align 2
  %16746 = add i64 %16708, -150
  %16747 = add i64 %16710, 24
  store i64 %16747, i64* %3, align 8
  %16748 = inttoptr i64 %16746 to i16*
  store i16 %16745, i16* %16748, align 2
  %16749 = load i64, i64* %RBP.i, align 8
  %16750 = add i64 %16749, -8
  %16751 = load i64, i64* %3, align 8
  %16752 = add i64 %16751, 4
  store i64 %16752, i64* %3, align 8
  %16753 = inttoptr i64 %16750 to i64*
  %16754 = load i64, i64* %16753, align 8
  %16755 = add i64 %16754, 51640
  store i64 %16755, i64* %RAX.i11582.pre-phi, align 8
  %16756 = icmp ugt i64 %16754, -51641
  %16757 = zext i1 %16756 to i8
  store i8 %16757, i8* %14, align 1
  %16758 = trunc i64 %16755 to i32
  %16759 = and i32 %16758, 255
  %16760 = tail call i32 @llvm.ctpop.i32(i32 %16759)
  %16761 = trunc i32 %16760 to i8
  %16762 = and i8 %16761, 1
  %16763 = xor i8 %16762, 1
  store i8 %16763, i8* %21, align 1
  %16764 = xor i64 %16754, 16
  %16765 = xor i64 %16764, %16755
  %16766 = lshr i64 %16765, 4
  %16767 = trunc i64 %16766 to i8
  %16768 = and i8 %16767, 1
  store i8 %16768, i8* %27, align 1
  %16769 = icmp eq i64 %16755, 0
  %16770 = zext i1 %16769 to i8
  store i8 %16770, i8* %30, align 1
  %16771 = lshr i64 %16755, 63
  %16772 = trunc i64 %16771 to i8
  store i8 %16772, i8* %33, align 1
  %16773 = lshr i64 %16754, 63
  %16774 = xor i64 %16771, %16773
  %16775 = add nuw nsw i64 %16774, %16771
  %16776 = icmp eq i64 %16775, 2
  %16777 = zext i1 %16776 to i8
  store i8 %16777, i8* %39, align 1
  %16778 = add i64 %16749, -150
  %16779 = add i64 %16751, 17
  store i64 %16779, i64* %3, align 8
  %16780 = inttoptr i64 %16778 to i16*
  %16781 = load i16, i16* %16780, align 2
  %16782 = zext i16 %16781 to i64
  store i64 %16782, i64* %RCX.i11580, align 8
  %16783 = zext i16 %16781 to i64
  %16784 = shl nuw nsw i64 %16783, 4
  store i64 %16784, i64* %573, align 8
  %16785 = add i64 %16784, %16755
  store i64 %16785, i64* %RAX.i11582.pre-phi, align 8
  %16786 = icmp ult i64 %16785, %16755
  %16787 = icmp ult i64 %16785, %16784
  %16788 = or i1 %16786, %16787
  %16789 = zext i1 %16788 to i8
  store i8 %16789, i8* %14, align 1
  %16790 = trunc i64 %16785 to i32
  %16791 = and i32 %16790, 255
  %16792 = tail call i32 @llvm.ctpop.i32(i32 %16791)
  %16793 = trunc i32 %16792 to i8
  %16794 = and i8 %16793, 1
  %16795 = xor i8 %16794, 1
  store i8 %16795, i8* %21, align 1
  %16796 = xor i64 %16784, %16755
  %16797 = xor i64 %16796, %16785
  %16798 = lshr i64 %16797, 4
  %16799 = trunc i64 %16798 to i8
  %16800 = and i8 %16799, 1
  store i8 %16800, i8* %27, align 1
  %16801 = icmp eq i64 %16785, 0
  %16802 = zext i1 %16801 to i8
  store i8 %16802, i8* %30, align 1
  %16803 = lshr i64 %16785, 63
  %16804 = trunc i64 %16803 to i8
  store i8 %16804, i8* %33, align 1
  %16805 = xor i64 %16803, %16771
  %16806 = add nuw nsw i64 %16805, %16803
  %16807 = icmp eq i64 %16806, 2
  %16808 = zext i1 %16807 to i8
  store i8 %16808, i8* %39, align 1
  %16809 = inttoptr i64 %16785 to i32*
  %16810 = add i64 %16751, 28
  store i64 %16810, i64* %3, align 8
  %16811 = load i32, i32* %16809, align 4
  %16812 = zext i32 %16811 to i64
  store i64 %16812, i64* %RCX.i11580, align 8
  %16813 = load i64, i64* %RBP.i, align 8
  %16814 = add i64 %16813, -140
  %16815 = add i64 %16751, 34
  store i64 %16815, i64* %3, align 8
  %16816 = inttoptr i64 %16814 to i32*
  %16817 = load i32, i32* %16816, align 4
  %16818 = add i32 %16817, %16811
  %16819 = zext i32 %16818 to i64
  store i64 %16819, i64* %RCX.i11580, align 8
  %16820 = icmp ult i32 %16818, %16811
  %16821 = icmp ult i32 %16818, %16817
  %16822 = or i1 %16820, %16821
  %16823 = zext i1 %16822 to i8
  store i8 %16823, i8* %14, align 1
  %16824 = and i32 %16818, 255
  %16825 = tail call i32 @llvm.ctpop.i32(i32 %16824)
  %16826 = trunc i32 %16825 to i8
  %16827 = and i8 %16826, 1
  %16828 = xor i8 %16827, 1
  store i8 %16828, i8* %21, align 1
  %16829 = xor i32 %16817, %16811
  %16830 = xor i32 %16829, %16818
  %16831 = lshr i32 %16830, 4
  %16832 = trunc i32 %16831 to i8
  %16833 = and i8 %16832, 1
  store i8 %16833, i8* %27, align 1
  %16834 = icmp eq i32 %16818, 0
  %16835 = zext i1 %16834 to i8
  store i8 %16835, i8* %30, align 1
  %16836 = lshr i32 %16818, 31
  %16837 = trunc i32 %16836 to i8
  store i8 %16837, i8* %33, align 1
  %16838 = lshr i32 %16811, 31
  %16839 = lshr i32 %16817, 31
  %16840 = xor i32 %16836, %16838
  %16841 = xor i32 %16836, %16839
  %16842 = add nuw nsw i32 %16840, %16841
  %16843 = icmp eq i32 %16842, 2
  %16844 = zext i1 %16843 to i8
  store i8 %16844, i8* %39, align 1
  %16845 = add i64 %16751, 40
  store i64 %16845, i64* %3, align 8
  store i32 %16818, i32* %16816, align 4
  %16846 = load i64, i64* %RBP.i, align 8
  %16847 = add i64 %16846, -8
  %16848 = load i64, i64* %3, align 8
  %16849 = add i64 %16848, 4
  store i64 %16849, i64* %3, align 8
  %16850 = inttoptr i64 %16847 to i64*
  %16851 = load i64, i64* %16850, align 8
  %16852 = add i64 %16851, 51640
  store i64 %16852, i64* %RAX.i11582.pre-phi, align 8
  %16853 = icmp ugt i64 %16851, -51641
  %16854 = zext i1 %16853 to i8
  store i8 %16854, i8* %14, align 1
  %16855 = trunc i64 %16852 to i32
  %16856 = and i32 %16855, 255
  %16857 = tail call i32 @llvm.ctpop.i32(i32 %16856)
  %16858 = trunc i32 %16857 to i8
  %16859 = and i8 %16858, 1
  %16860 = xor i8 %16859, 1
  store i8 %16860, i8* %21, align 1
  %16861 = xor i64 %16851, 16
  %16862 = xor i64 %16861, %16852
  %16863 = lshr i64 %16862, 4
  %16864 = trunc i64 %16863 to i8
  %16865 = and i8 %16864, 1
  store i8 %16865, i8* %27, align 1
  %16866 = icmp eq i64 %16852, 0
  %16867 = zext i1 %16866 to i8
  store i8 %16867, i8* %30, align 1
  %16868 = lshr i64 %16852, 63
  %16869 = trunc i64 %16868 to i8
  store i8 %16869, i8* %33, align 1
  %16870 = lshr i64 %16851, 63
  %16871 = xor i64 %16868, %16870
  %16872 = add nuw nsw i64 %16871, %16868
  %16873 = icmp eq i64 %16872, 2
  %16874 = zext i1 %16873 to i8
  store i8 %16874, i8* %39, align 1
  %16875 = add i64 %16846, -150
  %16876 = add i64 %16848, 17
  store i64 %16876, i64* %3, align 8
  %16877 = inttoptr i64 %16875 to i16*
  %16878 = load i16, i16* %16877, align 2
  %16879 = zext i16 %16878 to i64
  store i64 %16879, i64* %RCX.i11580, align 8
  %16880 = zext i16 %16878 to i64
  %16881 = shl nuw nsw i64 %16880, 4
  store i64 %16881, i64* %573, align 8
  %16882 = add i64 %16881, %16852
  store i64 %16882, i64* %RAX.i11582.pre-phi, align 8
  %16883 = icmp ult i64 %16882, %16852
  %16884 = icmp ult i64 %16882, %16881
  %16885 = or i1 %16883, %16884
  %16886 = zext i1 %16885 to i8
  store i8 %16886, i8* %14, align 1
  %16887 = trunc i64 %16882 to i32
  %16888 = and i32 %16887, 255
  %16889 = tail call i32 @llvm.ctpop.i32(i32 %16888)
  %16890 = trunc i32 %16889 to i8
  %16891 = and i8 %16890, 1
  %16892 = xor i8 %16891, 1
  store i8 %16892, i8* %21, align 1
  %16893 = xor i64 %16881, %16852
  %16894 = xor i64 %16893, %16882
  %16895 = lshr i64 %16894, 4
  %16896 = trunc i64 %16895 to i8
  %16897 = and i8 %16896, 1
  store i8 %16897, i8* %27, align 1
  %16898 = icmp eq i64 %16882, 0
  %16899 = zext i1 %16898 to i8
  store i8 %16899, i8* %30, align 1
  %16900 = lshr i64 %16882, 63
  %16901 = trunc i64 %16900 to i8
  store i8 %16901, i8* %33, align 1
  %16902 = xor i64 %16900, %16868
  %16903 = add nuw nsw i64 %16902, %16900
  %16904 = icmp eq i64 %16903, 2
  %16905 = zext i1 %16904 to i8
  store i8 %16905, i8* %39, align 1
  %16906 = add i64 %16882, 4
  %16907 = add i64 %16848, 29
  store i64 %16907, i64* %3, align 8
  %16908 = inttoptr i64 %16906 to i32*
  %16909 = load i32, i32* %16908, align 4
  %16910 = zext i32 %16909 to i64
  store i64 %16910, i64* %RCX.i11580, align 8
  %16911 = load i64, i64* %RBP.i, align 8
  %16912 = add i64 %16911, -144
  %16913 = add i64 %16848, 35
  store i64 %16913, i64* %3, align 8
  %16914 = inttoptr i64 %16912 to i32*
  %16915 = load i32, i32* %16914, align 4
  %16916 = add i32 %16915, %16909
  %16917 = zext i32 %16916 to i64
  store i64 %16917, i64* %RCX.i11580, align 8
  %16918 = icmp ult i32 %16916, %16909
  %16919 = icmp ult i32 %16916, %16915
  %16920 = or i1 %16918, %16919
  %16921 = zext i1 %16920 to i8
  store i8 %16921, i8* %14, align 1
  %16922 = and i32 %16916, 255
  %16923 = tail call i32 @llvm.ctpop.i32(i32 %16922)
  %16924 = trunc i32 %16923 to i8
  %16925 = and i8 %16924, 1
  %16926 = xor i8 %16925, 1
  store i8 %16926, i8* %21, align 1
  %16927 = xor i32 %16915, %16909
  %16928 = xor i32 %16927, %16916
  %16929 = lshr i32 %16928, 4
  %16930 = trunc i32 %16929 to i8
  %16931 = and i8 %16930, 1
  store i8 %16931, i8* %27, align 1
  %16932 = icmp eq i32 %16916, 0
  %16933 = zext i1 %16932 to i8
  store i8 %16933, i8* %30, align 1
  %16934 = lshr i32 %16916, 31
  %16935 = trunc i32 %16934 to i8
  store i8 %16935, i8* %33, align 1
  %16936 = lshr i32 %16909, 31
  %16937 = lshr i32 %16915, 31
  %16938 = xor i32 %16934, %16936
  %16939 = xor i32 %16934, %16937
  %16940 = add nuw nsw i32 %16938, %16939
  %16941 = icmp eq i32 %16940, 2
  %16942 = zext i1 %16941 to i8
  store i8 %16942, i8* %39, align 1
  %16943 = add i64 %16848, 41
  store i64 %16943, i64* %3, align 8
  store i32 %16916, i32* %16914, align 4
  %16944 = load i64, i64* %RBP.i, align 8
  %16945 = add i64 %16944, -8
  %16946 = load i64, i64* %3, align 8
  %16947 = add i64 %16946, 4
  store i64 %16947, i64* %3, align 8
  %16948 = inttoptr i64 %16945 to i64*
  %16949 = load i64, i64* %16948, align 8
  %16950 = add i64 %16949, 51640
  store i64 %16950, i64* %RAX.i11582.pre-phi, align 8
  %16951 = icmp ugt i64 %16949, -51641
  %16952 = zext i1 %16951 to i8
  store i8 %16952, i8* %14, align 1
  %16953 = trunc i64 %16950 to i32
  %16954 = and i32 %16953, 255
  %16955 = tail call i32 @llvm.ctpop.i32(i32 %16954)
  %16956 = trunc i32 %16955 to i8
  %16957 = and i8 %16956, 1
  %16958 = xor i8 %16957, 1
  store i8 %16958, i8* %21, align 1
  %16959 = xor i64 %16949, 16
  %16960 = xor i64 %16959, %16950
  %16961 = lshr i64 %16960, 4
  %16962 = trunc i64 %16961 to i8
  %16963 = and i8 %16962, 1
  store i8 %16963, i8* %27, align 1
  %16964 = icmp eq i64 %16950, 0
  %16965 = zext i1 %16964 to i8
  store i8 %16965, i8* %30, align 1
  %16966 = lshr i64 %16950, 63
  %16967 = trunc i64 %16966 to i8
  store i8 %16967, i8* %33, align 1
  %16968 = lshr i64 %16949, 63
  %16969 = xor i64 %16966, %16968
  %16970 = add nuw nsw i64 %16969, %16966
  %16971 = icmp eq i64 %16970, 2
  %16972 = zext i1 %16971 to i8
  store i8 %16972, i8* %39, align 1
  %16973 = add i64 %16944, -150
  %16974 = add i64 %16946, 17
  store i64 %16974, i64* %3, align 8
  %16975 = inttoptr i64 %16973 to i16*
  %16976 = load i16, i16* %16975, align 2
  %16977 = zext i16 %16976 to i64
  store i64 %16977, i64* %RCX.i11580, align 8
  %16978 = zext i16 %16976 to i64
  %16979 = shl nuw nsw i64 %16978, 4
  store i64 %16979, i64* %573, align 8
  %16980 = add i64 %16979, %16950
  store i64 %16980, i64* %RAX.i11582.pre-phi, align 8
  %16981 = icmp ult i64 %16980, %16950
  %16982 = icmp ult i64 %16980, %16979
  %16983 = or i1 %16981, %16982
  %16984 = zext i1 %16983 to i8
  store i8 %16984, i8* %14, align 1
  %16985 = trunc i64 %16980 to i32
  %16986 = and i32 %16985, 255
  %16987 = tail call i32 @llvm.ctpop.i32(i32 %16986)
  %16988 = trunc i32 %16987 to i8
  %16989 = and i8 %16988, 1
  %16990 = xor i8 %16989, 1
  store i8 %16990, i8* %21, align 1
  %16991 = xor i64 %16979, %16950
  %16992 = xor i64 %16991, %16980
  %16993 = lshr i64 %16992, 4
  %16994 = trunc i64 %16993 to i8
  %16995 = and i8 %16994, 1
  store i8 %16995, i8* %27, align 1
  %16996 = icmp eq i64 %16980, 0
  %16997 = zext i1 %16996 to i8
  store i8 %16997, i8* %30, align 1
  %16998 = lshr i64 %16980, 63
  %16999 = trunc i64 %16998 to i8
  store i8 %16999, i8* %33, align 1
  %17000 = xor i64 %16998, %16966
  %17001 = add nuw nsw i64 %17000, %16998
  %17002 = icmp eq i64 %17001, 2
  %17003 = zext i1 %17002 to i8
  store i8 %17003, i8* %39, align 1
  %17004 = add i64 %16980, 8
  %17005 = add i64 %16946, 29
  store i64 %17005, i64* %3, align 8
  %17006 = inttoptr i64 %17004 to i32*
  %17007 = load i32, i32* %17006, align 4
  %17008 = zext i32 %17007 to i64
  store i64 %17008, i64* %RCX.i11580, align 8
  %17009 = load i64, i64* %RBP.i, align 8
  %17010 = add i64 %17009, -148
  %17011 = add i64 %16946, 35
  store i64 %17011, i64* %3, align 8
  %17012 = inttoptr i64 %17010 to i32*
  %17013 = load i32, i32* %17012, align 4
  %17014 = add i32 %17013, %17007
  %17015 = zext i32 %17014 to i64
  store i64 %17015, i64* %RCX.i11580, align 8
  %17016 = icmp ult i32 %17014, %17007
  %17017 = icmp ult i32 %17014, %17013
  %17018 = or i1 %17016, %17017
  %17019 = zext i1 %17018 to i8
  store i8 %17019, i8* %14, align 1
  %17020 = and i32 %17014, 255
  %17021 = tail call i32 @llvm.ctpop.i32(i32 %17020)
  %17022 = trunc i32 %17021 to i8
  %17023 = and i8 %17022, 1
  %17024 = xor i8 %17023, 1
  store i8 %17024, i8* %21, align 1
  %17025 = xor i32 %17013, %17007
  %17026 = xor i32 %17025, %17014
  %17027 = lshr i32 %17026, 4
  %17028 = trunc i32 %17027 to i8
  %17029 = and i8 %17028, 1
  store i8 %17029, i8* %27, align 1
  %17030 = icmp eq i32 %17014, 0
  %17031 = zext i1 %17030 to i8
  store i8 %17031, i8* %30, align 1
  %17032 = lshr i32 %17014, 31
  %17033 = trunc i32 %17032 to i8
  store i8 %17033, i8* %33, align 1
  %17034 = lshr i32 %17007, 31
  %17035 = lshr i32 %17013, 31
  %17036 = xor i32 %17032, %17034
  %17037 = xor i32 %17032, %17035
  %17038 = add nuw nsw i32 %17036, %17037
  %17039 = icmp eq i32 %17038, 2
  %17040 = zext i1 %17039 to i8
  store i8 %17040, i8* %39, align 1
  %17041 = add i64 %16946, 41
  store i64 %17041, i64* %3, align 8
  store i32 %17014, i32* %17012, align 4
  %17042 = load i64, i64* %RBP.i, align 8
  %17043 = add i64 %17042, -120
  %17044 = load i64, i64* %3, align 8
  %17045 = add i64 %17044, 4
  store i64 %17045, i64* %3, align 8
  %17046 = inttoptr i64 %17043 to i64*
  %17047 = load i64, i64* %17046, align 8
  store i64 %17047, i64* %RAX.i11582.pre-phi, align 8
  %17048 = add i64 %17042, -28
  %17049 = add i64 %17044, 7
  store i64 %17049, i64* %3, align 8
  %17050 = inttoptr i64 %17048 to i32*
  %17051 = load i32, i32* %17050, align 4
  %17052 = add i32 %17051, 43
  %17053 = zext i32 %17052 to i64
  store i64 %17053, i64* %RCX.i11580, align 8
  %17054 = icmp ugt i32 %17051, -44
  %17055 = zext i1 %17054 to i8
  store i8 %17055, i8* %14, align 1
  %17056 = and i32 %17052, 255
  %17057 = tail call i32 @llvm.ctpop.i32(i32 %17056)
  %17058 = trunc i32 %17057 to i8
  %17059 = and i8 %17058, 1
  %17060 = xor i8 %17059, 1
  store i8 %17060, i8* %21, align 1
  %17061 = xor i32 %17052, %17051
  %17062 = lshr i32 %17061, 4
  %17063 = trunc i32 %17062 to i8
  %17064 = and i8 %17063, 1
  store i8 %17064, i8* %27, align 1
  %17065 = icmp eq i32 %17052, 0
  %17066 = zext i1 %17065 to i8
  store i8 %17066, i8* %30, align 1
  %17067 = lshr i32 %17052, 31
  %17068 = trunc i32 %17067 to i8
  store i8 %17068, i8* %33, align 1
  %17069 = lshr i32 %17051, 31
  %17070 = xor i32 %17067, %17069
  %17071 = add nuw nsw i32 %17070, %17067
  %17072 = icmp eq i32 %17071, 2
  %17073 = zext i1 %17072 to i8
  store i8 %17073, i8* %39, align 1
  %17074 = sext i32 %17052 to i64
  store i64 %17074, i64* %573, align 8
  %17075 = shl nsw i64 %17074, 1
  %17076 = add i64 %17047, %17075
  %17077 = add i64 %17044, 17
  store i64 %17077, i64* %3, align 8
  %17078 = inttoptr i64 %17076 to i16*
  %17079 = load i16, i16* %17078, align 2
  store i16 %17079, i16* %SI.i, align 2
  %17080 = add i64 %17042, -150
  %17081 = add i64 %17044, 24
  store i64 %17081, i64* %3, align 8
  %17082 = inttoptr i64 %17080 to i16*
  store i16 %17079, i16* %17082, align 2
  %17083 = load i64, i64* %RBP.i, align 8
  %17084 = add i64 %17083, -8
  %17085 = load i64, i64* %3, align 8
  %17086 = add i64 %17085, 4
  store i64 %17086, i64* %3, align 8
  %17087 = inttoptr i64 %17084 to i64*
  %17088 = load i64, i64* %17087, align 8
  %17089 = add i64 %17088, 51640
  store i64 %17089, i64* %RAX.i11582.pre-phi, align 8
  %17090 = icmp ugt i64 %17088, -51641
  %17091 = zext i1 %17090 to i8
  store i8 %17091, i8* %14, align 1
  %17092 = trunc i64 %17089 to i32
  %17093 = and i32 %17092, 255
  %17094 = tail call i32 @llvm.ctpop.i32(i32 %17093)
  %17095 = trunc i32 %17094 to i8
  %17096 = and i8 %17095, 1
  %17097 = xor i8 %17096, 1
  store i8 %17097, i8* %21, align 1
  %17098 = xor i64 %17088, 16
  %17099 = xor i64 %17098, %17089
  %17100 = lshr i64 %17099, 4
  %17101 = trunc i64 %17100 to i8
  %17102 = and i8 %17101, 1
  store i8 %17102, i8* %27, align 1
  %17103 = icmp eq i64 %17089, 0
  %17104 = zext i1 %17103 to i8
  store i8 %17104, i8* %30, align 1
  %17105 = lshr i64 %17089, 63
  %17106 = trunc i64 %17105 to i8
  store i8 %17106, i8* %33, align 1
  %17107 = lshr i64 %17088, 63
  %17108 = xor i64 %17105, %17107
  %17109 = add nuw nsw i64 %17108, %17105
  %17110 = icmp eq i64 %17109, 2
  %17111 = zext i1 %17110 to i8
  store i8 %17111, i8* %39, align 1
  %17112 = add i64 %17083, -150
  %17113 = add i64 %17085, 17
  store i64 %17113, i64* %3, align 8
  %17114 = inttoptr i64 %17112 to i16*
  %17115 = load i16, i16* %17114, align 2
  %17116 = zext i16 %17115 to i64
  store i64 %17116, i64* %RCX.i11580, align 8
  %17117 = zext i16 %17115 to i64
  %17118 = shl nuw nsw i64 %17117, 4
  store i64 %17118, i64* %573, align 8
  %17119 = add i64 %17118, %17089
  store i64 %17119, i64* %RAX.i11582.pre-phi, align 8
  %17120 = icmp ult i64 %17119, %17089
  %17121 = icmp ult i64 %17119, %17118
  %17122 = or i1 %17120, %17121
  %17123 = zext i1 %17122 to i8
  store i8 %17123, i8* %14, align 1
  %17124 = trunc i64 %17119 to i32
  %17125 = and i32 %17124, 255
  %17126 = tail call i32 @llvm.ctpop.i32(i32 %17125)
  %17127 = trunc i32 %17126 to i8
  %17128 = and i8 %17127, 1
  %17129 = xor i8 %17128, 1
  store i8 %17129, i8* %21, align 1
  %17130 = xor i64 %17118, %17089
  %17131 = xor i64 %17130, %17119
  %17132 = lshr i64 %17131, 4
  %17133 = trunc i64 %17132 to i8
  %17134 = and i8 %17133, 1
  store i8 %17134, i8* %27, align 1
  %17135 = icmp eq i64 %17119, 0
  %17136 = zext i1 %17135 to i8
  store i8 %17136, i8* %30, align 1
  %17137 = lshr i64 %17119, 63
  %17138 = trunc i64 %17137 to i8
  store i8 %17138, i8* %33, align 1
  %17139 = xor i64 %17137, %17105
  %17140 = add nuw nsw i64 %17139, %17137
  %17141 = icmp eq i64 %17140, 2
  %17142 = zext i1 %17141 to i8
  store i8 %17142, i8* %39, align 1
  %17143 = inttoptr i64 %17119 to i32*
  %17144 = add i64 %17085, 28
  store i64 %17144, i64* %3, align 8
  %17145 = load i32, i32* %17143, align 4
  %17146 = zext i32 %17145 to i64
  store i64 %17146, i64* %RCX.i11580, align 8
  %17147 = load i64, i64* %RBP.i, align 8
  %17148 = add i64 %17147, -140
  %17149 = add i64 %17085, 34
  store i64 %17149, i64* %3, align 8
  %17150 = inttoptr i64 %17148 to i32*
  %17151 = load i32, i32* %17150, align 4
  %17152 = add i32 %17151, %17145
  %17153 = zext i32 %17152 to i64
  store i64 %17153, i64* %RCX.i11580, align 8
  %17154 = icmp ult i32 %17152, %17145
  %17155 = icmp ult i32 %17152, %17151
  %17156 = or i1 %17154, %17155
  %17157 = zext i1 %17156 to i8
  store i8 %17157, i8* %14, align 1
  %17158 = and i32 %17152, 255
  %17159 = tail call i32 @llvm.ctpop.i32(i32 %17158)
  %17160 = trunc i32 %17159 to i8
  %17161 = and i8 %17160, 1
  %17162 = xor i8 %17161, 1
  store i8 %17162, i8* %21, align 1
  %17163 = xor i32 %17151, %17145
  %17164 = xor i32 %17163, %17152
  %17165 = lshr i32 %17164, 4
  %17166 = trunc i32 %17165 to i8
  %17167 = and i8 %17166, 1
  store i8 %17167, i8* %27, align 1
  %17168 = icmp eq i32 %17152, 0
  %17169 = zext i1 %17168 to i8
  store i8 %17169, i8* %30, align 1
  %17170 = lshr i32 %17152, 31
  %17171 = trunc i32 %17170 to i8
  store i8 %17171, i8* %33, align 1
  %17172 = lshr i32 %17145, 31
  %17173 = lshr i32 %17151, 31
  %17174 = xor i32 %17170, %17172
  %17175 = xor i32 %17170, %17173
  %17176 = add nuw nsw i32 %17174, %17175
  %17177 = icmp eq i32 %17176, 2
  %17178 = zext i1 %17177 to i8
  store i8 %17178, i8* %39, align 1
  %17179 = add i64 %17085, 40
  store i64 %17179, i64* %3, align 8
  store i32 %17152, i32* %17150, align 4
  %17180 = load i64, i64* %RBP.i, align 8
  %17181 = add i64 %17180, -8
  %17182 = load i64, i64* %3, align 8
  %17183 = add i64 %17182, 4
  store i64 %17183, i64* %3, align 8
  %17184 = inttoptr i64 %17181 to i64*
  %17185 = load i64, i64* %17184, align 8
  %17186 = add i64 %17185, 51640
  store i64 %17186, i64* %RAX.i11582.pre-phi, align 8
  %17187 = icmp ugt i64 %17185, -51641
  %17188 = zext i1 %17187 to i8
  store i8 %17188, i8* %14, align 1
  %17189 = trunc i64 %17186 to i32
  %17190 = and i32 %17189, 255
  %17191 = tail call i32 @llvm.ctpop.i32(i32 %17190)
  %17192 = trunc i32 %17191 to i8
  %17193 = and i8 %17192, 1
  %17194 = xor i8 %17193, 1
  store i8 %17194, i8* %21, align 1
  %17195 = xor i64 %17185, 16
  %17196 = xor i64 %17195, %17186
  %17197 = lshr i64 %17196, 4
  %17198 = trunc i64 %17197 to i8
  %17199 = and i8 %17198, 1
  store i8 %17199, i8* %27, align 1
  %17200 = icmp eq i64 %17186, 0
  %17201 = zext i1 %17200 to i8
  store i8 %17201, i8* %30, align 1
  %17202 = lshr i64 %17186, 63
  %17203 = trunc i64 %17202 to i8
  store i8 %17203, i8* %33, align 1
  %17204 = lshr i64 %17185, 63
  %17205 = xor i64 %17202, %17204
  %17206 = add nuw nsw i64 %17205, %17202
  %17207 = icmp eq i64 %17206, 2
  %17208 = zext i1 %17207 to i8
  store i8 %17208, i8* %39, align 1
  %17209 = add i64 %17180, -150
  %17210 = add i64 %17182, 17
  store i64 %17210, i64* %3, align 8
  %17211 = inttoptr i64 %17209 to i16*
  %17212 = load i16, i16* %17211, align 2
  %17213 = zext i16 %17212 to i64
  store i64 %17213, i64* %RCX.i11580, align 8
  %17214 = zext i16 %17212 to i64
  %17215 = shl nuw nsw i64 %17214, 4
  store i64 %17215, i64* %573, align 8
  %17216 = add i64 %17215, %17186
  store i64 %17216, i64* %RAX.i11582.pre-phi, align 8
  %17217 = icmp ult i64 %17216, %17186
  %17218 = icmp ult i64 %17216, %17215
  %17219 = or i1 %17217, %17218
  %17220 = zext i1 %17219 to i8
  store i8 %17220, i8* %14, align 1
  %17221 = trunc i64 %17216 to i32
  %17222 = and i32 %17221, 255
  %17223 = tail call i32 @llvm.ctpop.i32(i32 %17222)
  %17224 = trunc i32 %17223 to i8
  %17225 = and i8 %17224, 1
  %17226 = xor i8 %17225, 1
  store i8 %17226, i8* %21, align 1
  %17227 = xor i64 %17215, %17186
  %17228 = xor i64 %17227, %17216
  %17229 = lshr i64 %17228, 4
  %17230 = trunc i64 %17229 to i8
  %17231 = and i8 %17230, 1
  store i8 %17231, i8* %27, align 1
  %17232 = icmp eq i64 %17216, 0
  %17233 = zext i1 %17232 to i8
  store i8 %17233, i8* %30, align 1
  %17234 = lshr i64 %17216, 63
  %17235 = trunc i64 %17234 to i8
  store i8 %17235, i8* %33, align 1
  %17236 = xor i64 %17234, %17202
  %17237 = add nuw nsw i64 %17236, %17234
  %17238 = icmp eq i64 %17237, 2
  %17239 = zext i1 %17238 to i8
  store i8 %17239, i8* %39, align 1
  %17240 = add i64 %17216, 4
  %17241 = add i64 %17182, 29
  store i64 %17241, i64* %3, align 8
  %17242 = inttoptr i64 %17240 to i32*
  %17243 = load i32, i32* %17242, align 4
  %17244 = zext i32 %17243 to i64
  store i64 %17244, i64* %RCX.i11580, align 8
  %17245 = load i64, i64* %RBP.i, align 8
  %17246 = add i64 %17245, -144
  %17247 = add i64 %17182, 35
  store i64 %17247, i64* %3, align 8
  %17248 = inttoptr i64 %17246 to i32*
  %17249 = load i32, i32* %17248, align 4
  %17250 = add i32 %17249, %17243
  %17251 = zext i32 %17250 to i64
  store i64 %17251, i64* %RCX.i11580, align 8
  %17252 = icmp ult i32 %17250, %17243
  %17253 = icmp ult i32 %17250, %17249
  %17254 = or i1 %17252, %17253
  %17255 = zext i1 %17254 to i8
  store i8 %17255, i8* %14, align 1
  %17256 = and i32 %17250, 255
  %17257 = tail call i32 @llvm.ctpop.i32(i32 %17256)
  %17258 = trunc i32 %17257 to i8
  %17259 = and i8 %17258, 1
  %17260 = xor i8 %17259, 1
  store i8 %17260, i8* %21, align 1
  %17261 = xor i32 %17249, %17243
  %17262 = xor i32 %17261, %17250
  %17263 = lshr i32 %17262, 4
  %17264 = trunc i32 %17263 to i8
  %17265 = and i8 %17264, 1
  store i8 %17265, i8* %27, align 1
  %17266 = icmp eq i32 %17250, 0
  %17267 = zext i1 %17266 to i8
  store i8 %17267, i8* %30, align 1
  %17268 = lshr i32 %17250, 31
  %17269 = trunc i32 %17268 to i8
  store i8 %17269, i8* %33, align 1
  %17270 = lshr i32 %17243, 31
  %17271 = lshr i32 %17249, 31
  %17272 = xor i32 %17268, %17270
  %17273 = xor i32 %17268, %17271
  %17274 = add nuw nsw i32 %17272, %17273
  %17275 = icmp eq i32 %17274, 2
  %17276 = zext i1 %17275 to i8
  store i8 %17276, i8* %39, align 1
  %17277 = add i64 %17182, 41
  store i64 %17277, i64* %3, align 8
  store i32 %17250, i32* %17248, align 4
  %17278 = load i64, i64* %RBP.i, align 8
  %17279 = add i64 %17278, -8
  %17280 = load i64, i64* %3, align 8
  %17281 = add i64 %17280, 4
  store i64 %17281, i64* %3, align 8
  %17282 = inttoptr i64 %17279 to i64*
  %17283 = load i64, i64* %17282, align 8
  %17284 = add i64 %17283, 51640
  store i64 %17284, i64* %RAX.i11582.pre-phi, align 8
  %17285 = icmp ugt i64 %17283, -51641
  %17286 = zext i1 %17285 to i8
  store i8 %17286, i8* %14, align 1
  %17287 = trunc i64 %17284 to i32
  %17288 = and i32 %17287, 255
  %17289 = tail call i32 @llvm.ctpop.i32(i32 %17288)
  %17290 = trunc i32 %17289 to i8
  %17291 = and i8 %17290, 1
  %17292 = xor i8 %17291, 1
  store i8 %17292, i8* %21, align 1
  %17293 = xor i64 %17283, 16
  %17294 = xor i64 %17293, %17284
  %17295 = lshr i64 %17294, 4
  %17296 = trunc i64 %17295 to i8
  %17297 = and i8 %17296, 1
  store i8 %17297, i8* %27, align 1
  %17298 = icmp eq i64 %17284, 0
  %17299 = zext i1 %17298 to i8
  store i8 %17299, i8* %30, align 1
  %17300 = lshr i64 %17284, 63
  %17301 = trunc i64 %17300 to i8
  store i8 %17301, i8* %33, align 1
  %17302 = lshr i64 %17283, 63
  %17303 = xor i64 %17300, %17302
  %17304 = add nuw nsw i64 %17303, %17300
  %17305 = icmp eq i64 %17304, 2
  %17306 = zext i1 %17305 to i8
  store i8 %17306, i8* %39, align 1
  %17307 = add i64 %17278, -150
  %17308 = add i64 %17280, 17
  store i64 %17308, i64* %3, align 8
  %17309 = inttoptr i64 %17307 to i16*
  %17310 = load i16, i16* %17309, align 2
  %17311 = zext i16 %17310 to i64
  store i64 %17311, i64* %RCX.i11580, align 8
  %17312 = zext i16 %17310 to i64
  %17313 = shl nuw nsw i64 %17312, 4
  store i64 %17313, i64* %573, align 8
  %17314 = add i64 %17313, %17284
  store i64 %17314, i64* %RAX.i11582.pre-phi, align 8
  %17315 = icmp ult i64 %17314, %17284
  %17316 = icmp ult i64 %17314, %17313
  %17317 = or i1 %17315, %17316
  %17318 = zext i1 %17317 to i8
  store i8 %17318, i8* %14, align 1
  %17319 = trunc i64 %17314 to i32
  %17320 = and i32 %17319, 255
  %17321 = tail call i32 @llvm.ctpop.i32(i32 %17320)
  %17322 = trunc i32 %17321 to i8
  %17323 = and i8 %17322, 1
  %17324 = xor i8 %17323, 1
  store i8 %17324, i8* %21, align 1
  %17325 = xor i64 %17313, %17284
  %17326 = xor i64 %17325, %17314
  %17327 = lshr i64 %17326, 4
  %17328 = trunc i64 %17327 to i8
  %17329 = and i8 %17328, 1
  store i8 %17329, i8* %27, align 1
  %17330 = icmp eq i64 %17314, 0
  %17331 = zext i1 %17330 to i8
  store i8 %17331, i8* %30, align 1
  %17332 = lshr i64 %17314, 63
  %17333 = trunc i64 %17332 to i8
  store i8 %17333, i8* %33, align 1
  %17334 = xor i64 %17332, %17300
  %17335 = add nuw nsw i64 %17334, %17332
  %17336 = icmp eq i64 %17335, 2
  %17337 = zext i1 %17336 to i8
  store i8 %17337, i8* %39, align 1
  %17338 = add i64 %17314, 8
  %17339 = add i64 %17280, 29
  store i64 %17339, i64* %3, align 8
  %17340 = inttoptr i64 %17338 to i32*
  %17341 = load i32, i32* %17340, align 4
  %17342 = zext i32 %17341 to i64
  store i64 %17342, i64* %RCX.i11580, align 8
  %17343 = load i64, i64* %RBP.i, align 8
  %17344 = add i64 %17343, -148
  %17345 = add i64 %17280, 35
  store i64 %17345, i64* %3, align 8
  %17346 = inttoptr i64 %17344 to i32*
  %17347 = load i32, i32* %17346, align 4
  %17348 = add i32 %17347, %17341
  %17349 = zext i32 %17348 to i64
  store i64 %17349, i64* %RCX.i11580, align 8
  %17350 = icmp ult i32 %17348, %17341
  %17351 = icmp ult i32 %17348, %17347
  %17352 = or i1 %17350, %17351
  %17353 = zext i1 %17352 to i8
  store i8 %17353, i8* %14, align 1
  %17354 = and i32 %17348, 255
  %17355 = tail call i32 @llvm.ctpop.i32(i32 %17354)
  %17356 = trunc i32 %17355 to i8
  %17357 = and i8 %17356, 1
  %17358 = xor i8 %17357, 1
  store i8 %17358, i8* %21, align 1
  %17359 = xor i32 %17347, %17341
  %17360 = xor i32 %17359, %17348
  %17361 = lshr i32 %17360, 4
  %17362 = trunc i32 %17361 to i8
  %17363 = and i8 %17362, 1
  store i8 %17363, i8* %27, align 1
  %17364 = icmp eq i32 %17348, 0
  %17365 = zext i1 %17364 to i8
  store i8 %17365, i8* %30, align 1
  %17366 = lshr i32 %17348, 31
  %17367 = trunc i32 %17366 to i8
  store i8 %17367, i8* %33, align 1
  %17368 = lshr i32 %17341, 31
  %17369 = lshr i32 %17347, 31
  %17370 = xor i32 %17366, %17368
  %17371 = xor i32 %17366, %17369
  %17372 = add nuw nsw i32 %17370, %17371
  %17373 = icmp eq i32 %17372, 2
  %17374 = zext i1 %17373 to i8
  store i8 %17374, i8* %39, align 1
  %17375 = add i64 %17280, 41
  store i64 %17375, i64* %3, align 8
  store i32 %17348, i32* %17346, align 4
  %17376 = load i64, i64* %RBP.i, align 8
  %17377 = add i64 %17376, -120
  %17378 = load i64, i64* %3, align 8
  %17379 = add i64 %17378, 4
  store i64 %17379, i64* %3, align 8
  %17380 = inttoptr i64 %17377 to i64*
  %17381 = load i64, i64* %17380, align 8
  store i64 %17381, i64* %RAX.i11582.pre-phi, align 8
  %17382 = add i64 %17376, -28
  %17383 = add i64 %17378, 7
  store i64 %17383, i64* %3, align 8
  %17384 = inttoptr i64 %17382 to i32*
  %17385 = load i32, i32* %17384, align 4
  %17386 = add i32 %17385, 44
  %17387 = zext i32 %17386 to i64
  store i64 %17387, i64* %RCX.i11580, align 8
  %17388 = icmp ugt i32 %17385, -45
  %17389 = zext i1 %17388 to i8
  store i8 %17389, i8* %14, align 1
  %17390 = and i32 %17386, 255
  %17391 = tail call i32 @llvm.ctpop.i32(i32 %17390)
  %17392 = trunc i32 %17391 to i8
  %17393 = and i8 %17392, 1
  %17394 = xor i8 %17393, 1
  store i8 %17394, i8* %21, align 1
  %17395 = xor i32 %17386, %17385
  %17396 = lshr i32 %17395, 4
  %17397 = trunc i32 %17396 to i8
  %17398 = and i8 %17397, 1
  store i8 %17398, i8* %27, align 1
  %17399 = icmp eq i32 %17386, 0
  %17400 = zext i1 %17399 to i8
  store i8 %17400, i8* %30, align 1
  %17401 = lshr i32 %17386, 31
  %17402 = trunc i32 %17401 to i8
  store i8 %17402, i8* %33, align 1
  %17403 = lshr i32 %17385, 31
  %17404 = xor i32 %17401, %17403
  %17405 = add nuw nsw i32 %17404, %17401
  %17406 = icmp eq i32 %17405, 2
  %17407 = zext i1 %17406 to i8
  store i8 %17407, i8* %39, align 1
  %17408 = sext i32 %17386 to i64
  store i64 %17408, i64* %573, align 8
  %17409 = shl nsw i64 %17408, 1
  %17410 = add i64 %17381, %17409
  %17411 = add i64 %17378, 17
  store i64 %17411, i64* %3, align 8
  %17412 = inttoptr i64 %17410 to i16*
  %17413 = load i16, i16* %17412, align 2
  store i16 %17413, i16* %SI.i, align 2
  %17414 = add i64 %17376, -150
  %17415 = add i64 %17378, 24
  store i64 %17415, i64* %3, align 8
  %17416 = inttoptr i64 %17414 to i16*
  store i16 %17413, i16* %17416, align 2
  %17417 = load i64, i64* %RBP.i, align 8
  %17418 = add i64 %17417, -8
  %17419 = load i64, i64* %3, align 8
  %17420 = add i64 %17419, 4
  store i64 %17420, i64* %3, align 8
  %17421 = inttoptr i64 %17418 to i64*
  %17422 = load i64, i64* %17421, align 8
  %17423 = add i64 %17422, 51640
  store i64 %17423, i64* %RAX.i11582.pre-phi, align 8
  %17424 = icmp ugt i64 %17422, -51641
  %17425 = zext i1 %17424 to i8
  store i8 %17425, i8* %14, align 1
  %17426 = trunc i64 %17423 to i32
  %17427 = and i32 %17426, 255
  %17428 = tail call i32 @llvm.ctpop.i32(i32 %17427)
  %17429 = trunc i32 %17428 to i8
  %17430 = and i8 %17429, 1
  %17431 = xor i8 %17430, 1
  store i8 %17431, i8* %21, align 1
  %17432 = xor i64 %17422, 16
  %17433 = xor i64 %17432, %17423
  %17434 = lshr i64 %17433, 4
  %17435 = trunc i64 %17434 to i8
  %17436 = and i8 %17435, 1
  store i8 %17436, i8* %27, align 1
  %17437 = icmp eq i64 %17423, 0
  %17438 = zext i1 %17437 to i8
  store i8 %17438, i8* %30, align 1
  %17439 = lshr i64 %17423, 63
  %17440 = trunc i64 %17439 to i8
  store i8 %17440, i8* %33, align 1
  %17441 = lshr i64 %17422, 63
  %17442 = xor i64 %17439, %17441
  %17443 = add nuw nsw i64 %17442, %17439
  %17444 = icmp eq i64 %17443, 2
  %17445 = zext i1 %17444 to i8
  store i8 %17445, i8* %39, align 1
  %17446 = add i64 %17417, -150
  %17447 = add i64 %17419, 17
  store i64 %17447, i64* %3, align 8
  %17448 = inttoptr i64 %17446 to i16*
  %17449 = load i16, i16* %17448, align 2
  %17450 = zext i16 %17449 to i64
  store i64 %17450, i64* %RCX.i11580, align 8
  %17451 = zext i16 %17449 to i64
  %17452 = shl nuw nsw i64 %17451, 4
  store i64 %17452, i64* %573, align 8
  %17453 = add i64 %17452, %17423
  store i64 %17453, i64* %RAX.i11582.pre-phi, align 8
  %17454 = icmp ult i64 %17453, %17423
  %17455 = icmp ult i64 %17453, %17452
  %17456 = or i1 %17454, %17455
  %17457 = zext i1 %17456 to i8
  store i8 %17457, i8* %14, align 1
  %17458 = trunc i64 %17453 to i32
  %17459 = and i32 %17458, 255
  %17460 = tail call i32 @llvm.ctpop.i32(i32 %17459)
  %17461 = trunc i32 %17460 to i8
  %17462 = and i8 %17461, 1
  %17463 = xor i8 %17462, 1
  store i8 %17463, i8* %21, align 1
  %17464 = xor i64 %17452, %17423
  %17465 = xor i64 %17464, %17453
  %17466 = lshr i64 %17465, 4
  %17467 = trunc i64 %17466 to i8
  %17468 = and i8 %17467, 1
  store i8 %17468, i8* %27, align 1
  %17469 = icmp eq i64 %17453, 0
  %17470 = zext i1 %17469 to i8
  store i8 %17470, i8* %30, align 1
  %17471 = lshr i64 %17453, 63
  %17472 = trunc i64 %17471 to i8
  store i8 %17472, i8* %33, align 1
  %17473 = xor i64 %17471, %17439
  %17474 = add nuw nsw i64 %17473, %17471
  %17475 = icmp eq i64 %17474, 2
  %17476 = zext i1 %17475 to i8
  store i8 %17476, i8* %39, align 1
  %17477 = inttoptr i64 %17453 to i32*
  %17478 = add i64 %17419, 28
  store i64 %17478, i64* %3, align 8
  %17479 = load i32, i32* %17477, align 4
  %17480 = zext i32 %17479 to i64
  store i64 %17480, i64* %RCX.i11580, align 8
  %17481 = load i64, i64* %RBP.i, align 8
  %17482 = add i64 %17481, -140
  %17483 = add i64 %17419, 34
  store i64 %17483, i64* %3, align 8
  %17484 = inttoptr i64 %17482 to i32*
  %17485 = load i32, i32* %17484, align 4
  %17486 = add i32 %17485, %17479
  %17487 = zext i32 %17486 to i64
  store i64 %17487, i64* %RCX.i11580, align 8
  %17488 = icmp ult i32 %17486, %17479
  %17489 = icmp ult i32 %17486, %17485
  %17490 = or i1 %17488, %17489
  %17491 = zext i1 %17490 to i8
  store i8 %17491, i8* %14, align 1
  %17492 = and i32 %17486, 255
  %17493 = tail call i32 @llvm.ctpop.i32(i32 %17492)
  %17494 = trunc i32 %17493 to i8
  %17495 = and i8 %17494, 1
  %17496 = xor i8 %17495, 1
  store i8 %17496, i8* %21, align 1
  %17497 = xor i32 %17485, %17479
  %17498 = xor i32 %17497, %17486
  %17499 = lshr i32 %17498, 4
  %17500 = trunc i32 %17499 to i8
  %17501 = and i8 %17500, 1
  store i8 %17501, i8* %27, align 1
  %17502 = icmp eq i32 %17486, 0
  %17503 = zext i1 %17502 to i8
  store i8 %17503, i8* %30, align 1
  %17504 = lshr i32 %17486, 31
  %17505 = trunc i32 %17504 to i8
  store i8 %17505, i8* %33, align 1
  %17506 = lshr i32 %17479, 31
  %17507 = lshr i32 %17485, 31
  %17508 = xor i32 %17504, %17506
  %17509 = xor i32 %17504, %17507
  %17510 = add nuw nsw i32 %17508, %17509
  %17511 = icmp eq i32 %17510, 2
  %17512 = zext i1 %17511 to i8
  store i8 %17512, i8* %39, align 1
  %17513 = add i64 %17419, 40
  store i64 %17513, i64* %3, align 8
  store i32 %17486, i32* %17484, align 4
  %17514 = load i64, i64* %RBP.i, align 8
  %17515 = add i64 %17514, -8
  %17516 = load i64, i64* %3, align 8
  %17517 = add i64 %17516, 4
  store i64 %17517, i64* %3, align 8
  %17518 = inttoptr i64 %17515 to i64*
  %17519 = load i64, i64* %17518, align 8
  %17520 = add i64 %17519, 51640
  store i64 %17520, i64* %RAX.i11582.pre-phi, align 8
  %17521 = icmp ugt i64 %17519, -51641
  %17522 = zext i1 %17521 to i8
  store i8 %17522, i8* %14, align 1
  %17523 = trunc i64 %17520 to i32
  %17524 = and i32 %17523, 255
  %17525 = tail call i32 @llvm.ctpop.i32(i32 %17524)
  %17526 = trunc i32 %17525 to i8
  %17527 = and i8 %17526, 1
  %17528 = xor i8 %17527, 1
  store i8 %17528, i8* %21, align 1
  %17529 = xor i64 %17519, 16
  %17530 = xor i64 %17529, %17520
  %17531 = lshr i64 %17530, 4
  %17532 = trunc i64 %17531 to i8
  %17533 = and i8 %17532, 1
  store i8 %17533, i8* %27, align 1
  %17534 = icmp eq i64 %17520, 0
  %17535 = zext i1 %17534 to i8
  store i8 %17535, i8* %30, align 1
  %17536 = lshr i64 %17520, 63
  %17537 = trunc i64 %17536 to i8
  store i8 %17537, i8* %33, align 1
  %17538 = lshr i64 %17519, 63
  %17539 = xor i64 %17536, %17538
  %17540 = add nuw nsw i64 %17539, %17536
  %17541 = icmp eq i64 %17540, 2
  %17542 = zext i1 %17541 to i8
  store i8 %17542, i8* %39, align 1
  %17543 = add i64 %17514, -150
  %17544 = add i64 %17516, 17
  store i64 %17544, i64* %3, align 8
  %17545 = inttoptr i64 %17543 to i16*
  %17546 = load i16, i16* %17545, align 2
  %17547 = zext i16 %17546 to i64
  store i64 %17547, i64* %RCX.i11580, align 8
  %17548 = zext i16 %17546 to i64
  %17549 = shl nuw nsw i64 %17548, 4
  store i64 %17549, i64* %573, align 8
  %17550 = add i64 %17549, %17520
  store i64 %17550, i64* %RAX.i11582.pre-phi, align 8
  %17551 = icmp ult i64 %17550, %17520
  %17552 = icmp ult i64 %17550, %17549
  %17553 = or i1 %17551, %17552
  %17554 = zext i1 %17553 to i8
  store i8 %17554, i8* %14, align 1
  %17555 = trunc i64 %17550 to i32
  %17556 = and i32 %17555, 255
  %17557 = tail call i32 @llvm.ctpop.i32(i32 %17556)
  %17558 = trunc i32 %17557 to i8
  %17559 = and i8 %17558, 1
  %17560 = xor i8 %17559, 1
  store i8 %17560, i8* %21, align 1
  %17561 = xor i64 %17549, %17520
  %17562 = xor i64 %17561, %17550
  %17563 = lshr i64 %17562, 4
  %17564 = trunc i64 %17563 to i8
  %17565 = and i8 %17564, 1
  store i8 %17565, i8* %27, align 1
  %17566 = icmp eq i64 %17550, 0
  %17567 = zext i1 %17566 to i8
  store i8 %17567, i8* %30, align 1
  %17568 = lshr i64 %17550, 63
  %17569 = trunc i64 %17568 to i8
  store i8 %17569, i8* %33, align 1
  %17570 = xor i64 %17568, %17536
  %17571 = add nuw nsw i64 %17570, %17568
  %17572 = icmp eq i64 %17571, 2
  %17573 = zext i1 %17572 to i8
  store i8 %17573, i8* %39, align 1
  %17574 = add i64 %17550, 4
  %17575 = add i64 %17516, 29
  store i64 %17575, i64* %3, align 8
  %17576 = inttoptr i64 %17574 to i32*
  %17577 = load i32, i32* %17576, align 4
  %17578 = zext i32 %17577 to i64
  store i64 %17578, i64* %RCX.i11580, align 8
  %17579 = load i64, i64* %RBP.i, align 8
  %17580 = add i64 %17579, -144
  %17581 = add i64 %17516, 35
  store i64 %17581, i64* %3, align 8
  %17582 = inttoptr i64 %17580 to i32*
  %17583 = load i32, i32* %17582, align 4
  %17584 = add i32 %17583, %17577
  %17585 = zext i32 %17584 to i64
  store i64 %17585, i64* %RCX.i11580, align 8
  %17586 = icmp ult i32 %17584, %17577
  %17587 = icmp ult i32 %17584, %17583
  %17588 = or i1 %17586, %17587
  %17589 = zext i1 %17588 to i8
  store i8 %17589, i8* %14, align 1
  %17590 = and i32 %17584, 255
  %17591 = tail call i32 @llvm.ctpop.i32(i32 %17590)
  %17592 = trunc i32 %17591 to i8
  %17593 = and i8 %17592, 1
  %17594 = xor i8 %17593, 1
  store i8 %17594, i8* %21, align 1
  %17595 = xor i32 %17583, %17577
  %17596 = xor i32 %17595, %17584
  %17597 = lshr i32 %17596, 4
  %17598 = trunc i32 %17597 to i8
  %17599 = and i8 %17598, 1
  store i8 %17599, i8* %27, align 1
  %17600 = icmp eq i32 %17584, 0
  %17601 = zext i1 %17600 to i8
  store i8 %17601, i8* %30, align 1
  %17602 = lshr i32 %17584, 31
  %17603 = trunc i32 %17602 to i8
  store i8 %17603, i8* %33, align 1
  %17604 = lshr i32 %17577, 31
  %17605 = lshr i32 %17583, 31
  %17606 = xor i32 %17602, %17604
  %17607 = xor i32 %17602, %17605
  %17608 = add nuw nsw i32 %17606, %17607
  %17609 = icmp eq i32 %17608, 2
  %17610 = zext i1 %17609 to i8
  store i8 %17610, i8* %39, align 1
  %17611 = add i64 %17516, 41
  store i64 %17611, i64* %3, align 8
  store i32 %17584, i32* %17582, align 4
  %17612 = load i64, i64* %RBP.i, align 8
  %17613 = add i64 %17612, -8
  %17614 = load i64, i64* %3, align 8
  %17615 = add i64 %17614, 4
  store i64 %17615, i64* %3, align 8
  %17616 = inttoptr i64 %17613 to i64*
  %17617 = load i64, i64* %17616, align 8
  %17618 = add i64 %17617, 51640
  store i64 %17618, i64* %RAX.i11582.pre-phi, align 8
  %17619 = icmp ugt i64 %17617, -51641
  %17620 = zext i1 %17619 to i8
  store i8 %17620, i8* %14, align 1
  %17621 = trunc i64 %17618 to i32
  %17622 = and i32 %17621, 255
  %17623 = tail call i32 @llvm.ctpop.i32(i32 %17622)
  %17624 = trunc i32 %17623 to i8
  %17625 = and i8 %17624, 1
  %17626 = xor i8 %17625, 1
  store i8 %17626, i8* %21, align 1
  %17627 = xor i64 %17617, 16
  %17628 = xor i64 %17627, %17618
  %17629 = lshr i64 %17628, 4
  %17630 = trunc i64 %17629 to i8
  %17631 = and i8 %17630, 1
  store i8 %17631, i8* %27, align 1
  %17632 = icmp eq i64 %17618, 0
  %17633 = zext i1 %17632 to i8
  store i8 %17633, i8* %30, align 1
  %17634 = lshr i64 %17618, 63
  %17635 = trunc i64 %17634 to i8
  store i8 %17635, i8* %33, align 1
  %17636 = lshr i64 %17617, 63
  %17637 = xor i64 %17634, %17636
  %17638 = add nuw nsw i64 %17637, %17634
  %17639 = icmp eq i64 %17638, 2
  %17640 = zext i1 %17639 to i8
  store i8 %17640, i8* %39, align 1
  %17641 = add i64 %17612, -150
  %17642 = add i64 %17614, 17
  store i64 %17642, i64* %3, align 8
  %17643 = inttoptr i64 %17641 to i16*
  %17644 = load i16, i16* %17643, align 2
  %17645 = zext i16 %17644 to i64
  store i64 %17645, i64* %RCX.i11580, align 8
  %17646 = zext i16 %17644 to i64
  %17647 = shl nuw nsw i64 %17646, 4
  store i64 %17647, i64* %573, align 8
  %17648 = add i64 %17647, %17618
  store i64 %17648, i64* %RAX.i11582.pre-phi, align 8
  %17649 = icmp ult i64 %17648, %17618
  %17650 = icmp ult i64 %17648, %17647
  %17651 = or i1 %17649, %17650
  %17652 = zext i1 %17651 to i8
  store i8 %17652, i8* %14, align 1
  %17653 = trunc i64 %17648 to i32
  %17654 = and i32 %17653, 255
  %17655 = tail call i32 @llvm.ctpop.i32(i32 %17654)
  %17656 = trunc i32 %17655 to i8
  %17657 = and i8 %17656, 1
  %17658 = xor i8 %17657, 1
  store i8 %17658, i8* %21, align 1
  %17659 = xor i64 %17647, %17618
  %17660 = xor i64 %17659, %17648
  %17661 = lshr i64 %17660, 4
  %17662 = trunc i64 %17661 to i8
  %17663 = and i8 %17662, 1
  store i8 %17663, i8* %27, align 1
  %17664 = icmp eq i64 %17648, 0
  %17665 = zext i1 %17664 to i8
  store i8 %17665, i8* %30, align 1
  %17666 = lshr i64 %17648, 63
  %17667 = trunc i64 %17666 to i8
  store i8 %17667, i8* %33, align 1
  %17668 = xor i64 %17666, %17634
  %17669 = add nuw nsw i64 %17668, %17666
  %17670 = icmp eq i64 %17669, 2
  %17671 = zext i1 %17670 to i8
  store i8 %17671, i8* %39, align 1
  %17672 = add i64 %17648, 8
  %17673 = add i64 %17614, 29
  store i64 %17673, i64* %3, align 8
  %17674 = inttoptr i64 %17672 to i32*
  %17675 = load i32, i32* %17674, align 4
  %17676 = zext i32 %17675 to i64
  store i64 %17676, i64* %RCX.i11580, align 8
  %17677 = load i64, i64* %RBP.i, align 8
  %17678 = add i64 %17677, -148
  %17679 = add i64 %17614, 35
  store i64 %17679, i64* %3, align 8
  %17680 = inttoptr i64 %17678 to i32*
  %17681 = load i32, i32* %17680, align 4
  %17682 = add i32 %17681, %17675
  %17683 = zext i32 %17682 to i64
  store i64 %17683, i64* %RCX.i11580, align 8
  %17684 = icmp ult i32 %17682, %17675
  %17685 = icmp ult i32 %17682, %17681
  %17686 = or i1 %17684, %17685
  %17687 = zext i1 %17686 to i8
  store i8 %17687, i8* %14, align 1
  %17688 = and i32 %17682, 255
  %17689 = tail call i32 @llvm.ctpop.i32(i32 %17688)
  %17690 = trunc i32 %17689 to i8
  %17691 = and i8 %17690, 1
  %17692 = xor i8 %17691, 1
  store i8 %17692, i8* %21, align 1
  %17693 = xor i32 %17681, %17675
  %17694 = xor i32 %17693, %17682
  %17695 = lshr i32 %17694, 4
  %17696 = trunc i32 %17695 to i8
  %17697 = and i8 %17696, 1
  store i8 %17697, i8* %27, align 1
  %17698 = icmp eq i32 %17682, 0
  %17699 = zext i1 %17698 to i8
  store i8 %17699, i8* %30, align 1
  %17700 = lshr i32 %17682, 31
  %17701 = trunc i32 %17700 to i8
  store i8 %17701, i8* %33, align 1
  %17702 = lshr i32 %17675, 31
  %17703 = lshr i32 %17681, 31
  %17704 = xor i32 %17700, %17702
  %17705 = xor i32 %17700, %17703
  %17706 = add nuw nsw i32 %17704, %17705
  %17707 = icmp eq i32 %17706, 2
  %17708 = zext i1 %17707 to i8
  store i8 %17708, i8* %39, align 1
  %17709 = add i64 %17614, 41
  store i64 %17709, i64* %3, align 8
  store i32 %17682, i32* %17680, align 4
  %17710 = load i64, i64* %RBP.i, align 8
  %17711 = add i64 %17710, -120
  %17712 = load i64, i64* %3, align 8
  %17713 = add i64 %17712, 4
  store i64 %17713, i64* %3, align 8
  %17714 = inttoptr i64 %17711 to i64*
  %17715 = load i64, i64* %17714, align 8
  store i64 %17715, i64* %RAX.i11582.pre-phi, align 8
  %17716 = add i64 %17710, -28
  %17717 = add i64 %17712, 7
  store i64 %17717, i64* %3, align 8
  %17718 = inttoptr i64 %17716 to i32*
  %17719 = load i32, i32* %17718, align 4
  %17720 = add i32 %17719, 45
  %17721 = zext i32 %17720 to i64
  store i64 %17721, i64* %RCX.i11580, align 8
  %17722 = icmp ugt i32 %17719, -46
  %17723 = zext i1 %17722 to i8
  store i8 %17723, i8* %14, align 1
  %17724 = and i32 %17720, 255
  %17725 = tail call i32 @llvm.ctpop.i32(i32 %17724)
  %17726 = trunc i32 %17725 to i8
  %17727 = and i8 %17726, 1
  %17728 = xor i8 %17727, 1
  store i8 %17728, i8* %21, align 1
  %17729 = xor i32 %17720, %17719
  %17730 = lshr i32 %17729, 4
  %17731 = trunc i32 %17730 to i8
  %17732 = and i8 %17731, 1
  store i8 %17732, i8* %27, align 1
  %17733 = icmp eq i32 %17720, 0
  %17734 = zext i1 %17733 to i8
  store i8 %17734, i8* %30, align 1
  %17735 = lshr i32 %17720, 31
  %17736 = trunc i32 %17735 to i8
  store i8 %17736, i8* %33, align 1
  %17737 = lshr i32 %17719, 31
  %17738 = xor i32 %17735, %17737
  %17739 = add nuw nsw i32 %17738, %17735
  %17740 = icmp eq i32 %17739, 2
  %17741 = zext i1 %17740 to i8
  store i8 %17741, i8* %39, align 1
  %17742 = sext i32 %17720 to i64
  store i64 %17742, i64* %573, align 8
  %17743 = shl nsw i64 %17742, 1
  %17744 = add i64 %17715, %17743
  %17745 = add i64 %17712, 17
  store i64 %17745, i64* %3, align 8
  %17746 = inttoptr i64 %17744 to i16*
  %17747 = load i16, i16* %17746, align 2
  store i16 %17747, i16* %SI.i, align 2
  %17748 = add i64 %17710, -150
  %17749 = add i64 %17712, 24
  store i64 %17749, i64* %3, align 8
  %17750 = inttoptr i64 %17748 to i16*
  store i16 %17747, i16* %17750, align 2
  %17751 = load i64, i64* %RBP.i, align 8
  %17752 = add i64 %17751, -8
  %17753 = load i64, i64* %3, align 8
  %17754 = add i64 %17753, 4
  store i64 %17754, i64* %3, align 8
  %17755 = inttoptr i64 %17752 to i64*
  %17756 = load i64, i64* %17755, align 8
  %17757 = add i64 %17756, 51640
  store i64 %17757, i64* %RAX.i11582.pre-phi, align 8
  %17758 = icmp ugt i64 %17756, -51641
  %17759 = zext i1 %17758 to i8
  store i8 %17759, i8* %14, align 1
  %17760 = trunc i64 %17757 to i32
  %17761 = and i32 %17760, 255
  %17762 = tail call i32 @llvm.ctpop.i32(i32 %17761)
  %17763 = trunc i32 %17762 to i8
  %17764 = and i8 %17763, 1
  %17765 = xor i8 %17764, 1
  store i8 %17765, i8* %21, align 1
  %17766 = xor i64 %17756, 16
  %17767 = xor i64 %17766, %17757
  %17768 = lshr i64 %17767, 4
  %17769 = trunc i64 %17768 to i8
  %17770 = and i8 %17769, 1
  store i8 %17770, i8* %27, align 1
  %17771 = icmp eq i64 %17757, 0
  %17772 = zext i1 %17771 to i8
  store i8 %17772, i8* %30, align 1
  %17773 = lshr i64 %17757, 63
  %17774 = trunc i64 %17773 to i8
  store i8 %17774, i8* %33, align 1
  %17775 = lshr i64 %17756, 63
  %17776 = xor i64 %17773, %17775
  %17777 = add nuw nsw i64 %17776, %17773
  %17778 = icmp eq i64 %17777, 2
  %17779 = zext i1 %17778 to i8
  store i8 %17779, i8* %39, align 1
  %17780 = add i64 %17751, -150
  %17781 = add i64 %17753, 17
  store i64 %17781, i64* %3, align 8
  %17782 = inttoptr i64 %17780 to i16*
  %17783 = load i16, i16* %17782, align 2
  %17784 = zext i16 %17783 to i64
  store i64 %17784, i64* %RCX.i11580, align 8
  %17785 = zext i16 %17783 to i64
  %17786 = shl nuw nsw i64 %17785, 4
  store i64 %17786, i64* %573, align 8
  %17787 = add i64 %17786, %17757
  store i64 %17787, i64* %RAX.i11582.pre-phi, align 8
  %17788 = icmp ult i64 %17787, %17757
  %17789 = icmp ult i64 %17787, %17786
  %17790 = or i1 %17788, %17789
  %17791 = zext i1 %17790 to i8
  store i8 %17791, i8* %14, align 1
  %17792 = trunc i64 %17787 to i32
  %17793 = and i32 %17792, 255
  %17794 = tail call i32 @llvm.ctpop.i32(i32 %17793)
  %17795 = trunc i32 %17794 to i8
  %17796 = and i8 %17795, 1
  %17797 = xor i8 %17796, 1
  store i8 %17797, i8* %21, align 1
  %17798 = xor i64 %17786, %17757
  %17799 = xor i64 %17798, %17787
  %17800 = lshr i64 %17799, 4
  %17801 = trunc i64 %17800 to i8
  %17802 = and i8 %17801, 1
  store i8 %17802, i8* %27, align 1
  %17803 = icmp eq i64 %17787, 0
  %17804 = zext i1 %17803 to i8
  store i8 %17804, i8* %30, align 1
  %17805 = lshr i64 %17787, 63
  %17806 = trunc i64 %17805 to i8
  store i8 %17806, i8* %33, align 1
  %17807 = xor i64 %17805, %17773
  %17808 = add nuw nsw i64 %17807, %17805
  %17809 = icmp eq i64 %17808, 2
  %17810 = zext i1 %17809 to i8
  store i8 %17810, i8* %39, align 1
  %17811 = inttoptr i64 %17787 to i32*
  %17812 = add i64 %17753, 28
  store i64 %17812, i64* %3, align 8
  %17813 = load i32, i32* %17811, align 4
  %17814 = zext i32 %17813 to i64
  store i64 %17814, i64* %RCX.i11580, align 8
  %17815 = load i64, i64* %RBP.i, align 8
  %17816 = add i64 %17815, -140
  %17817 = add i64 %17753, 34
  store i64 %17817, i64* %3, align 8
  %17818 = inttoptr i64 %17816 to i32*
  %17819 = load i32, i32* %17818, align 4
  %17820 = add i32 %17819, %17813
  %17821 = zext i32 %17820 to i64
  store i64 %17821, i64* %RCX.i11580, align 8
  %17822 = icmp ult i32 %17820, %17813
  %17823 = icmp ult i32 %17820, %17819
  %17824 = or i1 %17822, %17823
  %17825 = zext i1 %17824 to i8
  store i8 %17825, i8* %14, align 1
  %17826 = and i32 %17820, 255
  %17827 = tail call i32 @llvm.ctpop.i32(i32 %17826)
  %17828 = trunc i32 %17827 to i8
  %17829 = and i8 %17828, 1
  %17830 = xor i8 %17829, 1
  store i8 %17830, i8* %21, align 1
  %17831 = xor i32 %17819, %17813
  %17832 = xor i32 %17831, %17820
  %17833 = lshr i32 %17832, 4
  %17834 = trunc i32 %17833 to i8
  %17835 = and i8 %17834, 1
  store i8 %17835, i8* %27, align 1
  %17836 = icmp eq i32 %17820, 0
  %17837 = zext i1 %17836 to i8
  store i8 %17837, i8* %30, align 1
  %17838 = lshr i32 %17820, 31
  %17839 = trunc i32 %17838 to i8
  store i8 %17839, i8* %33, align 1
  %17840 = lshr i32 %17813, 31
  %17841 = lshr i32 %17819, 31
  %17842 = xor i32 %17838, %17840
  %17843 = xor i32 %17838, %17841
  %17844 = add nuw nsw i32 %17842, %17843
  %17845 = icmp eq i32 %17844, 2
  %17846 = zext i1 %17845 to i8
  store i8 %17846, i8* %39, align 1
  %17847 = add i64 %17753, 40
  store i64 %17847, i64* %3, align 8
  store i32 %17820, i32* %17818, align 4
  %17848 = load i64, i64* %RBP.i, align 8
  %17849 = add i64 %17848, -8
  %17850 = load i64, i64* %3, align 8
  %17851 = add i64 %17850, 4
  store i64 %17851, i64* %3, align 8
  %17852 = inttoptr i64 %17849 to i64*
  %17853 = load i64, i64* %17852, align 8
  %17854 = add i64 %17853, 51640
  store i64 %17854, i64* %RAX.i11582.pre-phi, align 8
  %17855 = icmp ugt i64 %17853, -51641
  %17856 = zext i1 %17855 to i8
  store i8 %17856, i8* %14, align 1
  %17857 = trunc i64 %17854 to i32
  %17858 = and i32 %17857, 255
  %17859 = tail call i32 @llvm.ctpop.i32(i32 %17858)
  %17860 = trunc i32 %17859 to i8
  %17861 = and i8 %17860, 1
  %17862 = xor i8 %17861, 1
  store i8 %17862, i8* %21, align 1
  %17863 = xor i64 %17853, 16
  %17864 = xor i64 %17863, %17854
  %17865 = lshr i64 %17864, 4
  %17866 = trunc i64 %17865 to i8
  %17867 = and i8 %17866, 1
  store i8 %17867, i8* %27, align 1
  %17868 = icmp eq i64 %17854, 0
  %17869 = zext i1 %17868 to i8
  store i8 %17869, i8* %30, align 1
  %17870 = lshr i64 %17854, 63
  %17871 = trunc i64 %17870 to i8
  store i8 %17871, i8* %33, align 1
  %17872 = lshr i64 %17853, 63
  %17873 = xor i64 %17870, %17872
  %17874 = add nuw nsw i64 %17873, %17870
  %17875 = icmp eq i64 %17874, 2
  %17876 = zext i1 %17875 to i8
  store i8 %17876, i8* %39, align 1
  %17877 = add i64 %17848, -150
  %17878 = add i64 %17850, 17
  store i64 %17878, i64* %3, align 8
  %17879 = inttoptr i64 %17877 to i16*
  %17880 = load i16, i16* %17879, align 2
  %17881 = zext i16 %17880 to i64
  store i64 %17881, i64* %RCX.i11580, align 8
  %17882 = zext i16 %17880 to i64
  %17883 = shl nuw nsw i64 %17882, 4
  store i64 %17883, i64* %573, align 8
  %17884 = add i64 %17883, %17854
  store i64 %17884, i64* %RAX.i11582.pre-phi, align 8
  %17885 = icmp ult i64 %17884, %17854
  %17886 = icmp ult i64 %17884, %17883
  %17887 = or i1 %17885, %17886
  %17888 = zext i1 %17887 to i8
  store i8 %17888, i8* %14, align 1
  %17889 = trunc i64 %17884 to i32
  %17890 = and i32 %17889, 255
  %17891 = tail call i32 @llvm.ctpop.i32(i32 %17890)
  %17892 = trunc i32 %17891 to i8
  %17893 = and i8 %17892, 1
  %17894 = xor i8 %17893, 1
  store i8 %17894, i8* %21, align 1
  %17895 = xor i64 %17883, %17854
  %17896 = xor i64 %17895, %17884
  %17897 = lshr i64 %17896, 4
  %17898 = trunc i64 %17897 to i8
  %17899 = and i8 %17898, 1
  store i8 %17899, i8* %27, align 1
  %17900 = icmp eq i64 %17884, 0
  %17901 = zext i1 %17900 to i8
  store i8 %17901, i8* %30, align 1
  %17902 = lshr i64 %17884, 63
  %17903 = trunc i64 %17902 to i8
  store i8 %17903, i8* %33, align 1
  %17904 = xor i64 %17902, %17870
  %17905 = add nuw nsw i64 %17904, %17902
  %17906 = icmp eq i64 %17905, 2
  %17907 = zext i1 %17906 to i8
  store i8 %17907, i8* %39, align 1
  %17908 = add i64 %17884, 4
  %17909 = add i64 %17850, 29
  store i64 %17909, i64* %3, align 8
  %17910 = inttoptr i64 %17908 to i32*
  %17911 = load i32, i32* %17910, align 4
  %17912 = zext i32 %17911 to i64
  store i64 %17912, i64* %RCX.i11580, align 8
  %17913 = load i64, i64* %RBP.i, align 8
  %17914 = add i64 %17913, -144
  %17915 = add i64 %17850, 35
  store i64 %17915, i64* %3, align 8
  %17916 = inttoptr i64 %17914 to i32*
  %17917 = load i32, i32* %17916, align 4
  %17918 = add i32 %17917, %17911
  %17919 = zext i32 %17918 to i64
  store i64 %17919, i64* %RCX.i11580, align 8
  %17920 = icmp ult i32 %17918, %17911
  %17921 = icmp ult i32 %17918, %17917
  %17922 = or i1 %17920, %17921
  %17923 = zext i1 %17922 to i8
  store i8 %17923, i8* %14, align 1
  %17924 = and i32 %17918, 255
  %17925 = tail call i32 @llvm.ctpop.i32(i32 %17924)
  %17926 = trunc i32 %17925 to i8
  %17927 = and i8 %17926, 1
  %17928 = xor i8 %17927, 1
  store i8 %17928, i8* %21, align 1
  %17929 = xor i32 %17917, %17911
  %17930 = xor i32 %17929, %17918
  %17931 = lshr i32 %17930, 4
  %17932 = trunc i32 %17931 to i8
  %17933 = and i8 %17932, 1
  store i8 %17933, i8* %27, align 1
  %17934 = icmp eq i32 %17918, 0
  %17935 = zext i1 %17934 to i8
  store i8 %17935, i8* %30, align 1
  %17936 = lshr i32 %17918, 31
  %17937 = trunc i32 %17936 to i8
  store i8 %17937, i8* %33, align 1
  %17938 = lshr i32 %17911, 31
  %17939 = lshr i32 %17917, 31
  %17940 = xor i32 %17936, %17938
  %17941 = xor i32 %17936, %17939
  %17942 = add nuw nsw i32 %17940, %17941
  %17943 = icmp eq i32 %17942, 2
  %17944 = zext i1 %17943 to i8
  store i8 %17944, i8* %39, align 1
  %17945 = add i64 %17850, 41
  store i64 %17945, i64* %3, align 8
  store i32 %17918, i32* %17916, align 4
  %17946 = load i64, i64* %RBP.i, align 8
  %17947 = add i64 %17946, -8
  %17948 = load i64, i64* %3, align 8
  %17949 = add i64 %17948, 4
  store i64 %17949, i64* %3, align 8
  %17950 = inttoptr i64 %17947 to i64*
  %17951 = load i64, i64* %17950, align 8
  %17952 = add i64 %17951, 51640
  store i64 %17952, i64* %RAX.i11582.pre-phi, align 8
  %17953 = icmp ugt i64 %17951, -51641
  %17954 = zext i1 %17953 to i8
  store i8 %17954, i8* %14, align 1
  %17955 = trunc i64 %17952 to i32
  %17956 = and i32 %17955, 255
  %17957 = tail call i32 @llvm.ctpop.i32(i32 %17956)
  %17958 = trunc i32 %17957 to i8
  %17959 = and i8 %17958, 1
  %17960 = xor i8 %17959, 1
  store i8 %17960, i8* %21, align 1
  %17961 = xor i64 %17951, 16
  %17962 = xor i64 %17961, %17952
  %17963 = lshr i64 %17962, 4
  %17964 = trunc i64 %17963 to i8
  %17965 = and i8 %17964, 1
  store i8 %17965, i8* %27, align 1
  %17966 = icmp eq i64 %17952, 0
  %17967 = zext i1 %17966 to i8
  store i8 %17967, i8* %30, align 1
  %17968 = lshr i64 %17952, 63
  %17969 = trunc i64 %17968 to i8
  store i8 %17969, i8* %33, align 1
  %17970 = lshr i64 %17951, 63
  %17971 = xor i64 %17968, %17970
  %17972 = add nuw nsw i64 %17971, %17968
  %17973 = icmp eq i64 %17972, 2
  %17974 = zext i1 %17973 to i8
  store i8 %17974, i8* %39, align 1
  %17975 = add i64 %17946, -150
  %17976 = add i64 %17948, 17
  store i64 %17976, i64* %3, align 8
  %17977 = inttoptr i64 %17975 to i16*
  %17978 = load i16, i16* %17977, align 2
  %17979 = zext i16 %17978 to i64
  store i64 %17979, i64* %RCX.i11580, align 8
  %17980 = zext i16 %17978 to i64
  %17981 = shl nuw nsw i64 %17980, 4
  store i64 %17981, i64* %573, align 8
  %17982 = add i64 %17981, %17952
  store i64 %17982, i64* %RAX.i11582.pre-phi, align 8
  %17983 = icmp ult i64 %17982, %17952
  %17984 = icmp ult i64 %17982, %17981
  %17985 = or i1 %17983, %17984
  %17986 = zext i1 %17985 to i8
  store i8 %17986, i8* %14, align 1
  %17987 = trunc i64 %17982 to i32
  %17988 = and i32 %17987, 255
  %17989 = tail call i32 @llvm.ctpop.i32(i32 %17988)
  %17990 = trunc i32 %17989 to i8
  %17991 = and i8 %17990, 1
  %17992 = xor i8 %17991, 1
  store i8 %17992, i8* %21, align 1
  %17993 = xor i64 %17981, %17952
  %17994 = xor i64 %17993, %17982
  %17995 = lshr i64 %17994, 4
  %17996 = trunc i64 %17995 to i8
  %17997 = and i8 %17996, 1
  store i8 %17997, i8* %27, align 1
  %17998 = icmp eq i64 %17982, 0
  %17999 = zext i1 %17998 to i8
  store i8 %17999, i8* %30, align 1
  %18000 = lshr i64 %17982, 63
  %18001 = trunc i64 %18000 to i8
  store i8 %18001, i8* %33, align 1
  %18002 = xor i64 %18000, %17968
  %18003 = add nuw nsw i64 %18002, %18000
  %18004 = icmp eq i64 %18003, 2
  %18005 = zext i1 %18004 to i8
  store i8 %18005, i8* %39, align 1
  %18006 = add i64 %17982, 8
  %18007 = add i64 %17948, 29
  store i64 %18007, i64* %3, align 8
  %18008 = inttoptr i64 %18006 to i32*
  %18009 = load i32, i32* %18008, align 4
  %18010 = zext i32 %18009 to i64
  store i64 %18010, i64* %RCX.i11580, align 8
  %18011 = load i64, i64* %RBP.i, align 8
  %18012 = add i64 %18011, -148
  %18013 = add i64 %17948, 35
  store i64 %18013, i64* %3, align 8
  %18014 = inttoptr i64 %18012 to i32*
  %18015 = load i32, i32* %18014, align 4
  %18016 = add i32 %18015, %18009
  %18017 = zext i32 %18016 to i64
  store i64 %18017, i64* %RCX.i11580, align 8
  %18018 = icmp ult i32 %18016, %18009
  %18019 = icmp ult i32 %18016, %18015
  %18020 = or i1 %18018, %18019
  %18021 = zext i1 %18020 to i8
  store i8 %18021, i8* %14, align 1
  %18022 = and i32 %18016, 255
  %18023 = tail call i32 @llvm.ctpop.i32(i32 %18022)
  %18024 = trunc i32 %18023 to i8
  %18025 = and i8 %18024, 1
  %18026 = xor i8 %18025, 1
  store i8 %18026, i8* %21, align 1
  %18027 = xor i32 %18015, %18009
  %18028 = xor i32 %18027, %18016
  %18029 = lshr i32 %18028, 4
  %18030 = trunc i32 %18029 to i8
  %18031 = and i8 %18030, 1
  store i8 %18031, i8* %27, align 1
  %18032 = icmp eq i32 %18016, 0
  %18033 = zext i1 %18032 to i8
  store i8 %18033, i8* %30, align 1
  %18034 = lshr i32 %18016, 31
  %18035 = trunc i32 %18034 to i8
  store i8 %18035, i8* %33, align 1
  %18036 = lshr i32 %18009, 31
  %18037 = lshr i32 %18015, 31
  %18038 = xor i32 %18034, %18036
  %18039 = xor i32 %18034, %18037
  %18040 = add nuw nsw i32 %18038, %18039
  %18041 = icmp eq i32 %18040, 2
  %18042 = zext i1 %18041 to i8
  store i8 %18042, i8* %39, align 1
  %18043 = add i64 %17948, 41
  store i64 %18043, i64* %3, align 8
  store i32 %18016, i32* %18014, align 4
  %18044 = load i64, i64* %RBP.i, align 8
  %18045 = add i64 %18044, -120
  %18046 = load i64, i64* %3, align 8
  %18047 = add i64 %18046, 4
  store i64 %18047, i64* %3, align 8
  %18048 = inttoptr i64 %18045 to i64*
  %18049 = load i64, i64* %18048, align 8
  store i64 %18049, i64* %RAX.i11582.pre-phi, align 8
  %18050 = add i64 %18044, -28
  %18051 = add i64 %18046, 7
  store i64 %18051, i64* %3, align 8
  %18052 = inttoptr i64 %18050 to i32*
  %18053 = load i32, i32* %18052, align 4
  %18054 = add i32 %18053, 46
  %18055 = zext i32 %18054 to i64
  store i64 %18055, i64* %RCX.i11580, align 8
  %18056 = icmp ugt i32 %18053, -47
  %18057 = zext i1 %18056 to i8
  store i8 %18057, i8* %14, align 1
  %18058 = and i32 %18054, 255
  %18059 = tail call i32 @llvm.ctpop.i32(i32 %18058)
  %18060 = trunc i32 %18059 to i8
  %18061 = and i8 %18060, 1
  %18062 = xor i8 %18061, 1
  store i8 %18062, i8* %21, align 1
  %18063 = xor i32 %18054, %18053
  %18064 = lshr i32 %18063, 4
  %18065 = trunc i32 %18064 to i8
  %18066 = and i8 %18065, 1
  store i8 %18066, i8* %27, align 1
  %18067 = icmp eq i32 %18054, 0
  %18068 = zext i1 %18067 to i8
  store i8 %18068, i8* %30, align 1
  %18069 = lshr i32 %18054, 31
  %18070 = trunc i32 %18069 to i8
  store i8 %18070, i8* %33, align 1
  %18071 = lshr i32 %18053, 31
  %18072 = xor i32 %18069, %18071
  %18073 = add nuw nsw i32 %18072, %18069
  %18074 = icmp eq i32 %18073, 2
  %18075 = zext i1 %18074 to i8
  store i8 %18075, i8* %39, align 1
  %18076 = sext i32 %18054 to i64
  store i64 %18076, i64* %573, align 8
  %18077 = shl nsw i64 %18076, 1
  %18078 = add i64 %18049, %18077
  %18079 = add i64 %18046, 17
  store i64 %18079, i64* %3, align 8
  %18080 = inttoptr i64 %18078 to i16*
  %18081 = load i16, i16* %18080, align 2
  store i16 %18081, i16* %SI.i, align 2
  %18082 = add i64 %18044, -150
  %18083 = add i64 %18046, 24
  store i64 %18083, i64* %3, align 8
  %18084 = inttoptr i64 %18082 to i16*
  store i16 %18081, i16* %18084, align 2
  %18085 = load i64, i64* %RBP.i, align 8
  %18086 = add i64 %18085, -8
  %18087 = load i64, i64* %3, align 8
  %18088 = add i64 %18087, 4
  store i64 %18088, i64* %3, align 8
  %18089 = inttoptr i64 %18086 to i64*
  %18090 = load i64, i64* %18089, align 8
  %18091 = add i64 %18090, 51640
  store i64 %18091, i64* %RAX.i11582.pre-phi, align 8
  %18092 = icmp ugt i64 %18090, -51641
  %18093 = zext i1 %18092 to i8
  store i8 %18093, i8* %14, align 1
  %18094 = trunc i64 %18091 to i32
  %18095 = and i32 %18094, 255
  %18096 = tail call i32 @llvm.ctpop.i32(i32 %18095)
  %18097 = trunc i32 %18096 to i8
  %18098 = and i8 %18097, 1
  %18099 = xor i8 %18098, 1
  store i8 %18099, i8* %21, align 1
  %18100 = xor i64 %18090, 16
  %18101 = xor i64 %18100, %18091
  %18102 = lshr i64 %18101, 4
  %18103 = trunc i64 %18102 to i8
  %18104 = and i8 %18103, 1
  store i8 %18104, i8* %27, align 1
  %18105 = icmp eq i64 %18091, 0
  %18106 = zext i1 %18105 to i8
  store i8 %18106, i8* %30, align 1
  %18107 = lshr i64 %18091, 63
  %18108 = trunc i64 %18107 to i8
  store i8 %18108, i8* %33, align 1
  %18109 = lshr i64 %18090, 63
  %18110 = xor i64 %18107, %18109
  %18111 = add nuw nsw i64 %18110, %18107
  %18112 = icmp eq i64 %18111, 2
  %18113 = zext i1 %18112 to i8
  store i8 %18113, i8* %39, align 1
  %18114 = add i64 %18085, -150
  %18115 = add i64 %18087, 17
  store i64 %18115, i64* %3, align 8
  %18116 = inttoptr i64 %18114 to i16*
  %18117 = load i16, i16* %18116, align 2
  %18118 = zext i16 %18117 to i64
  store i64 %18118, i64* %RCX.i11580, align 8
  %18119 = zext i16 %18117 to i64
  %18120 = shl nuw nsw i64 %18119, 4
  store i64 %18120, i64* %573, align 8
  %18121 = add i64 %18120, %18091
  store i64 %18121, i64* %RAX.i11582.pre-phi, align 8
  %18122 = icmp ult i64 %18121, %18091
  %18123 = icmp ult i64 %18121, %18120
  %18124 = or i1 %18122, %18123
  %18125 = zext i1 %18124 to i8
  store i8 %18125, i8* %14, align 1
  %18126 = trunc i64 %18121 to i32
  %18127 = and i32 %18126, 255
  %18128 = tail call i32 @llvm.ctpop.i32(i32 %18127)
  %18129 = trunc i32 %18128 to i8
  %18130 = and i8 %18129, 1
  %18131 = xor i8 %18130, 1
  store i8 %18131, i8* %21, align 1
  %18132 = xor i64 %18120, %18091
  %18133 = xor i64 %18132, %18121
  %18134 = lshr i64 %18133, 4
  %18135 = trunc i64 %18134 to i8
  %18136 = and i8 %18135, 1
  store i8 %18136, i8* %27, align 1
  %18137 = icmp eq i64 %18121, 0
  %18138 = zext i1 %18137 to i8
  store i8 %18138, i8* %30, align 1
  %18139 = lshr i64 %18121, 63
  %18140 = trunc i64 %18139 to i8
  store i8 %18140, i8* %33, align 1
  %18141 = xor i64 %18139, %18107
  %18142 = add nuw nsw i64 %18141, %18139
  %18143 = icmp eq i64 %18142, 2
  %18144 = zext i1 %18143 to i8
  store i8 %18144, i8* %39, align 1
  %18145 = inttoptr i64 %18121 to i32*
  %18146 = add i64 %18087, 28
  store i64 %18146, i64* %3, align 8
  %18147 = load i32, i32* %18145, align 4
  %18148 = zext i32 %18147 to i64
  store i64 %18148, i64* %RCX.i11580, align 8
  %18149 = load i64, i64* %RBP.i, align 8
  %18150 = add i64 %18149, -140
  %18151 = add i64 %18087, 34
  store i64 %18151, i64* %3, align 8
  %18152 = inttoptr i64 %18150 to i32*
  %18153 = load i32, i32* %18152, align 4
  %18154 = add i32 %18153, %18147
  %18155 = zext i32 %18154 to i64
  store i64 %18155, i64* %RCX.i11580, align 8
  %18156 = icmp ult i32 %18154, %18147
  %18157 = icmp ult i32 %18154, %18153
  %18158 = or i1 %18156, %18157
  %18159 = zext i1 %18158 to i8
  store i8 %18159, i8* %14, align 1
  %18160 = and i32 %18154, 255
  %18161 = tail call i32 @llvm.ctpop.i32(i32 %18160)
  %18162 = trunc i32 %18161 to i8
  %18163 = and i8 %18162, 1
  %18164 = xor i8 %18163, 1
  store i8 %18164, i8* %21, align 1
  %18165 = xor i32 %18153, %18147
  %18166 = xor i32 %18165, %18154
  %18167 = lshr i32 %18166, 4
  %18168 = trunc i32 %18167 to i8
  %18169 = and i8 %18168, 1
  store i8 %18169, i8* %27, align 1
  %18170 = icmp eq i32 %18154, 0
  %18171 = zext i1 %18170 to i8
  store i8 %18171, i8* %30, align 1
  %18172 = lshr i32 %18154, 31
  %18173 = trunc i32 %18172 to i8
  store i8 %18173, i8* %33, align 1
  %18174 = lshr i32 %18147, 31
  %18175 = lshr i32 %18153, 31
  %18176 = xor i32 %18172, %18174
  %18177 = xor i32 %18172, %18175
  %18178 = add nuw nsw i32 %18176, %18177
  %18179 = icmp eq i32 %18178, 2
  %18180 = zext i1 %18179 to i8
  store i8 %18180, i8* %39, align 1
  %18181 = add i64 %18087, 40
  store i64 %18181, i64* %3, align 8
  store i32 %18154, i32* %18152, align 4
  %18182 = load i64, i64* %RBP.i, align 8
  %18183 = add i64 %18182, -8
  %18184 = load i64, i64* %3, align 8
  %18185 = add i64 %18184, 4
  store i64 %18185, i64* %3, align 8
  %18186 = inttoptr i64 %18183 to i64*
  %18187 = load i64, i64* %18186, align 8
  %18188 = add i64 %18187, 51640
  store i64 %18188, i64* %RAX.i11582.pre-phi, align 8
  %18189 = icmp ugt i64 %18187, -51641
  %18190 = zext i1 %18189 to i8
  store i8 %18190, i8* %14, align 1
  %18191 = trunc i64 %18188 to i32
  %18192 = and i32 %18191, 255
  %18193 = tail call i32 @llvm.ctpop.i32(i32 %18192)
  %18194 = trunc i32 %18193 to i8
  %18195 = and i8 %18194, 1
  %18196 = xor i8 %18195, 1
  store i8 %18196, i8* %21, align 1
  %18197 = xor i64 %18187, 16
  %18198 = xor i64 %18197, %18188
  %18199 = lshr i64 %18198, 4
  %18200 = trunc i64 %18199 to i8
  %18201 = and i8 %18200, 1
  store i8 %18201, i8* %27, align 1
  %18202 = icmp eq i64 %18188, 0
  %18203 = zext i1 %18202 to i8
  store i8 %18203, i8* %30, align 1
  %18204 = lshr i64 %18188, 63
  %18205 = trunc i64 %18204 to i8
  store i8 %18205, i8* %33, align 1
  %18206 = lshr i64 %18187, 63
  %18207 = xor i64 %18204, %18206
  %18208 = add nuw nsw i64 %18207, %18204
  %18209 = icmp eq i64 %18208, 2
  %18210 = zext i1 %18209 to i8
  store i8 %18210, i8* %39, align 1
  %18211 = add i64 %18182, -150
  %18212 = add i64 %18184, 17
  store i64 %18212, i64* %3, align 8
  %18213 = inttoptr i64 %18211 to i16*
  %18214 = load i16, i16* %18213, align 2
  %18215 = zext i16 %18214 to i64
  store i64 %18215, i64* %RCX.i11580, align 8
  %18216 = zext i16 %18214 to i64
  %18217 = shl nuw nsw i64 %18216, 4
  store i64 %18217, i64* %573, align 8
  %18218 = add i64 %18217, %18188
  store i64 %18218, i64* %RAX.i11582.pre-phi, align 8
  %18219 = icmp ult i64 %18218, %18188
  %18220 = icmp ult i64 %18218, %18217
  %18221 = or i1 %18219, %18220
  %18222 = zext i1 %18221 to i8
  store i8 %18222, i8* %14, align 1
  %18223 = trunc i64 %18218 to i32
  %18224 = and i32 %18223, 255
  %18225 = tail call i32 @llvm.ctpop.i32(i32 %18224)
  %18226 = trunc i32 %18225 to i8
  %18227 = and i8 %18226, 1
  %18228 = xor i8 %18227, 1
  store i8 %18228, i8* %21, align 1
  %18229 = xor i64 %18217, %18188
  %18230 = xor i64 %18229, %18218
  %18231 = lshr i64 %18230, 4
  %18232 = trunc i64 %18231 to i8
  %18233 = and i8 %18232, 1
  store i8 %18233, i8* %27, align 1
  %18234 = icmp eq i64 %18218, 0
  %18235 = zext i1 %18234 to i8
  store i8 %18235, i8* %30, align 1
  %18236 = lshr i64 %18218, 63
  %18237 = trunc i64 %18236 to i8
  store i8 %18237, i8* %33, align 1
  %18238 = xor i64 %18236, %18204
  %18239 = add nuw nsw i64 %18238, %18236
  %18240 = icmp eq i64 %18239, 2
  %18241 = zext i1 %18240 to i8
  store i8 %18241, i8* %39, align 1
  %18242 = add i64 %18218, 4
  %18243 = add i64 %18184, 29
  store i64 %18243, i64* %3, align 8
  %18244 = inttoptr i64 %18242 to i32*
  %18245 = load i32, i32* %18244, align 4
  %18246 = zext i32 %18245 to i64
  store i64 %18246, i64* %RCX.i11580, align 8
  %18247 = load i64, i64* %RBP.i, align 8
  %18248 = add i64 %18247, -144
  %18249 = add i64 %18184, 35
  store i64 %18249, i64* %3, align 8
  %18250 = inttoptr i64 %18248 to i32*
  %18251 = load i32, i32* %18250, align 4
  %18252 = add i32 %18251, %18245
  %18253 = zext i32 %18252 to i64
  store i64 %18253, i64* %RCX.i11580, align 8
  %18254 = icmp ult i32 %18252, %18245
  %18255 = icmp ult i32 %18252, %18251
  %18256 = or i1 %18254, %18255
  %18257 = zext i1 %18256 to i8
  store i8 %18257, i8* %14, align 1
  %18258 = and i32 %18252, 255
  %18259 = tail call i32 @llvm.ctpop.i32(i32 %18258)
  %18260 = trunc i32 %18259 to i8
  %18261 = and i8 %18260, 1
  %18262 = xor i8 %18261, 1
  store i8 %18262, i8* %21, align 1
  %18263 = xor i32 %18251, %18245
  %18264 = xor i32 %18263, %18252
  %18265 = lshr i32 %18264, 4
  %18266 = trunc i32 %18265 to i8
  %18267 = and i8 %18266, 1
  store i8 %18267, i8* %27, align 1
  %18268 = icmp eq i32 %18252, 0
  %18269 = zext i1 %18268 to i8
  store i8 %18269, i8* %30, align 1
  %18270 = lshr i32 %18252, 31
  %18271 = trunc i32 %18270 to i8
  store i8 %18271, i8* %33, align 1
  %18272 = lshr i32 %18245, 31
  %18273 = lshr i32 %18251, 31
  %18274 = xor i32 %18270, %18272
  %18275 = xor i32 %18270, %18273
  %18276 = add nuw nsw i32 %18274, %18275
  %18277 = icmp eq i32 %18276, 2
  %18278 = zext i1 %18277 to i8
  store i8 %18278, i8* %39, align 1
  %18279 = add i64 %18184, 41
  store i64 %18279, i64* %3, align 8
  store i32 %18252, i32* %18250, align 4
  %18280 = load i64, i64* %RBP.i, align 8
  %18281 = add i64 %18280, -8
  %18282 = load i64, i64* %3, align 8
  %18283 = add i64 %18282, 4
  store i64 %18283, i64* %3, align 8
  %18284 = inttoptr i64 %18281 to i64*
  %18285 = load i64, i64* %18284, align 8
  %18286 = add i64 %18285, 51640
  store i64 %18286, i64* %RAX.i11582.pre-phi, align 8
  %18287 = icmp ugt i64 %18285, -51641
  %18288 = zext i1 %18287 to i8
  store i8 %18288, i8* %14, align 1
  %18289 = trunc i64 %18286 to i32
  %18290 = and i32 %18289, 255
  %18291 = tail call i32 @llvm.ctpop.i32(i32 %18290)
  %18292 = trunc i32 %18291 to i8
  %18293 = and i8 %18292, 1
  %18294 = xor i8 %18293, 1
  store i8 %18294, i8* %21, align 1
  %18295 = xor i64 %18285, 16
  %18296 = xor i64 %18295, %18286
  %18297 = lshr i64 %18296, 4
  %18298 = trunc i64 %18297 to i8
  %18299 = and i8 %18298, 1
  store i8 %18299, i8* %27, align 1
  %18300 = icmp eq i64 %18286, 0
  %18301 = zext i1 %18300 to i8
  store i8 %18301, i8* %30, align 1
  %18302 = lshr i64 %18286, 63
  %18303 = trunc i64 %18302 to i8
  store i8 %18303, i8* %33, align 1
  %18304 = lshr i64 %18285, 63
  %18305 = xor i64 %18302, %18304
  %18306 = add nuw nsw i64 %18305, %18302
  %18307 = icmp eq i64 %18306, 2
  %18308 = zext i1 %18307 to i8
  store i8 %18308, i8* %39, align 1
  %18309 = add i64 %18280, -150
  %18310 = add i64 %18282, 17
  store i64 %18310, i64* %3, align 8
  %18311 = inttoptr i64 %18309 to i16*
  %18312 = load i16, i16* %18311, align 2
  %18313 = zext i16 %18312 to i64
  store i64 %18313, i64* %RCX.i11580, align 8
  %18314 = zext i16 %18312 to i64
  %18315 = shl nuw nsw i64 %18314, 4
  store i64 %18315, i64* %573, align 8
  %18316 = add i64 %18315, %18286
  store i64 %18316, i64* %RAX.i11582.pre-phi, align 8
  %18317 = icmp ult i64 %18316, %18286
  %18318 = icmp ult i64 %18316, %18315
  %18319 = or i1 %18317, %18318
  %18320 = zext i1 %18319 to i8
  store i8 %18320, i8* %14, align 1
  %18321 = trunc i64 %18316 to i32
  %18322 = and i32 %18321, 255
  %18323 = tail call i32 @llvm.ctpop.i32(i32 %18322)
  %18324 = trunc i32 %18323 to i8
  %18325 = and i8 %18324, 1
  %18326 = xor i8 %18325, 1
  store i8 %18326, i8* %21, align 1
  %18327 = xor i64 %18315, %18286
  %18328 = xor i64 %18327, %18316
  %18329 = lshr i64 %18328, 4
  %18330 = trunc i64 %18329 to i8
  %18331 = and i8 %18330, 1
  store i8 %18331, i8* %27, align 1
  %18332 = icmp eq i64 %18316, 0
  %18333 = zext i1 %18332 to i8
  store i8 %18333, i8* %30, align 1
  %18334 = lshr i64 %18316, 63
  %18335 = trunc i64 %18334 to i8
  store i8 %18335, i8* %33, align 1
  %18336 = xor i64 %18334, %18302
  %18337 = add nuw nsw i64 %18336, %18334
  %18338 = icmp eq i64 %18337, 2
  %18339 = zext i1 %18338 to i8
  store i8 %18339, i8* %39, align 1
  %18340 = add i64 %18316, 8
  %18341 = add i64 %18282, 29
  store i64 %18341, i64* %3, align 8
  %18342 = inttoptr i64 %18340 to i32*
  %18343 = load i32, i32* %18342, align 4
  %18344 = zext i32 %18343 to i64
  store i64 %18344, i64* %RCX.i11580, align 8
  %18345 = load i64, i64* %RBP.i, align 8
  %18346 = add i64 %18345, -148
  %18347 = add i64 %18282, 35
  store i64 %18347, i64* %3, align 8
  %18348 = inttoptr i64 %18346 to i32*
  %18349 = load i32, i32* %18348, align 4
  %18350 = add i32 %18349, %18343
  %18351 = zext i32 %18350 to i64
  store i64 %18351, i64* %RCX.i11580, align 8
  %18352 = icmp ult i32 %18350, %18343
  %18353 = icmp ult i32 %18350, %18349
  %18354 = or i1 %18352, %18353
  %18355 = zext i1 %18354 to i8
  store i8 %18355, i8* %14, align 1
  %18356 = and i32 %18350, 255
  %18357 = tail call i32 @llvm.ctpop.i32(i32 %18356)
  %18358 = trunc i32 %18357 to i8
  %18359 = and i8 %18358, 1
  %18360 = xor i8 %18359, 1
  store i8 %18360, i8* %21, align 1
  %18361 = xor i32 %18349, %18343
  %18362 = xor i32 %18361, %18350
  %18363 = lshr i32 %18362, 4
  %18364 = trunc i32 %18363 to i8
  %18365 = and i8 %18364, 1
  store i8 %18365, i8* %27, align 1
  %18366 = icmp eq i32 %18350, 0
  %18367 = zext i1 %18366 to i8
  store i8 %18367, i8* %30, align 1
  %18368 = lshr i32 %18350, 31
  %18369 = trunc i32 %18368 to i8
  store i8 %18369, i8* %33, align 1
  %18370 = lshr i32 %18343, 31
  %18371 = lshr i32 %18349, 31
  %18372 = xor i32 %18368, %18370
  %18373 = xor i32 %18368, %18371
  %18374 = add nuw nsw i32 %18372, %18373
  %18375 = icmp eq i32 %18374, 2
  %18376 = zext i1 %18375 to i8
  store i8 %18376, i8* %39, align 1
  %18377 = add i64 %18282, 41
  store i64 %18377, i64* %3, align 8
  store i32 %18350, i32* %18348, align 4
  %18378 = load i64, i64* %RBP.i, align 8
  %18379 = add i64 %18378, -120
  %18380 = load i64, i64* %3, align 8
  %18381 = add i64 %18380, 4
  store i64 %18381, i64* %3, align 8
  %18382 = inttoptr i64 %18379 to i64*
  %18383 = load i64, i64* %18382, align 8
  store i64 %18383, i64* %RAX.i11582.pre-phi, align 8
  %18384 = add i64 %18378, -28
  %18385 = add i64 %18380, 7
  store i64 %18385, i64* %3, align 8
  %18386 = inttoptr i64 %18384 to i32*
  %18387 = load i32, i32* %18386, align 4
  %18388 = add i32 %18387, 47
  %18389 = zext i32 %18388 to i64
  store i64 %18389, i64* %RCX.i11580, align 8
  %18390 = icmp ugt i32 %18387, -48
  %18391 = zext i1 %18390 to i8
  store i8 %18391, i8* %14, align 1
  %18392 = and i32 %18388, 255
  %18393 = tail call i32 @llvm.ctpop.i32(i32 %18392)
  %18394 = trunc i32 %18393 to i8
  %18395 = and i8 %18394, 1
  %18396 = xor i8 %18395, 1
  store i8 %18396, i8* %21, align 1
  %18397 = xor i32 %18388, %18387
  %18398 = lshr i32 %18397, 4
  %18399 = trunc i32 %18398 to i8
  %18400 = and i8 %18399, 1
  store i8 %18400, i8* %27, align 1
  %18401 = icmp eq i32 %18388, 0
  %18402 = zext i1 %18401 to i8
  store i8 %18402, i8* %30, align 1
  %18403 = lshr i32 %18388, 31
  %18404 = trunc i32 %18403 to i8
  store i8 %18404, i8* %33, align 1
  %18405 = lshr i32 %18387, 31
  %18406 = xor i32 %18403, %18405
  %18407 = add nuw nsw i32 %18406, %18403
  %18408 = icmp eq i32 %18407, 2
  %18409 = zext i1 %18408 to i8
  store i8 %18409, i8* %39, align 1
  %18410 = sext i32 %18388 to i64
  store i64 %18410, i64* %573, align 8
  %18411 = shl nsw i64 %18410, 1
  %18412 = add i64 %18383, %18411
  %18413 = add i64 %18380, 17
  store i64 %18413, i64* %3, align 8
  %18414 = inttoptr i64 %18412 to i16*
  %18415 = load i16, i16* %18414, align 2
  store i16 %18415, i16* %SI.i, align 2
  %18416 = add i64 %18378, -150
  %18417 = add i64 %18380, 24
  store i64 %18417, i64* %3, align 8
  %18418 = inttoptr i64 %18416 to i16*
  store i16 %18415, i16* %18418, align 2
  %18419 = load i64, i64* %RBP.i, align 8
  %18420 = add i64 %18419, -8
  %18421 = load i64, i64* %3, align 8
  %18422 = add i64 %18421, 4
  store i64 %18422, i64* %3, align 8
  %18423 = inttoptr i64 %18420 to i64*
  %18424 = load i64, i64* %18423, align 8
  %18425 = add i64 %18424, 51640
  store i64 %18425, i64* %RAX.i11582.pre-phi, align 8
  %18426 = icmp ugt i64 %18424, -51641
  %18427 = zext i1 %18426 to i8
  store i8 %18427, i8* %14, align 1
  %18428 = trunc i64 %18425 to i32
  %18429 = and i32 %18428, 255
  %18430 = tail call i32 @llvm.ctpop.i32(i32 %18429)
  %18431 = trunc i32 %18430 to i8
  %18432 = and i8 %18431, 1
  %18433 = xor i8 %18432, 1
  store i8 %18433, i8* %21, align 1
  %18434 = xor i64 %18424, 16
  %18435 = xor i64 %18434, %18425
  %18436 = lshr i64 %18435, 4
  %18437 = trunc i64 %18436 to i8
  %18438 = and i8 %18437, 1
  store i8 %18438, i8* %27, align 1
  %18439 = icmp eq i64 %18425, 0
  %18440 = zext i1 %18439 to i8
  store i8 %18440, i8* %30, align 1
  %18441 = lshr i64 %18425, 63
  %18442 = trunc i64 %18441 to i8
  store i8 %18442, i8* %33, align 1
  %18443 = lshr i64 %18424, 63
  %18444 = xor i64 %18441, %18443
  %18445 = add nuw nsw i64 %18444, %18441
  %18446 = icmp eq i64 %18445, 2
  %18447 = zext i1 %18446 to i8
  store i8 %18447, i8* %39, align 1
  %18448 = add i64 %18419, -150
  %18449 = add i64 %18421, 17
  store i64 %18449, i64* %3, align 8
  %18450 = inttoptr i64 %18448 to i16*
  %18451 = load i16, i16* %18450, align 2
  %18452 = zext i16 %18451 to i64
  store i64 %18452, i64* %RCX.i11580, align 8
  %18453 = zext i16 %18451 to i64
  %18454 = shl nuw nsw i64 %18453, 4
  store i64 %18454, i64* %573, align 8
  %18455 = add i64 %18454, %18425
  store i64 %18455, i64* %RAX.i11582.pre-phi, align 8
  %18456 = icmp ult i64 %18455, %18425
  %18457 = icmp ult i64 %18455, %18454
  %18458 = or i1 %18456, %18457
  %18459 = zext i1 %18458 to i8
  store i8 %18459, i8* %14, align 1
  %18460 = trunc i64 %18455 to i32
  %18461 = and i32 %18460, 255
  %18462 = tail call i32 @llvm.ctpop.i32(i32 %18461)
  %18463 = trunc i32 %18462 to i8
  %18464 = and i8 %18463, 1
  %18465 = xor i8 %18464, 1
  store i8 %18465, i8* %21, align 1
  %18466 = xor i64 %18454, %18425
  %18467 = xor i64 %18466, %18455
  %18468 = lshr i64 %18467, 4
  %18469 = trunc i64 %18468 to i8
  %18470 = and i8 %18469, 1
  store i8 %18470, i8* %27, align 1
  %18471 = icmp eq i64 %18455, 0
  %18472 = zext i1 %18471 to i8
  store i8 %18472, i8* %30, align 1
  %18473 = lshr i64 %18455, 63
  %18474 = trunc i64 %18473 to i8
  store i8 %18474, i8* %33, align 1
  %18475 = xor i64 %18473, %18441
  %18476 = add nuw nsw i64 %18475, %18473
  %18477 = icmp eq i64 %18476, 2
  %18478 = zext i1 %18477 to i8
  store i8 %18478, i8* %39, align 1
  %18479 = inttoptr i64 %18455 to i32*
  %18480 = add i64 %18421, 28
  store i64 %18480, i64* %3, align 8
  %18481 = load i32, i32* %18479, align 4
  %18482 = zext i32 %18481 to i64
  store i64 %18482, i64* %RCX.i11580, align 8
  %18483 = load i64, i64* %RBP.i, align 8
  %18484 = add i64 %18483, -140
  %18485 = add i64 %18421, 34
  store i64 %18485, i64* %3, align 8
  %18486 = inttoptr i64 %18484 to i32*
  %18487 = load i32, i32* %18486, align 4
  %18488 = add i32 %18487, %18481
  %18489 = zext i32 %18488 to i64
  store i64 %18489, i64* %RCX.i11580, align 8
  %18490 = icmp ult i32 %18488, %18481
  %18491 = icmp ult i32 %18488, %18487
  %18492 = or i1 %18490, %18491
  %18493 = zext i1 %18492 to i8
  store i8 %18493, i8* %14, align 1
  %18494 = and i32 %18488, 255
  %18495 = tail call i32 @llvm.ctpop.i32(i32 %18494)
  %18496 = trunc i32 %18495 to i8
  %18497 = and i8 %18496, 1
  %18498 = xor i8 %18497, 1
  store i8 %18498, i8* %21, align 1
  %18499 = xor i32 %18487, %18481
  %18500 = xor i32 %18499, %18488
  %18501 = lshr i32 %18500, 4
  %18502 = trunc i32 %18501 to i8
  %18503 = and i8 %18502, 1
  store i8 %18503, i8* %27, align 1
  %18504 = icmp eq i32 %18488, 0
  %18505 = zext i1 %18504 to i8
  store i8 %18505, i8* %30, align 1
  %18506 = lshr i32 %18488, 31
  %18507 = trunc i32 %18506 to i8
  store i8 %18507, i8* %33, align 1
  %18508 = lshr i32 %18481, 31
  %18509 = lshr i32 %18487, 31
  %18510 = xor i32 %18506, %18508
  %18511 = xor i32 %18506, %18509
  %18512 = add nuw nsw i32 %18510, %18511
  %18513 = icmp eq i32 %18512, 2
  %18514 = zext i1 %18513 to i8
  store i8 %18514, i8* %39, align 1
  %18515 = add i64 %18421, 40
  store i64 %18515, i64* %3, align 8
  store i32 %18488, i32* %18486, align 4
  %18516 = load i64, i64* %RBP.i, align 8
  %18517 = add i64 %18516, -8
  %18518 = load i64, i64* %3, align 8
  %18519 = add i64 %18518, 4
  store i64 %18519, i64* %3, align 8
  %18520 = inttoptr i64 %18517 to i64*
  %18521 = load i64, i64* %18520, align 8
  %18522 = add i64 %18521, 51640
  store i64 %18522, i64* %RAX.i11582.pre-phi, align 8
  %18523 = icmp ugt i64 %18521, -51641
  %18524 = zext i1 %18523 to i8
  store i8 %18524, i8* %14, align 1
  %18525 = trunc i64 %18522 to i32
  %18526 = and i32 %18525, 255
  %18527 = tail call i32 @llvm.ctpop.i32(i32 %18526)
  %18528 = trunc i32 %18527 to i8
  %18529 = and i8 %18528, 1
  %18530 = xor i8 %18529, 1
  store i8 %18530, i8* %21, align 1
  %18531 = xor i64 %18521, 16
  %18532 = xor i64 %18531, %18522
  %18533 = lshr i64 %18532, 4
  %18534 = trunc i64 %18533 to i8
  %18535 = and i8 %18534, 1
  store i8 %18535, i8* %27, align 1
  %18536 = icmp eq i64 %18522, 0
  %18537 = zext i1 %18536 to i8
  store i8 %18537, i8* %30, align 1
  %18538 = lshr i64 %18522, 63
  %18539 = trunc i64 %18538 to i8
  store i8 %18539, i8* %33, align 1
  %18540 = lshr i64 %18521, 63
  %18541 = xor i64 %18538, %18540
  %18542 = add nuw nsw i64 %18541, %18538
  %18543 = icmp eq i64 %18542, 2
  %18544 = zext i1 %18543 to i8
  store i8 %18544, i8* %39, align 1
  %18545 = add i64 %18516, -150
  %18546 = add i64 %18518, 17
  store i64 %18546, i64* %3, align 8
  %18547 = inttoptr i64 %18545 to i16*
  %18548 = load i16, i16* %18547, align 2
  %18549 = zext i16 %18548 to i64
  store i64 %18549, i64* %RCX.i11580, align 8
  %18550 = zext i16 %18548 to i64
  %18551 = shl nuw nsw i64 %18550, 4
  store i64 %18551, i64* %573, align 8
  %18552 = add i64 %18551, %18522
  store i64 %18552, i64* %RAX.i11582.pre-phi, align 8
  %18553 = icmp ult i64 %18552, %18522
  %18554 = icmp ult i64 %18552, %18551
  %18555 = or i1 %18553, %18554
  %18556 = zext i1 %18555 to i8
  store i8 %18556, i8* %14, align 1
  %18557 = trunc i64 %18552 to i32
  %18558 = and i32 %18557, 255
  %18559 = tail call i32 @llvm.ctpop.i32(i32 %18558)
  %18560 = trunc i32 %18559 to i8
  %18561 = and i8 %18560, 1
  %18562 = xor i8 %18561, 1
  store i8 %18562, i8* %21, align 1
  %18563 = xor i64 %18551, %18522
  %18564 = xor i64 %18563, %18552
  %18565 = lshr i64 %18564, 4
  %18566 = trunc i64 %18565 to i8
  %18567 = and i8 %18566, 1
  store i8 %18567, i8* %27, align 1
  %18568 = icmp eq i64 %18552, 0
  %18569 = zext i1 %18568 to i8
  store i8 %18569, i8* %30, align 1
  %18570 = lshr i64 %18552, 63
  %18571 = trunc i64 %18570 to i8
  store i8 %18571, i8* %33, align 1
  %18572 = xor i64 %18570, %18538
  %18573 = add nuw nsw i64 %18572, %18570
  %18574 = icmp eq i64 %18573, 2
  %18575 = zext i1 %18574 to i8
  store i8 %18575, i8* %39, align 1
  %18576 = add i64 %18552, 4
  %18577 = add i64 %18518, 29
  store i64 %18577, i64* %3, align 8
  %18578 = inttoptr i64 %18576 to i32*
  %18579 = load i32, i32* %18578, align 4
  %18580 = zext i32 %18579 to i64
  store i64 %18580, i64* %RCX.i11580, align 8
  %18581 = load i64, i64* %RBP.i, align 8
  %18582 = add i64 %18581, -144
  %18583 = add i64 %18518, 35
  store i64 %18583, i64* %3, align 8
  %18584 = inttoptr i64 %18582 to i32*
  %18585 = load i32, i32* %18584, align 4
  %18586 = add i32 %18585, %18579
  %18587 = zext i32 %18586 to i64
  store i64 %18587, i64* %RCX.i11580, align 8
  %18588 = icmp ult i32 %18586, %18579
  %18589 = icmp ult i32 %18586, %18585
  %18590 = or i1 %18588, %18589
  %18591 = zext i1 %18590 to i8
  store i8 %18591, i8* %14, align 1
  %18592 = and i32 %18586, 255
  %18593 = tail call i32 @llvm.ctpop.i32(i32 %18592)
  %18594 = trunc i32 %18593 to i8
  %18595 = and i8 %18594, 1
  %18596 = xor i8 %18595, 1
  store i8 %18596, i8* %21, align 1
  %18597 = xor i32 %18585, %18579
  %18598 = xor i32 %18597, %18586
  %18599 = lshr i32 %18598, 4
  %18600 = trunc i32 %18599 to i8
  %18601 = and i8 %18600, 1
  store i8 %18601, i8* %27, align 1
  %18602 = icmp eq i32 %18586, 0
  %18603 = zext i1 %18602 to i8
  store i8 %18603, i8* %30, align 1
  %18604 = lshr i32 %18586, 31
  %18605 = trunc i32 %18604 to i8
  store i8 %18605, i8* %33, align 1
  %18606 = lshr i32 %18579, 31
  %18607 = lshr i32 %18585, 31
  %18608 = xor i32 %18604, %18606
  %18609 = xor i32 %18604, %18607
  %18610 = add nuw nsw i32 %18608, %18609
  %18611 = icmp eq i32 %18610, 2
  %18612 = zext i1 %18611 to i8
  store i8 %18612, i8* %39, align 1
  %18613 = add i64 %18518, 41
  store i64 %18613, i64* %3, align 8
  store i32 %18586, i32* %18584, align 4
  %18614 = load i64, i64* %RBP.i, align 8
  %18615 = add i64 %18614, -8
  %18616 = load i64, i64* %3, align 8
  %18617 = add i64 %18616, 4
  store i64 %18617, i64* %3, align 8
  %18618 = inttoptr i64 %18615 to i64*
  %18619 = load i64, i64* %18618, align 8
  %18620 = add i64 %18619, 51640
  store i64 %18620, i64* %RAX.i11582.pre-phi, align 8
  %18621 = icmp ugt i64 %18619, -51641
  %18622 = zext i1 %18621 to i8
  store i8 %18622, i8* %14, align 1
  %18623 = trunc i64 %18620 to i32
  %18624 = and i32 %18623, 255
  %18625 = tail call i32 @llvm.ctpop.i32(i32 %18624)
  %18626 = trunc i32 %18625 to i8
  %18627 = and i8 %18626, 1
  %18628 = xor i8 %18627, 1
  store i8 %18628, i8* %21, align 1
  %18629 = xor i64 %18619, 16
  %18630 = xor i64 %18629, %18620
  %18631 = lshr i64 %18630, 4
  %18632 = trunc i64 %18631 to i8
  %18633 = and i8 %18632, 1
  store i8 %18633, i8* %27, align 1
  %18634 = icmp eq i64 %18620, 0
  %18635 = zext i1 %18634 to i8
  store i8 %18635, i8* %30, align 1
  %18636 = lshr i64 %18620, 63
  %18637 = trunc i64 %18636 to i8
  store i8 %18637, i8* %33, align 1
  %18638 = lshr i64 %18619, 63
  %18639 = xor i64 %18636, %18638
  %18640 = add nuw nsw i64 %18639, %18636
  %18641 = icmp eq i64 %18640, 2
  %18642 = zext i1 %18641 to i8
  store i8 %18642, i8* %39, align 1
  %18643 = add i64 %18614, -150
  %18644 = add i64 %18616, 17
  store i64 %18644, i64* %3, align 8
  %18645 = inttoptr i64 %18643 to i16*
  %18646 = load i16, i16* %18645, align 2
  %18647 = zext i16 %18646 to i64
  store i64 %18647, i64* %RCX.i11580, align 8
  %18648 = zext i16 %18646 to i64
  %18649 = shl nuw nsw i64 %18648, 4
  store i64 %18649, i64* %573, align 8
  %18650 = add i64 %18649, %18620
  store i64 %18650, i64* %RAX.i11582.pre-phi, align 8
  %18651 = icmp ult i64 %18650, %18620
  %18652 = icmp ult i64 %18650, %18649
  %18653 = or i1 %18651, %18652
  %18654 = zext i1 %18653 to i8
  store i8 %18654, i8* %14, align 1
  %18655 = trunc i64 %18650 to i32
  %18656 = and i32 %18655, 255
  %18657 = tail call i32 @llvm.ctpop.i32(i32 %18656)
  %18658 = trunc i32 %18657 to i8
  %18659 = and i8 %18658, 1
  %18660 = xor i8 %18659, 1
  store i8 %18660, i8* %21, align 1
  %18661 = xor i64 %18649, %18620
  %18662 = xor i64 %18661, %18650
  %18663 = lshr i64 %18662, 4
  %18664 = trunc i64 %18663 to i8
  %18665 = and i8 %18664, 1
  store i8 %18665, i8* %27, align 1
  %18666 = icmp eq i64 %18650, 0
  %18667 = zext i1 %18666 to i8
  store i8 %18667, i8* %30, align 1
  %18668 = lshr i64 %18650, 63
  %18669 = trunc i64 %18668 to i8
  store i8 %18669, i8* %33, align 1
  %18670 = xor i64 %18668, %18636
  %18671 = add nuw nsw i64 %18670, %18668
  %18672 = icmp eq i64 %18671, 2
  %18673 = zext i1 %18672 to i8
  store i8 %18673, i8* %39, align 1
  %18674 = add i64 %18650, 8
  %18675 = add i64 %18616, 29
  store i64 %18675, i64* %3, align 8
  %18676 = inttoptr i64 %18674 to i32*
  %18677 = load i32, i32* %18676, align 4
  %18678 = zext i32 %18677 to i64
  store i64 %18678, i64* %RCX.i11580, align 8
  %18679 = load i64, i64* %RBP.i, align 8
  %18680 = add i64 %18679, -148
  %18681 = add i64 %18616, 35
  store i64 %18681, i64* %3, align 8
  %18682 = inttoptr i64 %18680 to i32*
  %18683 = load i32, i32* %18682, align 4
  %18684 = add i32 %18683, %18677
  %18685 = zext i32 %18684 to i64
  store i64 %18685, i64* %RCX.i11580, align 8
  %18686 = icmp ult i32 %18684, %18677
  %18687 = icmp ult i32 %18684, %18683
  %18688 = or i1 %18686, %18687
  %18689 = zext i1 %18688 to i8
  store i8 %18689, i8* %14, align 1
  %18690 = and i32 %18684, 255
  %18691 = tail call i32 @llvm.ctpop.i32(i32 %18690)
  %18692 = trunc i32 %18691 to i8
  %18693 = and i8 %18692, 1
  %18694 = xor i8 %18693, 1
  store i8 %18694, i8* %21, align 1
  %18695 = xor i32 %18683, %18677
  %18696 = xor i32 %18695, %18684
  %18697 = lshr i32 %18696, 4
  %18698 = trunc i32 %18697 to i8
  %18699 = and i8 %18698, 1
  store i8 %18699, i8* %27, align 1
  %18700 = icmp eq i32 %18684, 0
  %18701 = zext i1 %18700 to i8
  store i8 %18701, i8* %30, align 1
  %18702 = lshr i32 %18684, 31
  %18703 = trunc i32 %18702 to i8
  store i8 %18703, i8* %33, align 1
  %18704 = lshr i32 %18677, 31
  %18705 = lshr i32 %18683, 31
  %18706 = xor i32 %18702, %18704
  %18707 = xor i32 %18702, %18705
  %18708 = add nuw nsw i32 %18706, %18707
  %18709 = icmp eq i32 %18708, 2
  %18710 = zext i1 %18709 to i8
  store i8 %18710, i8* %39, align 1
  %18711 = add i64 %18616, 41
  store i64 %18711, i64* %3, align 8
  store i32 %18684, i32* %18682, align 4
  %18712 = load i64, i64* %RBP.i, align 8
  %18713 = add i64 %18712, -120
  %18714 = load i64, i64* %3, align 8
  %18715 = add i64 %18714, 4
  store i64 %18715, i64* %3, align 8
  %18716 = inttoptr i64 %18713 to i64*
  %18717 = load i64, i64* %18716, align 8
  store i64 %18717, i64* %RAX.i11582.pre-phi, align 8
  %18718 = add i64 %18712, -28
  %18719 = add i64 %18714, 7
  store i64 %18719, i64* %3, align 8
  %18720 = inttoptr i64 %18718 to i32*
  %18721 = load i32, i32* %18720, align 4
  %18722 = add i32 %18721, 48
  %18723 = zext i32 %18722 to i64
  store i64 %18723, i64* %RCX.i11580, align 8
  %18724 = icmp ugt i32 %18721, -49
  %18725 = zext i1 %18724 to i8
  store i8 %18725, i8* %14, align 1
  %18726 = and i32 %18722, 255
  %18727 = tail call i32 @llvm.ctpop.i32(i32 %18726)
  %18728 = trunc i32 %18727 to i8
  %18729 = and i8 %18728, 1
  %18730 = xor i8 %18729, 1
  store i8 %18730, i8* %21, align 1
  %18731 = xor i32 %18721, 16
  %18732 = xor i32 %18731, %18722
  %18733 = lshr i32 %18732, 4
  %18734 = trunc i32 %18733 to i8
  %18735 = and i8 %18734, 1
  store i8 %18735, i8* %27, align 1
  %18736 = icmp eq i32 %18722, 0
  %18737 = zext i1 %18736 to i8
  store i8 %18737, i8* %30, align 1
  %18738 = lshr i32 %18722, 31
  %18739 = trunc i32 %18738 to i8
  store i8 %18739, i8* %33, align 1
  %18740 = lshr i32 %18721, 31
  %18741 = xor i32 %18738, %18740
  %18742 = add nuw nsw i32 %18741, %18738
  %18743 = icmp eq i32 %18742, 2
  %18744 = zext i1 %18743 to i8
  store i8 %18744, i8* %39, align 1
  %18745 = sext i32 %18722 to i64
  store i64 %18745, i64* %573, align 8
  %18746 = shl nsw i64 %18745, 1
  %18747 = add i64 %18717, %18746
  %18748 = add i64 %18714, 17
  store i64 %18748, i64* %3, align 8
  %18749 = inttoptr i64 %18747 to i16*
  %18750 = load i16, i16* %18749, align 2
  store i16 %18750, i16* %SI.i, align 2
  %18751 = add i64 %18712, -150
  %18752 = add i64 %18714, 24
  store i64 %18752, i64* %3, align 8
  %18753 = inttoptr i64 %18751 to i16*
  store i16 %18750, i16* %18753, align 2
  %18754 = load i64, i64* %RBP.i, align 8
  %18755 = add i64 %18754, -8
  %18756 = load i64, i64* %3, align 8
  %18757 = add i64 %18756, 4
  store i64 %18757, i64* %3, align 8
  %18758 = inttoptr i64 %18755 to i64*
  %18759 = load i64, i64* %18758, align 8
  %18760 = add i64 %18759, 51640
  store i64 %18760, i64* %RAX.i11582.pre-phi, align 8
  %18761 = icmp ugt i64 %18759, -51641
  %18762 = zext i1 %18761 to i8
  store i8 %18762, i8* %14, align 1
  %18763 = trunc i64 %18760 to i32
  %18764 = and i32 %18763, 255
  %18765 = tail call i32 @llvm.ctpop.i32(i32 %18764)
  %18766 = trunc i32 %18765 to i8
  %18767 = and i8 %18766, 1
  %18768 = xor i8 %18767, 1
  store i8 %18768, i8* %21, align 1
  %18769 = xor i64 %18759, 16
  %18770 = xor i64 %18769, %18760
  %18771 = lshr i64 %18770, 4
  %18772 = trunc i64 %18771 to i8
  %18773 = and i8 %18772, 1
  store i8 %18773, i8* %27, align 1
  %18774 = icmp eq i64 %18760, 0
  %18775 = zext i1 %18774 to i8
  store i8 %18775, i8* %30, align 1
  %18776 = lshr i64 %18760, 63
  %18777 = trunc i64 %18776 to i8
  store i8 %18777, i8* %33, align 1
  %18778 = lshr i64 %18759, 63
  %18779 = xor i64 %18776, %18778
  %18780 = add nuw nsw i64 %18779, %18776
  %18781 = icmp eq i64 %18780, 2
  %18782 = zext i1 %18781 to i8
  store i8 %18782, i8* %39, align 1
  %18783 = add i64 %18754, -150
  %18784 = add i64 %18756, 17
  store i64 %18784, i64* %3, align 8
  %18785 = inttoptr i64 %18783 to i16*
  %18786 = load i16, i16* %18785, align 2
  %18787 = zext i16 %18786 to i64
  store i64 %18787, i64* %RCX.i11580, align 8
  %18788 = zext i16 %18786 to i64
  %18789 = shl nuw nsw i64 %18788, 4
  store i64 %18789, i64* %573, align 8
  %18790 = add i64 %18789, %18760
  store i64 %18790, i64* %RAX.i11582.pre-phi, align 8
  %18791 = icmp ult i64 %18790, %18760
  %18792 = icmp ult i64 %18790, %18789
  %18793 = or i1 %18791, %18792
  %18794 = zext i1 %18793 to i8
  store i8 %18794, i8* %14, align 1
  %18795 = trunc i64 %18790 to i32
  %18796 = and i32 %18795, 255
  %18797 = tail call i32 @llvm.ctpop.i32(i32 %18796)
  %18798 = trunc i32 %18797 to i8
  %18799 = and i8 %18798, 1
  %18800 = xor i8 %18799, 1
  store i8 %18800, i8* %21, align 1
  %18801 = xor i64 %18789, %18760
  %18802 = xor i64 %18801, %18790
  %18803 = lshr i64 %18802, 4
  %18804 = trunc i64 %18803 to i8
  %18805 = and i8 %18804, 1
  store i8 %18805, i8* %27, align 1
  %18806 = icmp eq i64 %18790, 0
  %18807 = zext i1 %18806 to i8
  store i8 %18807, i8* %30, align 1
  %18808 = lshr i64 %18790, 63
  %18809 = trunc i64 %18808 to i8
  store i8 %18809, i8* %33, align 1
  %18810 = xor i64 %18808, %18776
  %18811 = add nuw nsw i64 %18810, %18808
  %18812 = icmp eq i64 %18811, 2
  %18813 = zext i1 %18812 to i8
  store i8 %18813, i8* %39, align 1
  %18814 = inttoptr i64 %18790 to i32*
  %18815 = add i64 %18756, 28
  store i64 %18815, i64* %3, align 8
  %18816 = load i32, i32* %18814, align 4
  %18817 = zext i32 %18816 to i64
  store i64 %18817, i64* %RCX.i11580, align 8
  %18818 = load i64, i64* %RBP.i, align 8
  %18819 = add i64 %18818, -140
  %18820 = add i64 %18756, 34
  store i64 %18820, i64* %3, align 8
  %18821 = inttoptr i64 %18819 to i32*
  %18822 = load i32, i32* %18821, align 4
  %18823 = add i32 %18822, %18816
  %18824 = zext i32 %18823 to i64
  store i64 %18824, i64* %RCX.i11580, align 8
  %18825 = icmp ult i32 %18823, %18816
  %18826 = icmp ult i32 %18823, %18822
  %18827 = or i1 %18825, %18826
  %18828 = zext i1 %18827 to i8
  store i8 %18828, i8* %14, align 1
  %18829 = and i32 %18823, 255
  %18830 = tail call i32 @llvm.ctpop.i32(i32 %18829)
  %18831 = trunc i32 %18830 to i8
  %18832 = and i8 %18831, 1
  %18833 = xor i8 %18832, 1
  store i8 %18833, i8* %21, align 1
  %18834 = xor i32 %18822, %18816
  %18835 = xor i32 %18834, %18823
  %18836 = lshr i32 %18835, 4
  %18837 = trunc i32 %18836 to i8
  %18838 = and i8 %18837, 1
  store i8 %18838, i8* %27, align 1
  %18839 = icmp eq i32 %18823, 0
  %18840 = zext i1 %18839 to i8
  store i8 %18840, i8* %30, align 1
  %18841 = lshr i32 %18823, 31
  %18842 = trunc i32 %18841 to i8
  store i8 %18842, i8* %33, align 1
  %18843 = lshr i32 %18816, 31
  %18844 = lshr i32 %18822, 31
  %18845 = xor i32 %18841, %18843
  %18846 = xor i32 %18841, %18844
  %18847 = add nuw nsw i32 %18845, %18846
  %18848 = icmp eq i32 %18847, 2
  %18849 = zext i1 %18848 to i8
  store i8 %18849, i8* %39, align 1
  %18850 = add i64 %18756, 40
  store i64 %18850, i64* %3, align 8
  store i32 %18823, i32* %18821, align 4
  %18851 = load i64, i64* %RBP.i, align 8
  %18852 = add i64 %18851, -8
  %18853 = load i64, i64* %3, align 8
  %18854 = add i64 %18853, 4
  store i64 %18854, i64* %3, align 8
  %18855 = inttoptr i64 %18852 to i64*
  %18856 = load i64, i64* %18855, align 8
  %18857 = add i64 %18856, 51640
  store i64 %18857, i64* %RAX.i11582.pre-phi, align 8
  %18858 = icmp ugt i64 %18856, -51641
  %18859 = zext i1 %18858 to i8
  store i8 %18859, i8* %14, align 1
  %18860 = trunc i64 %18857 to i32
  %18861 = and i32 %18860, 255
  %18862 = tail call i32 @llvm.ctpop.i32(i32 %18861)
  %18863 = trunc i32 %18862 to i8
  %18864 = and i8 %18863, 1
  %18865 = xor i8 %18864, 1
  store i8 %18865, i8* %21, align 1
  %18866 = xor i64 %18856, 16
  %18867 = xor i64 %18866, %18857
  %18868 = lshr i64 %18867, 4
  %18869 = trunc i64 %18868 to i8
  %18870 = and i8 %18869, 1
  store i8 %18870, i8* %27, align 1
  %18871 = icmp eq i64 %18857, 0
  %18872 = zext i1 %18871 to i8
  store i8 %18872, i8* %30, align 1
  %18873 = lshr i64 %18857, 63
  %18874 = trunc i64 %18873 to i8
  store i8 %18874, i8* %33, align 1
  %18875 = lshr i64 %18856, 63
  %18876 = xor i64 %18873, %18875
  %18877 = add nuw nsw i64 %18876, %18873
  %18878 = icmp eq i64 %18877, 2
  %18879 = zext i1 %18878 to i8
  store i8 %18879, i8* %39, align 1
  %18880 = add i64 %18851, -150
  %18881 = add i64 %18853, 17
  store i64 %18881, i64* %3, align 8
  %18882 = inttoptr i64 %18880 to i16*
  %18883 = load i16, i16* %18882, align 2
  %18884 = zext i16 %18883 to i64
  store i64 %18884, i64* %RCX.i11580, align 8
  %18885 = zext i16 %18883 to i64
  %18886 = shl nuw nsw i64 %18885, 4
  store i64 %18886, i64* %573, align 8
  %18887 = add i64 %18886, %18857
  store i64 %18887, i64* %RAX.i11582.pre-phi, align 8
  %18888 = icmp ult i64 %18887, %18857
  %18889 = icmp ult i64 %18887, %18886
  %18890 = or i1 %18888, %18889
  %18891 = zext i1 %18890 to i8
  store i8 %18891, i8* %14, align 1
  %18892 = trunc i64 %18887 to i32
  %18893 = and i32 %18892, 255
  %18894 = tail call i32 @llvm.ctpop.i32(i32 %18893)
  %18895 = trunc i32 %18894 to i8
  %18896 = and i8 %18895, 1
  %18897 = xor i8 %18896, 1
  store i8 %18897, i8* %21, align 1
  %18898 = xor i64 %18886, %18857
  %18899 = xor i64 %18898, %18887
  %18900 = lshr i64 %18899, 4
  %18901 = trunc i64 %18900 to i8
  %18902 = and i8 %18901, 1
  store i8 %18902, i8* %27, align 1
  %18903 = icmp eq i64 %18887, 0
  %18904 = zext i1 %18903 to i8
  store i8 %18904, i8* %30, align 1
  %18905 = lshr i64 %18887, 63
  %18906 = trunc i64 %18905 to i8
  store i8 %18906, i8* %33, align 1
  %18907 = xor i64 %18905, %18873
  %18908 = add nuw nsw i64 %18907, %18905
  %18909 = icmp eq i64 %18908, 2
  %18910 = zext i1 %18909 to i8
  store i8 %18910, i8* %39, align 1
  %18911 = add i64 %18887, 4
  %18912 = add i64 %18853, 29
  store i64 %18912, i64* %3, align 8
  %18913 = inttoptr i64 %18911 to i32*
  %18914 = load i32, i32* %18913, align 4
  %18915 = zext i32 %18914 to i64
  store i64 %18915, i64* %RCX.i11580, align 8
  %18916 = load i64, i64* %RBP.i, align 8
  %18917 = add i64 %18916, -144
  %18918 = add i64 %18853, 35
  store i64 %18918, i64* %3, align 8
  %18919 = inttoptr i64 %18917 to i32*
  %18920 = load i32, i32* %18919, align 4
  %18921 = add i32 %18920, %18914
  %18922 = zext i32 %18921 to i64
  store i64 %18922, i64* %RCX.i11580, align 8
  %18923 = icmp ult i32 %18921, %18914
  %18924 = icmp ult i32 %18921, %18920
  %18925 = or i1 %18923, %18924
  %18926 = zext i1 %18925 to i8
  store i8 %18926, i8* %14, align 1
  %18927 = and i32 %18921, 255
  %18928 = tail call i32 @llvm.ctpop.i32(i32 %18927)
  %18929 = trunc i32 %18928 to i8
  %18930 = and i8 %18929, 1
  %18931 = xor i8 %18930, 1
  store i8 %18931, i8* %21, align 1
  %18932 = xor i32 %18920, %18914
  %18933 = xor i32 %18932, %18921
  %18934 = lshr i32 %18933, 4
  %18935 = trunc i32 %18934 to i8
  %18936 = and i8 %18935, 1
  store i8 %18936, i8* %27, align 1
  %18937 = icmp eq i32 %18921, 0
  %18938 = zext i1 %18937 to i8
  store i8 %18938, i8* %30, align 1
  %18939 = lshr i32 %18921, 31
  %18940 = trunc i32 %18939 to i8
  store i8 %18940, i8* %33, align 1
  %18941 = lshr i32 %18914, 31
  %18942 = lshr i32 %18920, 31
  %18943 = xor i32 %18939, %18941
  %18944 = xor i32 %18939, %18942
  %18945 = add nuw nsw i32 %18943, %18944
  %18946 = icmp eq i32 %18945, 2
  %18947 = zext i1 %18946 to i8
  store i8 %18947, i8* %39, align 1
  %18948 = add i64 %18853, 41
  store i64 %18948, i64* %3, align 8
  store i32 %18921, i32* %18919, align 4
  %18949 = load i64, i64* %RBP.i, align 8
  %18950 = add i64 %18949, -8
  %18951 = load i64, i64* %3, align 8
  %18952 = add i64 %18951, 4
  store i64 %18952, i64* %3, align 8
  %18953 = inttoptr i64 %18950 to i64*
  %18954 = load i64, i64* %18953, align 8
  %18955 = add i64 %18954, 51640
  store i64 %18955, i64* %RAX.i11582.pre-phi, align 8
  %18956 = icmp ugt i64 %18954, -51641
  %18957 = zext i1 %18956 to i8
  store i8 %18957, i8* %14, align 1
  %18958 = trunc i64 %18955 to i32
  %18959 = and i32 %18958, 255
  %18960 = tail call i32 @llvm.ctpop.i32(i32 %18959)
  %18961 = trunc i32 %18960 to i8
  %18962 = and i8 %18961, 1
  %18963 = xor i8 %18962, 1
  store i8 %18963, i8* %21, align 1
  %18964 = xor i64 %18954, 16
  %18965 = xor i64 %18964, %18955
  %18966 = lshr i64 %18965, 4
  %18967 = trunc i64 %18966 to i8
  %18968 = and i8 %18967, 1
  store i8 %18968, i8* %27, align 1
  %18969 = icmp eq i64 %18955, 0
  %18970 = zext i1 %18969 to i8
  store i8 %18970, i8* %30, align 1
  %18971 = lshr i64 %18955, 63
  %18972 = trunc i64 %18971 to i8
  store i8 %18972, i8* %33, align 1
  %18973 = lshr i64 %18954, 63
  %18974 = xor i64 %18971, %18973
  %18975 = add nuw nsw i64 %18974, %18971
  %18976 = icmp eq i64 %18975, 2
  %18977 = zext i1 %18976 to i8
  store i8 %18977, i8* %39, align 1
  %18978 = add i64 %18949, -150
  %18979 = add i64 %18951, 17
  store i64 %18979, i64* %3, align 8
  %18980 = inttoptr i64 %18978 to i16*
  %18981 = load i16, i16* %18980, align 2
  %18982 = zext i16 %18981 to i64
  store i64 %18982, i64* %RCX.i11580, align 8
  %18983 = zext i16 %18981 to i64
  %18984 = shl nuw nsw i64 %18983, 4
  store i64 %18984, i64* %573, align 8
  %18985 = add i64 %18984, %18955
  store i64 %18985, i64* %RAX.i11582.pre-phi, align 8
  %18986 = icmp ult i64 %18985, %18955
  %18987 = icmp ult i64 %18985, %18984
  %18988 = or i1 %18986, %18987
  %18989 = zext i1 %18988 to i8
  store i8 %18989, i8* %14, align 1
  %18990 = trunc i64 %18985 to i32
  %18991 = and i32 %18990, 255
  %18992 = tail call i32 @llvm.ctpop.i32(i32 %18991)
  %18993 = trunc i32 %18992 to i8
  %18994 = and i8 %18993, 1
  %18995 = xor i8 %18994, 1
  store i8 %18995, i8* %21, align 1
  %18996 = xor i64 %18984, %18955
  %18997 = xor i64 %18996, %18985
  %18998 = lshr i64 %18997, 4
  %18999 = trunc i64 %18998 to i8
  %19000 = and i8 %18999, 1
  store i8 %19000, i8* %27, align 1
  %19001 = icmp eq i64 %18985, 0
  %19002 = zext i1 %19001 to i8
  store i8 %19002, i8* %30, align 1
  %19003 = lshr i64 %18985, 63
  %19004 = trunc i64 %19003 to i8
  store i8 %19004, i8* %33, align 1
  %19005 = xor i64 %19003, %18971
  %19006 = add nuw nsw i64 %19005, %19003
  %19007 = icmp eq i64 %19006, 2
  %19008 = zext i1 %19007 to i8
  store i8 %19008, i8* %39, align 1
  %19009 = add i64 %18985, 8
  %19010 = add i64 %18951, 29
  store i64 %19010, i64* %3, align 8
  %19011 = inttoptr i64 %19009 to i32*
  %19012 = load i32, i32* %19011, align 4
  %19013 = zext i32 %19012 to i64
  store i64 %19013, i64* %RCX.i11580, align 8
  %19014 = load i64, i64* %RBP.i, align 8
  %19015 = add i64 %19014, -148
  %19016 = add i64 %18951, 35
  store i64 %19016, i64* %3, align 8
  %19017 = inttoptr i64 %19015 to i32*
  %19018 = load i32, i32* %19017, align 4
  %19019 = add i32 %19018, %19012
  %19020 = zext i32 %19019 to i64
  store i64 %19020, i64* %RCX.i11580, align 8
  %19021 = icmp ult i32 %19019, %19012
  %19022 = icmp ult i32 %19019, %19018
  %19023 = or i1 %19021, %19022
  %19024 = zext i1 %19023 to i8
  store i8 %19024, i8* %14, align 1
  %19025 = and i32 %19019, 255
  %19026 = tail call i32 @llvm.ctpop.i32(i32 %19025)
  %19027 = trunc i32 %19026 to i8
  %19028 = and i8 %19027, 1
  %19029 = xor i8 %19028, 1
  store i8 %19029, i8* %21, align 1
  %19030 = xor i32 %19018, %19012
  %19031 = xor i32 %19030, %19019
  %19032 = lshr i32 %19031, 4
  %19033 = trunc i32 %19032 to i8
  %19034 = and i8 %19033, 1
  store i8 %19034, i8* %27, align 1
  %19035 = icmp eq i32 %19019, 0
  %19036 = zext i1 %19035 to i8
  store i8 %19036, i8* %30, align 1
  %19037 = lshr i32 %19019, 31
  %19038 = trunc i32 %19037 to i8
  store i8 %19038, i8* %33, align 1
  %19039 = lshr i32 %19012, 31
  %19040 = lshr i32 %19018, 31
  %19041 = xor i32 %19037, %19039
  %19042 = xor i32 %19037, %19040
  %19043 = add nuw nsw i32 %19041, %19042
  %19044 = icmp eq i32 %19043, 2
  %19045 = zext i1 %19044 to i8
  store i8 %19045, i8* %39, align 1
  %19046 = add i64 %18951, 41
  store i64 %19046, i64* %3, align 8
  store i32 %19019, i32* %19017, align 4
  %19047 = load i64, i64* %RBP.i, align 8
  %19048 = add i64 %19047, -120
  %19049 = load i64, i64* %3, align 8
  %19050 = add i64 %19049, 4
  store i64 %19050, i64* %3, align 8
  %19051 = inttoptr i64 %19048 to i64*
  %19052 = load i64, i64* %19051, align 8
  store i64 %19052, i64* %RAX.i11582.pre-phi, align 8
  %19053 = add i64 %19047, -28
  %19054 = add i64 %19049, 7
  store i64 %19054, i64* %3, align 8
  %19055 = inttoptr i64 %19053 to i32*
  %19056 = load i32, i32* %19055, align 4
  %19057 = add i32 %19056, 49
  %19058 = zext i32 %19057 to i64
  store i64 %19058, i64* %RCX.i11580, align 8
  %19059 = icmp ugt i32 %19056, -50
  %19060 = zext i1 %19059 to i8
  store i8 %19060, i8* %14, align 1
  %19061 = and i32 %19057, 255
  %19062 = tail call i32 @llvm.ctpop.i32(i32 %19061)
  %19063 = trunc i32 %19062 to i8
  %19064 = and i8 %19063, 1
  %19065 = xor i8 %19064, 1
  store i8 %19065, i8* %21, align 1
  %19066 = xor i32 %19056, 16
  %19067 = xor i32 %19066, %19057
  %19068 = lshr i32 %19067, 4
  %19069 = trunc i32 %19068 to i8
  %19070 = and i8 %19069, 1
  store i8 %19070, i8* %27, align 1
  %19071 = icmp eq i32 %19057, 0
  %19072 = zext i1 %19071 to i8
  store i8 %19072, i8* %30, align 1
  %19073 = lshr i32 %19057, 31
  %19074 = trunc i32 %19073 to i8
  store i8 %19074, i8* %33, align 1
  %19075 = lshr i32 %19056, 31
  %19076 = xor i32 %19073, %19075
  %19077 = add nuw nsw i32 %19076, %19073
  %19078 = icmp eq i32 %19077, 2
  %19079 = zext i1 %19078 to i8
  store i8 %19079, i8* %39, align 1
  %19080 = sext i32 %19057 to i64
  store i64 %19080, i64* %573, align 8
  %19081 = shl nsw i64 %19080, 1
  %19082 = add i64 %19052, %19081
  %19083 = add i64 %19049, 17
  store i64 %19083, i64* %3, align 8
  %19084 = inttoptr i64 %19082 to i16*
  %19085 = load i16, i16* %19084, align 2
  store i16 %19085, i16* %SI.i, align 2
  %19086 = add i64 %19047, -150
  %19087 = add i64 %19049, 24
  store i64 %19087, i64* %3, align 8
  %19088 = inttoptr i64 %19086 to i16*
  store i16 %19085, i16* %19088, align 2
  %19089 = load i64, i64* %RBP.i, align 8
  %19090 = add i64 %19089, -8
  %19091 = load i64, i64* %3, align 8
  %19092 = add i64 %19091, 4
  store i64 %19092, i64* %3, align 8
  %19093 = inttoptr i64 %19090 to i64*
  %19094 = load i64, i64* %19093, align 8
  %19095 = add i64 %19094, 51640
  store i64 %19095, i64* %RAX.i11582.pre-phi, align 8
  %19096 = icmp ugt i64 %19094, -51641
  %19097 = zext i1 %19096 to i8
  store i8 %19097, i8* %14, align 1
  %19098 = trunc i64 %19095 to i32
  %19099 = and i32 %19098, 255
  %19100 = tail call i32 @llvm.ctpop.i32(i32 %19099)
  %19101 = trunc i32 %19100 to i8
  %19102 = and i8 %19101, 1
  %19103 = xor i8 %19102, 1
  store i8 %19103, i8* %21, align 1
  %19104 = xor i64 %19094, 16
  %19105 = xor i64 %19104, %19095
  %19106 = lshr i64 %19105, 4
  %19107 = trunc i64 %19106 to i8
  %19108 = and i8 %19107, 1
  store i8 %19108, i8* %27, align 1
  %19109 = icmp eq i64 %19095, 0
  %19110 = zext i1 %19109 to i8
  store i8 %19110, i8* %30, align 1
  %19111 = lshr i64 %19095, 63
  %19112 = trunc i64 %19111 to i8
  store i8 %19112, i8* %33, align 1
  %19113 = lshr i64 %19094, 63
  %19114 = xor i64 %19111, %19113
  %19115 = add nuw nsw i64 %19114, %19111
  %19116 = icmp eq i64 %19115, 2
  %19117 = zext i1 %19116 to i8
  store i8 %19117, i8* %39, align 1
  %19118 = add i64 %19089, -150
  %19119 = add i64 %19091, 17
  store i64 %19119, i64* %3, align 8
  %19120 = inttoptr i64 %19118 to i16*
  %19121 = load i16, i16* %19120, align 2
  %19122 = zext i16 %19121 to i64
  store i64 %19122, i64* %RCX.i11580, align 8
  %19123 = zext i16 %19121 to i64
  %19124 = shl nuw nsw i64 %19123, 4
  store i64 %19124, i64* %573, align 8
  %19125 = add i64 %19124, %19095
  store i64 %19125, i64* %RAX.i11582.pre-phi, align 8
  %19126 = icmp ult i64 %19125, %19095
  %19127 = icmp ult i64 %19125, %19124
  %19128 = or i1 %19126, %19127
  %19129 = zext i1 %19128 to i8
  store i8 %19129, i8* %14, align 1
  %19130 = trunc i64 %19125 to i32
  %19131 = and i32 %19130, 255
  %19132 = tail call i32 @llvm.ctpop.i32(i32 %19131)
  %19133 = trunc i32 %19132 to i8
  %19134 = and i8 %19133, 1
  %19135 = xor i8 %19134, 1
  store i8 %19135, i8* %21, align 1
  %19136 = xor i64 %19124, %19095
  %19137 = xor i64 %19136, %19125
  %19138 = lshr i64 %19137, 4
  %19139 = trunc i64 %19138 to i8
  %19140 = and i8 %19139, 1
  store i8 %19140, i8* %27, align 1
  %19141 = icmp eq i64 %19125, 0
  %19142 = zext i1 %19141 to i8
  store i8 %19142, i8* %30, align 1
  %19143 = lshr i64 %19125, 63
  %19144 = trunc i64 %19143 to i8
  store i8 %19144, i8* %33, align 1
  %19145 = xor i64 %19143, %19111
  %19146 = add nuw nsw i64 %19145, %19143
  %19147 = icmp eq i64 %19146, 2
  %19148 = zext i1 %19147 to i8
  store i8 %19148, i8* %39, align 1
  %19149 = inttoptr i64 %19125 to i32*
  %19150 = add i64 %19091, 28
  store i64 %19150, i64* %3, align 8
  %19151 = load i32, i32* %19149, align 4
  %19152 = zext i32 %19151 to i64
  store i64 %19152, i64* %RCX.i11580, align 8
  %19153 = load i64, i64* %RBP.i, align 8
  %19154 = add i64 %19153, -140
  %19155 = add i64 %19091, 34
  store i64 %19155, i64* %3, align 8
  %19156 = inttoptr i64 %19154 to i32*
  %19157 = load i32, i32* %19156, align 4
  %19158 = add i32 %19157, %19151
  %19159 = zext i32 %19158 to i64
  store i64 %19159, i64* %RCX.i11580, align 8
  %19160 = icmp ult i32 %19158, %19151
  %19161 = icmp ult i32 %19158, %19157
  %19162 = or i1 %19160, %19161
  %19163 = zext i1 %19162 to i8
  store i8 %19163, i8* %14, align 1
  %19164 = and i32 %19158, 255
  %19165 = tail call i32 @llvm.ctpop.i32(i32 %19164)
  %19166 = trunc i32 %19165 to i8
  %19167 = and i8 %19166, 1
  %19168 = xor i8 %19167, 1
  store i8 %19168, i8* %21, align 1
  %19169 = xor i32 %19157, %19151
  %19170 = xor i32 %19169, %19158
  %19171 = lshr i32 %19170, 4
  %19172 = trunc i32 %19171 to i8
  %19173 = and i8 %19172, 1
  store i8 %19173, i8* %27, align 1
  %19174 = icmp eq i32 %19158, 0
  %19175 = zext i1 %19174 to i8
  store i8 %19175, i8* %30, align 1
  %19176 = lshr i32 %19158, 31
  %19177 = trunc i32 %19176 to i8
  store i8 %19177, i8* %33, align 1
  %19178 = lshr i32 %19151, 31
  %19179 = lshr i32 %19157, 31
  %19180 = xor i32 %19176, %19178
  %19181 = xor i32 %19176, %19179
  %19182 = add nuw nsw i32 %19180, %19181
  %19183 = icmp eq i32 %19182, 2
  %19184 = zext i1 %19183 to i8
  store i8 %19184, i8* %39, align 1
  %19185 = add i64 %19091, 40
  store i64 %19185, i64* %3, align 8
  store i32 %19158, i32* %19156, align 4
  %19186 = load i64, i64* %RBP.i, align 8
  %19187 = add i64 %19186, -8
  %19188 = load i64, i64* %3, align 8
  %19189 = add i64 %19188, 4
  store i64 %19189, i64* %3, align 8
  %19190 = inttoptr i64 %19187 to i64*
  %19191 = load i64, i64* %19190, align 8
  %19192 = add i64 %19191, 51640
  store i64 %19192, i64* %RAX.i11582.pre-phi, align 8
  %19193 = icmp ugt i64 %19191, -51641
  %19194 = zext i1 %19193 to i8
  store i8 %19194, i8* %14, align 1
  %19195 = trunc i64 %19192 to i32
  %19196 = and i32 %19195, 255
  %19197 = tail call i32 @llvm.ctpop.i32(i32 %19196)
  %19198 = trunc i32 %19197 to i8
  %19199 = and i8 %19198, 1
  %19200 = xor i8 %19199, 1
  store i8 %19200, i8* %21, align 1
  %19201 = xor i64 %19191, 16
  %19202 = xor i64 %19201, %19192
  %19203 = lshr i64 %19202, 4
  %19204 = trunc i64 %19203 to i8
  %19205 = and i8 %19204, 1
  store i8 %19205, i8* %27, align 1
  %19206 = icmp eq i64 %19192, 0
  %19207 = zext i1 %19206 to i8
  store i8 %19207, i8* %30, align 1
  %19208 = lshr i64 %19192, 63
  %19209 = trunc i64 %19208 to i8
  store i8 %19209, i8* %33, align 1
  %19210 = lshr i64 %19191, 63
  %19211 = xor i64 %19208, %19210
  %19212 = add nuw nsw i64 %19211, %19208
  %19213 = icmp eq i64 %19212, 2
  %19214 = zext i1 %19213 to i8
  store i8 %19214, i8* %39, align 1
  %19215 = add i64 %19186, -150
  %19216 = add i64 %19188, 17
  store i64 %19216, i64* %3, align 8
  %19217 = inttoptr i64 %19215 to i16*
  %19218 = load i16, i16* %19217, align 2
  %19219 = zext i16 %19218 to i64
  store i64 %19219, i64* %RCX.i11580, align 8
  %19220 = zext i16 %19218 to i64
  %19221 = shl nuw nsw i64 %19220, 4
  store i64 %19221, i64* %573, align 8
  %19222 = add i64 %19221, %19192
  store i64 %19222, i64* %RAX.i11582.pre-phi, align 8
  %19223 = icmp ult i64 %19222, %19192
  %19224 = icmp ult i64 %19222, %19221
  %19225 = or i1 %19223, %19224
  %19226 = zext i1 %19225 to i8
  store i8 %19226, i8* %14, align 1
  %19227 = trunc i64 %19222 to i32
  %19228 = and i32 %19227, 255
  %19229 = tail call i32 @llvm.ctpop.i32(i32 %19228)
  %19230 = trunc i32 %19229 to i8
  %19231 = and i8 %19230, 1
  %19232 = xor i8 %19231, 1
  store i8 %19232, i8* %21, align 1
  %19233 = xor i64 %19221, %19192
  %19234 = xor i64 %19233, %19222
  %19235 = lshr i64 %19234, 4
  %19236 = trunc i64 %19235 to i8
  %19237 = and i8 %19236, 1
  store i8 %19237, i8* %27, align 1
  %19238 = icmp eq i64 %19222, 0
  %19239 = zext i1 %19238 to i8
  store i8 %19239, i8* %30, align 1
  %19240 = lshr i64 %19222, 63
  %19241 = trunc i64 %19240 to i8
  store i8 %19241, i8* %33, align 1
  %19242 = xor i64 %19240, %19208
  %19243 = add nuw nsw i64 %19242, %19240
  %19244 = icmp eq i64 %19243, 2
  %19245 = zext i1 %19244 to i8
  store i8 %19245, i8* %39, align 1
  %19246 = add i64 %19222, 4
  %19247 = add i64 %19188, 29
  store i64 %19247, i64* %3, align 8
  %19248 = inttoptr i64 %19246 to i32*
  %19249 = load i32, i32* %19248, align 4
  %19250 = zext i32 %19249 to i64
  store i64 %19250, i64* %RCX.i11580, align 8
  %19251 = load i64, i64* %RBP.i, align 8
  %19252 = add i64 %19251, -144
  %19253 = add i64 %19188, 35
  store i64 %19253, i64* %3, align 8
  %19254 = inttoptr i64 %19252 to i32*
  %19255 = load i32, i32* %19254, align 4
  %19256 = add i32 %19255, %19249
  %19257 = zext i32 %19256 to i64
  store i64 %19257, i64* %RCX.i11580, align 8
  %19258 = icmp ult i32 %19256, %19249
  %19259 = icmp ult i32 %19256, %19255
  %19260 = or i1 %19258, %19259
  %19261 = zext i1 %19260 to i8
  store i8 %19261, i8* %14, align 1
  %19262 = and i32 %19256, 255
  %19263 = tail call i32 @llvm.ctpop.i32(i32 %19262)
  %19264 = trunc i32 %19263 to i8
  %19265 = and i8 %19264, 1
  %19266 = xor i8 %19265, 1
  store i8 %19266, i8* %21, align 1
  %19267 = xor i32 %19255, %19249
  %19268 = xor i32 %19267, %19256
  %19269 = lshr i32 %19268, 4
  %19270 = trunc i32 %19269 to i8
  %19271 = and i8 %19270, 1
  store i8 %19271, i8* %27, align 1
  %19272 = icmp eq i32 %19256, 0
  %19273 = zext i1 %19272 to i8
  store i8 %19273, i8* %30, align 1
  %19274 = lshr i32 %19256, 31
  %19275 = trunc i32 %19274 to i8
  store i8 %19275, i8* %33, align 1
  %19276 = lshr i32 %19249, 31
  %19277 = lshr i32 %19255, 31
  %19278 = xor i32 %19274, %19276
  %19279 = xor i32 %19274, %19277
  %19280 = add nuw nsw i32 %19278, %19279
  %19281 = icmp eq i32 %19280, 2
  %19282 = zext i1 %19281 to i8
  store i8 %19282, i8* %39, align 1
  %19283 = add i64 %19188, 41
  store i64 %19283, i64* %3, align 8
  store i32 %19256, i32* %19254, align 4
  %19284 = load i64, i64* %RBP.i, align 8
  %19285 = add i64 %19284, -8
  %19286 = load i64, i64* %3, align 8
  %19287 = add i64 %19286, 4
  store i64 %19287, i64* %3, align 8
  %19288 = inttoptr i64 %19285 to i64*
  %19289 = load i64, i64* %19288, align 8
  %19290 = add i64 %19289, 51640
  store i64 %19290, i64* %RAX.i11582.pre-phi, align 8
  %19291 = icmp ugt i64 %19289, -51641
  %19292 = zext i1 %19291 to i8
  store i8 %19292, i8* %14, align 1
  %19293 = trunc i64 %19290 to i32
  %19294 = and i32 %19293, 255
  %19295 = tail call i32 @llvm.ctpop.i32(i32 %19294)
  %19296 = trunc i32 %19295 to i8
  %19297 = and i8 %19296, 1
  %19298 = xor i8 %19297, 1
  store i8 %19298, i8* %21, align 1
  %19299 = xor i64 %19289, 16
  %19300 = xor i64 %19299, %19290
  %19301 = lshr i64 %19300, 4
  %19302 = trunc i64 %19301 to i8
  %19303 = and i8 %19302, 1
  store i8 %19303, i8* %27, align 1
  %19304 = icmp eq i64 %19290, 0
  %19305 = zext i1 %19304 to i8
  store i8 %19305, i8* %30, align 1
  %19306 = lshr i64 %19290, 63
  %19307 = trunc i64 %19306 to i8
  store i8 %19307, i8* %33, align 1
  %19308 = lshr i64 %19289, 63
  %19309 = xor i64 %19306, %19308
  %19310 = add nuw nsw i64 %19309, %19306
  %19311 = icmp eq i64 %19310, 2
  %19312 = zext i1 %19311 to i8
  store i8 %19312, i8* %39, align 1
  %19313 = add i64 %19284, -150
  %19314 = add i64 %19286, 17
  store i64 %19314, i64* %3, align 8
  %19315 = inttoptr i64 %19313 to i16*
  %19316 = load i16, i16* %19315, align 2
  %19317 = zext i16 %19316 to i64
  store i64 %19317, i64* %RCX.i11580, align 8
  %19318 = zext i16 %19316 to i64
  %19319 = shl nuw nsw i64 %19318, 4
  store i64 %19319, i64* %573, align 8
  %19320 = add i64 %19319, %19290
  store i64 %19320, i64* %RAX.i11582.pre-phi, align 8
  %19321 = icmp ult i64 %19320, %19290
  %19322 = icmp ult i64 %19320, %19319
  %19323 = or i1 %19321, %19322
  %19324 = zext i1 %19323 to i8
  store i8 %19324, i8* %14, align 1
  %19325 = trunc i64 %19320 to i32
  %19326 = and i32 %19325, 255
  %19327 = tail call i32 @llvm.ctpop.i32(i32 %19326)
  %19328 = trunc i32 %19327 to i8
  %19329 = and i8 %19328, 1
  %19330 = xor i8 %19329, 1
  store i8 %19330, i8* %21, align 1
  %19331 = xor i64 %19319, %19290
  %19332 = xor i64 %19331, %19320
  %19333 = lshr i64 %19332, 4
  %19334 = trunc i64 %19333 to i8
  %19335 = and i8 %19334, 1
  store i8 %19335, i8* %27, align 1
  %19336 = icmp eq i64 %19320, 0
  %19337 = zext i1 %19336 to i8
  store i8 %19337, i8* %30, align 1
  %19338 = lshr i64 %19320, 63
  %19339 = trunc i64 %19338 to i8
  store i8 %19339, i8* %33, align 1
  %19340 = xor i64 %19338, %19306
  %19341 = add nuw nsw i64 %19340, %19338
  %19342 = icmp eq i64 %19341, 2
  %19343 = zext i1 %19342 to i8
  store i8 %19343, i8* %39, align 1
  %19344 = add i64 %19320, 8
  %19345 = add i64 %19286, 29
  store i64 %19345, i64* %3, align 8
  %19346 = inttoptr i64 %19344 to i32*
  %19347 = load i32, i32* %19346, align 4
  %19348 = zext i32 %19347 to i64
  store i64 %19348, i64* %RCX.i11580, align 8
  %19349 = load i64, i64* %RBP.i, align 8
  %19350 = add i64 %19349, -148
  %19351 = add i64 %19286, 35
  store i64 %19351, i64* %3, align 8
  %19352 = inttoptr i64 %19350 to i32*
  %19353 = load i32, i32* %19352, align 4
  %19354 = add i32 %19353, %19347
  %19355 = zext i32 %19354 to i64
  store i64 %19355, i64* %RCX.i11580, align 8
  %19356 = icmp ult i32 %19354, %19347
  %19357 = icmp ult i32 %19354, %19353
  %19358 = or i1 %19356, %19357
  %19359 = zext i1 %19358 to i8
  store i8 %19359, i8* %14, align 1
  %19360 = and i32 %19354, 255
  %19361 = tail call i32 @llvm.ctpop.i32(i32 %19360)
  %19362 = trunc i32 %19361 to i8
  %19363 = and i8 %19362, 1
  %19364 = xor i8 %19363, 1
  store i8 %19364, i8* %21, align 1
  %19365 = xor i32 %19353, %19347
  %19366 = xor i32 %19365, %19354
  %19367 = lshr i32 %19366, 4
  %19368 = trunc i32 %19367 to i8
  %19369 = and i8 %19368, 1
  store i8 %19369, i8* %27, align 1
  %19370 = icmp eq i32 %19354, 0
  %19371 = zext i1 %19370 to i8
  store i8 %19371, i8* %30, align 1
  %19372 = lshr i32 %19354, 31
  %19373 = trunc i32 %19372 to i8
  store i8 %19373, i8* %33, align 1
  %19374 = lshr i32 %19347, 31
  %19375 = lshr i32 %19353, 31
  %19376 = xor i32 %19372, %19374
  %19377 = xor i32 %19372, %19375
  %19378 = add nuw nsw i32 %19376, %19377
  %19379 = icmp eq i32 %19378, 2
  %19380 = zext i1 %19379 to i8
  store i8 %19380, i8* %39, align 1
  %19381 = add i64 %19286, 41
  store i64 %19381, i64* %3, align 8
  store i32 %19354, i32* %19352, align 4
  %19382 = load i64, i64* %RBP.i, align 8
  %19383 = add i64 %19382, -140
  %19384 = load i64, i64* %3, align 8
  %19385 = add i64 %19384, 6
  store i64 %19385, i64* %3, align 8
  %19386 = inttoptr i64 %19383 to i32*
  %19387 = load i32, i32* %19386, align 4
  %19388 = and i32 %19387, 65535
  %19389 = zext i32 %19388 to i64
  store i64 %19389, i64* %RCX.i11580, align 8
  store i8 0, i8* %14, align 1
  %19390 = and i32 %19387, 255
  %19391 = tail call i32 @llvm.ctpop.i32(i32 %19390)
  %19392 = trunc i32 %19391 to i8
  %19393 = and i8 %19392, 1
  %19394 = xor i8 %19393, 1
  store i8 %19394, i8* %21, align 1
  %19395 = icmp eq i32 %19388, 0
  %19396 = zext i1 %19395 to i8
  store i8 %19396, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %19397 = trunc i32 %19387 to i16
  store i16 %19397, i16* %SI.i, align 2
  %19398 = add i64 %19382, -88
  %19399 = add i64 %19384, 19
  store i64 %19399, i64* %3, align 8
  %19400 = inttoptr i64 %19398 to i16*
  store i16 %19397, i16* %19400, align 2
  %19401 = load i64, i64* %RBP.i, align 8
  %19402 = add i64 %19401, -140
  %19403 = load i64, i64* %3, align 8
  %19404 = add i64 %19403, 6
  store i64 %19404, i64* %3, align 8
  %19405 = inttoptr i64 %19402 to i32*
  %19406 = load i32, i32* %19405, align 4
  %19407 = lshr i32 %19406, 15
  %19408 = trunc i32 %19407 to i8
  %19409 = and i8 %19408, 1
  %19410 = lshr i32 %19406, 16
  %19411 = zext i32 %19410 to i64
  store i64 %19411, i64* %RCX.i11580, align 8
  store i8 %19409, i8* %14, align 1
  %19412 = and i32 %19410, 255
  %19413 = tail call i32 @llvm.ctpop.i32(i32 %19412)
  %19414 = trunc i32 %19413 to i8
  %19415 = and i8 %19414, 1
  %19416 = xor i8 %19415, 1
  store i8 %19416, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %19417 = icmp eq i32 %19410, 0
  %19418 = zext i1 %19417 to i8
  store i8 %19418, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %19419 = trunc i32 %19410 to i16
  store i16 %19419, i16* %SI.i, align 2
  %19420 = add i64 %19401, -86
  %19421 = add i64 %19403, 16
  store i64 %19421, i64* %3, align 8
  %19422 = inttoptr i64 %19420 to i16*
  store i16 %19419, i16* %19422, align 2
  %19423 = load i64, i64* %RBP.i, align 8
  %19424 = add i64 %19423, -144
  %19425 = load i64, i64* %3, align 8
  %19426 = add i64 %19425, 6
  store i64 %19426, i64* %3, align 8
  %19427 = inttoptr i64 %19424 to i32*
  %19428 = load i32, i32* %19427, align 4
  %19429 = and i32 %19428, 65535
  %19430 = zext i32 %19429 to i64
  store i64 %19430, i64* %RCX.i11580, align 8
  store i8 0, i8* %14, align 1
  %19431 = and i32 %19428, 255
  %19432 = tail call i32 @llvm.ctpop.i32(i32 %19431)
  %19433 = trunc i32 %19432 to i8
  %19434 = and i8 %19433, 1
  %19435 = xor i8 %19434, 1
  store i8 %19435, i8* %21, align 1
  %19436 = icmp eq i32 %19429, 0
  %19437 = zext i1 %19436 to i8
  store i8 %19437, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %19438 = trunc i32 %19428 to i16
  store i16 %19438, i16* %SI.i, align 2
  %19439 = add i64 %19423, -84
  %19440 = add i64 %19425, 19
  store i64 %19440, i64* %3, align 8
  %19441 = inttoptr i64 %19439 to i16*
  store i16 %19438, i16* %19441, align 2
  %19442 = load i64, i64* %RBP.i, align 8
  %19443 = add i64 %19442, -144
  %19444 = load i64, i64* %3, align 8
  %19445 = add i64 %19444, 6
  store i64 %19445, i64* %3, align 8
  %19446 = inttoptr i64 %19443 to i32*
  %19447 = load i32, i32* %19446, align 4
  %19448 = lshr i32 %19447, 15
  %19449 = trunc i32 %19448 to i8
  %19450 = and i8 %19449, 1
  %19451 = lshr i32 %19447, 16
  %19452 = zext i32 %19451 to i64
  store i64 %19452, i64* %RCX.i11580, align 8
  store i8 %19450, i8* %14, align 1
  %19453 = and i32 %19451, 255
  %19454 = tail call i32 @llvm.ctpop.i32(i32 %19453)
  %19455 = trunc i32 %19454 to i8
  %19456 = and i8 %19455, 1
  %19457 = xor i8 %19456, 1
  store i8 %19457, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %19458 = icmp eq i32 %19451, 0
  %19459 = zext i1 %19458 to i8
  store i8 %19459, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %19460 = trunc i32 %19451 to i16
  store i16 %19460, i16* %SI.i, align 2
  %19461 = add i64 %19442, -82
  %19462 = add i64 %19444, 16
  store i64 %19462, i64* %3, align 8
  %19463 = inttoptr i64 %19461 to i16*
  store i16 %19460, i16* %19463, align 2
  %19464 = load i64, i64* %RBP.i, align 8
  %19465 = add i64 %19464, -148
  %19466 = load i64, i64* %3, align 8
  %19467 = add i64 %19466, 6
  store i64 %19467, i64* %3, align 8
  %19468 = inttoptr i64 %19465 to i32*
  %19469 = load i32, i32* %19468, align 4
  %19470 = and i32 %19469, 65535
  %19471 = zext i32 %19470 to i64
  store i64 %19471, i64* %RCX.i11580, align 8
  store i8 0, i8* %14, align 1
  %19472 = and i32 %19469, 255
  %19473 = tail call i32 @llvm.ctpop.i32(i32 %19472)
  %19474 = trunc i32 %19473 to i8
  %19475 = and i8 %19474, 1
  %19476 = xor i8 %19475, 1
  store i8 %19476, i8* %21, align 1
  %19477 = icmp eq i32 %19470, 0
  %19478 = zext i1 %19477 to i8
  store i8 %19478, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %19479 = trunc i32 %19469 to i16
  store i16 %19479, i16* %SI.i, align 2
  %19480 = add i64 %19464, -80
  %19481 = add i64 %19466, 19
  store i64 %19481, i64* %3, align 8
  %19482 = inttoptr i64 %19480 to i16*
  store i16 %19479, i16* %19482, align 2
  %19483 = load i64, i64* %RBP.i, align 8
  %19484 = add i64 %19483, -148
  %19485 = load i64, i64* %3, align 8
  %19486 = add i64 %19485, 6
  store i64 %19486, i64* %3, align 8
  %19487 = inttoptr i64 %19484 to i32*
  %19488 = load i32, i32* %19487, align 4
  %19489 = lshr i32 %19488, 15
  %19490 = trunc i32 %19489 to i8
  %19491 = and i8 %19490, 1
  %19492 = lshr i32 %19488, 16
  %19493 = zext i32 %19492 to i64
  store i64 %19493, i64* %RCX.i11580, align 8
  store i8 %19491, i8* %14, align 1
  %19494 = and i32 %19492, 255
  %19495 = tail call i32 @llvm.ctpop.i32(i32 %19494)
  %19496 = trunc i32 %19495 to i8
  %19497 = and i8 %19496, 1
  %19498 = xor i8 %19497, 1
  store i8 %19498, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %19499 = icmp eq i32 %19492, 0
  %19500 = zext i1 %19499 to i8
  store i8 %19500, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %19501 = trunc i32 %19492 to i16
  store i16 %19501, i16* %SI.i, align 2
  %19502 = add i64 %19483, -78
  %19503 = add i64 %19485, 16
  store i64 %19503, i64* %3, align 8
  %19504 = inttoptr i64 %19502 to i16*
  store i16 %19501, i16* %19504, align 2
  %19505 = load i64, i64* %3, align 8
  %19506 = add i64 %19505, 155
  %.pre271 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40c5ef

block_.L_40c559:                                  ; preds = %block_40a833, %block_.L_40a829
  %19507 = phi i64 [ %2626, %block_.L_40a829 ], [ %2660, %block_40a833 ]
  %19508 = phi i64 [ %2527, %block_.L_40a829 ], [ %2661, %block_40a833 ]
  %19509 = add i64 %19508, -28
  %19510 = add i64 %19507, 3
  store i64 %19510, i64* %3, align 8
  %19511 = inttoptr i64 %19509 to i32*
  %19512 = load i32, i32* %19511, align 4
  %19513 = zext i32 %19512 to i64
  store i64 %19513, i64* %RAX.i11582.pre-phi, align 8
  %19514 = add i64 %19508, -20
  %19515 = add i64 %19507, 6
  store i64 %19515, i64* %3, align 8
  %19516 = inttoptr i64 %19514 to i32*
  store i32 %19512, i32* %19516, align 4
  %.pre269 = load i64, i64* %3, align 8
  br label %block_.L_40c55f

block_.L_40c55f:                                  ; preds = %block_.L_40c5d7, %block_.L_40c559
  %19517 = phi i64 [ %19773, %block_.L_40c5d7 ], [ %.pre269, %block_.L_40c559 ]
  %19518 = load i64, i64* %RBP.i, align 8
  %19519 = add i64 %19518, -20
  %19520 = add i64 %19517, 3
  store i64 %19520, i64* %3, align 8
  %19521 = inttoptr i64 %19519 to i32*
  %19522 = load i32, i32* %19521, align 4
  %19523 = zext i32 %19522 to i64
  store i64 %19523, i64* %RAX.i11582.pre-phi, align 8
  %19524 = add i64 %19518, -32
  %19525 = add i64 %19517, 6
  store i64 %19525, i64* %3, align 8
  %19526 = inttoptr i64 %19524 to i32*
  %19527 = load i32, i32* %19526, align 4
  %19528 = sub i32 %19522, %19527
  %19529 = icmp ult i32 %19522, %19527
  %19530 = zext i1 %19529 to i8
  store i8 %19530, i8* %14, align 1
  %19531 = and i32 %19528, 255
  %19532 = tail call i32 @llvm.ctpop.i32(i32 %19531)
  %19533 = trunc i32 %19532 to i8
  %19534 = and i8 %19533, 1
  %19535 = xor i8 %19534, 1
  store i8 %19535, i8* %21, align 1
  %19536 = xor i32 %19527, %19522
  %19537 = xor i32 %19536, %19528
  %19538 = lshr i32 %19537, 4
  %19539 = trunc i32 %19538 to i8
  %19540 = and i8 %19539, 1
  store i8 %19540, i8* %27, align 1
  %19541 = icmp eq i32 %19528, 0
  %19542 = zext i1 %19541 to i8
  store i8 %19542, i8* %30, align 1
  %19543 = lshr i32 %19528, 31
  %19544 = trunc i32 %19543 to i8
  store i8 %19544, i8* %33, align 1
  %19545 = lshr i32 %19522, 31
  %19546 = lshr i32 %19527, 31
  %19547 = xor i32 %19546, %19545
  %19548 = xor i32 %19543, %19545
  %19549 = add nuw nsw i32 %19548, %19547
  %19550 = icmp eq i32 %19549, 2
  %19551 = zext i1 %19550 to i8
  store i8 %19551, i8* %39, align 1
  %19552 = icmp ne i8 %19544, 0
  %19553 = xor i1 %19552, %19550
  %.demorgan292 = or i1 %19541, %19553
  %.v354 = select i1 %.demorgan292, i64 12, i64 139
  %19554 = add i64 %19517, %.v354
  store i64 %19554, i64* %3, align 8
  br i1 %.demorgan292, label %block_40c56b, label %block_.L_40c5ea

block_40c56b:                                     ; preds = %block_.L_40c55f
  %19555 = add i64 %19518, -120
  %19556 = add i64 %19554, 4
  store i64 %19556, i64* %3, align 8
  %19557 = inttoptr i64 %19555 to i64*
  %19558 = load i64, i64* %19557, align 8
  store i64 %19558, i64* %RAX.i11582.pre-phi, align 8
  %19559 = add i64 %19554, 8
  store i64 %19559, i64* %3, align 8
  %19560 = load i32, i32* %19521, align 4
  %19561 = sext i32 %19560 to i64
  store i64 %19561, i64* %RCX.i11580, align 8
  %19562 = shl nsw i64 %19561, 1
  %19563 = add i64 %19562, %19558
  %19564 = add i64 %19554, 12
  store i64 %19564, i64* %3, align 8
  %19565 = inttoptr i64 %19563 to i16*
  %19566 = load i16, i16* %19565, align 2
  store i16 %19566, i16* %DX.i6075, align 2
  %19567 = add i64 %19518, -152
  %19568 = add i64 %19554, 19
  store i64 %19568, i64* %3, align 8
  %19569 = inttoptr i64 %19567 to i16*
  store i16 %19566, i16* %19569, align 2
  %19570 = load i64, i64* %RBP.i, align 8
  %19571 = add i64 %19570, -16
  %19572 = load i64, i64* %3, align 8
  %19573 = add i64 %19572, 7
  store i64 %19573, i64* %3, align 8
  %19574 = inttoptr i64 %19571 to i32*
  store i32 0, i32* %19574, align 4
  %.pre270 = load i64, i64* %3, align 8
  br label %block_.L_40c585

block_.L_40c585:                                  ; preds = %block_40c591, %block_40c56b
  %19575 = phi i64 [ %19743, %block_40c591 ], [ %.pre270, %block_40c56b ]
  %19576 = load i64, i64* %RBP.i, align 8
  %19577 = add i64 %19576, -16
  %19578 = add i64 %19575, 3
  store i64 %19578, i64* %3, align 8
  %19579 = inttoptr i64 %19577 to i32*
  %19580 = load i32, i32* %19579, align 4
  %19581 = zext i32 %19580 to i64
  store i64 %19581, i64* %RAX.i11582.pre-phi, align 8
  %19582 = add i64 %19576, -72
  %19583 = add i64 %19575, 6
  store i64 %19583, i64* %3, align 8
  %19584 = inttoptr i64 %19582 to i32*
  %19585 = load i32, i32* %19584, align 4
  %19586 = sub i32 %19580, %19585
  %19587 = icmp ult i32 %19580, %19585
  %19588 = zext i1 %19587 to i8
  store i8 %19588, i8* %14, align 1
  %19589 = and i32 %19586, 255
  %19590 = tail call i32 @llvm.ctpop.i32(i32 %19589)
  %19591 = trunc i32 %19590 to i8
  %19592 = and i8 %19591, 1
  %19593 = xor i8 %19592, 1
  store i8 %19593, i8* %21, align 1
  %19594 = xor i32 %19585, %19580
  %19595 = xor i32 %19594, %19586
  %19596 = lshr i32 %19595, 4
  %19597 = trunc i32 %19596 to i8
  %19598 = and i8 %19597, 1
  store i8 %19598, i8* %27, align 1
  %19599 = icmp eq i32 %19586, 0
  %19600 = zext i1 %19599 to i8
  store i8 %19600, i8* %30, align 1
  %19601 = lshr i32 %19586, 31
  %19602 = trunc i32 %19601 to i8
  store i8 %19602, i8* %33, align 1
  %19603 = lshr i32 %19580, 31
  %19604 = lshr i32 %19585, 31
  %19605 = xor i32 %19604, %19603
  %19606 = xor i32 %19601, %19603
  %19607 = add nuw nsw i32 %19606, %19605
  %19608 = icmp eq i32 %19607, 2
  %19609 = zext i1 %19608 to i8
  store i8 %19609, i8* %39, align 1
  %19610 = icmp ne i8 %19602, 0
  %19611 = xor i1 %19610, %19608
  %.v293 = select i1 %19611, i64 12, i64 82
  %19612 = add i64 %19575, %.v293
  store i64 %19612, i64* %3, align 8
  br i1 %19611, label %block_40c591, label %block_.L_40c5d7

block_40c591:                                     ; preds = %block_.L_40c585
  %19613 = add i64 %19576, -8
  %19614 = add i64 %19612, 4
  store i64 %19614, i64* %3, align 8
  %19615 = inttoptr i64 %19613 to i64*
  %19616 = load i64, i64* %19615, align 8
  %19617 = add i64 %19616, 37708
  store i64 %19617, i64* %RAX.i11582.pre-phi, align 8
  %19618 = icmp ugt i64 %19616, -37709
  %19619 = zext i1 %19618 to i8
  store i8 %19619, i8* %14, align 1
  %19620 = trunc i64 %19617 to i32
  %19621 = and i32 %19620, 255
  %19622 = tail call i32 @llvm.ctpop.i32(i32 %19621)
  %19623 = trunc i32 %19622 to i8
  %19624 = and i8 %19623, 1
  %19625 = xor i8 %19624, 1
  store i8 %19625, i8* %21, align 1
  %19626 = xor i64 %19617, %19616
  %19627 = lshr i64 %19626, 4
  %19628 = trunc i64 %19627 to i8
  %19629 = and i8 %19628, 1
  store i8 %19629, i8* %27, align 1
  %19630 = icmp eq i64 %19617, 0
  %19631 = zext i1 %19630 to i8
  store i8 %19631, i8* %30, align 1
  %19632 = lshr i64 %19617, 63
  %19633 = trunc i64 %19632 to i8
  store i8 %19633, i8* %33, align 1
  %19634 = lshr i64 %19616, 63
  %19635 = xor i64 %19632, %19634
  %19636 = add nuw nsw i64 %19635, %19632
  %19637 = icmp eq i64 %19636, 2
  %19638 = zext i1 %19637 to i8
  store i8 %19638, i8* %39, align 1
  %19639 = add i64 %19612, 14
  store i64 %19639, i64* %3, align 8
  %19640 = load i32, i32* %19579, align 4
  %19641 = sext i32 %19640 to i64
  %19642 = mul nsw i64 %19641, 258
  store i64 %19642, i64* %RCX.i11580, align 8
  %19643 = lshr i64 %19642, 63
  %19644 = add i64 %19642, %19617
  store i64 %19644, i64* %RAX.i11582.pre-phi, align 8
  %19645 = icmp ult i64 %19644, %19617
  %19646 = icmp ult i64 %19644, %19642
  %19647 = or i1 %19645, %19646
  %19648 = zext i1 %19647 to i8
  store i8 %19648, i8* %14, align 1
  %19649 = trunc i64 %19644 to i32
  %19650 = and i32 %19649, 255
  %19651 = tail call i32 @llvm.ctpop.i32(i32 %19650)
  %19652 = trunc i32 %19651 to i8
  %19653 = and i8 %19652, 1
  %19654 = xor i8 %19653, 1
  store i8 %19654, i8* %21, align 1
  %19655 = xor i64 %19642, %19617
  %19656 = xor i64 %19655, %19644
  %19657 = lshr i64 %19656, 4
  %19658 = trunc i64 %19657 to i8
  %19659 = and i8 %19658, 1
  store i8 %19659, i8* %27, align 1
  %19660 = icmp eq i64 %19644, 0
  %19661 = zext i1 %19660 to i8
  store i8 %19661, i8* %30, align 1
  %19662 = lshr i64 %19644, 63
  %19663 = trunc i64 %19662 to i8
  store i8 %19663, i8* %33, align 1
  %19664 = xor i64 %19662, %19632
  %19665 = xor i64 %19662, %19643
  %19666 = add nuw nsw i64 %19664, %19665
  %19667 = icmp eq i64 %19666, 2
  %19668 = zext i1 %19667 to i8
  store i8 %19668, i8* %39, align 1
  %19669 = load i64, i64* %RBP.i, align 8
  %19670 = add i64 %19669, -152
  %19671 = add i64 %19612, 31
  store i64 %19671, i64* %3, align 8
  %19672 = inttoptr i64 %19670 to i16*
  %19673 = load i16, i16* %19672, align 2
  %19674 = zext i16 %19673 to i64
  store i64 %19674, i64* %576, align 8
  %19675 = zext i16 %19673 to i64
  store i64 %19675, i64* %RCX.i11580, align 8
  %19676 = add i64 %19644, %19675
  %19677 = add i64 %19612, 37
  store i64 %19677, i64* %3, align 8
  %19678 = inttoptr i64 %19676 to i8*
  %19679 = load i8, i8* %19678, align 1
  %19680 = zext i8 %19679 to i64
  store i64 %19680, i64* %576, align 8
  %19681 = add i64 %19669, -16
  %19682 = add i64 %19612, 41
  store i64 %19682, i64* %3, align 8
  %19683 = inttoptr i64 %19681 to i32*
  %19684 = load i32, i32* %19683, align 4
  %19685 = sext i32 %19684 to i64
  store i64 %19685, i64* %RAX.i11582.pre-phi, align 8
  %19686 = shl nsw i64 %19685, 1
  %19687 = add i64 %19669, -88
  %19688 = add i64 %19687, %19686
  %19689 = add i64 %19612, 46
  store i64 %19689, i64* %3, align 8
  %19690 = inttoptr i64 %19688 to i16*
  %19691 = load i16, i16* %19690, align 2
  %19692 = zext i16 %19691 to i32
  %19693 = zext i8 %19679 to i32
  %19694 = zext i8 %19679 to i32
  %19695 = zext i16 %19691 to i32
  %19696 = add nuw nsw i32 %19693, %19695
  %19697 = zext i32 %19696 to i64
  store i64 %19697, i64* %RSI.i11312, align 8
  store i8 0, i8* %14, align 1
  %19698 = and i32 %19696, 255
  %19699 = tail call i32 @llvm.ctpop.i32(i32 %19698)
  %19700 = trunc i32 %19699 to i8
  %19701 = and i8 %19700, 1
  %19702 = xor i8 %19701, 1
  store i8 %19702, i8* %21, align 1
  %19703 = xor i32 %19694, %19692
  %19704 = xor i32 %19703, %19696
  %19705 = lshr i32 %19704, 4
  %19706 = trunc i32 %19705 to i8
  %19707 = and i8 %19706, 1
  store i8 %19707, i8* %27, align 1
  %19708 = icmp eq i32 %19696, 0
  %19709 = zext i1 %19708 to i8
  store i8 %19709, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %19710 = trunc i32 %19696 to i16
  store i16 %19710, i16* %DI.i6029, align 2
  %19711 = add i64 %19612, 56
  store i64 %19711, i64* %3, align 8
  store i16 %19710, i16* %19690, align 2
  %19712 = load i64, i64* %RBP.i, align 8
  %19713 = add i64 %19712, -16
  %19714 = load i64, i64* %3, align 8
  %19715 = add i64 %19714, 3
  store i64 %19715, i64* %3, align 8
  %19716 = inttoptr i64 %19713 to i32*
  %19717 = load i32, i32* %19716, align 4
  %19718 = add i32 %19717, 1
  %19719 = zext i32 %19718 to i64
  store i64 %19719, i64* %RAX.i11582.pre-phi, align 8
  %19720 = icmp eq i32 %19717, -1
  %19721 = icmp eq i32 %19718, 0
  %19722 = or i1 %19720, %19721
  %19723 = zext i1 %19722 to i8
  store i8 %19723, i8* %14, align 1
  %19724 = and i32 %19718, 255
  %19725 = tail call i32 @llvm.ctpop.i32(i32 %19724)
  %19726 = trunc i32 %19725 to i8
  %19727 = and i8 %19726, 1
  %19728 = xor i8 %19727, 1
  store i8 %19728, i8* %21, align 1
  %19729 = xor i32 %19718, %19717
  %19730 = lshr i32 %19729, 4
  %19731 = trunc i32 %19730 to i8
  %19732 = and i8 %19731, 1
  store i8 %19732, i8* %27, align 1
  %19733 = zext i1 %19721 to i8
  store i8 %19733, i8* %30, align 1
  %19734 = lshr i32 %19718, 31
  %19735 = trunc i32 %19734 to i8
  store i8 %19735, i8* %33, align 1
  %19736 = lshr i32 %19717, 31
  %19737 = xor i32 %19734, %19736
  %19738 = add nuw nsw i32 %19737, %19734
  %19739 = icmp eq i32 %19738, 2
  %19740 = zext i1 %19739 to i8
  store i8 %19740, i8* %39, align 1
  %19741 = add i64 %19714, 9
  store i64 %19741, i64* %3, align 8
  store i32 %19718, i32* %19716, align 4
  %19742 = load i64, i64* %3, align 8
  %19743 = add i64 %19742, -77
  store i64 %19743, i64* %3, align 8
  br label %block_.L_40c585

block_.L_40c5d7:                                  ; preds = %block_.L_40c585
  %19744 = add i64 %19576, -20
  %19745 = add i64 %19612, 8
  store i64 %19745, i64* %3, align 8
  %19746 = inttoptr i64 %19744 to i32*
  %19747 = load i32, i32* %19746, align 4
  %19748 = add i32 %19747, 1
  %19749 = zext i32 %19748 to i64
  store i64 %19749, i64* %RAX.i11582.pre-phi, align 8
  %19750 = icmp eq i32 %19747, -1
  %19751 = icmp eq i32 %19748, 0
  %19752 = or i1 %19750, %19751
  %19753 = zext i1 %19752 to i8
  store i8 %19753, i8* %14, align 1
  %19754 = and i32 %19748, 255
  %19755 = tail call i32 @llvm.ctpop.i32(i32 %19754)
  %19756 = trunc i32 %19755 to i8
  %19757 = and i8 %19756, 1
  %19758 = xor i8 %19757, 1
  store i8 %19758, i8* %21, align 1
  %19759 = xor i32 %19748, %19747
  %19760 = lshr i32 %19759, 4
  %19761 = trunc i32 %19760 to i8
  %19762 = and i8 %19761, 1
  store i8 %19762, i8* %27, align 1
  %19763 = zext i1 %19751 to i8
  store i8 %19763, i8* %30, align 1
  %19764 = lshr i32 %19748, 31
  %19765 = trunc i32 %19764 to i8
  store i8 %19765, i8* %33, align 1
  %19766 = lshr i32 %19747, 31
  %19767 = xor i32 %19764, %19766
  %19768 = add nuw nsw i32 %19767, %19764
  %19769 = icmp eq i32 %19768, 2
  %19770 = zext i1 %19769 to i8
  store i8 %19770, i8* %39, align 1
  %19771 = add i64 %19612, 14
  store i64 %19771, i64* %3, align 8
  store i32 %19748, i32* %19746, align 4
  %19772 = load i64, i64* %3, align 8
  %19773 = add i64 %19772, -134
  store i64 %19773, i64* %3, align 8
  br label %block_.L_40c55f

block_.L_40c5ea:                                  ; preds = %block_.L_40c55f
  %19774 = add i64 %19554, 5
  store i64 %19774, i64* %3, align 8
  br label %block_.L_40c5ef

block_.L_40c5ef:                                  ; preds = %block_.L_40c5ea, %block_40a849
  %19775 = phi i64 [ %.pre271, %block_40a849 ], [ %19518, %block_.L_40c5ea ]
  %storemerge82 = phi i64 [ %19506, %block_40a849 ], [ %19774, %block_.L_40c5ea ]
  %19776 = add i64 %19775, -44
  %19777 = add i64 %storemerge82, 7
  store i64 %19777, i64* %3, align 8
  %19778 = inttoptr i64 %19776 to i32*
  store i32 999999999, i32* %19778, align 4
  %19779 = load i64, i64* %RBP.i, align 8
  %19780 = add i64 %19779, -40
  %19781 = load i64, i64* %3, align 8
  %19782 = add i64 %19781, 7
  store i64 %19782, i64* %3, align 8
  %19783 = inttoptr i64 %19780 to i32*
  store i32 -1, i32* %19783, align 4
  %19784 = load i64, i64* %RBP.i, align 8
  %19785 = add i64 %19784, -16
  %19786 = load i64, i64* %3, align 8
  %19787 = add i64 %19786, 7
  store i64 %19787, i64* %3, align 8
  %19788 = inttoptr i64 %19785 to i32*
  store i32 0, i32* %19788, align 4
  %.pre272 = load i64, i64* %3, align 8
  br label %block_.L_40c604

block_.L_40c604:                                  ; preds = %block_.L_40c634, %block_.L_40c5ef
  %19789 = phi i64 [ %19918, %block_.L_40c634 ], [ %.pre272, %block_.L_40c5ef ]
  %19790 = load i64, i64* %RBP.i, align 8
  %19791 = add i64 %19790, -16
  %19792 = add i64 %19789, 3
  store i64 %19792, i64* %3, align 8
  %19793 = inttoptr i64 %19791 to i32*
  %19794 = load i32, i32* %19793, align 4
  %19795 = zext i32 %19794 to i64
  store i64 %19795, i64* %RAX.i11582.pre-phi, align 8
  %19796 = add i64 %19790, -72
  %19797 = add i64 %19789, 6
  store i64 %19797, i64* %3, align 8
  %19798 = inttoptr i64 %19796 to i32*
  %19799 = load i32, i32* %19798, align 4
  %19800 = sub i32 %19794, %19799
  %19801 = icmp ult i32 %19794, %19799
  %19802 = zext i1 %19801 to i8
  store i8 %19802, i8* %14, align 1
  %19803 = and i32 %19800, 255
  %19804 = tail call i32 @llvm.ctpop.i32(i32 %19803)
  %19805 = trunc i32 %19804 to i8
  %19806 = and i8 %19805, 1
  %19807 = xor i8 %19806, 1
  store i8 %19807, i8* %21, align 1
  %19808 = xor i32 %19799, %19794
  %19809 = xor i32 %19808, %19800
  %19810 = lshr i32 %19809, 4
  %19811 = trunc i32 %19810 to i8
  %19812 = and i8 %19811, 1
  store i8 %19812, i8* %27, align 1
  %19813 = icmp eq i32 %19800, 0
  %19814 = zext i1 %19813 to i8
  store i8 %19814, i8* %30, align 1
  %19815 = lshr i32 %19800, 31
  %19816 = trunc i32 %19815 to i8
  store i8 %19816, i8* %33, align 1
  %19817 = lshr i32 %19794, 31
  %19818 = lshr i32 %19799, 31
  %19819 = xor i32 %19818, %19817
  %19820 = xor i32 %19815, %19817
  %19821 = add nuw nsw i32 %19820, %19819
  %19822 = icmp eq i32 %19821, 2
  %19823 = zext i1 %19822 to i8
  store i8 %19823, i8* %39, align 1
  %19824 = icmp ne i8 %19816, 0
  %19825 = xor i1 %19824, %19822
  %.v355 = select i1 %19825, i64 12, i64 67
  %19826 = add i64 %19789, %.v355
  store i64 %19826, i64* %3, align 8
  br i1 %19825, label %block_40c610, label %block_.L_40c647

block_40c610:                                     ; preds = %block_.L_40c604
  %19827 = add i64 %19826, 4
  store i64 %19827, i64* %3, align 8
  %19828 = load i32, i32* %19793, align 4
  %19829 = sext i32 %19828 to i64
  store i64 %19829, i64* %RAX.i11582.pre-phi, align 8
  %19830 = shl nsw i64 %19829, 1
  %19831 = add i64 %19790, -88
  %19832 = add i64 %19831, %19830
  %19833 = add i64 %19826, 9
  store i64 %19833, i64* %3, align 8
  %19834 = inttoptr i64 %19832 to i16*
  %19835 = load i16, i16* %19834, align 2
  %19836 = zext i16 %19835 to i64
  store i64 %19836, i64* %RCX.i11580, align 8
  %19837 = zext i16 %19835 to i32
  %19838 = add i64 %19790, -44
  %19839 = add i64 %19826, 12
  store i64 %19839, i64* %3, align 8
  %19840 = inttoptr i64 %19838 to i32*
  %19841 = load i32, i32* %19840, align 4
  %19842 = sub i32 %19837, %19841
  %19843 = icmp ult i32 %19837, %19841
  %19844 = zext i1 %19843 to i8
  store i8 %19844, i8* %14, align 1
  %19845 = and i32 %19842, 255
  %19846 = tail call i32 @llvm.ctpop.i32(i32 %19845)
  %19847 = trunc i32 %19846 to i8
  %19848 = and i8 %19847, 1
  %19849 = xor i8 %19848, 1
  store i8 %19849, i8* %21, align 1
  %19850 = xor i32 %19841, %19837
  %19851 = xor i32 %19850, %19842
  %19852 = lshr i32 %19851, 4
  %19853 = trunc i32 %19852 to i8
  %19854 = and i8 %19853, 1
  store i8 %19854, i8* %27, align 1
  %19855 = icmp eq i32 %19842, 0
  %19856 = zext i1 %19855 to i8
  store i8 %19856, i8* %30, align 1
  %19857 = lshr i32 %19842, 31
  %19858 = trunc i32 %19857 to i8
  store i8 %19858, i8* %33, align 1
  %19859 = lshr i32 %19841, 31
  %19860 = add nuw nsw i32 %19857, %19859
  %19861 = icmp eq i32 %19860, 2
  %19862 = zext i1 %19861 to i8
  store i8 %19862, i8* %39, align 1
  %19863 = icmp ne i8 %19858, 0
  %19864 = xor i1 %19863, %19861
  %.v359 = select i1 %19864, i64 18, i64 36
  %19865 = add i64 %19826, %.v359
  store i64 %19865, i64* %3, align 8
  br i1 %19864, label %block_40c622, label %block_.L_40c634

block_40c622:                                     ; preds = %block_40c610
  %19866 = add i64 %19865, 4
  store i64 %19866, i64* %3, align 8
  %19867 = load i32, i32* %19793, align 4
  %19868 = sext i32 %19867 to i64
  store i64 %19868, i64* %RAX.i11582.pre-phi, align 8
  %19869 = shl nsw i64 %19868, 1
  %19870 = add i64 %19831, %19869
  %19871 = add i64 %19865, 9
  store i64 %19871, i64* %3, align 8
  %19872 = inttoptr i64 %19870 to i16*
  %19873 = load i16, i16* %19872, align 2
  %19874 = zext i16 %19873 to i64
  store i64 %19874, i64* %RCX.i11580, align 8
  %19875 = zext i16 %19873 to i32
  %19876 = add i64 %19865, 12
  store i64 %19876, i64* %3, align 8
  store i32 %19875, i32* %19840, align 4
  %19877 = load i64, i64* %RBP.i, align 8
  %19878 = add i64 %19877, -16
  %19879 = load i64, i64* %3, align 8
  %19880 = add i64 %19879, 3
  store i64 %19880, i64* %3, align 8
  %19881 = inttoptr i64 %19878 to i32*
  %19882 = load i32, i32* %19881, align 4
  %19883 = zext i32 %19882 to i64
  store i64 %19883, i64* %RCX.i11580, align 8
  %19884 = add i64 %19877, -40
  %19885 = add i64 %19879, 6
  store i64 %19885, i64* %3, align 8
  %19886 = inttoptr i64 %19884 to i32*
  store i32 %19882, i32* %19886, align 4
  %.pre276 = load i64, i64* %3, align 8
  %.pre277 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40c634

block_.L_40c634:                                  ; preds = %block_40c610, %block_40c622
  %19887 = phi i64 [ %19790, %block_40c610 ], [ %.pre277, %block_40c622 ]
  %19888 = phi i64 [ %19865, %block_40c610 ], [ %.pre276, %block_40c622 ]
  %19889 = add i64 %19887, -16
  %19890 = add i64 %19888, 8
  store i64 %19890, i64* %3, align 8
  %19891 = inttoptr i64 %19889 to i32*
  %19892 = load i32, i32* %19891, align 4
  %19893 = add i32 %19892, 1
  %19894 = zext i32 %19893 to i64
  store i64 %19894, i64* %RAX.i11582.pre-phi, align 8
  %19895 = icmp eq i32 %19892, -1
  %19896 = icmp eq i32 %19893, 0
  %19897 = or i1 %19895, %19896
  %19898 = zext i1 %19897 to i8
  store i8 %19898, i8* %14, align 1
  %19899 = and i32 %19893, 255
  %19900 = tail call i32 @llvm.ctpop.i32(i32 %19899)
  %19901 = trunc i32 %19900 to i8
  %19902 = and i8 %19901, 1
  %19903 = xor i8 %19902, 1
  store i8 %19903, i8* %21, align 1
  %19904 = xor i32 %19893, %19892
  %19905 = lshr i32 %19904, 4
  %19906 = trunc i32 %19905 to i8
  %19907 = and i8 %19906, 1
  store i8 %19907, i8* %27, align 1
  %19908 = zext i1 %19896 to i8
  store i8 %19908, i8* %30, align 1
  %19909 = lshr i32 %19893, 31
  %19910 = trunc i32 %19909 to i8
  store i8 %19910, i8* %33, align 1
  %19911 = lshr i32 %19892, 31
  %19912 = xor i32 %19909, %19911
  %19913 = add nuw nsw i32 %19912, %19909
  %19914 = icmp eq i32 %19913, 2
  %19915 = zext i1 %19914 to i8
  store i8 %19915, i8* %39, align 1
  %19916 = add i64 %19888, 14
  store i64 %19916, i64* %3, align 8
  store i32 %19893, i32* %19891, align 4
  %19917 = load i64, i64* %3, align 8
  %19918 = add i64 %19917, -62
  store i64 %19918, i64* %3, align 8
  br label %block_.L_40c604

block_.L_40c647:                                  ; preds = %block_.L_40c604
  %19919 = add i64 %19790, -44
  %19920 = add i64 %19826, 3
  store i64 %19920, i64* %3, align 8
  %19921 = inttoptr i64 %19919 to i32*
  %19922 = load i32, i32* %19921, align 4
  %19923 = zext i32 %19922 to i64
  store i64 %19923, i64* %RAX.i11582.pre-phi, align 8
  %19924 = add i64 %19790, -36
  %19925 = add i64 %19826, 6
  store i64 %19925, i64* %3, align 8
  %19926 = inttoptr i64 %19924 to i32*
  %19927 = load i32, i32* %19926, align 4
  %19928 = add i32 %19927, %19922
  %19929 = zext i32 %19928 to i64
  store i64 %19929, i64* %RAX.i11582.pre-phi, align 8
  %19930 = icmp ult i32 %19928, %19922
  %19931 = icmp ult i32 %19928, %19927
  %19932 = or i1 %19930, %19931
  %19933 = zext i1 %19932 to i8
  store i8 %19933, i8* %14, align 1
  %19934 = and i32 %19928, 255
  %19935 = tail call i32 @llvm.ctpop.i32(i32 %19934)
  %19936 = trunc i32 %19935 to i8
  %19937 = and i8 %19936, 1
  %19938 = xor i8 %19937, 1
  store i8 %19938, i8* %21, align 1
  %19939 = xor i32 %19927, %19922
  %19940 = xor i32 %19939, %19928
  %19941 = lshr i32 %19940, 4
  %19942 = trunc i32 %19941 to i8
  %19943 = and i8 %19942, 1
  store i8 %19943, i8* %27, align 1
  %19944 = icmp eq i32 %19928, 0
  %19945 = zext i1 %19944 to i8
  store i8 %19945, i8* %30, align 1
  %19946 = lshr i32 %19928, 31
  %19947 = trunc i32 %19946 to i8
  store i8 %19947, i8* %33, align 1
  %19948 = lshr i32 %19922, 31
  %19949 = lshr i32 %19927, 31
  %19950 = xor i32 %19946, %19948
  %19951 = xor i32 %19946, %19949
  %19952 = add nuw nsw i32 %19950, %19951
  %19953 = icmp eq i32 %19952, 2
  %19954 = zext i1 %19953 to i8
  store i8 %19954, i8* %39, align 1
  %19955 = add i64 %19826, 9
  store i64 %19955, i64* %3, align 8
  store i32 %19928, i32* %19926, align 4
  %19956 = load i64, i64* %RBP.i, align 8
  %19957 = add i64 %19956, -40
  %19958 = load i64, i64* %3, align 8
  %19959 = add i64 %19958, 4
  store i64 %19959, i64* %3, align 8
  %19960 = inttoptr i64 %19957 to i32*
  %19961 = load i32, i32* %19960, align 4
  %19962 = sext i32 %19961 to i64
  store i64 %19962, i64* %RCX.i11580, align 8
  %19963 = shl nsw i64 %19962, 2
  %19964 = add i64 %19956, -112
  %19965 = add i64 %19964, %19963
  %19966 = add i64 %19958, 8
  store i64 %19966, i64* %3, align 8
  %19967 = inttoptr i64 %19965 to i32*
  %19968 = load i32, i32* %19967, align 4
  %19969 = add i32 %19968, 1
  %19970 = zext i32 %19969 to i64
  store i64 %19970, i64* %RAX.i11582.pre-phi, align 8
  %19971 = icmp eq i32 %19968, -1
  %19972 = icmp eq i32 %19969, 0
  %19973 = or i1 %19971, %19972
  %19974 = zext i1 %19973 to i8
  store i8 %19974, i8* %14, align 1
  %19975 = and i32 %19969, 255
  %19976 = tail call i32 @llvm.ctpop.i32(i32 %19975)
  %19977 = trunc i32 %19976 to i8
  %19978 = and i8 %19977, 1
  %19979 = xor i8 %19978, 1
  store i8 %19979, i8* %21, align 1
  %19980 = xor i32 %19969, %19968
  %19981 = lshr i32 %19980, 4
  %19982 = trunc i32 %19981 to i8
  %19983 = and i8 %19982, 1
  store i8 %19983, i8* %27, align 1
  %19984 = zext i1 %19972 to i8
  store i8 %19984, i8* %30, align 1
  %19985 = lshr i32 %19969, 31
  %19986 = trunc i32 %19985 to i8
  store i8 %19986, i8* %33, align 1
  %19987 = lshr i32 %19968, 31
  %19988 = xor i32 %19985, %19987
  %19989 = add nuw nsw i32 %19988, %19985
  %19990 = icmp eq i32 %19989, 2
  %19991 = zext i1 %19990 to i8
  store i8 %19991, i8* %39, align 1
  %19992 = add i64 %19958, 15
  store i64 %19992, i64* %3, align 8
  store i32 %19969, i32* %19967, align 4
  %19993 = load i64, i64* %RBP.i, align 8
  %19994 = add i64 %19993, -40
  %19995 = load i64, i64* %3, align 8
  %19996 = add i64 %19995, 3
  store i64 %19996, i64* %3, align 8
  %19997 = inttoptr i64 %19994 to i32*
  %19998 = load i32, i32* %19997, align 4
  %19999 = zext i32 %19998 to i64
  store i64 %19999, i64* %RAX.i11582.pre-phi, align 8
  %20000 = trunc i32 %19998 to i8
  store i8 %20000, i8* %DL.i11402, align 1
  %20001 = add i64 %19993, -8
  %20002 = add i64 %19995, 9
  store i64 %20002, i64* %3, align 8
  %20003 = inttoptr i64 %20001 to i64*
  %20004 = load i64, i64* %20003, align 8
  store i64 %20004, i64* %RCX.i11580, align 8
  %20005 = add i64 %19993, -52
  %20006 = add i64 %19995, 13
  store i64 %20006, i64* %3, align 8
  %20007 = inttoptr i64 %20005 to i32*
  %20008 = load i32, i32* %20007, align 4
  %20009 = sext i32 %20008 to i64
  store i64 %20009, i64* %RSI.i11312, align 8
  %20010 = add nsw i64 %20009, 1704
  %20011 = add i64 %20010, %20004
  %20012 = add i64 %19995, 20
  store i64 %20012, i64* %3, align 8
  %20013 = inttoptr i64 %20011 to i8*
  store i8 %20000, i8* %20013, align 1
  %20014 = load i64, i64* %RBP.i, align 8
  %20015 = add i64 %20014, -52
  %20016 = load i64, i64* %3, align 8
  %20017 = add i64 %20016, 3
  store i64 %20017, i64* %3, align 8
  %20018 = inttoptr i64 %20015 to i32*
  %20019 = load i32, i32* %20018, align 4
  %20020 = add i32 %20019, 1
  %20021 = zext i32 %20020 to i64
  store i64 %20021, i64* %RAX.i11582.pre-phi, align 8
  %20022 = icmp eq i32 %20019, -1
  %20023 = icmp eq i32 %20020, 0
  %20024 = or i1 %20022, %20023
  %20025 = zext i1 %20024 to i8
  store i8 %20025, i8* %14, align 1
  %20026 = and i32 %20020, 255
  %20027 = tail call i32 @llvm.ctpop.i32(i32 %20026)
  %20028 = trunc i32 %20027 to i8
  %20029 = and i8 %20028, 1
  %20030 = xor i8 %20029, 1
  store i8 %20030, i8* %21, align 1
  %20031 = xor i32 %20020, %20019
  %20032 = lshr i32 %20031, 4
  %20033 = trunc i32 %20032 to i8
  %20034 = and i8 %20033, 1
  store i8 %20034, i8* %27, align 1
  %20035 = zext i1 %20023 to i8
  store i8 %20035, i8* %30, align 1
  %20036 = lshr i32 %20020, 31
  %20037 = trunc i32 %20036 to i8
  store i8 %20037, i8* %33, align 1
  %20038 = lshr i32 %20019, 31
  %20039 = xor i32 %20036, %20038
  %20040 = add nuw nsw i32 %20039, %20036
  %20041 = icmp eq i32 %20040, 2
  %20042 = zext i1 %20041 to i8
  store i8 %20042, i8* %39, align 1
  %20043 = add i64 %20016, 9
  store i64 %20043, i64* %3, align 8
  store i32 %20020, i32* %20018, align 4
  %20044 = load i64, i64* %RBP.i, align 8
  %20045 = add i64 %20044, -72
  %20046 = load i64, i64* %3, align 8
  %20047 = add i64 %20046, 4
  store i64 %20047, i64* %3, align 8
  %20048 = inttoptr i64 %20045 to i32*
  %20049 = load i32, i32* %20048, align 4
  %20050 = add i32 %20049, -6
  %20051 = icmp ult i32 %20049, 6
  %20052 = zext i1 %20051 to i8
  store i8 %20052, i8* %14, align 1
  %20053 = and i32 %20050, 255
  %20054 = tail call i32 @llvm.ctpop.i32(i32 %20053)
  %20055 = trunc i32 %20054 to i8
  %20056 = and i8 %20055, 1
  %20057 = xor i8 %20056, 1
  store i8 %20057, i8* %21, align 1
  %20058 = xor i32 %20050, %20049
  %20059 = lshr i32 %20058, 4
  %20060 = trunc i32 %20059 to i8
  %20061 = and i8 %20060, 1
  store i8 %20061, i8* %27, align 1
  %20062 = icmp eq i32 %20050, 0
  %20063 = zext i1 %20062 to i8
  store i8 %20063, i8* %30, align 1
  %20064 = lshr i32 %20050, 31
  %20065 = trunc i32 %20064 to i8
  store i8 %20065, i8* %33, align 1
  %20066 = lshr i32 %20049, 31
  %20067 = xor i32 %20064, %20066
  %20068 = add nuw nsw i32 %20067, %20066
  %20069 = icmp eq i32 %20068, 2
  %20070 = zext i1 %20069 to i8
  store i8 %20070, i8* %39, align 1
  %.v356 = select i1 %20062, i64 10, i64 2637
  %20071 = add i64 %20046, %.v356
  store i64 %20071, i64* %3, align 8
  br i1 %20062, label %block_40c686, label %block_.L_40d0c9

block_40c686:                                     ; preds = %block_.L_40c647
  store i64 50, i64* %RAX.i11582.pre-phi, align 8
  %20072 = add i64 %20044, -32
  %20073 = add i64 %20071, 8
  store i64 %20073, i64* %3, align 8
  %20074 = inttoptr i64 %20072 to i32*
  %20075 = load i32, i32* %20074, align 4
  %20076 = zext i32 %20075 to i64
  store i64 %20076, i64* %RCX.i11580, align 8
  %20077 = add i64 %20044, -28
  %20078 = add i64 %20071, 11
  store i64 %20078, i64* %3, align 8
  %20079 = inttoptr i64 %20077 to i32*
  %20080 = load i32, i32* %20079, align 4
  %20081 = sub i32 %20075, %20080
  %20082 = add i32 %20081, 1
  %20083 = zext i32 %20082 to i64
  store i64 %20083, i64* %RCX.i11580, align 8
  %20084 = lshr i32 %20082, 31
  %20085 = sub i32 49, %20081
  %20086 = icmp ugt i32 %20082, 50
  %20087 = zext i1 %20086 to i8
  store i8 %20087, i8* %14, align 1
  %20088 = and i32 %20085, 255
  %20089 = tail call i32 @llvm.ctpop.i32(i32 %20088)
  %20090 = trunc i32 %20089 to i8
  %20091 = and i8 %20090, 1
  %20092 = xor i8 %20091, 1
  store i8 %20092, i8* %21, align 1
  %20093 = xor i32 %20082, 16
  %20094 = xor i32 %20093, %20085
  %20095 = lshr i32 %20094, 4
  %20096 = trunc i32 %20095 to i8
  %20097 = and i8 %20096, 1
  store i8 %20097, i8* %27, align 1
  %20098 = icmp eq i32 %20085, 0
  %20099 = zext i1 %20098 to i8
  store i8 %20099, i8* %30, align 1
  %20100 = lshr i32 %20085, 31
  %20101 = trunc i32 %20100 to i8
  store i8 %20101, i8* %33, align 1
  %20102 = add nuw nsw i32 %20100, %20084
  %20103 = icmp eq i32 %20102, 2
  %20104 = zext i1 %20103 to i8
  store i8 %20104, i8* %39, align 1
  %.v358 = select i1 %20098, i64 22, i64 2627
  %20105 = add i64 %20071, %.v358
  store i64 %20105, i64* %3, align 8
  %20106 = load i64, i64* %RBP.i, align 8
  br i1 %20098, label %block_40c69c, label %block_.L_40d0c9

block_40c69c:                                     ; preds = %block_40c686
  %20107 = add i64 %20106, -8
  %20108 = add i64 %20105, 4
  store i64 %20108, i64* %3, align 8
  %20109 = inttoptr i64 %20107 to i64*
  %20110 = load i64, i64* %20109, align 8
  %20111 = add i64 %20110, 45448
  store i64 %20111, i64* %RAX.i11582.pre-phi, align 8
  %20112 = icmp ugt i64 %20110, -45449
  %20113 = zext i1 %20112 to i8
  store i8 %20113, i8* %14, align 1
  %20114 = trunc i64 %20111 to i32
  %20115 = and i32 %20114, 255
  %20116 = tail call i32 @llvm.ctpop.i32(i32 %20115)
  %20117 = trunc i32 %20116 to i8
  %20118 = and i8 %20117, 1
  %20119 = xor i8 %20118, 1
  store i8 %20119, i8* %21, align 1
  %20120 = xor i64 %20111, %20110
  %20121 = lshr i64 %20120, 4
  %20122 = trunc i64 %20121 to i8
  %20123 = and i8 %20122, 1
  store i8 %20123, i8* %27, align 1
  %20124 = icmp eq i64 %20111, 0
  %20125 = zext i1 %20124 to i8
  store i8 %20125, i8* %30, align 1
  %20126 = lshr i64 %20111, 63
  %20127 = trunc i64 %20126 to i8
  store i8 %20127, i8* %33, align 1
  %20128 = lshr i64 %20110, 63
  %20129 = xor i64 %20126, %20128
  %20130 = add nuw nsw i64 %20129, %20126
  %20131 = icmp eq i64 %20130, 2
  %20132 = zext i1 %20131 to i8
  store i8 %20132, i8* %39, align 1
  %20133 = add i64 %20106, -40
  %20134 = add i64 %20105, 14
  store i64 %20134, i64* %3, align 8
  %20135 = inttoptr i64 %20133 to i32*
  %20136 = load i32, i32* %20135, align 4
  %20137 = sext i32 %20136 to i64
  %20138 = mul nsw i64 %20137, 1032
  store i64 %20138, i64* %RCX.i11580, align 8
  %20139 = lshr i64 %20138, 63
  %20140 = add i64 %20138, %20111
  store i64 %20140, i64* %RAX.i11582.pre-phi, align 8
  %20141 = icmp ult i64 %20140, %20111
  %20142 = icmp ult i64 %20140, %20138
  %20143 = or i1 %20141, %20142
  %20144 = zext i1 %20143 to i8
  store i8 %20144, i8* %14, align 1
  %20145 = trunc i64 %20140 to i32
  %20146 = and i32 %20145, 255
  %20147 = tail call i32 @llvm.ctpop.i32(i32 %20146)
  %20148 = trunc i32 %20147 to i8
  %20149 = and i8 %20148, 1
  %20150 = xor i8 %20149, 1
  store i8 %20150, i8* %21, align 1
  %20151 = xor i64 %20138, %20111
  %20152 = xor i64 %20151, %20140
  %20153 = lshr i64 %20152, 4
  %20154 = trunc i64 %20153 to i8
  %20155 = and i8 %20154, 1
  store i8 %20155, i8* %27, align 1
  %20156 = icmp eq i64 %20140, 0
  %20157 = zext i1 %20156 to i8
  store i8 %20157, i8* %30, align 1
  %20158 = lshr i64 %20140, 63
  %20159 = trunc i64 %20158 to i8
  store i8 %20159, i8* %33, align 1
  %20160 = xor i64 %20158, %20126
  %20161 = xor i64 %20158, %20139
  %20162 = add nuw nsw i64 %20160, %20161
  %20163 = icmp eq i64 %20162, 2
  %20164 = zext i1 %20163 to i8
  store i8 %20164, i8* %39, align 1
  %20165 = load i64, i64* %RBP.i, align 8
  %20166 = add i64 %20165, -120
  %20167 = add i64 %20105, 28
  store i64 %20167, i64* %3, align 8
  %20168 = inttoptr i64 %20166 to i64*
  %20169 = load i64, i64* %20168, align 8
  store i64 %20169, i64* %RCX.i11580, align 8
  %20170 = add i64 %20165, -28
  %20171 = add i64 %20105, 31
  store i64 %20171, i64* %3, align 8
  %20172 = inttoptr i64 %20170 to i32*
  %20173 = load i32, i32* %20172, align 4
  %20174 = zext i32 %20173 to i64
  store i64 %20174, i64* %576, align 8
  store i8 0, i8* %14, align 1
  %20175 = and i32 %20173, 255
  %20176 = tail call i32 @llvm.ctpop.i32(i32 %20175)
  %20177 = trunc i32 %20176 to i8
  %20178 = and i8 %20177, 1
  %20179 = xor i8 %20178, 1
  store i8 %20179, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %20180 = icmp eq i32 %20173, 0
  %20181 = zext i1 %20180 to i8
  store i8 %20181, i8* %30, align 1
  %20182 = lshr i32 %20173, 31
  %20183 = trunc i32 %20182 to i8
  store i8 %20183, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %20184 = sext i32 %20173 to i64
  store i64 %20184, i64* %RSI.i11312, align 8
  %20185 = shl nsw i64 %20184, 1
  %20186 = add i64 %20169, %20185
  %20187 = add i64 %20105, 41
  store i64 %20187, i64* %3, align 8
  %20188 = inttoptr i64 %20186 to i16*
  %20189 = load i16, i16* %20188, align 2
  %20190 = zext i16 %20189 to i64
  store i64 %20190, i64* %576, align 8
  %20191 = zext i16 %20189 to i64
  store i64 %20191, i64* %RCX.i11580, align 8
  %20192 = shl nuw nsw i64 %20191, 2
  %20193 = add i64 %20140, %20192
  %20194 = add i64 %20105, 46
  store i64 %20194, i64* %3, align 8
  %20195 = inttoptr i64 %20193 to i32*
  %20196 = load i32, i32* %20195, align 4
  %20197 = add i32 %20196, 1
  %20198 = zext i32 %20197 to i64
  store i64 %20198, i64* %576, align 8
  %20199 = icmp eq i32 %20196, -1
  %20200 = icmp eq i32 %20197, 0
  %20201 = or i1 %20199, %20200
  %20202 = zext i1 %20201 to i8
  store i8 %20202, i8* %14, align 1
  %20203 = and i32 %20197, 255
  %20204 = tail call i32 @llvm.ctpop.i32(i32 %20203)
  %20205 = trunc i32 %20204 to i8
  %20206 = and i8 %20205, 1
  %20207 = xor i8 %20206, 1
  store i8 %20207, i8* %21, align 1
  %20208 = xor i32 %20197, %20196
  %20209 = lshr i32 %20208, 4
  %20210 = trunc i32 %20209 to i8
  %20211 = and i8 %20210, 1
  store i8 %20211, i8* %27, align 1
  %20212 = zext i1 %20200 to i8
  store i8 %20212, i8* %30, align 1
  %20213 = lshr i32 %20197, 31
  %20214 = trunc i32 %20213 to i8
  store i8 %20214, i8* %33, align 1
  %20215 = lshr i32 %20196, 31
  %20216 = xor i32 %20213, %20215
  %20217 = add nuw nsw i32 %20216, %20213
  %20218 = icmp eq i32 %20217, 2
  %20219 = zext i1 %20218 to i8
  store i8 %20219, i8* %39, align 1
  %20220 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %20221 = add i64 %20192, %20220
  %20222 = add i64 %20105, 52
  store i64 %20222, i64* %3, align 8
  %20223 = inttoptr i64 %20221 to i32*
  store i32 %20197, i32* %20223, align 4
  %20224 = load i64, i64* %RBP.i, align 8
  %20225 = add i64 %20224, -8
  %20226 = load i64, i64* %3, align 8
  %20227 = add i64 %20226, 4
  store i64 %20227, i64* %3, align 8
  %20228 = inttoptr i64 %20225 to i64*
  %20229 = load i64, i64* %20228, align 8
  %20230 = add i64 %20229, 45448
  store i64 %20230, i64* %RAX.i11582.pre-phi, align 8
  %20231 = icmp ugt i64 %20229, -45449
  %20232 = zext i1 %20231 to i8
  store i8 %20232, i8* %14, align 1
  %20233 = trunc i64 %20230 to i32
  %20234 = and i32 %20233, 255
  %20235 = tail call i32 @llvm.ctpop.i32(i32 %20234)
  %20236 = trunc i32 %20235 to i8
  %20237 = and i8 %20236, 1
  %20238 = xor i8 %20237, 1
  store i8 %20238, i8* %21, align 1
  %20239 = xor i64 %20230, %20229
  %20240 = lshr i64 %20239, 4
  %20241 = trunc i64 %20240 to i8
  %20242 = and i8 %20241, 1
  store i8 %20242, i8* %27, align 1
  %20243 = icmp eq i64 %20230, 0
  %20244 = zext i1 %20243 to i8
  store i8 %20244, i8* %30, align 1
  %20245 = lshr i64 %20230, 63
  %20246 = trunc i64 %20245 to i8
  store i8 %20246, i8* %33, align 1
  %20247 = lshr i64 %20229, 63
  %20248 = xor i64 %20245, %20247
  %20249 = add nuw nsw i64 %20248, %20245
  %20250 = icmp eq i64 %20249, 2
  %20251 = zext i1 %20250 to i8
  store i8 %20251, i8* %39, align 1
  %20252 = add i64 %20224, -40
  %20253 = add i64 %20226, 14
  store i64 %20253, i64* %3, align 8
  %20254 = inttoptr i64 %20252 to i32*
  %20255 = load i32, i32* %20254, align 4
  %20256 = sext i32 %20255 to i64
  %20257 = mul nsw i64 %20256, 1032
  store i64 %20257, i64* %RCX.i11580, align 8
  %20258 = lshr i64 %20257, 63
  %20259 = add i64 %20257, %20230
  store i64 %20259, i64* %RAX.i11582.pre-phi, align 8
  %20260 = icmp ult i64 %20259, %20230
  %20261 = icmp ult i64 %20259, %20257
  %20262 = or i1 %20260, %20261
  %20263 = zext i1 %20262 to i8
  store i8 %20263, i8* %14, align 1
  %20264 = trunc i64 %20259 to i32
  %20265 = and i32 %20264, 255
  %20266 = tail call i32 @llvm.ctpop.i32(i32 %20265)
  %20267 = trunc i32 %20266 to i8
  %20268 = and i8 %20267, 1
  %20269 = xor i8 %20268, 1
  store i8 %20269, i8* %21, align 1
  %20270 = xor i64 %20257, %20230
  %20271 = xor i64 %20270, %20259
  %20272 = lshr i64 %20271, 4
  %20273 = trunc i64 %20272 to i8
  %20274 = and i8 %20273, 1
  store i8 %20274, i8* %27, align 1
  %20275 = icmp eq i64 %20259, 0
  %20276 = zext i1 %20275 to i8
  store i8 %20276, i8* %30, align 1
  %20277 = lshr i64 %20259, 63
  %20278 = trunc i64 %20277 to i8
  store i8 %20278, i8* %33, align 1
  %20279 = xor i64 %20277, %20245
  %20280 = xor i64 %20277, %20258
  %20281 = add nuw nsw i64 %20279, %20280
  %20282 = icmp eq i64 %20281, 2
  %20283 = zext i1 %20282 to i8
  store i8 %20283, i8* %39, align 1
  %20284 = load i64, i64* %RBP.i, align 8
  %20285 = add i64 %20284, -120
  %20286 = add i64 %20226, 28
  store i64 %20286, i64* %3, align 8
  %20287 = inttoptr i64 %20285 to i64*
  %20288 = load i64, i64* %20287, align 8
  store i64 %20288, i64* %RCX.i11580, align 8
  %20289 = add i64 %20284, -28
  %20290 = add i64 %20226, 31
  store i64 %20290, i64* %3, align 8
  %20291 = inttoptr i64 %20289 to i32*
  %20292 = load i32, i32* %20291, align 4
  %20293 = add i32 %20292, 1
  %20294 = zext i32 %20293 to i64
  store i64 %20294, i64* %576, align 8
  %20295 = icmp eq i32 %20292, -1
  %20296 = icmp eq i32 %20293, 0
  %20297 = or i1 %20295, %20296
  %20298 = zext i1 %20297 to i8
  store i8 %20298, i8* %14, align 1
  %20299 = and i32 %20293, 255
  %20300 = tail call i32 @llvm.ctpop.i32(i32 %20299)
  %20301 = trunc i32 %20300 to i8
  %20302 = and i8 %20301, 1
  %20303 = xor i8 %20302, 1
  store i8 %20303, i8* %21, align 1
  %20304 = xor i32 %20293, %20292
  %20305 = lshr i32 %20304, 4
  %20306 = trunc i32 %20305 to i8
  %20307 = and i8 %20306, 1
  store i8 %20307, i8* %27, align 1
  %20308 = zext i1 %20296 to i8
  store i8 %20308, i8* %30, align 1
  %20309 = lshr i32 %20293, 31
  %20310 = trunc i32 %20309 to i8
  store i8 %20310, i8* %33, align 1
  %20311 = lshr i32 %20292, 31
  %20312 = xor i32 %20309, %20311
  %20313 = add nuw nsw i32 %20312, %20309
  %20314 = icmp eq i32 %20313, 2
  %20315 = zext i1 %20314 to i8
  store i8 %20315, i8* %39, align 1
  %20316 = sext i32 %20293 to i64
  store i64 %20316, i64* %RSI.i11312, align 8
  %20317 = shl nsw i64 %20316, 1
  %20318 = add i64 %20288, %20317
  %20319 = add i64 %20226, 41
  store i64 %20319, i64* %3, align 8
  %20320 = inttoptr i64 %20318 to i16*
  %20321 = load i16, i16* %20320, align 2
  %20322 = zext i16 %20321 to i64
  store i64 %20322, i64* %576, align 8
  %20323 = zext i16 %20321 to i64
  store i64 %20323, i64* %RCX.i11580, align 8
  %20324 = shl nuw nsw i64 %20323, 2
  %20325 = add i64 %20259, %20324
  %20326 = add i64 %20226, 46
  store i64 %20326, i64* %3, align 8
  %20327 = inttoptr i64 %20325 to i32*
  %20328 = load i32, i32* %20327, align 4
  %20329 = add i32 %20328, 1
  %20330 = zext i32 %20329 to i64
  store i64 %20330, i64* %576, align 8
  %20331 = icmp eq i32 %20328, -1
  %20332 = icmp eq i32 %20329, 0
  %20333 = or i1 %20331, %20332
  %20334 = zext i1 %20333 to i8
  store i8 %20334, i8* %14, align 1
  %20335 = and i32 %20329, 255
  %20336 = tail call i32 @llvm.ctpop.i32(i32 %20335)
  %20337 = trunc i32 %20336 to i8
  %20338 = and i8 %20337, 1
  %20339 = xor i8 %20338, 1
  store i8 %20339, i8* %21, align 1
  %20340 = xor i32 %20329, %20328
  %20341 = lshr i32 %20340, 4
  %20342 = trunc i32 %20341 to i8
  %20343 = and i8 %20342, 1
  store i8 %20343, i8* %27, align 1
  %20344 = zext i1 %20332 to i8
  store i8 %20344, i8* %30, align 1
  %20345 = lshr i32 %20329, 31
  %20346 = trunc i32 %20345 to i8
  store i8 %20346, i8* %33, align 1
  %20347 = lshr i32 %20328, 31
  %20348 = xor i32 %20345, %20347
  %20349 = add nuw nsw i32 %20348, %20345
  %20350 = icmp eq i32 %20349, 2
  %20351 = zext i1 %20350 to i8
  store i8 %20351, i8* %39, align 1
  %20352 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %20353 = add i64 %20324, %20352
  %20354 = add i64 %20226, 52
  store i64 %20354, i64* %3, align 8
  %20355 = inttoptr i64 %20353 to i32*
  store i32 %20329, i32* %20355, align 4
  %20356 = load i64, i64* %RBP.i, align 8
  %20357 = add i64 %20356, -8
  %20358 = load i64, i64* %3, align 8
  %20359 = add i64 %20358, 4
  store i64 %20359, i64* %3, align 8
  %20360 = inttoptr i64 %20357 to i64*
  %20361 = load i64, i64* %20360, align 8
  %20362 = add i64 %20361, 45448
  store i64 %20362, i64* %RAX.i11582.pre-phi, align 8
  %20363 = icmp ugt i64 %20361, -45449
  %20364 = zext i1 %20363 to i8
  store i8 %20364, i8* %14, align 1
  %20365 = trunc i64 %20362 to i32
  %20366 = and i32 %20365, 255
  %20367 = tail call i32 @llvm.ctpop.i32(i32 %20366)
  %20368 = trunc i32 %20367 to i8
  %20369 = and i8 %20368, 1
  %20370 = xor i8 %20369, 1
  store i8 %20370, i8* %21, align 1
  %20371 = xor i64 %20362, %20361
  %20372 = lshr i64 %20371, 4
  %20373 = trunc i64 %20372 to i8
  %20374 = and i8 %20373, 1
  store i8 %20374, i8* %27, align 1
  %20375 = icmp eq i64 %20362, 0
  %20376 = zext i1 %20375 to i8
  store i8 %20376, i8* %30, align 1
  %20377 = lshr i64 %20362, 63
  %20378 = trunc i64 %20377 to i8
  store i8 %20378, i8* %33, align 1
  %20379 = lshr i64 %20361, 63
  %20380 = xor i64 %20377, %20379
  %20381 = add nuw nsw i64 %20380, %20377
  %20382 = icmp eq i64 %20381, 2
  %20383 = zext i1 %20382 to i8
  store i8 %20383, i8* %39, align 1
  %20384 = add i64 %20356, -40
  %20385 = add i64 %20358, 14
  store i64 %20385, i64* %3, align 8
  %20386 = inttoptr i64 %20384 to i32*
  %20387 = load i32, i32* %20386, align 4
  %20388 = sext i32 %20387 to i64
  %20389 = mul nsw i64 %20388, 1032
  store i64 %20389, i64* %RCX.i11580, align 8
  %20390 = lshr i64 %20389, 63
  %20391 = add i64 %20389, %20362
  store i64 %20391, i64* %RAX.i11582.pre-phi, align 8
  %20392 = icmp ult i64 %20391, %20362
  %20393 = icmp ult i64 %20391, %20389
  %20394 = or i1 %20392, %20393
  %20395 = zext i1 %20394 to i8
  store i8 %20395, i8* %14, align 1
  %20396 = trunc i64 %20391 to i32
  %20397 = and i32 %20396, 255
  %20398 = tail call i32 @llvm.ctpop.i32(i32 %20397)
  %20399 = trunc i32 %20398 to i8
  %20400 = and i8 %20399, 1
  %20401 = xor i8 %20400, 1
  store i8 %20401, i8* %21, align 1
  %20402 = xor i64 %20389, %20362
  %20403 = xor i64 %20402, %20391
  %20404 = lshr i64 %20403, 4
  %20405 = trunc i64 %20404 to i8
  %20406 = and i8 %20405, 1
  store i8 %20406, i8* %27, align 1
  %20407 = icmp eq i64 %20391, 0
  %20408 = zext i1 %20407 to i8
  store i8 %20408, i8* %30, align 1
  %20409 = lshr i64 %20391, 63
  %20410 = trunc i64 %20409 to i8
  store i8 %20410, i8* %33, align 1
  %20411 = xor i64 %20409, %20377
  %20412 = xor i64 %20409, %20390
  %20413 = add nuw nsw i64 %20411, %20412
  %20414 = icmp eq i64 %20413, 2
  %20415 = zext i1 %20414 to i8
  store i8 %20415, i8* %39, align 1
  %20416 = load i64, i64* %RBP.i, align 8
  %20417 = add i64 %20416, -120
  %20418 = add i64 %20358, 28
  store i64 %20418, i64* %3, align 8
  %20419 = inttoptr i64 %20417 to i64*
  %20420 = load i64, i64* %20419, align 8
  store i64 %20420, i64* %RCX.i11580, align 8
  %20421 = add i64 %20416, -28
  %20422 = add i64 %20358, 31
  store i64 %20422, i64* %3, align 8
  %20423 = inttoptr i64 %20421 to i32*
  %20424 = load i32, i32* %20423, align 4
  %20425 = add i32 %20424, 2
  %20426 = zext i32 %20425 to i64
  store i64 %20426, i64* %576, align 8
  %20427 = icmp ugt i32 %20424, -3
  %20428 = zext i1 %20427 to i8
  store i8 %20428, i8* %14, align 1
  %20429 = and i32 %20425, 255
  %20430 = tail call i32 @llvm.ctpop.i32(i32 %20429)
  %20431 = trunc i32 %20430 to i8
  %20432 = and i8 %20431, 1
  %20433 = xor i8 %20432, 1
  store i8 %20433, i8* %21, align 1
  %20434 = xor i32 %20425, %20424
  %20435 = lshr i32 %20434, 4
  %20436 = trunc i32 %20435 to i8
  %20437 = and i8 %20436, 1
  store i8 %20437, i8* %27, align 1
  %20438 = icmp eq i32 %20425, 0
  %20439 = zext i1 %20438 to i8
  store i8 %20439, i8* %30, align 1
  %20440 = lshr i32 %20425, 31
  %20441 = trunc i32 %20440 to i8
  store i8 %20441, i8* %33, align 1
  %20442 = lshr i32 %20424, 31
  %20443 = xor i32 %20440, %20442
  %20444 = add nuw nsw i32 %20443, %20440
  %20445 = icmp eq i32 %20444, 2
  %20446 = zext i1 %20445 to i8
  store i8 %20446, i8* %39, align 1
  %20447 = sext i32 %20425 to i64
  store i64 %20447, i64* %RSI.i11312, align 8
  %20448 = shl nsw i64 %20447, 1
  %20449 = add i64 %20420, %20448
  %20450 = add i64 %20358, 41
  store i64 %20450, i64* %3, align 8
  %20451 = inttoptr i64 %20449 to i16*
  %20452 = load i16, i16* %20451, align 2
  %20453 = zext i16 %20452 to i64
  store i64 %20453, i64* %576, align 8
  %20454 = zext i16 %20452 to i64
  store i64 %20454, i64* %RCX.i11580, align 8
  %20455 = shl nuw nsw i64 %20454, 2
  %20456 = add i64 %20391, %20455
  %20457 = add i64 %20358, 46
  store i64 %20457, i64* %3, align 8
  %20458 = inttoptr i64 %20456 to i32*
  %20459 = load i32, i32* %20458, align 4
  %20460 = add i32 %20459, 1
  %20461 = zext i32 %20460 to i64
  store i64 %20461, i64* %576, align 8
  %20462 = icmp eq i32 %20459, -1
  %20463 = icmp eq i32 %20460, 0
  %20464 = or i1 %20462, %20463
  %20465 = zext i1 %20464 to i8
  store i8 %20465, i8* %14, align 1
  %20466 = and i32 %20460, 255
  %20467 = tail call i32 @llvm.ctpop.i32(i32 %20466)
  %20468 = trunc i32 %20467 to i8
  %20469 = and i8 %20468, 1
  %20470 = xor i8 %20469, 1
  store i8 %20470, i8* %21, align 1
  %20471 = xor i32 %20460, %20459
  %20472 = lshr i32 %20471, 4
  %20473 = trunc i32 %20472 to i8
  %20474 = and i8 %20473, 1
  store i8 %20474, i8* %27, align 1
  %20475 = zext i1 %20463 to i8
  store i8 %20475, i8* %30, align 1
  %20476 = lshr i32 %20460, 31
  %20477 = trunc i32 %20476 to i8
  store i8 %20477, i8* %33, align 1
  %20478 = lshr i32 %20459, 31
  %20479 = xor i32 %20476, %20478
  %20480 = add nuw nsw i32 %20479, %20476
  %20481 = icmp eq i32 %20480, 2
  %20482 = zext i1 %20481 to i8
  store i8 %20482, i8* %39, align 1
  %20483 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %20484 = add i64 %20455, %20483
  %20485 = add i64 %20358, 52
  store i64 %20485, i64* %3, align 8
  %20486 = inttoptr i64 %20484 to i32*
  store i32 %20460, i32* %20486, align 4
  %20487 = load i64, i64* %RBP.i, align 8
  %20488 = add i64 %20487, -8
  %20489 = load i64, i64* %3, align 8
  %20490 = add i64 %20489, 4
  store i64 %20490, i64* %3, align 8
  %20491 = inttoptr i64 %20488 to i64*
  %20492 = load i64, i64* %20491, align 8
  %20493 = add i64 %20492, 45448
  store i64 %20493, i64* %RAX.i11582.pre-phi, align 8
  %20494 = icmp ugt i64 %20492, -45449
  %20495 = zext i1 %20494 to i8
  store i8 %20495, i8* %14, align 1
  %20496 = trunc i64 %20493 to i32
  %20497 = and i32 %20496, 255
  %20498 = tail call i32 @llvm.ctpop.i32(i32 %20497)
  %20499 = trunc i32 %20498 to i8
  %20500 = and i8 %20499, 1
  %20501 = xor i8 %20500, 1
  store i8 %20501, i8* %21, align 1
  %20502 = xor i64 %20493, %20492
  %20503 = lshr i64 %20502, 4
  %20504 = trunc i64 %20503 to i8
  %20505 = and i8 %20504, 1
  store i8 %20505, i8* %27, align 1
  %20506 = icmp eq i64 %20493, 0
  %20507 = zext i1 %20506 to i8
  store i8 %20507, i8* %30, align 1
  %20508 = lshr i64 %20493, 63
  %20509 = trunc i64 %20508 to i8
  store i8 %20509, i8* %33, align 1
  %20510 = lshr i64 %20492, 63
  %20511 = xor i64 %20508, %20510
  %20512 = add nuw nsw i64 %20511, %20508
  %20513 = icmp eq i64 %20512, 2
  %20514 = zext i1 %20513 to i8
  store i8 %20514, i8* %39, align 1
  %20515 = add i64 %20487, -40
  %20516 = add i64 %20489, 14
  store i64 %20516, i64* %3, align 8
  %20517 = inttoptr i64 %20515 to i32*
  %20518 = load i32, i32* %20517, align 4
  %20519 = sext i32 %20518 to i64
  %20520 = mul nsw i64 %20519, 1032
  store i64 %20520, i64* %RCX.i11580, align 8
  %20521 = lshr i64 %20520, 63
  %20522 = add i64 %20520, %20493
  store i64 %20522, i64* %RAX.i11582.pre-phi, align 8
  %20523 = icmp ult i64 %20522, %20493
  %20524 = icmp ult i64 %20522, %20520
  %20525 = or i1 %20523, %20524
  %20526 = zext i1 %20525 to i8
  store i8 %20526, i8* %14, align 1
  %20527 = trunc i64 %20522 to i32
  %20528 = and i32 %20527, 255
  %20529 = tail call i32 @llvm.ctpop.i32(i32 %20528)
  %20530 = trunc i32 %20529 to i8
  %20531 = and i8 %20530, 1
  %20532 = xor i8 %20531, 1
  store i8 %20532, i8* %21, align 1
  %20533 = xor i64 %20520, %20493
  %20534 = xor i64 %20533, %20522
  %20535 = lshr i64 %20534, 4
  %20536 = trunc i64 %20535 to i8
  %20537 = and i8 %20536, 1
  store i8 %20537, i8* %27, align 1
  %20538 = icmp eq i64 %20522, 0
  %20539 = zext i1 %20538 to i8
  store i8 %20539, i8* %30, align 1
  %20540 = lshr i64 %20522, 63
  %20541 = trunc i64 %20540 to i8
  store i8 %20541, i8* %33, align 1
  %20542 = xor i64 %20540, %20508
  %20543 = xor i64 %20540, %20521
  %20544 = add nuw nsw i64 %20542, %20543
  %20545 = icmp eq i64 %20544, 2
  %20546 = zext i1 %20545 to i8
  store i8 %20546, i8* %39, align 1
  %20547 = load i64, i64* %RBP.i, align 8
  %20548 = add i64 %20547, -120
  %20549 = add i64 %20489, 28
  store i64 %20549, i64* %3, align 8
  %20550 = inttoptr i64 %20548 to i64*
  %20551 = load i64, i64* %20550, align 8
  store i64 %20551, i64* %RCX.i11580, align 8
  %20552 = add i64 %20547, -28
  %20553 = add i64 %20489, 31
  store i64 %20553, i64* %3, align 8
  %20554 = inttoptr i64 %20552 to i32*
  %20555 = load i32, i32* %20554, align 4
  %20556 = add i32 %20555, 3
  %20557 = zext i32 %20556 to i64
  store i64 %20557, i64* %576, align 8
  %20558 = icmp ugt i32 %20555, -4
  %20559 = zext i1 %20558 to i8
  store i8 %20559, i8* %14, align 1
  %20560 = and i32 %20556, 255
  %20561 = tail call i32 @llvm.ctpop.i32(i32 %20560)
  %20562 = trunc i32 %20561 to i8
  %20563 = and i8 %20562, 1
  %20564 = xor i8 %20563, 1
  store i8 %20564, i8* %21, align 1
  %20565 = xor i32 %20556, %20555
  %20566 = lshr i32 %20565, 4
  %20567 = trunc i32 %20566 to i8
  %20568 = and i8 %20567, 1
  store i8 %20568, i8* %27, align 1
  %20569 = icmp eq i32 %20556, 0
  %20570 = zext i1 %20569 to i8
  store i8 %20570, i8* %30, align 1
  %20571 = lshr i32 %20556, 31
  %20572 = trunc i32 %20571 to i8
  store i8 %20572, i8* %33, align 1
  %20573 = lshr i32 %20555, 31
  %20574 = xor i32 %20571, %20573
  %20575 = add nuw nsw i32 %20574, %20571
  %20576 = icmp eq i32 %20575, 2
  %20577 = zext i1 %20576 to i8
  store i8 %20577, i8* %39, align 1
  %20578 = sext i32 %20556 to i64
  store i64 %20578, i64* %RSI.i11312, align 8
  %20579 = shl nsw i64 %20578, 1
  %20580 = add i64 %20551, %20579
  %20581 = add i64 %20489, 41
  store i64 %20581, i64* %3, align 8
  %20582 = inttoptr i64 %20580 to i16*
  %20583 = load i16, i16* %20582, align 2
  %20584 = zext i16 %20583 to i64
  store i64 %20584, i64* %576, align 8
  %20585 = zext i16 %20583 to i64
  store i64 %20585, i64* %RCX.i11580, align 8
  %20586 = shl nuw nsw i64 %20585, 2
  %20587 = add i64 %20522, %20586
  %20588 = add i64 %20489, 46
  store i64 %20588, i64* %3, align 8
  %20589 = inttoptr i64 %20587 to i32*
  %20590 = load i32, i32* %20589, align 4
  %20591 = add i32 %20590, 1
  %20592 = zext i32 %20591 to i64
  store i64 %20592, i64* %576, align 8
  %20593 = icmp eq i32 %20590, -1
  %20594 = icmp eq i32 %20591, 0
  %20595 = or i1 %20593, %20594
  %20596 = zext i1 %20595 to i8
  store i8 %20596, i8* %14, align 1
  %20597 = and i32 %20591, 255
  %20598 = tail call i32 @llvm.ctpop.i32(i32 %20597)
  %20599 = trunc i32 %20598 to i8
  %20600 = and i8 %20599, 1
  %20601 = xor i8 %20600, 1
  store i8 %20601, i8* %21, align 1
  %20602 = xor i32 %20591, %20590
  %20603 = lshr i32 %20602, 4
  %20604 = trunc i32 %20603 to i8
  %20605 = and i8 %20604, 1
  store i8 %20605, i8* %27, align 1
  %20606 = zext i1 %20594 to i8
  store i8 %20606, i8* %30, align 1
  %20607 = lshr i32 %20591, 31
  %20608 = trunc i32 %20607 to i8
  store i8 %20608, i8* %33, align 1
  %20609 = lshr i32 %20590, 31
  %20610 = xor i32 %20607, %20609
  %20611 = add nuw nsw i32 %20610, %20607
  %20612 = icmp eq i32 %20611, 2
  %20613 = zext i1 %20612 to i8
  store i8 %20613, i8* %39, align 1
  %20614 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %20615 = add i64 %20586, %20614
  %20616 = add i64 %20489, 52
  store i64 %20616, i64* %3, align 8
  %20617 = inttoptr i64 %20615 to i32*
  store i32 %20591, i32* %20617, align 4
  %20618 = load i64, i64* %RBP.i, align 8
  %20619 = add i64 %20618, -8
  %20620 = load i64, i64* %3, align 8
  %20621 = add i64 %20620, 4
  store i64 %20621, i64* %3, align 8
  %20622 = inttoptr i64 %20619 to i64*
  %20623 = load i64, i64* %20622, align 8
  %20624 = add i64 %20623, 45448
  store i64 %20624, i64* %RAX.i11582.pre-phi, align 8
  %20625 = icmp ugt i64 %20623, -45449
  %20626 = zext i1 %20625 to i8
  store i8 %20626, i8* %14, align 1
  %20627 = trunc i64 %20624 to i32
  %20628 = and i32 %20627, 255
  %20629 = tail call i32 @llvm.ctpop.i32(i32 %20628)
  %20630 = trunc i32 %20629 to i8
  %20631 = and i8 %20630, 1
  %20632 = xor i8 %20631, 1
  store i8 %20632, i8* %21, align 1
  %20633 = xor i64 %20624, %20623
  %20634 = lshr i64 %20633, 4
  %20635 = trunc i64 %20634 to i8
  %20636 = and i8 %20635, 1
  store i8 %20636, i8* %27, align 1
  %20637 = icmp eq i64 %20624, 0
  %20638 = zext i1 %20637 to i8
  store i8 %20638, i8* %30, align 1
  %20639 = lshr i64 %20624, 63
  %20640 = trunc i64 %20639 to i8
  store i8 %20640, i8* %33, align 1
  %20641 = lshr i64 %20623, 63
  %20642 = xor i64 %20639, %20641
  %20643 = add nuw nsw i64 %20642, %20639
  %20644 = icmp eq i64 %20643, 2
  %20645 = zext i1 %20644 to i8
  store i8 %20645, i8* %39, align 1
  %20646 = add i64 %20618, -40
  %20647 = add i64 %20620, 14
  store i64 %20647, i64* %3, align 8
  %20648 = inttoptr i64 %20646 to i32*
  %20649 = load i32, i32* %20648, align 4
  %20650 = sext i32 %20649 to i64
  %20651 = mul nsw i64 %20650, 1032
  store i64 %20651, i64* %RCX.i11580, align 8
  %20652 = lshr i64 %20651, 63
  %20653 = add i64 %20651, %20624
  store i64 %20653, i64* %RAX.i11582.pre-phi, align 8
  %20654 = icmp ult i64 %20653, %20624
  %20655 = icmp ult i64 %20653, %20651
  %20656 = or i1 %20654, %20655
  %20657 = zext i1 %20656 to i8
  store i8 %20657, i8* %14, align 1
  %20658 = trunc i64 %20653 to i32
  %20659 = and i32 %20658, 255
  %20660 = tail call i32 @llvm.ctpop.i32(i32 %20659)
  %20661 = trunc i32 %20660 to i8
  %20662 = and i8 %20661, 1
  %20663 = xor i8 %20662, 1
  store i8 %20663, i8* %21, align 1
  %20664 = xor i64 %20651, %20624
  %20665 = xor i64 %20664, %20653
  %20666 = lshr i64 %20665, 4
  %20667 = trunc i64 %20666 to i8
  %20668 = and i8 %20667, 1
  store i8 %20668, i8* %27, align 1
  %20669 = icmp eq i64 %20653, 0
  %20670 = zext i1 %20669 to i8
  store i8 %20670, i8* %30, align 1
  %20671 = lshr i64 %20653, 63
  %20672 = trunc i64 %20671 to i8
  store i8 %20672, i8* %33, align 1
  %20673 = xor i64 %20671, %20639
  %20674 = xor i64 %20671, %20652
  %20675 = add nuw nsw i64 %20673, %20674
  %20676 = icmp eq i64 %20675, 2
  %20677 = zext i1 %20676 to i8
  store i8 %20677, i8* %39, align 1
  %20678 = load i64, i64* %RBP.i, align 8
  %20679 = add i64 %20678, -120
  %20680 = add i64 %20620, 28
  store i64 %20680, i64* %3, align 8
  %20681 = inttoptr i64 %20679 to i64*
  %20682 = load i64, i64* %20681, align 8
  store i64 %20682, i64* %RCX.i11580, align 8
  %20683 = add i64 %20678, -28
  %20684 = add i64 %20620, 31
  store i64 %20684, i64* %3, align 8
  %20685 = inttoptr i64 %20683 to i32*
  %20686 = load i32, i32* %20685, align 4
  %20687 = add i32 %20686, 4
  %20688 = zext i32 %20687 to i64
  store i64 %20688, i64* %576, align 8
  %20689 = icmp ugt i32 %20686, -5
  %20690 = zext i1 %20689 to i8
  store i8 %20690, i8* %14, align 1
  %20691 = and i32 %20687, 255
  %20692 = tail call i32 @llvm.ctpop.i32(i32 %20691)
  %20693 = trunc i32 %20692 to i8
  %20694 = and i8 %20693, 1
  %20695 = xor i8 %20694, 1
  store i8 %20695, i8* %21, align 1
  %20696 = xor i32 %20687, %20686
  %20697 = lshr i32 %20696, 4
  %20698 = trunc i32 %20697 to i8
  %20699 = and i8 %20698, 1
  store i8 %20699, i8* %27, align 1
  %20700 = icmp eq i32 %20687, 0
  %20701 = zext i1 %20700 to i8
  store i8 %20701, i8* %30, align 1
  %20702 = lshr i32 %20687, 31
  %20703 = trunc i32 %20702 to i8
  store i8 %20703, i8* %33, align 1
  %20704 = lshr i32 %20686, 31
  %20705 = xor i32 %20702, %20704
  %20706 = add nuw nsw i32 %20705, %20702
  %20707 = icmp eq i32 %20706, 2
  %20708 = zext i1 %20707 to i8
  store i8 %20708, i8* %39, align 1
  %20709 = sext i32 %20687 to i64
  store i64 %20709, i64* %RSI.i11312, align 8
  %20710 = shl nsw i64 %20709, 1
  %20711 = add i64 %20682, %20710
  %20712 = add i64 %20620, 41
  store i64 %20712, i64* %3, align 8
  %20713 = inttoptr i64 %20711 to i16*
  %20714 = load i16, i16* %20713, align 2
  %20715 = zext i16 %20714 to i64
  store i64 %20715, i64* %576, align 8
  %20716 = zext i16 %20714 to i64
  store i64 %20716, i64* %RCX.i11580, align 8
  %20717 = shl nuw nsw i64 %20716, 2
  %20718 = add i64 %20653, %20717
  %20719 = add i64 %20620, 46
  store i64 %20719, i64* %3, align 8
  %20720 = inttoptr i64 %20718 to i32*
  %20721 = load i32, i32* %20720, align 4
  %20722 = add i32 %20721, 1
  %20723 = zext i32 %20722 to i64
  store i64 %20723, i64* %576, align 8
  %20724 = icmp eq i32 %20721, -1
  %20725 = icmp eq i32 %20722, 0
  %20726 = or i1 %20724, %20725
  %20727 = zext i1 %20726 to i8
  store i8 %20727, i8* %14, align 1
  %20728 = and i32 %20722, 255
  %20729 = tail call i32 @llvm.ctpop.i32(i32 %20728)
  %20730 = trunc i32 %20729 to i8
  %20731 = and i8 %20730, 1
  %20732 = xor i8 %20731, 1
  store i8 %20732, i8* %21, align 1
  %20733 = xor i32 %20722, %20721
  %20734 = lshr i32 %20733, 4
  %20735 = trunc i32 %20734 to i8
  %20736 = and i8 %20735, 1
  store i8 %20736, i8* %27, align 1
  %20737 = zext i1 %20725 to i8
  store i8 %20737, i8* %30, align 1
  %20738 = lshr i32 %20722, 31
  %20739 = trunc i32 %20738 to i8
  store i8 %20739, i8* %33, align 1
  %20740 = lshr i32 %20721, 31
  %20741 = xor i32 %20738, %20740
  %20742 = add nuw nsw i32 %20741, %20738
  %20743 = icmp eq i32 %20742, 2
  %20744 = zext i1 %20743 to i8
  store i8 %20744, i8* %39, align 1
  %20745 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %20746 = add i64 %20717, %20745
  %20747 = add i64 %20620, 52
  store i64 %20747, i64* %3, align 8
  %20748 = inttoptr i64 %20746 to i32*
  store i32 %20722, i32* %20748, align 4
  %20749 = load i64, i64* %RBP.i, align 8
  %20750 = add i64 %20749, -8
  %20751 = load i64, i64* %3, align 8
  %20752 = add i64 %20751, 4
  store i64 %20752, i64* %3, align 8
  %20753 = inttoptr i64 %20750 to i64*
  %20754 = load i64, i64* %20753, align 8
  %20755 = add i64 %20754, 45448
  store i64 %20755, i64* %RAX.i11582.pre-phi, align 8
  %20756 = icmp ugt i64 %20754, -45449
  %20757 = zext i1 %20756 to i8
  store i8 %20757, i8* %14, align 1
  %20758 = trunc i64 %20755 to i32
  %20759 = and i32 %20758, 255
  %20760 = tail call i32 @llvm.ctpop.i32(i32 %20759)
  %20761 = trunc i32 %20760 to i8
  %20762 = and i8 %20761, 1
  %20763 = xor i8 %20762, 1
  store i8 %20763, i8* %21, align 1
  %20764 = xor i64 %20755, %20754
  %20765 = lshr i64 %20764, 4
  %20766 = trunc i64 %20765 to i8
  %20767 = and i8 %20766, 1
  store i8 %20767, i8* %27, align 1
  %20768 = icmp eq i64 %20755, 0
  %20769 = zext i1 %20768 to i8
  store i8 %20769, i8* %30, align 1
  %20770 = lshr i64 %20755, 63
  %20771 = trunc i64 %20770 to i8
  store i8 %20771, i8* %33, align 1
  %20772 = lshr i64 %20754, 63
  %20773 = xor i64 %20770, %20772
  %20774 = add nuw nsw i64 %20773, %20770
  %20775 = icmp eq i64 %20774, 2
  %20776 = zext i1 %20775 to i8
  store i8 %20776, i8* %39, align 1
  %20777 = add i64 %20749, -40
  %20778 = add i64 %20751, 14
  store i64 %20778, i64* %3, align 8
  %20779 = inttoptr i64 %20777 to i32*
  %20780 = load i32, i32* %20779, align 4
  %20781 = sext i32 %20780 to i64
  %20782 = mul nsw i64 %20781, 1032
  store i64 %20782, i64* %RCX.i11580, align 8
  %20783 = lshr i64 %20782, 63
  %20784 = add i64 %20782, %20755
  store i64 %20784, i64* %RAX.i11582.pre-phi, align 8
  %20785 = icmp ult i64 %20784, %20755
  %20786 = icmp ult i64 %20784, %20782
  %20787 = or i1 %20785, %20786
  %20788 = zext i1 %20787 to i8
  store i8 %20788, i8* %14, align 1
  %20789 = trunc i64 %20784 to i32
  %20790 = and i32 %20789, 255
  %20791 = tail call i32 @llvm.ctpop.i32(i32 %20790)
  %20792 = trunc i32 %20791 to i8
  %20793 = and i8 %20792, 1
  %20794 = xor i8 %20793, 1
  store i8 %20794, i8* %21, align 1
  %20795 = xor i64 %20782, %20755
  %20796 = xor i64 %20795, %20784
  %20797 = lshr i64 %20796, 4
  %20798 = trunc i64 %20797 to i8
  %20799 = and i8 %20798, 1
  store i8 %20799, i8* %27, align 1
  %20800 = icmp eq i64 %20784, 0
  %20801 = zext i1 %20800 to i8
  store i8 %20801, i8* %30, align 1
  %20802 = lshr i64 %20784, 63
  %20803 = trunc i64 %20802 to i8
  store i8 %20803, i8* %33, align 1
  %20804 = xor i64 %20802, %20770
  %20805 = xor i64 %20802, %20783
  %20806 = add nuw nsw i64 %20804, %20805
  %20807 = icmp eq i64 %20806, 2
  %20808 = zext i1 %20807 to i8
  store i8 %20808, i8* %39, align 1
  %20809 = load i64, i64* %RBP.i, align 8
  %20810 = add i64 %20809, -120
  %20811 = add i64 %20751, 28
  store i64 %20811, i64* %3, align 8
  %20812 = inttoptr i64 %20810 to i64*
  %20813 = load i64, i64* %20812, align 8
  store i64 %20813, i64* %RCX.i11580, align 8
  %20814 = add i64 %20809, -28
  %20815 = add i64 %20751, 31
  store i64 %20815, i64* %3, align 8
  %20816 = inttoptr i64 %20814 to i32*
  %20817 = load i32, i32* %20816, align 4
  %20818 = add i32 %20817, 5
  %20819 = zext i32 %20818 to i64
  store i64 %20819, i64* %576, align 8
  %20820 = icmp ugt i32 %20817, -6
  %20821 = zext i1 %20820 to i8
  store i8 %20821, i8* %14, align 1
  %20822 = and i32 %20818, 255
  %20823 = tail call i32 @llvm.ctpop.i32(i32 %20822)
  %20824 = trunc i32 %20823 to i8
  %20825 = and i8 %20824, 1
  %20826 = xor i8 %20825, 1
  store i8 %20826, i8* %21, align 1
  %20827 = xor i32 %20818, %20817
  %20828 = lshr i32 %20827, 4
  %20829 = trunc i32 %20828 to i8
  %20830 = and i8 %20829, 1
  store i8 %20830, i8* %27, align 1
  %20831 = icmp eq i32 %20818, 0
  %20832 = zext i1 %20831 to i8
  store i8 %20832, i8* %30, align 1
  %20833 = lshr i32 %20818, 31
  %20834 = trunc i32 %20833 to i8
  store i8 %20834, i8* %33, align 1
  %20835 = lshr i32 %20817, 31
  %20836 = xor i32 %20833, %20835
  %20837 = add nuw nsw i32 %20836, %20833
  %20838 = icmp eq i32 %20837, 2
  %20839 = zext i1 %20838 to i8
  store i8 %20839, i8* %39, align 1
  %20840 = sext i32 %20818 to i64
  store i64 %20840, i64* %RSI.i11312, align 8
  %20841 = shl nsw i64 %20840, 1
  %20842 = add i64 %20813, %20841
  %20843 = add i64 %20751, 41
  store i64 %20843, i64* %3, align 8
  %20844 = inttoptr i64 %20842 to i16*
  %20845 = load i16, i16* %20844, align 2
  %20846 = zext i16 %20845 to i64
  store i64 %20846, i64* %576, align 8
  %20847 = zext i16 %20845 to i64
  store i64 %20847, i64* %RCX.i11580, align 8
  %20848 = shl nuw nsw i64 %20847, 2
  %20849 = add i64 %20784, %20848
  %20850 = add i64 %20751, 46
  store i64 %20850, i64* %3, align 8
  %20851 = inttoptr i64 %20849 to i32*
  %20852 = load i32, i32* %20851, align 4
  %20853 = add i32 %20852, 1
  %20854 = zext i32 %20853 to i64
  store i64 %20854, i64* %576, align 8
  %20855 = icmp eq i32 %20852, -1
  %20856 = icmp eq i32 %20853, 0
  %20857 = or i1 %20855, %20856
  %20858 = zext i1 %20857 to i8
  store i8 %20858, i8* %14, align 1
  %20859 = and i32 %20853, 255
  %20860 = tail call i32 @llvm.ctpop.i32(i32 %20859)
  %20861 = trunc i32 %20860 to i8
  %20862 = and i8 %20861, 1
  %20863 = xor i8 %20862, 1
  store i8 %20863, i8* %21, align 1
  %20864 = xor i32 %20853, %20852
  %20865 = lshr i32 %20864, 4
  %20866 = trunc i32 %20865 to i8
  %20867 = and i8 %20866, 1
  store i8 %20867, i8* %27, align 1
  %20868 = zext i1 %20856 to i8
  store i8 %20868, i8* %30, align 1
  %20869 = lshr i32 %20853, 31
  %20870 = trunc i32 %20869 to i8
  store i8 %20870, i8* %33, align 1
  %20871 = lshr i32 %20852, 31
  %20872 = xor i32 %20869, %20871
  %20873 = add nuw nsw i32 %20872, %20869
  %20874 = icmp eq i32 %20873, 2
  %20875 = zext i1 %20874 to i8
  store i8 %20875, i8* %39, align 1
  %20876 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %20877 = add i64 %20848, %20876
  %20878 = add i64 %20751, 52
  store i64 %20878, i64* %3, align 8
  %20879 = inttoptr i64 %20877 to i32*
  store i32 %20853, i32* %20879, align 4
  %20880 = load i64, i64* %RBP.i, align 8
  %20881 = add i64 %20880, -8
  %20882 = load i64, i64* %3, align 8
  %20883 = add i64 %20882, 4
  store i64 %20883, i64* %3, align 8
  %20884 = inttoptr i64 %20881 to i64*
  %20885 = load i64, i64* %20884, align 8
  %20886 = add i64 %20885, 45448
  store i64 %20886, i64* %RAX.i11582.pre-phi, align 8
  %20887 = icmp ugt i64 %20885, -45449
  %20888 = zext i1 %20887 to i8
  store i8 %20888, i8* %14, align 1
  %20889 = trunc i64 %20886 to i32
  %20890 = and i32 %20889, 255
  %20891 = tail call i32 @llvm.ctpop.i32(i32 %20890)
  %20892 = trunc i32 %20891 to i8
  %20893 = and i8 %20892, 1
  %20894 = xor i8 %20893, 1
  store i8 %20894, i8* %21, align 1
  %20895 = xor i64 %20886, %20885
  %20896 = lshr i64 %20895, 4
  %20897 = trunc i64 %20896 to i8
  %20898 = and i8 %20897, 1
  store i8 %20898, i8* %27, align 1
  %20899 = icmp eq i64 %20886, 0
  %20900 = zext i1 %20899 to i8
  store i8 %20900, i8* %30, align 1
  %20901 = lshr i64 %20886, 63
  %20902 = trunc i64 %20901 to i8
  store i8 %20902, i8* %33, align 1
  %20903 = lshr i64 %20885, 63
  %20904 = xor i64 %20901, %20903
  %20905 = add nuw nsw i64 %20904, %20901
  %20906 = icmp eq i64 %20905, 2
  %20907 = zext i1 %20906 to i8
  store i8 %20907, i8* %39, align 1
  %20908 = add i64 %20880, -40
  %20909 = add i64 %20882, 14
  store i64 %20909, i64* %3, align 8
  %20910 = inttoptr i64 %20908 to i32*
  %20911 = load i32, i32* %20910, align 4
  %20912 = sext i32 %20911 to i64
  %20913 = mul nsw i64 %20912, 1032
  store i64 %20913, i64* %RCX.i11580, align 8
  %20914 = lshr i64 %20913, 63
  %20915 = add i64 %20913, %20886
  store i64 %20915, i64* %RAX.i11582.pre-phi, align 8
  %20916 = icmp ult i64 %20915, %20886
  %20917 = icmp ult i64 %20915, %20913
  %20918 = or i1 %20916, %20917
  %20919 = zext i1 %20918 to i8
  store i8 %20919, i8* %14, align 1
  %20920 = trunc i64 %20915 to i32
  %20921 = and i32 %20920, 255
  %20922 = tail call i32 @llvm.ctpop.i32(i32 %20921)
  %20923 = trunc i32 %20922 to i8
  %20924 = and i8 %20923, 1
  %20925 = xor i8 %20924, 1
  store i8 %20925, i8* %21, align 1
  %20926 = xor i64 %20913, %20886
  %20927 = xor i64 %20926, %20915
  %20928 = lshr i64 %20927, 4
  %20929 = trunc i64 %20928 to i8
  %20930 = and i8 %20929, 1
  store i8 %20930, i8* %27, align 1
  %20931 = icmp eq i64 %20915, 0
  %20932 = zext i1 %20931 to i8
  store i8 %20932, i8* %30, align 1
  %20933 = lshr i64 %20915, 63
  %20934 = trunc i64 %20933 to i8
  store i8 %20934, i8* %33, align 1
  %20935 = xor i64 %20933, %20901
  %20936 = xor i64 %20933, %20914
  %20937 = add nuw nsw i64 %20935, %20936
  %20938 = icmp eq i64 %20937, 2
  %20939 = zext i1 %20938 to i8
  store i8 %20939, i8* %39, align 1
  %20940 = load i64, i64* %RBP.i, align 8
  %20941 = add i64 %20940, -120
  %20942 = add i64 %20882, 28
  store i64 %20942, i64* %3, align 8
  %20943 = inttoptr i64 %20941 to i64*
  %20944 = load i64, i64* %20943, align 8
  store i64 %20944, i64* %RCX.i11580, align 8
  %20945 = add i64 %20940, -28
  %20946 = add i64 %20882, 31
  store i64 %20946, i64* %3, align 8
  %20947 = inttoptr i64 %20945 to i32*
  %20948 = load i32, i32* %20947, align 4
  %20949 = add i32 %20948, 6
  %20950 = zext i32 %20949 to i64
  store i64 %20950, i64* %576, align 8
  %20951 = icmp ugt i32 %20948, -7
  %20952 = zext i1 %20951 to i8
  store i8 %20952, i8* %14, align 1
  %20953 = and i32 %20949, 255
  %20954 = tail call i32 @llvm.ctpop.i32(i32 %20953)
  %20955 = trunc i32 %20954 to i8
  %20956 = and i8 %20955, 1
  %20957 = xor i8 %20956, 1
  store i8 %20957, i8* %21, align 1
  %20958 = xor i32 %20949, %20948
  %20959 = lshr i32 %20958, 4
  %20960 = trunc i32 %20959 to i8
  %20961 = and i8 %20960, 1
  store i8 %20961, i8* %27, align 1
  %20962 = icmp eq i32 %20949, 0
  %20963 = zext i1 %20962 to i8
  store i8 %20963, i8* %30, align 1
  %20964 = lshr i32 %20949, 31
  %20965 = trunc i32 %20964 to i8
  store i8 %20965, i8* %33, align 1
  %20966 = lshr i32 %20948, 31
  %20967 = xor i32 %20964, %20966
  %20968 = add nuw nsw i32 %20967, %20964
  %20969 = icmp eq i32 %20968, 2
  %20970 = zext i1 %20969 to i8
  store i8 %20970, i8* %39, align 1
  %20971 = sext i32 %20949 to i64
  store i64 %20971, i64* %RSI.i11312, align 8
  %20972 = shl nsw i64 %20971, 1
  %20973 = add i64 %20944, %20972
  %20974 = add i64 %20882, 41
  store i64 %20974, i64* %3, align 8
  %20975 = inttoptr i64 %20973 to i16*
  %20976 = load i16, i16* %20975, align 2
  %20977 = zext i16 %20976 to i64
  store i64 %20977, i64* %576, align 8
  %20978 = zext i16 %20976 to i64
  store i64 %20978, i64* %RCX.i11580, align 8
  %20979 = shl nuw nsw i64 %20978, 2
  %20980 = add i64 %20915, %20979
  %20981 = add i64 %20882, 46
  store i64 %20981, i64* %3, align 8
  %20982 = inttoptr i64 %20980 to i32*
  %20983 = load i32, i32* %20982, align 4
  %20984 = add i32 %20983, 1
  %20985 = zext i32 %20984 to i64
  store i64 %20985, i64* %576, align 8
  %20986 = icmp eq i32 %20983, -1
  %20987 = icmp eq i32 %20984, 0
  %20988 = or i1 %20986, %20987
  %20989 = zext i1 %20988 to i8
  store i8 %20989, i8* %14, align 1
  %20990 = and i32 %20984, 255
  %20991 = tail call i32 @llvm.ctpop.i32(i32 %20990)
  %20992 = trunc i32 %20991 to i8
  %20993 = and i8 %20992, 1
  %20994 = xor i8 %20993, 1
  store i8 %20994, i8* %21, align 1
  %20995 = xor i32 %20984, %20983
  %20996 = lshr i32 %20995, 4
  %20997 = trunc i32 %20996 to i8
  %20998 = and i8 %20997, 1
  store i8 %20998, i8* %27, align 1
  %20999 = zext i1 %20987 to i8
  store i8 %20999, i8* %30, align 1
  %21000 = lshr i32 %20984, 31
  %21001 = trunc i32 %21000 to i8
  store i8 %21001, i8* %33, align 1
  %21002 = lshr i32 %20983, 31
  %21003 = xor i32 %21000, %21002
  %21004 = add nuw nsw i32 %21003, %21000
  %21005 = icmp eq i32 %21004, 2
  %21006 = zext i1 %21005 to i8
  store i8 %21006, i8* %39, align 1
  %21007 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21008 = add i64 %20979, %21007
  %21009 = add i64 %20882, 52
  store i64 %21009, i64* %3, align 8
  %21010 = inttoptr i64 %21008 to i32*
  store i32 %20984, i32* %21010, align 4
  %21011 = load i64, i64* %RBP.i, align 8
  %21012 = add i64 %21011, -8
  %21013 = load i64, i64* %3, align 8
  %21014 = add i64 %21013, 4
  store i64 %21014, i64* %3, align 8
  %21015 = inttoptr i64 %21012 to i64*
  %21016 = load i64, i64* %21015, align 8
  %21017 = add i64 %21016, 45448
  store i64 %21017, i64* %RAX.i11582.pre-phi, align 8
  %21018 = icmp ugt i64 %21016, -45449
  %21019 = zext i1 %21018 to i8
  store i8 %21019, i8* %14, align 1
  %21020 = trunc i64 %21017 to i32
  %21021 = and i32 %21020, 255
  %21022 = tail call i32 @llvm.ctpop.i32(i32 %21021)
  %21023 = trunc i32 %21022 to i8
  %21024 = and i8 %21023, 1
  %21025 = xor i8 %21024, 1
  store i8 %21025, i8* %21, align 1
  %21026 = xor i64 %21017, %21016
  %21027 = lshr i64 %21026, 4
  %21028 = trunc i64 %21027 to i8
  %21029 = and i8 %21028, 1
  store i8 %21029, i8* %27, align 1
  %21030 = icmp eq i64 %21017, 0
  %21031 = zext i1 %21030 to i8
  store i8 %21031, i8* %30, align 1
  %21032 = lshr i64 %21017, 63
  %21033 = trunc i64 %21032 to i8
  store i8 %21033, i8* %33, align 1
  %21034 = lshr i64 %21016, 63
  %21035 = xor i64 %21032, %21034
  %21036 = add nuw nsw i64 %21035, %21032
  %21037 = icmp eq i64 %21036, 2
  %21038 = zext i1 %21037 to i8
  store i8 %21038, i8* %39, align 1
  %21039 = add i64 %21011, -40
  %21040 = add i64 %21013, 14
  store i64 %21040, i64* %3, align 8
  %21041 = inttoptr i64 %21039 to i32*
  %21042 = load i32, i32* %21041, align 4
  %21043 = sext i32 %21042 to i64
  %21044 = mul nsw i64 %21043, 1032
  store i64 %21044, i64* %RCX.i11580, align 8
  %21045 = lshr i64 %21044, 63
  %21046 = add i64 %21044, %21017
  store i64 %21046, i64* %RAX.i11582.pre-phi, align 8
  %21047 = icmp ult i64 %21046, %21017
  %21048 = icmp ult i64 %21046, %21044
  %21049 = or i1 %21047, %21048
  %21050 = zext i1 %21049 to i8
  store i8 %21050, i8* %14, align 1
  %21051 = trunc i64 %21046 to i32
  %21052 = and i32 %21051, 255
  %21053 = tail call i32 @llvm.ctpop.i32(i32 %21052)
  %21054 = trunc i32 %21053 to i8
  %21055 = and i8 %21054, 1
  %21056 = xor i8 %21055, 1
  store i8 %21056, i8* %21, align 1
  %21057 = xor i64 %21044, %21017
  %21058 = xor i64 %21057, %21046
  %21059 = lshr i64 %21058, 4
  %21060 = trunc i64 %21059 to i8
  %21061 = and i8 %21060, 1
  store i8 %21061, i8* %27, align 1
  %21062 = icmp eq i64 %21046, 0
  %21063 = zext i1 %21062 to i8
  store i8 %21063, i8* %30, align 1
  %21064 = lshr i64 %21046, 63
  %21065 = trunc i64 %21064 to i8
  store i8 %21065, i8* %33, align 1
  %21066 = xor i64 %21064, %21032
  %21067 = xor i64 %21064, %21045
  %21068 = add nuw nsw i64 %21066, %21067
  %21069 = icmp eq i64 %21068, 2
  %21070 = zext i1 %21069 to i8
  store i8 %21070, i8* %39, align 1
  %21071 = load i64, i64* %RBP.i, align 8
  %21072 = add i64 %21071, -120
  %21073 = add i64 %21013, 28
  store i64 %21073, i64* %3, align 8
  %21074 = inttoptr i64 %21072 to i64*
  %21075 = load i64, i64* %21074, align 8
  store i64 %21075, i64* %RCX.i11580, align 8
  %21076 = add i64 %21071, -28
  %21077 = add i64 %21013, 31
  store i64 %21077, i64* %3, align 8
  %21078 = inttoptr i64 %21076 to i32*
  %21079 = load i32, i32* %21078, align 4
  %21080 = add i32 %21079, 7
  %21081 = zext i32 %21080 to i64
  store i64 %21081, i64* %576, align 8
  %21082 = icmp ugt i32 %21079, -8
  %21083 = zext i1 %21082 to i8
  store i8 %21083, i8* %14, align 1
  %21084 = and i32 %21080, 255
  %21085 = tail call i32 @llvm.ctpop.i32(i32 %21084)
  %21086 = trunc i32 %21085 to i8
  %21087 = and i8 %21086, 1
  %21088 = xor i8 %21087, 1
  store i8 %21088, i8* %21, align 1
  %21089 = xor i32 %21080, %21079
  %21090 = lshr i32 %21089, 4
  %21091 = trunc i32 %21090 to i8
  %21092 = and i8 %21091, 1
  store i8 %21092, i8* %27, align 1
  %21093 = icmp eq i32 %21080, 0
  %21094 = zext i1 %21093 to i8
  store i8 %21094, i8* %30, align 1
  %21095 = lshr i32 %21080, 31
  %21096 = trunc i32 %21095 to i8
  store i8 %21096, i8* %33, align 1
  %21097 = lshr i32 %21079, 31
  %21098 = xor i32 %21095, %21097
  %21099 = add nuw nsw i32 %21098, %21095
  %21100 = icmp eq i32 %21099, 2
  %21101 = zext i1 %21100 to i8
  store i8 %21101, i8* %39, align 1
  %21102 = sext i32 %21080 to i64
  store i64 %21102, i64* %RSI.i11312, align 8
  %21103 = shl nsw i64 %21102, 1
  %21104 = add i64 %21075, %21103
  %21105 = add i64 %21013, 41
  store i64 %21105, i64* %3, align 8
  %21106 = inttoptr i64 %21104 to i16*
  %21107 = load i16, i16* %21106, align 2
  %21108 = zext i16 %21107 to i64
  store i64 %21108, i64* %576, align 8
  %21109 = zext i16 %21107 to i64
  store i64 %21109, i64* %RCX.i11580, align 8
  %21110 = shl nuw nsw i64 %21109, 2
  %21111 = add i64 %21046, %21110
  %21112 = add i64 %21013, 46
  store i64 %21112, i64* %3, align 8
  %21113 = inttoptr i64 %21111 to i32*
  %21114 = load i32, i32* %21113, align 4
  %21115 = add i32 %21114, 1
  %21116 = zext i32 %21115 to i64
  store i64 %21116, i64* %576, align 8
  %21117 = icmp eq i32 %21114, -1
  %21118 = icmp eq i32 %21115, 0
  %21119 = or i1 %21117, %21118
  %21120 = zext i1 %21119 to i8
  store i8 %21120, i8* %14, align 1
  %21121 = and i32 %21115, 255
  %21122 = tail call i32 @llvm.ctpop.i32(i32 %21121)
  %21123 = trunc i32 %21122 to i8
  %21124 = and i8 %21123, 1
  %21125 = xor i8 %21124, 1
  store i8 %21125, i8* %21, align 1
  %21126 = xor i32 %21115, %21114
  %21127 = lshr i32 %21126, 4
  %21128 = trunc i32 %21127 to i8
  %21129 = and i8 %21128, 1
  store i8 %21129, i8* %27, align 1
  %21130 = zext i1 %21118 to i8
  store i8 %21130, i8* %30, align 1
  %21131 = lshr i32 %21115, 31
  %21132 = trunc i32 %21131 to i8
  store i8 %21132, i8* %33, align 1
  %21133 = lshr i32 %21114, 31
  %21134 = xor i32 %21131, %21133
  %21135 = add nuw nsw i32 %21134, %21131
  %21136 = icmp eq i32 %21135, 2
  %21137 = zext i1 %21136 to i8
  store i8 %21137, i8* %39, align 1
  %21138 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21139 = add i64 %21110, %21138
  %21140 = add i64 %21013, 52
  store i64 %21140, i64* %3, align 8
  %21141 = inttoptr i64 %21139 to i32*
  store i32 %21115, i32* %21141, align 4
  %21142 = load i64, i64* %RBP.i, align 8
  %21143 = add i64 %21142, -8
  %21144 = load i64, i64* %3, align 8
  %21145 = add i64 %21144, 4
  store i64 %21145, i64* %3, align 8
  %21146 = inttoptr i64 %21143 to i64*
  %21147 = load i64, i64* %21146, align 8
  %21148 = add i64 %21147, 45448
  store i64 %21148, i64* %RAX.i11582.pre-phi, align 8
  %21149 = icmp ugt i64 %21147, -45449
  %21150 = zext i1 %21149 to i8
  store i8 %21150, i8* %14, align 1
  %21151 = trunc i64 %21148 to i32
  %21152 = and i32 %21151, 255
  %21153 = tail call i32 @llvm.ctpop.i32(i32 %21152)
  %21154 = trunc i32 %21153 to i8
  %21155 = and i8 %21154, 1
  %21156 = xor i8 %21155, 1
  store i8 %21156, i8* %21, align 1
  %21157 = xor i64 %21148, %21147
  %21158 = lshr i64 %21157, 4
  %21159 = trunc i64 %21158 to i8
  %21160 = and i8 %21159, 1
  store i8 %21160, i8* %27, align 1
  %21161 = icmp eq i64 %21148, 0
  %21162 = zext i1 %21161 to i8
  store i8 %21162, i8* %30, align 1
  %21163 = lshr i64 %21148, 63
  %21164 = trunc i64 %21163 to i8
  store i8 %21164, i8* %33, align 1
  %21165 = lshr i64 %21147, 63
  %21166 = xor i64 %21163, %21165
  %21167 = add nuw nsw i64 %21166, %21163
  %21168 = icmp eq i64 %21167, 2
  %21169 = zext i1 %21168 to i8
  store i8 %21169, i8* %39, align 1
  %21170 = add i64 %21142, -40
  %21171 = add i64 %21144, 14
  store i64 %21171, i64* %3, align 8
  %21172 = inttoptr i64 %21170 to i32*
  %21173 = load i32, i32* %21172, align 4
  %21174 = sext i32 %21173 to i64
  %21175 = mul nsw i64 %21174, 1032
  store i64 %21175, i64* %RCX.i11580, align 8
  %21176 = lshr i64 %21175, 63
  %21177 = add i64 %21175, %21148
  store i64 %21177, i64* %RAX.i11582.pre-phi, align 8
  %21178 = icmp ult i64 %21177, %21148
  %21179 = icmp ult i64 %21177, %21175
  %21180 = or i1 %21178, %21179
  %21181 = zext i1 %21180 to i8
  store i8 %21181, i8* %14, align 1
  %21182 = trunc i64 %21177 to i32
  %21183 = and i32 %21182, 255
  %21184 = tail call i32 @llvm.ctpop.i32(i32 %21183)
  %21185 = trunc i32 %21184 to i8
  %21186 = and i8 %21185, 1
  %21187 = xor i8 %21186, 1
  store i8 %21187, i8* %21, align 1
  %21188 = xor i64 %21175, %21148
  %21189 = xor i64 %21188, %21177
  %21190 = lshr i64 %21189, 4
  %21191 = trunc i64 %21190 to i8
  %21192 = and i8 %21191, 1
  store i8 %21192, i8* %27, align 1
  %21193 = icmp eq i64 %21177, 0
  %21194 = zext i1 %21193 to i8
  store i8 %21194, i8* %30, align 1
  %21195 = lshr i64 %21177, 63
  %21196 = trunc i64 %21195 to i8
  store i8 %21196, i8* %33, align 1
  %21197 = xor i64 %21195, %21163
  %21198 = xor i64 %21195, %21176
  %21199 = add nuw nsw i64 %21197, %21198
  %21200 = icmp eq i64 %21199, 2
  %21201 = zext i1 %21200 to i8
  store i8 %21201, i8* %39, align 1
  %21202 = load i64, i64* %RBP.i, align 8
  %21203 = add i64 %21202, -120
  %21204 = add i64 %21144, 28
  store i64 %21204, i64* %3, align 8
  %21205 = inttoptr i64 %21203 to i64*
  %21206 = load i64, i64* %21205, align 8
  store i64 %21206, i64* %RCX.i11580, align 8
  %21207 = add i64 %21202, -28
  %21208 = add i64 %21144, 31
  store i64 %21208, i64* %3, align 8
  %21209 = inttoptr i64 %21207 to i32*
  %21210 = load i32, i32* %21209, align 4
  %21211 = add i32 %21210, 8
  %21212 = zext i32 %21211 to i64
  store i64 %21212, i64* %576, align 8
  %21213 = icmp ugt i32 %21210, -9
  %21214 = zext i1 %21213 to i8
  store i8 %21214, i8* %14, align 1
  %21215 = and i32 %21211, 255
  %21216 = tail call i32 @llvm.ctpop.i32(i32 %21215)
  %21217 = trunc i32 %21216 to i8
  %21218 = and i8 %21217, 1
  %21219 = xor i8 %21218, 1
  store i8 %21219, i8* %21, align 1
  %21220 = xor i32 %21211, %21210
  %21221 = lshr i32 %21220, 4
  %21222 = trunc i32 %21221 to i8
  %21223 = and i8 %21222, 1
  store i8 %21223, i8* %27, align 1
  %21224 = icmp eq i32 %21211, 0
  %21225 = zext i1 %21224 to i8
  store i8 %21225, i8* %30, align 1
  %21226 = lshr i32 %21211, 31
  %21227 = trunc i32 %21226 to i8
  store i8 %21227, i8* %33, align 1
  %21228 = lshr i32 %21210, 31
  %21229 = xor i32 %21226, %21228
  %21230 = add nuw nsw i32 %21229, %21226
  %21231 = icmp eq i32 %21230, 2
  %21232 = zext i1 %21231 to i8
  store i8 %21232, i8* %39, align 1
  %21233 = sext i32 %21211 to i64
  store i64 %21233, i64* %RSI.i11312, align 8
  %21234 = shl nsw i64 %21233, 1
  %21235 = add i64 %21206, %21234
  %21236 = add i64 %21144, 41
  store i64 %21236, i64* %3, align 8
  %21237 = inttoptr i64 %21235 to i16*
  %21238 = load i16, i16* %21237, align 2
  %21239 = zext i16 %21238 to i64
  store i64 %21239, i64* %576, align 8
  %21240 = zext i16 %21238 to i64
  store i64 %21240, i64* %RCX.i11580, align 8
  %21241 = shl nuw nsw i64 %21240, 2
  %21242 = add i64 %21177, %21241
  %21243 = add i64 %21144, 46
  store i64 %21243, i64* %3, align 8
  %21244 = inttoptr i64 %21242 to i32*
  %21245 = load i32, i32* %21244, align 4
  %21246 = add i32 %21245, 1
  %21247 = zext i32 %21246 to i64
  store i64 %21247, i64* %576, align 8
  %21248 = icmp eq i32 %21245, -1
  %21249 = icmp eq i32 %21246, 0
  %21250 = or i1 %21248, %21249
  %21251 = zext i1 %21250 to i8
  store i8 %21251, i8* %14, align 1
  %21252 = and i32 %21246, 255
  %21253 = tail call i32 @llvm.ctpop.i32(i32 %21252)
  %21254 = trunc i32 %21253 to i8
  %21255 = and i8 %21254, 1
  %21256 = xor i8 %21255, 1
  store i8 %21256, i8* %21, align 1
  %21257 = xor i32 %21246, %21245
  %21258 = lshr i32 %21257, 4
  %21259 = trunc i32 %21258 to i8
  %21260 = and i8 %21259, 1
  store i8 %21260, i8* %27, align 1
  %21261 = zext i1 %21249 to i8
  store i8 %21261, i8* %30, align 1
  %21262 = lshr i32 %21246, 31
  %21263 = trunc i32 %21262 to i8
  store i8 %21263, i8* %33, align 1
  %21264 = lshr i32 %21245, 31
  %21265 = xor i32 %21262, %21264
  %21266 = add nuw nsw i32 %21265, %21262
  %21267 = icmp eq i32 %21266, 2
  %21268 = zext i1 %21267 to i8
  store i8 %21268, i8* %39, align 1
  %21269 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21270 = add i64 %21241, %21269
  %21271 = add i64 %21144, 52
  store i64 %21271, i64* %3, align 8
  %21272 = inttoptr i64 %21270 to i32*
  store i32 %21246, i32* %21272, align 4
  %21273 = load i64, i64* %RBP.i, align 8
  %21274 = add i64 %21273, -8
  %21275 = load i64, i64* %3, align 8
  %21276 = add i64 %21275, 4
  store i64 %21276, i64* %3, align 8
  %21277 = inttoptr i64 %21274 to i64*
  %21278 = load i64, i64* %21277, align 8
  %21279 = add i64 %21278, 45448
  store i64 %21279, i64* %RAX.i11582.pre-phi, align 8
  %21280 = icmp ugt i64 %21278, -45449
  %21281 = zext i1 %21280 to i8
  store i8 %21281, i8* %14, align 1
  %21282 = trunc i64 %21279 to i32
  %21283 = and i32 %21282, 255
  %21284 = tail call i32 @llvm.ctpop.i32(i32 %21283)
  %21285 = trunc i32 %21284 to i8
  %21286 = and i8 %21285, 1
  %21287 = xor i8 %21286, 1
  store i8 %21287, i8* %21, align 1
  %21288 = xor i64 %21279, %21278
  %21289 = lshr i64 %21288, 4
  %21290 = trunc i64 %21289 to i8
  %21291 = and i8 %21290, 1
  store i8 %21291, i8* %27, align 1
  %21292 = icmp eq i64 %21279, 0
  %21293 = zext i1 %21292 to i8
  store i8 %21293, i8* %30, align 1
  %21294 = lshr i64 %21279, 63
  %21295 = trunc i64 %21294 to i8
  store i8 %21295, i8* %33, align 1
  %21296 = lshr i64 %21278, 63
  %21297 = xor i64 %21294, %21296
  %21298 = add nuw nsw i64 %21297, %21294
  %21299 = icmp eq i64 %21298, 2
  %21300 = zext i1 %21299 to i8
  store i8 %21300, i8* %39, align 1
  %21301 = add i64 %21273, -40
  %21302 = add i64 %21275, 14
  store i64 %21302, i64* %3, align 8
  %21303 = inttoptr i64 %21301 to i32*
  %21304 = load i32, i32* %21303, align 4
  %21305 = sext i32 %21304 to i64
  %21306 = mul nsw i64 %21305, 1032
  store i64 %21306, i64* %RCX.i11580, align 8
  %21307 = lshr i64 %21306, 63
  %21308 = add i64 %21306, %21279
  store i64 %21308, i64* %RAX.i11582.pre-phi, align 8
  %21309 = icmp ult i64 %21308, %21279
  %21310 = icmp ult i64 %21308, %21306
  %21311 = or i1 %21309, %21310
  %21312 = zext i1 %21311 to i8
  store i8 %21312, i8* %14, align 1
  %21313 = trunc i64 %21308 to i32
  %21314 = and i32 %21313, 255
  %21315 = tail call i32 @llvm.ctpop.i32(i32 %21314)
  %21316 = trunc i32 %21315 to i8
  %21317 = and i8 %21316, 1
  %21318 = xor i8 %21317, 1
  store i8 %21318, i8* %21, align 1
  %21319 = xor i64 %21306, %21279
  %21320 = xor i64 %21319, %21308
  %21321 = lshr i64 %21320, 4
  %21322 = trunc i64 %21321 to i8
  %21323 = and i8 %21322, 1
  store i8 %21323, i8* %27, align 1
  %21324 = icmp eq i64 %21308, 0
  %21325 = zext i1 %21324 to i8
  store i8 %21325, i8* %30, align 1
  %21326 = lshr i64 %21308, 63
  %21327 = trunc i64 %21326 to i8
  store i8 %21327, i8* %33, align 1
  %21328 = xor i64 %21326, %21294
  %21329 = xor i64 %21326, %21307
  %21330 = add nuw nsw i64 %21328, %21329
  %21331 = icmp eq i64 %21330, 2
  %21332 = zext i1 %21331 to i8
  store i8 %21332, i8* %39, align 1
  %21333 = load i64, i64* %RBP.i, align 8
  %21334 = add i64 %21333, -120
  %21335 = add i64 %21275, 28
  store i64 %21335, i64* %3, align 8
  %21336 = inttoptr i64 %21334 to i64*
  %21337 = load i64, i64* %21336, align 8
  store i64 %21337, i64* %RCX.i11580, align 8
  %21338 = add i64 %21333, -28
  %21339 = add i64 %21275, 31
  store i64 %21339, i64* %3, align 8
  %21340 = inttoptr i64 %21338 to i32*
  %21341 = load i32, i32* %21340, align 4
  %21342 = add i32 %21341, 9
  %21343 = zext i32 %21342 to i64
  store i64 %21343, i64* %576, align 8
  %21344 = icmp ugt i32 %21341, -10
  %21345 = zext i1 %21344 to i8
  store i8 %21345, i8* %14, align 1
  %21346 = and i32 %21342, 255
  %21347 = tail call i32 @llvm.ctpop.i32(i32 %21346)
  %21348 = trunc i32 %21347 to i8
  %21349 = and i8 %21348, 1
  %21350 = xor i8 %21349, 1
  store i8 %21350, i8* %21, align 1
  %21351 = xor i32 %21342, %21341
  %21352 = lshr i32 %21351, 4
  %21353 = trunc i32 %21352 to i8
  %21354 = and i8 %21353, 1
  store i8 %21354, i8* %27, align 1
  %21355 = icmp eq i32 %21342, 0
  %21356 = zext i1 %21355 to i8
  store i8 %21356, i8* %30, align 1
  %21357 = lshr i32 %21342, 31
  %21358 = trunc i32 %21357 to i8
  store i8 %21358, i8* %33, align 1
  %21359 = lshr i32 %21341, 31
  %21360 = xor i32 %21357, %21359
  %21361 = add nuw nsw i32 %21360, %21357
  %21362 = icmp eq i32 %21361, 2
  %21363 = zext i1 %21362 to i8
  store i8 %21363, i8* %39, align 1
  %21364 = sext i32 %21342 to i64
  store i64 %21364, i64* %RSI.i11312, align 8
  %21365 = shl nsw i64 %21364, 1
  %21366 = add i64 %21337, %21365
  %21367 = add i64 %21275, 41
  store i64 %21367, i64* %3, align 8
  %21368 = inttoptr i64 %21366 to i16*
  %21369 = load i16, i16* %21368, align 2
  %21370 = zext i16 %21369 to i64
  store i64 %21370, i64* %576, align 8
  %21371 = zext i16 %21369 to i64
  store i64 %21371, i64* %RCX.i11580, align 8
  %21372 = shl nuw nsw i64 %21371, 2
  %21373 = add i64 %21308, %21372
  %21374 = add i64 %21275, 46
  store i64 %21374, i64* %3, align 8
  %21375 = inttoptr i64 %21373 to i32*
  %21376 = load i32, i32* %21375, align 4
  %21377 = add i32 %21376, 1
  %21378 = zext i32 %21377 to i64
  store i64 %21378, i64* %576, align 8
  %21379 = icmp eq i32 %21376, -1
  %21380 = icmp eq i32 %21377, 0
  %21381 = or i1 %21379, %21380
  %21382 = zext i1 %21381 to i8
  store i8 %21382, i8* %14, align 1
  %21383 = and i32 %21377, 255
  %21384 = tail call i32 @llvm.ctpop.i32(i32 %21383)
  %21385 = trunc i32 %21384 to i8
  %21386 = and i8 %21385, 1
  %21387 = xor i8 %21386, 1
  store i8 %21387, i8* %21, align 1
  %21388 = xor i32 %21377, %21376
  %21389 = lshr i32 %21388, 4
  %21390 = trunc i32 %21389 to i8
  %21391 = and i8 %21390, 1
  store i8 %21391, i8* %27, align 1
  %21392 = zext i1 %21380 to i8
  store i8 %21392, i8* %30, align 1
  %21393 = lshr i32 %21377, 31
  %21394 = trunc i32 %21393 to i8
  store i8 %21394, i8* %33, align 1
  %21395 = lshr i32 %21376, 31
  %21396 = xor i32 %21393, %21395
  %21397 = add nuw nsw i32 %21396, %21393
  %21398 = icmp eq i32 %21397, 2
  %21399 = zext i1 %21398 to i8
  store i8 %21399, i8* %39, align 1
  %21400 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21401 = add i64 %21372, %21400
  %21402 = add i64 %21275, 52
  store i64 %21402, i64* %3, align 8
  %21403 = inttoptr i64 %21401 to i32*
  store i32 %21377, i32* %21403, align 4
  %21404 = load i64, i64* %RBP.i, align 8
  %21405 = add i64 %21404, -8
  %21406 = load i64, i64* %3, align 8
  %21407 = add i64 %21406, 4
  store i64 %21407, i64* %3, align 8
  %21408 = inttoptr i64 %21405 to i64*
  %21409 = load i64, i64* %21408, align 8
  %21410 = add i64 %21409, 45448
  store i64 %21410, i64* %RAX.i11582.pre-phi, align 8
  %21411 = icmp ugt i64 %21409, -45449
  %21412 = zext i1 %21411 to i8
  store i8 %21412, i8* %14, align 1
  %21413 = trunc i64 %21410 to i32
  %21414 = and i32 %21413, 255
  %21415 = tail call i32 @llvm.ctpop.i32(i32 %21414)
  %21416 = trunc i32 %21415 to i8
  %21417 = and i8 %21416, 1
  %21418 = xor i8 %21417, 1
  store i8 %21418, i8* %21, align 1
  %21419 = xor i64 %21410, %21409
  %21420 = lshr i64 %21419, 4
  %21421 = trunc i64 %21420 to i8
  %21422 = and i8 %21421, 1
  store i8 %21422, i8* %27, align 1
  %21423 = icmp eq i64 %21410, 0
  %21424 = zext i1 %21423 to i8
  store i8 %21424, i8* %30, align 1
  %21425 = lshr i64 %21410, 63
  %21426 = trunc i64 %21425 to i8
  store i8 %21426, i8* %33, align 1
  %21427 = lshr i64 %21409, 63
  %21428 = xor i64 %21425, %21427
  %21429 = add nuw nsw i64 %21428, %21425
  %21430 = icmp eq i64 %21429, 2
  %21431 = zext i1 %21430 to i8
  store i8 %21431, i8* %39, align 1
  %21432 = add i64 %21404, -40
  %21433 = add i64 %21406, 14
  store i64 %21433, i64* %3, align 8
  %21434 = inttoptr i64 %21432 to i32*
  %21435 = load i32, i32* %21434, align 4
  %21436 = sext i32 %21435 to i64
  %21437 = mul nsw i64 %21436, 1032
  store i64 %21437, i64* %RCX.i11580, align 8
  %21438 = lshr i64 %21437, 63
  %21439 = add i64 %21437, %21410
  store i64 %21439, i64* %RAX.i11582.pre-phi, align 8
  %21440 = icmp ult i64 %21439, %21410
  %21441 = icmp ult i64 %21439, %21437
  %21442 = or i1 %21440, %21441
  %21443 = zext i1 %21442 to i8
  store i8 %21443, i8* %14, align 1
  %21444 = trunc i64 %21439 to i32
  %21445 = and i32 %21444, 255
  %21446 = tail call i32 @llvm.ctpop.i32(i32 %21445)
  %21447 = trunc i32 %21446 to i8
  %21448 = and i8 %21447, 1
  %21449 = xor i8 %21448, 1
  store i8 %21449, i8* %21, align 1
  %21450 = xor i64 %21437, %21410
  %21451 = xor i64 %21450, %21439
  %21452 = lshr i64 %21451, 4
  %21453 = trunc i64 %21452 to i8
  %21454 = and i8 %21453, 1
  store i8 %21454, i8* %27, align 1
  %21455 = icmp eq i64 %21439, 0
  %21456 = zext i1 %21455 to i8
  store i8 %21456, i8* %30, align 1
  %21457 = lshr i64 %21439, 63
  %21458 = trunc i64 %21457 to i8
  store i8 %21458, i8* %33, align 1
  %21459 = xor i64 %21457, %21425
  %21460 = xor i64 %21457, %21438
  %21461 = add nuw nsw i64 %21459, %21460
  %21462 = icmp eq i64 %21461, 2
  %21463 = zext i1 %21462 to i8
  store i8 %21463, i8* %39, align 1
  %21464 = load i64, i64* %RBP.i, align 8
  %21465 = add i64 %21464, -120
  %21466 = add i64 %21406, 28
  store i64 %21466, i64* %3, align 8
  %21467 = inttoptr i64 %21465 to i64*
  %21468 = load i64, i64* %21467, align 8
  store i64 %21468, i64* %RCX.i11580, align 8
  %21469 = add i64 %21464, -28
  %21470 = add i64 %21406, 31
  store i64 %21470, i64* %3, align 8
  %21471 = inttoptr i64 %21469 to i32*
  %21472 = load i32, i32* %21471, align 4
  %21473 = add i32 %21472, 10
  %21474 = zext i32 %21473 to i64
  store i64 %21474, i64* %576, align 8
  %21475 = icmp ugt i32 %21472, -11
  %21476 = zext i1 %21475 to i8
  store i8 %21476, i8* %14, align 1
  %21477 = and i32 %21473, 255
  %21478 = tail call i32 @llvm.ctpop.i32(i32 %21477)
  %21479 = trunc i32 %21478 to i8
  %21480 = and i8 %21479, 1
  %21481 = xor i8 %21480, 1
  store i8 %21481, i8* %21, align 1
  %21482 = xor i32 %21473, %21472
  %21483 = lshr i32 %21482, 4
  %21484 = trunc i32 %21483 to i8
  %21485 = and i8 %21484, 1
  store i8 %21485, i8* %27, align 1
  %21486 = icmp eq i32 %21473, 0
  %21487 = zext i1 %21486 to i8
  store i8 %21487, i8* %30, align 1
  %21488 = lshr i32 %21473, 31
  %21489 = trunc i32 %21488 to i8
  store i8 %21489, i8* %33, align 1
  %21490 = lshr i32 %21472, 31
  %21491 = xor i32 %21488, %21490
  %21492 = add nuw nsw i32 %21491, %21488
  %21493 = icmp eq i32 %21492, 2
  %21494 = zext i1 %21493 to i8
  store i8 %21494, i8* %39, align 1
  %21495 = sext i32 %21473 to i64
  store i64 %21495, i64* %RSI.i11312, align 8
  %21496 = shl nsw i64 %21495, 1
  %21497 = add i64 %21468, %21496
  %21498 = add i64 %21406, 41
  store i64 %21498, i64* %3, align 8
  %21499 = inttoptr i64 %21497 to i16*
  %21500 = load i16, i16* %21499, align 2
  %21501 = zext i16 %21500 to i64
  store i64 %21501, i64* %576, align 8
  %21502 = zext i16 %21500 to i64
  store i64 %21502, i64* %RCX.i11580, align 8
  %21503 = shl nuw nsw i64 %21502, 2
  %21504 = add i64 %21439, %21503
  %21505 = add i64 %21406, 46
  store i64 %21505, i64* %3, align 8
  %21506 = inttoptr i64 %21504 to i32*
  %21507 = load i32, i32* %21506, align 4
  %21508 = add i32 %21507, 1
  %21509 = zext i32 %21508 to i64
  store i64 %21509, i64* %576, align 8
  %21510 = icmp eq i32 %21507, -1
  %21511 = icmp eq i32 %21508, 0
  %21512 = or i1 %21510, %21511
  %21513 = zext i1 %21512 to i8
  store i8 %21513, i8* %14, align 1
  %21514 = and i32 %21508, 255
  %21515 = tail call i32 @llvm.ctpop.i32(i32 %21514)
  %21516 = trunc i32 %21515 to i8
  %21517 = and i8 %21516, 1
  %21518 = xor i8 %21517, 1
  store i8 %21518, i8* %21, align 1
  %21519 = xor i32 %21508, %21507
  %21520 = lshr i32 %21519, 4
  %21521 = trunc i32 %21520 to i8
  %21522 = and i8 %21521, 1
  store i8 %21522, i8* %27, align 1
  %21523 = zext i1 %21511 to i8
  store i8 %21523, i8* %30, align 1
  %21524 = lshr i32 %21508, 31
  %21525 = trunc i32 %21524 to i8
  store i8 %21525, i8* %33, align 1
  %21526 = lshr i32 %21507, 31
  %21527 = xor i32 %21524, %21526
  %21528 = add nuw nsw i32 %21527, %21524
  %21529 = icmp eq i32 %21528, 2
  %21530 = zext i1 %21529 to i8
  store i8 %21530, i8* %39, align 1
  %21531 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21532 = add i64 %21503, %21531
  %21533 = add i64 %21406, 52
  store i64 %21533, i64* %3, align 8
  %21534 = inttoptr i64 %21532 to i32*
  store i32 %21508, i32* %21534, align 4
  %21535 = load i64, i64* %RBP.i, align 8
  %21536 = add i64 %21535, -8
  %21537 = load i64, i64* %3, align 8
  %21538 = add i64 %21537, 4
  store i64 %21538, i64* %3, align 8
  %21539 = inttoptr i64 %21536 to i64*
  %21540 = load i64, i64* %21539, align 8
  %21541 = add i64 %21540, 45448
  store i64 %21541, i64* %RAX.i11582.pre-phi, align 8
  %21542 = icmp ugt i64 %21540, -45449
  %21543 = zext i1 %21542 to i8
  store i8 %21543, i8* %14, align 1
  %21544 = trunc i64 %21541 to i32
  %21545 = and i32 %21544, 255
  %21546 = tail call i32 @llvm.ctpop.i32(i32 %21545)
  %21547 = trunc i32 %21546 to i8
  %21548 = and i8 %21547, 1
  %21549 = xor i8 %21548, 1
  store i8 %21549, i8* %21, align 1
  %21550 = xor i64 %21541, %21540
  %21551 = lshr i64 %21550, 4
  %21552 = trunc i64 %21551 to i8
  %21553 = and i8 %21552, 1
  store i8 %21553, i8* %27, align 1
  %21554 = icmp eq i64 %21541, 0
  %21555 = zext i1 %21554 to i8
  store i8 %21555, i8* %30, align 1
  %21556 = lshr i64 %21541, 63
  %21557 = trunc i64 %21556 to i8
  store i8 %21557, i8* %33, align 1
  %21558 = lshr i64 %21540, 63
  %21559 = xor i64 %21556, %21558
  %21560 = add nuw nsw i64 %21559, %21556
  %21561 = icmp eq i64 %21560, 2
  %21562 = zext i1 %21561 to i8
  store i8 %21562, i8* %39, align 1
  %21563 = add i64 %21535, -40
  %21564 = add i64 %21537, 14
  store i64 %21564, i64* %3, align 8
  %21565 = inttoptr i64 %21563 to i32*
  %21566 = load i32, i32* %21565, align 4
  %21567 = sext i32 %21566 to i64
  %21568 = mul nsw i64 %21567, 1032
  store i64 %21568, i64* %RCX.i11580, align 8
  %21569 = lshr i64 %21568, 63
  %21570 = add i64 %21568, %21541
  store i64 %21570, i64* %RAX.i11582.pre-phi, align 8
  %21571 = icmp ult i64 %21570, %21541
  %21572 = icmp ult i64 %21570, %21568
  %21573 = or i1 %21571, %21572
  %21574 = zext i1 %21573 to i8
  store i8 %21574, i8* %14, align 1
  %21575 = trunc i64 %21570 to i32
  %21576 = and i32 %21575, 255
  %21577 = tail call i32 @llvm.ctpop.i32(i32 %21576)
  %21578 = trunc i32 %21577 to i8
  %21579 = and i8 %21578, 1
  %21580 = xor i8 %21579, 1
  store i8 %21580, i8* %21, align 1
  %21581 = xor i64 %21568, %21541
  %21582 = xor i64 %21581, %21570
  %21583 = lshr i64 %21582, 4
  %21584 = trunc i64 %21583 to i8
  %21585 = and i8 %21584, 1
  store i8 %21585, i8* %27, align 1
  %21586 = icmp eq i64 %21570, 0
  %21587 = zext i1 %21586 to i8
  store i8 %21587, i8* %30, align 1
  %21588 = lshr i64 %21570, 63
  %21589 = trunc i64 %21588 to i8
  store i8 %21589, i8* %33, align 1
  %21590 = xor i64 %21588, %21556
  %21591 = xor i64 %21588, %21569
  %21592 = add nuw nsw i64 %21590, %21591
  %21593 = icmp eq i64 %21592, 2
  %21594 = zext i1 %21593 to i8
  store i8 %21594, i8* %39, align 1
  %21595 = load i64, i64* %RBP.i, align 8
  %21596 = add i64 %21595, -120
  %21597 = add i64 %21537, 28
  store i64 %21597, i64* %3, align 8
  %21598 = inttoptr i64 %21596 to i64*
  %21599 = load i64, i64* %21598, align 8
  store i64 %21599, i64* %RCX.i11580, align 8
  %21600 = add i64 %21595, -28
  %21601 = add i64 %21537, 31
  store i64 %21601, i64* %3, align 8
  %21602 = inttoptr i64 %21600 to i32*
  %21603 = load i32, i32* %21602, align 4
  %21604 = add i32 %21603, 11
  %21605 = zext i32 %21604 to i64
  store i64 %21605, i64* %576, align 8
  %21606 = icmp ugt i32 %21603, -12
  %21607 = zext i1 %21606 to i8
  store i8 %21607, i8* %14, align 1
  %21608 = and i32 %21604, 255
  %21609 = tail call i32 @llvm.ctpop.i32(i32 %21608)
  %21610 = trunc i32 %21609 to i8
  %21611 = and i8 %21610, 1
  %21612 = xor i8 %21611, 1
  store i8 %21612, i8* %21, align 1
  %21613 = xor i32 %21604, %21603
  %21614 = lshr i32 %21613, 4
  %21615 = trunc i32 %21614 to i8
  %21616 = and i8 %21615, 1
  store i8 %21616, i8* %27, align 1
  %21617 = icmp eq i32 %21604, 0
  %21618 = zext i1 %21617 to i8
  store i8 %21618, i8* %30, align 1
  %21619 = lshr i32 %21604, 31
  %21620 = trunc i32 %21619 to i8
  store i8 %21620, i8* %33, align 1
  %21621 = lshr i32 %21603, 31
  %21622 = xor i32 %21619, %21621
  %21623 = add nuw nsw i32 %21622, %21619
  %21624 = icmp eq i32 %21623, 2
  %21625 = zext i1 %21624 to i8
  store i8 %21625, i8* %39, align 1
  %21626 = sext i32 %21604 to i64
  store i64 %21626, i64* %RSI.i11312, align 8
  %21627 = shl nsw i64 %21626, 1
  %21628 = add i64 %21599, %21627
  %21629 = add i64 %21537, 41
  store i64 %21629, i64* %3, align 8
  %21630 = inttoptr i64 %21628 to i16*
  %21631 = load i16, i16* %21630, align 2
  %21632 = zext i16 %21631 to i64
  store i64 %21632, i64* %576, align 8
  %21633 = zext i16 %21631 to i64
  store i64 %21633, i64* %RCX.i11580, align 8
  %21634 = shl nuw nsw i64 %21633, 2
  %21635 = add i64 %21570, %21634
  %21636 = add i64 %21537, 46
  store i64 %21636, i64* %3, align 8
  %21637 = inttoptr i64 %21635 to i32*
  %21638 = load i32, i32* %21637, align 4
  %21639 = add i32 %21638, 1
  %21640 = zext i32 %21639 to i64
  store i64 %21640, i64* %576, align 8
  %21641 = icmp eq i32 %21638, -1
  %21642 = icmp eq i32 %21639, 0
  %21643 = or i1 %21641, %21642
  %21644 = zext i1 %21643 to i8
  store i8 %21644, i8* %14, align 1
  %21645 = and i32 %21639, 255
  %21646 = tail call i32 @llvm.ctpop.i32(i32 %21645)
  %21647 = trunc i32 %21646 to i8
  %21648 = and i8 %21647, 1
  %21649 = xor i8 %21648, 1
  store i8 %21649, i8* %21, align 1
  %21650 = xor i32 %21639, %21638
  %21651 = lshr i32 %21650, 4
  %21652 = trunc i32 %21651 to i8
  %21653 = and i8 %21652, 1
  store i8 %21653, i8* %27, align 1
  %21654 = zext i1 %21642 to i8
  store i8 %21654, i8* %30, align 1
  %21655 = lshr i32 %21639, 31
  %21656 = trunc i32 %21655 to i8
  store i8 %21656, i8* %33, align 1
  %21657 = lshr i32 %21638, 31
  %21658 = xor i32 %21655, %21657
  %21659 = add nuw nsw i32 %21658, %21655
  %21660 = icmp eq i32 %21659, 2
  %21661 = zext i1 %21660 to i8
  store i8 %21661, i8* %39, align 1
  %21662 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21663 = add i64 %21634, %21662
  %21664 = add i64 %21537, 52
  store i64 %21664, i64* %3, align 8
  %21665 = inttoptr i64 %21663 to i32*
  store i32 %21639, i32* %21665, align 4
  %21666 = load i64, i64* %RBP.i, align 8
  %21667 = add i64 %21666, -8
  %21668 = load i64, i64* %3, align 8
  %21669 = add i64 %21668, 4
  store i64 %21669, i64* %3, align 8
  %21670 = inttoptr i64 %21667 to i64*
  %21671 = load i64, i64* %21670, align 8
  %21672 = add i64 %21671, 45448
  store i64 %21672, i64* %RAX.i11582.pre-phi, align 8
  %21673 = icmp ugt i64 %21671, -45449
  %21674 = zext i1 %21673 to i8
  store i8 %21674, i8* %14, align 1
  %21675 = trunc i64 %21672 to i32
  %21676 = and i32 %21675, 255
  %21677 = tail call i32 @llvm.ctpop.i32(i32 %21676)
  %21678 = trunc i32 %21677 to i8
  %21679 = and i8 %21678, 1
  %21680 = xor i8 %21679, 1
  store i8 %21680, i8* %21, align 1
  %21681 = xor i64 %21672, %21671
  %21682 = lshr i64 %21681, 4
  %21683 = trunc i64 %21682 to i8
  %21684 = and i8 %21683, 1
  store i8 %21684, i8* %27, align 1
  %21685 = icmp eq i64 %21672, 0
  %21686 = zext i1 %21685 to i8
  store i8 %21686, i8* %30, align 1
  %21687 = lshr i64 %21672, 63
  %21688 = trunc i64 %21687 to i8
  store i8 %21688, i8* %33, align 1
  %21689 = lshr i64 %21671, 63
  %21690 = xor i64 %21687, %21689
  %21691 = add nuw nsw i64 %21690, %21687
  %21692 = icmp eq i64 %21691, 2
  %21693 = zext i1 %21692 to i8
  store i8 %21693, i8* %39, align 1
  %21694 = add i64 %21666, -40
  %21695 = add i64 %21668, 14
  store i64 %21695, i64* %3, align 8
  %21696 = inttoptr i64 %21694 to i32*
  %21697 = load i32, i32* %21696, align 4
  %21698 = sext i32 %21697 to i64
  %21699 = mul nsw i64 %21698, 1032
  store i64 %21699, i64* %RCX.i11580, align 8
  %21700 = lshr i64 %21699, 63
  %21701 = add i64 %21699, %21672
  store i64 %21701, i64* %RAX.i11582.pre-phi, align 8
  %21702 = icmp ult i64 %21701, %21672
  %21703 = icmp ult i64 %21701, %21699
  %21704 = or i1 %21702, %21703
  %21705 = zext i1 %21704 to i8
  store i8 %21705, i8* %14, align 1
  %21706 = trunc i64 %21701 to i32
  %21707 = and i32 %21706, 255
  %21708 = tail call i32 @llvm.ctpop.i32(i32 %21707)
  %21709 = trunc i32 %21708 to i8
  %21710 = and i8 %21709, 1
  %21711 = xor i8 %21710, 1
  store i8 %21711, i8* %21, align 1
  %21712 = xor i64 %21699, %21672
  %21713 = xor i64 %21712, %21701
  %21714 = lshr i64 %21713, 4
  %21715 = trunc i64 %21714 to i8
  %21716 = and i8 %21715, 1
  store i8 %21716, i8* %27, align 1
  %21717 = icmp eq i64 %21701, 0
  %21718 = zext i1 %21717 to i8
  store i8 %21718, i8* %30, align 1
  %21719 = lshr i64 %21701, 63
  %21720 = trunc i64 %21719 to i8
  store i8 %21720, i8* %33, align 1
  %21721 = xor i64 %21719, %21687
  %21722 = xor i64 %21719, %21700
  %21723 = add nuw nsw i64 %21721, %21722
  %21724 = icmp eq i64 %21723, 2
  %21725 = zext i1 %21724 to i8
  store i8 %21725, i8* %39, align 1
  %21726 = load i64, i64* %RBP.i, align 8
  %21727 = add i64 %21726, -120
  %21728 = add i64 %21668, 28
  store i64 %21728, i64* %3, align 8
  %21729 = inttoptr i64 %21727 to i64*
  %21730 = load i64, i64* %21729, align 8
  store i64 %21730, i64* %RCX.i11580, align 8
  %21731 = add i64 %21726, -28
  %21732 = add i64 %21668, 31
  store i64 %21732, i64* %3, align 8
  %21733 = inttoptr i64 %21731 to i32*
  %21734 = load i32, i32* %21733, align 4
  %21735 = add i32 %21734, 12
  %21736 = zext i32 %21735 to i64
  store i64 %21736, i64* %576, align 8
  %21737 = icmp ugt i32 %21734, -13
  %21738 = zext i1 %21737 to i8
  store i8 %21738, i8* %14, align 1
  %21739 = and i32 %21735, 255
  %21740 = tail call i32 @llvm.ctpop.i32(i32 %21739)
  %21741 = trunc i32 %21740 to i8
  %21742 = and i8 %21741, 1
  %21743 = xor i8 %21742, 1
  store i8 %21743, i8* %21, align 1
  %21744 = xor i32 %21735, %21734
  %21745 = lshr i32 %21744, 4
  %21746 = trunc i32 %21745 to i8
  %21747 = and i8 %21746, 1
  store i8 %21747, i8* %27, align 1
  %21748 = icmp eq i32 %21735, 0
  %21749 = zext i1 %21748 to i8
  store i8 %21749, i8* %30, align 1
  %21750 = lshr i32 %21735, 31
  %21751 = trunc i32 %21750 to i8
  store i8 %21751, i8* %33, align 1
  %21752 = lshr i32 %21734, 31
  %21753 = xor i32 %21750, %21752
  %21754 = add nuw nsw i32 %21753, %21750
  %21755 = icmp eq i32 %21754, 2
  %21756 = zext i1 %21755 to i8
  store i8 %21756, i8* %39, align 1
  %21757 = sext i32 %21735 to i64
  store i64 %21757, i64* %RSI.i11312, align 8
  %21758 = shl nsw i64 %21757, 1
  %21759 = add i64 %21730, %21758
  %21760 = add i64 %21668, 41
  store i64 %21760, i64* %3, align 8
  %21761 = inttoptr i64 %21759 to i16*
  %21762 = load i16, i16* %21761, align 2
  %21763 = zext i16 %21762 to i64
  store i64 %21763, i64* %576, align 8
  %21764 = zext i16 %21762 to i64
  store i64 %21764, i64* %RCX.i11580, align 8
  %21765 = shl nuw nsw i64 %21764, 2
  %21766 = add i64 %21701, %21765
  %21767 = add i64 %21668, 46
  store i64 %21767, i64* %3, align 8
  %21768 = inttoptr i64 %21766 to i32*
  %21769 = load i32, i32* %21768, align 4
  %21770 = add i32 %21769, 1
  %21771 = zext i32 %21770 to i64
  store i64 %21771, i64* %576, align 8
  %21772 = icmp eq i32 %21769, -1
  %21773 = icmp eq i32 %21770, 0
  %21774 = or i1 %21772, %21773
  %21775 = zext i1 %21774 to i8
  store i8 %21775, i8* %14, align 1
  %21776 = and i32 %21770, 255
  %21777 = tail call i32 @llvm.ctpop.i32(i32 %21776)
  %21778 = trunc i32 %21777 to i8
  %21779 = and i8 %21778, 1
  %21780 = xor i8 %21779, 1
  store i8 %21780, i8* %21, align 1
  %21781 = xor i32 %21770, %21769
  %21782 = lshr i32 %21781, 4
  %21783 = trunc i32 %21782 to i8
  %21784 = and i8 %21783, 1
  store i8 %21784, i8* %27, align 1
  %21785 = zext i1 %21773 to i8
  store i8 %21785, i8* %30, align 1
  %21786 = lshr i32 %21770, 31
  %21787 = trunc i32 %21786 to i8
  store i8 %21787, i8* %33, align 1
  %21788 = lshr i32 %21769, 31
  %21789 = xor i32 %21786, %21788
  %21790 = add nuw nsw i32 %21789, %21786
  %21791 = icmp eq i32 %21790, 2
  %21792 = zext i1 %21791 to i8
  store i8 %21792, i8* %39, align 1
  %21793 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21794 = add i64 %21765, %21793
  %21795 = add i64 %21668, 52
  store i64 %21795, i64* %3, align 8
  %21796 = inttoptr i64 %21794 to i32*
  store i32 %21770, i32* %21796, align 4
  %21797 = load i64, i64* %RBP.i, align 8
  %21798 = add i64 %21797, -8
  %21799 = load i64, i64* %3, align 8
  %21800 = add i64 %21799, 4
  store i64 %21800, i64* %3, align 8
  %21801 = inttoptr i64 %21798 to i64*
  %21802 = load i64, i64* %21801, align 8
  %21803 = add i64 %21802, 45448
  store i64 %21803, i64* %RAX.i11582.pre-phi, align 8
  %21804 = icmp ugt i64 %21802, -45449
  %21805 = zext i1 %21804 to i8
  store i8 %21805, i8* %14, align 1
  %21806 = trunc i64 %21803 to i32
  %21807 = and i32 %21806, 255
  %21808 = tail call i32 @llvm.ctpop.i32(i32 %21807)
  %21809 = trunc i32 %21808 to i8
  %21810 = and i8 %21809, 1
  %21811 = xor i8 %21810, 1
  store i8 %21811, i8* %21, align 1
  %21812 = xor i64 %21803, %21802
  %21813 = lshr i64 %21812, 4
  %21814 = trunc i64 %21813 to i8
  %21815 = and i8 %21814, 1
  store i8 %21815, i8* %27, align 1
  %21816 = icmp eq i64 %21803, 0
  %21817 = zext i1 %21816 to i8
  store i8 %21817, i8* %30, align 1
  %21818 = lshr i64 %21803, 63
  %21819 = trunc i64 %21818 to i8
  store i8 %21819, i8* %33, align 1
  %21820 = lshr i64 %21802, 63
  %21821 = xor i64 %21818, %21820
  %21822 = add nuw nsw i64 %21821, %21818
  %21823 = icmp eq i64 %21822, 2
  %21824 = zext i1 %21823 to i8
  store i8 %21824, i8* %39, align 1
  %21825 = add i64 %21797, -40
  %21826 = add i64 %21799, 14
  store i64 %21826, i64* %3, align 8
  %21827 = inttoptr i64 %21825 to i32*
  %21828 = load i32, i32* %21827, align 4
  %21829 = sext i32 %21828 to i64
  %21830 = mul nsw i64 %21829, 1032
  store i64 %21830, i64* %RCX.i11580, align 8
  %21831 = lshr i64 %21830, 63
  %21832 = add i64 %21830, %21803
  store i64 %21832, i64* %RAX.i11582.pre-phi, align 8
  %21833 = icmp ult i64 %21832, %21803
  %21834 = icmp ult i64 %21832, %21830
  %21835 = or i1 %21833, %21834
  %21836 = zext i1 %21835 to i8
  store i8 %21836, i8* %14, align 1
  %21837 = trunc i64 %21832 to i32
  %21838 = and i32 %21837, 255
  %21839 = tail call i32 @llvm.ctpop.i32(i32 %21838)
  %21840 = trunc i32 %21839 to i8
  %21841 = and i8 %21840, 1
  %21842 = xor i8 %21841, 1
  store i8 %21842, i8* %21, align 1
  %21843 = xor i64 %21830, %21803
  %21844 = xor i64 %21843, %21832
  %21845 = lshr i64 %21844, 4
  %21846 = trunc i64 %21845 to i8
  %21847 = and i8 %21846, 1
  store i8 %21847, i8* %27, align 1
  %21848 = icmp eq i64 %21832, 0
  %21849 = zext i1 %21848 to i8
  store i8 %21849, i8* %30, align 1
  %21850 = lshr i64 %21832, 63
  %21851 = trunc i64 %21850 to i8
  store i8 %21851, i8* %33, align 1
  %21852 = xor i64 %21850, %21818
  %21853 = xor i64 %21850, %21831
  %21854 = add nuw nsw i64 %21852, %21853
  %21855 = icmp eq i64 %21854, 2
  %21856 = zext i1 %21855 to i8
  store i8 %21856, i8* %39, align 1
  %21857 = load i64, i64* %RBP.i, align 8
  %21858 = add i64 %21857, -120
  %21859 = add i64 %21799, 28
  store i64 %21859, i64* %3, align 8
  %21860 = inttoptr i64 %21858 to i64*
  %21861 = load i64, i64* %21860, align 8
  store i64 %21861, i64* %RCX.i11580, align 8
  %21862 = add i64 %21857, -28
  %21863 = add i64 %21799, 31
  store i64 %21863, i64* %3, align 8
  %21864 = inttoptr i64 %21862 to i32*
  %21865 = load i32, i32* %21864, align 4
  %21866 = add i32 %21865, 13
  %21867 = zext i32 %21866 to i64
  store i64 %21867, i64* %576, align 8
  %21868 = icmp ugt i32 %21865, -14
  %21869 = zext i1 %21868 to i8
  store i8 %21869, i8* %14, align 1
  %21870 = and i32 %21866, 255
  %21871 = tail call i32 @llvm.ctpop.i32(i32 %21870)
  %21872 = trunc i32 %21871 to i8
  %21873 = and i8 %21872, 1
  %21874 = xor i8 %21873, 1
  store i8 %21874, i8* %21, align 1
  %21875 = xor i32 %21866, %21865
  %21876 = lshr i32 %21875, 4
  %21877 = trunc i32 %21876 to i8
  %21878 = and i8 %21877, 1
  store i8 %21878, i8* %27, align 1
  %21879 = icmp eq i32 %21866, 0
  %21880 = zext i1 %21879 to i8
  store i8 %21880, i8* %30, align 1
  %21881 = lshr i32 %21866, 31
  %21882 = trunc i32 %21881 to i8
  store i8 %21882, i8* %33, align 1
  %21883 = lshr i32 %21865, 31
  %21884 = xor i32 %21881, %21883
  %21885 = add nuw nsw i32 %21884, %21881
  %21886 = icmp eq i32 %21885, 2
  %21887 = zext i1 %21886 to i8
  store i8 %21887, i8* %39, align 1
  %21888 = sext i32 %21866 to i64
  store i64 %21888, i64* %RSI.i11312, align 8
  %21889 = shl nsw i64 %21888, 1
  %21890 = add i64 %21861, %21889
  %21891 = add i64 %21799, 41
  store i64 %21891, i64* %3, align 8
  %21892 = inttoptr i64 %21890 to i16*
  %21893 = load i16, i16* %21892, align 2
  %21894 = zext i16 %21893 to i64
  store i64 %21894, i64* %576, align 8
  %21895 = zext i16 %21893 to i64
  store i64 %21895, i64* %RCX.i11580, align 8
  %21896 = shl nuw nsw i64 %21895, 2
  %21897 = add i64 %21832, %21896
  %21898 = add i64 %21799, 46
  store i64 %21898, i64* %3, align 8
  %21899 = inttoptr i64 %21897 to i32*
  %21900 = load i32, i32* %21899, align 4
  %21901 = add i32 %21900, 1
  %21902 = zext i32 %21901 to i64
  store i64 %21902, i64* %576, align 8
  %21903 = icmp eq i32 %21900, -1
  %21904 = icmp eq i32 %21901, 0
  %21905 = or i1 %21903, %21904
  %21906 = zext i1 %21905 to i8
  store i8 %21906, i8* %14, align 1
  %21907 = and i32 %21901, 255
  %21908 = tail call i32 @llvm.ctpop.i32(i32 %21907)
  %21909 = trunc i32 %21908 to i8
  %21910 = and i8 %21909, 1
  %21911 = xor i8 %21910, 1
  store i8 %21911, i8* %21, align 1
  %21912 = xor i32 %21901, %21900
  %21913 = lshr i32 %21912, 4
  %21914 = trunc i32 %21913 to i8
  %21915 = and i8 %21914, 1
  store i8 %21915, i8* %27, align 1
  %21916 = zext i1 %21904 to i8
  store i8 %21916, i8* %30, align 1
  %21917 = lshr i32 %21901, 31
  %21918 = trunc i32 %21917 to i8
  store i8 %21918, i8* %33, align 1
  %21919 = lshr i32 %21900, 31
  %21920 = xor i32 %21917, %21919
  %21921 = add nuw nsw i32 %21920, %21917
  %21922 = icmp eq i32 %21921, 2
  %21923 = zext i1 %21922 to i8
  store i8 %21923, i8* %39, align 1
  %21924 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %21925 = add i64 %21896, %21924
  %21926 = add i64 %21799, 52
  store i64 %21926, i64* %3, align 8
  %21927 = inttoptr i64 %21925 to i32*
  store i32 %21901, i32* %21927, align 4
  %21928 = load i64, i64* %RBP.i, align 8
  %21929 = add i64 %21928, -8
  %21930 = load i64, i64* %3, align 8
  %21931 = add i64 %21930, 4
  store i64 %21931, i64* %3, align 8
  %21932 = inttoptr i64 %21929 to i64*
  %21933 = load i64, i64* %21932, align 8
  %21934 = add i64 %21933, 45448
  store i64 %21934, i64* %RAX.i11582.pre-phi, align 8
  %21935 = icmp ugt i64 %21933, -45449
  %21936 = zext i1 %21935 to i8
  store i8 %21936, i8* %14, align 1
  %21937 = trunc i64 %21934 to i32
  %21938 = and i32 %21937, 255
  %21939 = tail call i32 @llvm.ctpop.i32(i32 %21938)
  %21940 = trunc i32 %21939 to i8
  %21941 = and i8 %21940, 1
  %21942 = xor i8 %21941, 1
  store i8 %21942, i8* %21, align 1
  %21943 = xor i64 %21934, %21933
  %21944 = lshr i64 %21943, 4
  %21945 = trunc i64 %21944 to i8
  %21946 = and i8 %21945, 1
  store i8 %21946, i8* %27, align 1
  %21947 = icmp eq i64 %21934, 0
  %21948 = zext i1 %21947 to i8
  store i8 %21948, i8* %30, align 1
  %21949 = lshr i64 %21934, 63
  %21950 = trunc i64 %21949 to i8
  store i8 %21950, i8* %33, align 1
  %21951 = lshr i64 %21933, 63
  %21952 = xor i64 %21949, %21951
  %21953 = add nuw nsw i64 %21952, %21949
  %21954 = icmp eq i64 %21953, 2
  %21955 = zext i1 %21954 to i8
  store i8 %21955, i8* %39, align 1
  %21956 = add i64 %21928, -40
  %21957 = add i64 %21930, 14
  store i64 %21957, i64* %3, align 8
  %21958 = inttoptr i64 %21956 to i32*
  %21959 = load i32, i32* %21958, align 4
  %21960 = sext i32 %21959 to i64
  %21961 = mul nsw i64 %21960, 1032
  store i64 %21961, i64* %RCX.i11580, align 8
  %21962 = lshr i64 %21961, 63
  %21963 = add i64 %21961, %21934
  store i64 %21963, i64* %RAX.i11582.pre-phi, align 8
  %21964 = icmp ult i64 %21963, %21934
  %21965 = icmp ult i64 %21963, %21961
  %21966 = or i1 %21964, %21965
  %21967 = zext i1 %21966 to i8
  store i8 %21967, i8* %14, align 1
  %21968 = trunc i64 %21963 to i32
  %21969 = and i32 %21968, 255
  %21970 = tail call i32 @llvm.ctpop.i32(i32 %21969)
  %21971 = trunc i32 %21970 to i8
  %21972 = and i8 %21971, 1
  %21973 = xor i8 %21972, 1
  store i8 %21973, i8* %21, align 1
  %21974 = xor i64 %21961, %21934
  %21975 = xor i64 %21974, %21963
  %21976 = lshr i64 %21975, 4
  %21977 = trunc i64 %21976 to i8
  %21978 = and i8 %21977, 1
  store i8 %21978, i8* %27, align 1
  %21979 = icmp eq i64 %21963, 0
  %21980 = zext i1 %21979 to i8
  store i8 %21980, i8* %30, align 1
  %21981 = lshr i64 %21963, 63
  %21982 = trunc i64 %21981 to i8
  store i8 %21982, i8* %33, align 1
  %21983 = xor i64 %21981, %21949
  %21984 = xor i64 %21981, %21962
  %21985 = add nuw nsw i64 %21983, %21984
  %21986 = icmp eq i64 %21985, 2
  %21987 = zext i1 %21986 to i8
  store i8 %21987, i8* %39, align 1
  %21988 = load i64, i64* %RBP.i, align 8
  %21989 = add i64 %21988, -120
  %21990 = add i64 %21930, 28
  store i64 %21990, i64* %3, align 8
  %21991 = inttoptr i64 %21989 to i64*
  %21992 = load i64, i64* %21991, align 8
  store i64 %21992, i64* %RCX.i11580, align 8
  %21993 = add i64 %21988, -28
  %21994 = add i64 %21930, 31
  store i64 %21994, i64* %3, align 8
  %21995 = inttoptr i64 %21993 to i32*
  %21996 = load i32, i32* %21995, align 4
  %21997 = add i32 %21996, 14
  %21998 = zext i32 %21997 to i64
  store i64 %21998, i64* %576, align 8
  %21999 = icmp ugt i32 %21996, -15
  %22000 = zext i1 %21999 to i8
  store i8 %22000, i8* %14, align 1
  %22001 = and i32 %21997, 255
  %22002 = tail call i32 @llvm.ctpop.i32(i32 %22001)
  %22003 = trunc i32 %22002 to i8
  %22004 = and i8 %22003, 1
  %22005 = xor i8 %22004, 1
  store i8 %22005, i8* %21, align 1
  %22006 = xor i32 %21997, %21996
  %22007 = lshr i32 %22006, 4
  %22008 = trunc i32 %22007 to i8
  %22009 = and i8 %22008, 1
  store i8 %22009, i8* %27, align 1
  %22010 = icmp eq i32 %21997, 0
  %22011 = zext i1 %22010 to i8
  store i8 %22011, i8* %30, align 1
  %22012 = lshr i32 %21997, 31
  %22013 = trunc i32 %22012 to i8
  store i8 %22013, i8* %33, align 1
  %22014 = lshr i32 %21996, 31
  %22015 = xor i32 %22012, %22014
  %22016 = add nuw nsw i32 %22015, %22012
  %22017 = icmp eq i32 %22016, 2
  %22018 = zext i1 %22017 to i8
  store i8 %22018, i8* %39, align 1
  %22019 = sext i32 %21997 to i64
  store i64 %22019, i64* %RSI.i11312, align 8
  %22020 = shl nsw i64 %22019, 1
  %22021 = add i64 %21992, %22020
  %22022 = add i64 %21930, 41
  store i64 %22022, i64* %3, align 8
  %22023 = inttoptr i64 %22021 to i16*
  %22024 = load i16, i16* %22023, align 2
  %22025 = zext i16 %22024 to i64
  store i64 %22025, i64* %576, align 8
  %22026 = zext i16 %22024 to i64
  store i64 %22026, i64* %RCX.i11580, align 8
  %22027 = shl nuw nsw i64 %22026, 2
  %22028 = add i64 %21963, %22027
  %22029 = add i64 %21930, 46
  store i64 %22029, i64* %3, align 8
  %22030 = inttoptr i64 %22028 to i32*
  %22031 = load i32, i32* %22030, align 4
  %22032 = add i32 %22031, 1
  %22033 = zext i32 %22032 to i64
  store i64 %22033, i64* %576, align 8
  %22034 = icmp eq i32 %22031, -1
  %22035 = icmp eq i32 %22032, 0
  %22036 = or i1 %22034, %22035
  %22037 = zext i1 %22036 to i8
  store i8 %22037, i8* %14, align 1
  %22038 = and i32 %22032, 255
  %22039 = tail call i32 @llvm.ctpop.i32(i32 %22038)
  %22040 = trunc i32 %22039 to i8
  %22041 = and i8 %22040, 1
  %22042 = xor i8 %22041, 1
  store i8 %22042, i8* %21, align 1
  %22043 = xor i32 %22032, %22031
  %22044 = lshr i32 %22043, 4
  %22045 = trunc i32 %22044 to i8
  %22046 = and i8 %22045, 1
  store i8 %22046, i8* %27, align 1
  %22047 = zext i1 %22035 to i8
  store i8 %22047, i8* %30, align 1
  %22048 = lshr i32 %22032, 31
  %22049 = trunc i32 %22048 to i8
  store i8 %22049, i8* %33, align 1
  %22050 = lshr i32 %22031, 31
  %22051 = xor i32 %22048, %22050
  %22052 = add nuw nsw i32 %22051, %22048
  %22053 = icmp eq i32 %22052, 2
  %22054 = zext i1 %22053 to i8
  store i8 %22054, i8* %39, align 1
  %22055 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22056 = add i64 %22027, %22055
  %22057 = add i64 %21930, 52
  store i64 %22057, i64* %3, align 8
  %22058 = inttoptr i64 %22056 to i32*
  store i32 %22032, i32* %22058, align 4
  %22059 = load i64, i64* %RBP.i, align 8
  %22060 = add i64 %22059, -8
  %22061 = load i64, i64* %3, align 8
  %22062 = add i64 %22061, 4
  store i64 %22062, i64* %3, align 8
  %22063 = inttoptr i64 %22060 to i64*
  %22064 = load i64, i64* %22063, align 8
  %22065 = add i64 %22064, 45448
  store i64 %22065, i64* %RAX.i11582.pre-phi, align 8
  %22066 = icmp ugt i64 %22064, -45449
  %22067 = zext i1 %22066 to i8
  store i8 %22067, i8* %14, align 1
  %22068 = trunc i64 %22065 to i32
  %22069 = and i32 %22068, 255
  %22070 = tail call i32 @llvm.ctpop.i32(i32 %22069)
  %22071 = trunc i32 %22070 to i8
  %22072 = and i8 %22071, 1
  %22073 = xor i8 %22072, 1
  store i8 %22073, i8* %21, align 1
  %22074 = xor i64 %22065, %22064
  %22075 = lshr i64 %22074, 4
  %22076 = trunc i64 %22075 to i8
  %22077 = and i8 %22076, 1
  store i8 %22077, i8* %27, align 1
  %22078 = icmp eq i64 %22065, 0
  %22079 = zext i1 %22078 to i8
  store i8 %22079, i8* %30, align 1
  %22080 = lshr i64 %22065, 63
  %22081 = trunc i64 %22080 to i8
  store i8 %22081, i8* %33, align 1
  %22082 = lshr i64 %22064, 63
  %22083 = xor i64 %22080, %22082
  %22084 = add nuw nsw i64 %22083, %22080
  %22085 = icmp eq i64 %22084, 2
  %22086 = zext i1 %22085 to i8
  store i8 %22086, i8* %39, align 1
  %22087 = add i64 %22059, -40
  %22088 = add i64 %22061, 14
  store i64 %22088, i64* %3, align 8
  %22089 = inttoptr i64 %22087 to i32*
  %22090 = load i32, i32* %22089, align 4
  %22091 = sext i32 %22090 to i64
  %22092 = mul nsw i64 %22091, 1032
  store i64 %22092, i64* %RCX.i11580, align 8
  %22093 = lshr i64 %22092, 63
  %22094 = add i64 %22092, %22065
  store i64 %22094, i64* %RAX.i11582.pre-phi, align 8
  %22095 = icmp ult i64 %22094, %22065
  %22096 = icmp ult i64 %22094, %22092
  %22097 = or i1 %22095, %22096
  %22098 = zext i1 %22097 to i8
  store i8 %22098, i8* %14, align 1
  %22099 = trunc i64 %22094 to i32
  %22100 = and i32 %22099, 255
  %22101 = tail call i32 @llvm.ctpop.i32(i32 %22100)
  %22102 = trunc i32 %22101 to i8
  %22103 = and i8 %22102, 1
  %22104 = xor i8 %22103, 1
  store i8 %22104, i8* %21, align 1
  %22105 = xor i64 %22092, %22065
  %22106 = xor i64 %22105, %22094
  %22107 = lshr i64 %22106, 4
  %22108 = trunc i64 %22107 to i8
  %22109 = and i8 %22108, 1
  store i8 %22109, i8* %27, align 1
  %22110 = icmp eq i64 %22094, 0
  %22111 = zext i1 %22110 to i8
  store i8 %22111, i8* %30, align 1
  %22112 = lshr i64 %22094, 63
  %22113 = trunc i64 %22112 to i8
  store i8 %22113, i8* %33, align 1
  %22114 = xor i64 %22112, %22080
  %22115 = xor i64 %22112, %22093
  %22116 = add nuw nsw i64 %22114, %22115
  %22117 = icmp eq i64 %22116, 2
  %22118 = zext i1 %22117 to i8
  store i8 %22118, i8* %39, align 1
  %22119 = load i64, i64* %RBP.i, align 8
  %22120 = add i64 %22119, -120
  %22121 = add i64 %22061, 28
  store i64 %22121, i64* %3, align 8
  %22122 = inttoptr i64 %22120 to i64*
  %22123 = load i64, i64* %22122, align 8
  store i64 %22123, i64* %RCX.i11580, align 8
  %22124 = add i64 %22119, -28
  %22125 = add i64 %22061, 31
  store i64 %22125, i64* %3, align 8
  %22126 = inttoptr i64 %22124 to i32*
  %22127 = load i32, i32* %22126, align 4
  %22128 = add i32 %22127, 15
  %22129 = zext i32 %22128 to i64
  store i64 %22129, i64* %576, align 8
  %22130 = icmp ugt i32 %22127, -16
  %22131 = zext i1 %22130 to i8
  store i8 %22131, i8* %14, align 1
  %22132 = and i32 %22128, 255
  %22133 = tail call i32 @llvm.ctpop.i32(i32 %22132)
  %22134 = trunc i32 %22133 to i8
  %22135 = and i8 %22134, 1
  %22136 = xor i8 %22135, 1
  store i8 %22136, i8* %21, align 1
  %22137 = xor i32 %22128, %22127
  %22138 = lshr i32 %22137, 4
  %22139 = trunc i32 %22138 to i8
  %22140 = and i8 %22139, 1
  store i8 %22140, i8* %27, align 1
  %22141 = icmp eq i32 %22128, 0
  %22142 = zext i1 %22141 to i8
  store i8 %22142, i8* %30, align 1
  %22143 = lshr i32 %22128, 31
  %22144 = trunc i32 %22143 to i8
  store i8 %22144, i8* %33, align 1
  %22145 = lshr i32 %22127, 31
  %22146 = xor i32 %22143, %22145
  %22147 = add nuw nsw i32 %22146, %22143
  %22148 = icmp eq i32 %22147, 2
  %22149 = zext i1 %22148 to i8
  store i8 %22149, i8* %39, align 1
  %22150 = sext i32 %22128 to i64
  store i64 %22150, i64* %RSI.i11312, align 8
  %22151 = shl nsw i64 %22150, 1
  %22152 = add i64 %22123, %22151
  %22153 = add i64 %22061, 41
  store i64 %22153, i64* %3, align 8
  %22154 = inttoptr i64 %22152 to i16*
  %22155 = load i16, i16* %22154, align 2
  %22156 = zext i16 %22155 to i64
  store i64 %22156, i64* %576, align 8
  %22157 = zext i16 %22155 to i64
  store i64 %22157, i64* %RCX.i11580, align 8
  %22158 = shl nuw nsw i64 %22157, 2
  %22159 = add i64 %22094, %22158
  %22160 = add i64 %22061, 46
  store i64 %22160, i64* %3, align 8
  %22161 = inttoptr i64 %22159 to i32*
  %22162 = load i32, i32* %22161, align 4
  %22163 = add i32 %22162, 1
  %22164 = zext i32 %22163 to i64
  store i64 %22164, i64* %576, align 8
  %22165 = icmp eq i32 %22162, -1
  %22166 = icmp eq i32 %22163, 0
  %22167 = or i1 %22165, %22166
  %22168 = zext i1 %22167 to i8
  store i8 %22168, i8* %14, align 1
  %22169 = and i32 %22163, 255
  %22170 = tail call i32 @llvm.ctpop.i32(i32 %22169)
  %22171 = trunc i32 %22170 to i8
  %22172 = and i8 %22171, 1
  %22173 = xor i8 %22172, 1
  store i8 %22173, i8* %21, align 1
  %22174 = xor i32 %22163, %22162
  %22175 = lshr i32 %22174, 4
  %22176 = trunc i32 %22175 to i8
  %22177 = and i8 %22176, 1
  store i8 %22177, i8* %27, align 1
  %22178 = zext i1 %22166 to i8
  store i8 %22178, i8* %30, align 1
  %22179 = lshr i32 %22163, 31
  %22180 = trunc i32 %22179 to i8
  store i8 %22180, i8* %33, align 1
  %22181 = lshr i32 %22162, 31
  %22182 = xor i32 %22179, %22181
  %22183 = add nuw nsw i32 %22182, %22179
  %22184 = icmp eq i32 %22183, 2
  %22185 = zext i1 %22184 to i8
  store i8 %22185, i8* %39, align 1
  %22186 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22187 = add i64 %22158, %22186
  %22188 = add i64 %22061, 52
  store i64 %22188, i64* %3, align 8
  %22189 = inttoptr i64 %22187 to i32*
  store i32 %22163, i32* %22189, align 4
  %22190 = load i64, i64* %RBP.i, align 8
  %22191 = add i64 %22190, -8
  %22192 = load i64, i64* %3, align 8
  %22193 = add i64 %22192, 4
  store i64 %22193, i64* %3, align 8
  %22194 = inttoptr i64 %22191 to i64*
  %22195 = load i64, i64* %22194, align 8
  %22196 = add i64 %22195, 45448
  store i64 %22196, i64* %RAX.i11582.pre-phi, align 8
  %22197 = icmp ugt i64 %22195, -45449
  %22198 = zext i1 %22197 to i8
  store i8 %22198, i8* %14, align 1
  %22199 = trunc i64 %22196 to i32
  %22200 = and i32 %22199, 255
  %22201 = tail call i32 @llvm.ctpop.i32(i32 %22200)
  %22202 = trunc i32 %22201 to i8
  %22203 = and i8 %22202, 1
  %22204 = xor i8 %22203, 1
  store i8 %22204, i8* %21, align 1
  %22205 = xor i64 %22196, %22195
  %22206 = lshr i64 %22205, 4
  %22207 = trunc i64 %22206 to i8
  %22208 = and i8 %22207, 1
  store i8 %22208, i8* %27, align 1
  %22209 = icmp eq i64 %22196, 0
  %22210 = zext i1 %22209 to i8
  store i8 %22210, i8* %30, align 1
  %22211 = lshr i64 %22196, 63
  %22212 = trunc i64 %22211 to i8
  store i8 %22212, i8* %33, align 1
  %22213 = lshr i64 %22195, 63
  %22214 = xor i64 %22211, %22213
  %22215 = add nuw nsw i64 %22214, %22211
  %22216 = icmp eq i64 %22215, 2
  %22217 = zext i1 %22216 to i8
  store i8 %22217, i8* %39, align 1
  %22218 = add i64 %22190, -40
  %22219 = add i64 %22192, 14
  store i64 %22219, i64* %3, align 8
  %22220 = inttoptr i64 %22218 to i32*
  %22221 = load i32, i32* %22220, align 4
  %22222 = sext i32 %22221 to i64
  %22223 = mul nsw i64 %22222, 1032
  store i64 %22223, i64* %RCX.i11580, align 8
  %22224 = lshr i64 %22223, 63
  %22225 = add i64 %22223, %22196
  store i64 %22225, i64* %RAX.i11582.pre-phi, align 8
  %22226 = icmp ult i64 %22225, %22196
  %22227 = icmp ult i64 %22225, %22223
  %22228 = or i1 %22226, %22227
  %22229 = zext i1 %22228 to i8
  store i8 %22229, i8* %14, align 1
  %22230 = trunc i64 %22225 to i32
  %22231 = and i32 %22230, 255
  %22232 = tail call i32 @llvm.ctpop.i32(i32 %22231)
  %22233 = trunc i32 %22232 to i8
  %22234 = and i8 %22233, 1
  %22235 = xor i8 %22234, 1
  store i8 %22235, i8* %21, align 1
  %22236 = xor i64 %22223, %22196
  %22237 = xor i64 %22236, %22225
  %22238 = lshr i64 %22237, 4
  %22239 = trunc i64 %22238 to i8
  %22240 = and i8 %22239, 1
  store i8 %22240, i8* %27, align 1
  %22241 = icmp eq i64 %22225, 0
  %22242 = zext i1 %22241 to i8
  store i8 %22242, i8* %30, align 1
  %22243 = lshr i64 %22225, 63
  %22244 = trunc i64 %22243 to i8
  store i8 %22244, i8* %33, align 1
  %22245 = xor i64 %22243, %22211
  %22246 = xor i64 %22243, %22224
  %22247 = add nuw nsw i64 %22245, %22246
  %22248 = icmp eq i64 %22247, 2
  %22249 = zext i1 %22248 to i8
  store i8 %22249, i8* %39, align 1
  %22250 = load i64, i64* %RBP.i, align 8
  %22251 = add i64 %22250, -120
  %22252 = add i64 %22192, 28
  store i64 %22252, i64* %3, align 8
  %22253 = inttoptr i64 %22251 to i64*
  %22254 = load i64, i64* %22253, align 8
  store i64 %22254, i64* %RCX.i11580, align 8
  %22255 = add i64 %22250, -28
  %22256 = add i64 %22192, 31
  store i64 %22256, i64* %3, align 8
  %22257 = inttoptr i64 %22255 to i32*
  %22258 = load i32, i32* %22257, align 4
  %22259 = add i32 %22258, 16
  %22260 = zext i32 %22259 to i64
  store i64 %22260, i64* %576, align 8
  %22261 = icmp ugt i32 %22258, -17
  %22262 = zext i1 %22261 to i8
  store i8 %22262, i8* %14, align 1
  %22263 = and i32 %22259, 255
  %22264 = tail call i32 @llvm.ctpop.i32(i32 %22263)
  %22265 = trunc i32 %22264 to i8
  %22266 = and i8 %22265, 1
  %22267 = xor i8 %22266, 1
  store i8 %22267, i8* %21, align 1
  %22268 = xor i32 %22258, 16
  %22269 = xor i32 %22268, %22259
  %22270 = lshr i32 %22269, 4
  %22271 = trunc i32 %22270 to i8
  %22272 = and i8 %22271, 1
  store i8 %22272, i8* %27, align 1
  %22273 = icmp eq i32 %22259, 0
  %22274 = zext i1 %22273 to i8
  store i8 %22274, i8* %30, align 1
  %22275 = lshr i32 %22259, 31
  %22276 = trunc i32 %22275 to i8
  store i8 %22276, i8* %33, align 1
  %22277 = lshr i32 %22258, 31
  %22278 = xor i32 %22275, %22277
  %22279 = add nuw nsw i32 %22278, %22275
  %22280 = icmp eq i32 %22279, 2
  %22281 = zext i1 %22280 to i8
  store i8 %22281, i8* %39, align 1
  %22282 = sext i32 %22259 to i64
  store i64 %22282, i64* %RSI.i11312, align 8
  %22283 = shl nsw i64 %22282, 1
  %22284 = add i64 %22254, %22283
  %22285 = add i64 %22192, 41
  store i64 %22285, i64* %3, align 8
  %22286 = inttoptr i64 %22284 to i16*
  %22287 = load i16, i16* %22286, align 2
  %22288 = zext i16 %22287 to i64
  store i64 %22288, i64* %576, align 8
  %22289 = zext i16 %22287 to i64
  store i64 %22289, i64* %RCX.i11580, align 8
  %22290 = shl nuw nsw i64 %22289, 2
  %22291 = add i64 %22225, %22290
  %22292 = add i64 %22192, 46
  store i64 %22292, i64* %3, align 8
  %22293 = inttoptr i64 %22291 to i32*
  %22294 = load i32, i32* %22293, align 4
  %22295 = add i32 %22294, 1
  %22296 = zext i32 %22295 to i64
  store i64 %22296, i64* %576, align 8
  %22297 = icmp eq i32 %22294, -1
  %22298 = icmp eq i32 %22295, 0
  %22299 = or i1 %22297, %22298
  %22300 = zext i1 %22299 to i8
  store i8 %22300, i8* %14, align 1
  %22301 = and i32 %22295, 255
  %22302 = tail call i32 @llvm.ctpop.i32(i32 %22301)
  %22303 = trunc i32 %22302 to i8
  %22304 = and i8 %22303, 1
  %22305 = xor i8 %22304, 1
  store i8 %22305, i8* %21, align 1
  %22306 = xor i32 %22295, %22294
  %22307 = lshr i32 %22306, 4
  %22308 = trunc i32 %22307 to i8
  %22309 = and i8 %22308, 1
  store i8 %22309, i8* %27, align 1
  %22310 = zext i1 %22298 to i8
  store i8 %22310, i8* %30, align 1
  %22311 = lshr i32 %22295, 31
  %22312 = trunc i32 %22311 to i8
  store i8 %22312, i8* %33, align 1
  %22313 = lshr i32 %22294, 31
  %22314 = xor i32 %22311, %22313
  %22315 = add nuw nsw i32 %22314, %22311
  %22316 = icmp eq i32 %22315, 2
  %22317 = zext i1 %22316 to i8
  store i8 %22317, i8* %39, align 1
  %22318 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22319 = add i64 %22290, %22318
  %22320 = add i64 %22192, 52
  store i64 %22320, i64* %3, align 8
  %22321 = inttoptr i64 %22319 to i32*
  store i32 %22295, i32* %22321, align 4
  %22322 = load i64, i64* %RBP.i, align 8
  %22323 = add i64 %22322, -8
  %22324 = load i64, i64* %3, align 8
  %22325 = add i64 %22324, 4
  store i64 %22325, i64* %3, align 8
  %22326 = inttoptr i64 %22323 to i64*
  %22327 = load i64, i64* %22326, align 8
  %22328 = add i64 %22327, 45448
  store i64 %22328, i64* %RAX.i11582.pre-phi, align 8
  %22329 = icmp ugt i64 %22327, -45449
  %22330 = zext i1 %22329 to i8
  store i8 %22330, i8* %14, align 1
  %22331 = trunc i64 %22328 to i32
  %22332 = and i32 %22331, 255
  %22333 = tail call i32 @llvm.ctpop.i32(i32 %22332)
  %22334 = trunc i32 %22333 to i8
  %22335 = and i8 %22334, 1
  %22336 = xor i8 %22335, 1
  store i8 %22336, i8* %21, align 1
  %22337 = xor i64 %22328, %22327
  %22338 = lshr i64 %22337, 4
  %22339 = trunc i64 %22338 to i8
  %22340 = and i8 %22339, 1
  store i8 %22340, i8* %27, align 1
  %22341 = icmp eq i64 %22328, 0
  %22342 = zext i1 %22341 to i8
  store i8 %22342, i8* %30, align 1
  %22343 = lshr i64 %22328, 63
  %22344 = trunc i64 %22343 to i8
  store i8 %22344, i8* %33, align 1
  %22345 = lshr i64 %22327, 63
  %22346 = xor i64 %22343, %22345
  %22347 = add nuw nsw i64 %22346, %22343
  %22348 = icmp eq i64 %22347, 2
  %22349 = zext i1 %22348 to i8
  store i8 %22349, i8* %39, align 1
  %22350 = add i64 %22322, -40
  %22351 = add i64 %22324, 14
  store i64 %22351, i64* %3, align 8
  %22352 = inttoptr i64 %22350 to i32*
  %22353 = load i32, i32* %22352, align 4
  %22354 = sext i32 %22353 to i64
  %22355 = mul nsw i64 %22354, 1032
  store i64 %22355, i64* %RCX.i11580, align 8
  %22356 = lshr i64 %22355, 63
  %22357 = add i64 %22355, %22328
  store i64 %22357, i64* %RAX.i11582.pre-phi, align 8
  %22358 = icmp ult i64 %22357, %22328
  %22359 = icmp ult i64 %22357, %22355
  %22360 = or i1 %22358, %22359
  %22361 = zext i1 %22360 to i8
  store i8 %22361, i8* %14, align 1
  %22362 = trunc i64 %22357 to i32
  %22363 = and i32 %22362, 255
  %22364 = tail call i32 @llvm.ctpop.i32(i32 %22363)
  %22365 = trunc i32 %22364 to i8
  %22366 = and i8 %22365, 1
  %22367 = xor i8 %22366, 1
  store i8 %22367, i8* %21, align 1
  %22368 = xor i64 %22355, %22328
  %22369 = xor i64 %22368, %22357
  %22370 = lshr i64 %22369, 4
  %22371 = trunc i64 %22370 to i8
  %22372 = and i8 %22371, 1
  store i8 %22372, i8* %27, align 1
  %22373 = icmp eq i64 %22357, 0
  %22374 = zext i1 %22373 to i8
  store i8 %22374, i8* %30, align 1
  %22375 = lshr i64 %22357, 63
  %22376 = trunc i64 %22375 to i8
  store i8 %22376, i8* %33, align 1
  %22377 = xor i64 %22375, %22343
  %22378 = xor i64 %22375, %22356
  %22379 = add nuw nsw i64 %22377, %22378
  %22380 = icmp eq i64 %22379, 2
  %22381 = zext i1 %22380 to i8
  store i8 %22381, i8* %39, align 1
  %22382 = load i64, i64* %RBP.i, align 8
  %22383 = add i64 %22382, -120
  %22384 = add i64 %22324, 28
  store i64 %22384, i64* %3, align 8
  %22385 = inttoptr i64 %22383 to i64*
  %22386 = load i64, i64* %22385, align 8
  store i64 %22386, i64* %RCX.i11580, align 8
  %22387 = add i64 %22382, -28
  %22388 = add i64 %22324, 31
  store i64 %22388, i64* %3, align 8
  %22389 = inttoptr i64 %22387 to i32*
  %22390 = load i32, i32* %22389, align 4
  %22391 = add i32 %22390, 17
  %22392 = zext i32 %22391 to i64
  store i64 %22392, i64* %576, align 8
  %22393 = icmp ugt i32 %22390, -18
  %22394 = zext i1 %22393 to i8
  store i8 %22394, i8* %14, align 1
  %22395 = and i32 %22391, 255
  %22396 = tail call i32 @llvm.ctpop.i32(i32 %22395)
  %22397 = trunc i32 %22396 to i8
  %22398 = and i8 %22397, 1
  %22399 = xor i8 %22398, 1
  store i8 %22399, i8* %21, align 1
  %22400 = xor i32 %22390, 16
  %22401 = xor i32 %22400, %22391
  %22402 = lshr i32 %22401, 4
  %22403 = trunc i32 %22402 to i8
  %22404 = and i8 %22403, 1
  store i8 %22404, i8* %27, align 1
  %22405 = icmp eq i32 %22391, 0
  %22406 = zext i1 %22405 to i8
  store i8 %22406, i8* %30, align 1
  %22407 = lshr i32 %22391, 31
  %22408 = trunc i32 %22407 to i8
  store i8 %22408, i8* %33, align 1
  %22409 = lshr i32 %22390, 31
  %22410 = xor i32 %22407, %22409
  %22411 = add nuw nsw i32 %22410, %22407
  %22412 = icmp eq i32 %22411, 2
  %22413 = zext i1 %22412 to i8
  store i8 %22413, i8* %39, align 1
  %22414 = sext i32 %22391 to i64
  store i64 %22414, i64* %RSI.i11312, align 8
  %22415 = shl nsw i64 %22414, 1
  %22416 = add i64 %22386, %22415
  %22417 = add i64 %22324, 41
  store i64 %22417, i64* %3, align 8
  %22418 = inttoptr i64 %22416 to i16*
  %22419 = load i16, i16* %22418, align 2
  %22420 = zext i16 %22419 to i64
  store i64 %22420, i64* %576, align 8
  %22421 = zext i16 %22419 to i64
  store i64 %22421, i64* %RCX.i11580, align 8
  %22422 = shl nuw nsw i64 %22421, 2
  %22423 = add i64 %22357, %22422
  %22424 = add i64 %22324, 46
  store i64 %22424, i64* %3, align 8
  %22425 = inttoptr i64 %22423 to i32*
  %22426 = load i32, i32* %22425, align 4
  %22427 = add i32 %22426, 1
  %22428 = zext i32 %22427 to i64
  store i64 %22428, i64* %576, align 8
  %22429 = icmp eq i32 %22426, -1
  %22430 = icmp eq i32 %22427, 0
  %22431 = or i1 %22429, %22430
  %22432 = zext i1 %22431 to i8
  store i8 %22432, i8* %14, align 1
  %22433 = and i32 %22427, 255
  %22434 = tail call i32 @llvm.ctpop.i32(i32 %22433)
  %22435 = trunc i32 %22434 to i8
  %22436 = and i8 %22435, 1
  %22437 = xor i8 %22436, 1
  store i8 %22437, i8* %21, align 1
  %22438 = xor i32 %22427, %22426
  %22439 = lshr i32 %22438, 4
  %22440 = trunc i32 %22439 to i8
  %22441 = and i8 %22440, 1
  store i8 %22441, i8* %27, align 1
  %22442 = zext i1 %22430 to i8
  store i8 %22442, i8* %30, align 1
  %22443 = lshr i32 %22427, 31
  %22444 = trunc i32 %22443 to i8
  store i8 %22444, i8* %33, align 1
  %22445 = lshr i32 %22426, 31
  %22446 = xor i32 %22443, %22445
  %22447 = add nuw nsw i32 %22446, %22443
  %22448 = icmp eq i32 %22447, 2
  %22449 = zext i1 %22448 to i8
  store i8 %22449, i8* %39, align 1
  %22450 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22451 = add i64 %22422, %22450
  %22452 = add i64 %22324, 52
  store i64 %22452, i64* %3, align 8
  %22453 = inttoptr i64 %22451 to i32*
  store i32 %22427, i32* %22453, align 4
  %22454 = load i64, i64* %RBP.i, align 8
  %22455 = add i64 %22454, -8
  %22456 = load i64, i64* %3, align 8
  %22457 = add i64 %22456, 4
  store i64 %22457, i64* %3, align 8
  %22458 = inttoptr i64 %22455 to i64*
  %22459 = load i64, i64* %22458, align 8
  %22460 = add i64 %22459, 45448
  store i64 %22460, i64* %RAX.i11582.pre-phi, align 8
  %22461 = icmp ugt i64 %22459, -45449
  %22462 = zext i1 %22461 to i8
  store i8 %22462, i8* %14, align 1
  %22463 = trunc i64 %22460 to i32
  %22464 = and i32 %22463, 255
  %22465 = tail call i32 @llvm.ctpop.i32(i32 %22464)
  %22466 = trunc i32 %22465 to i8
  %22467 = and i8 %22466, 1
  %22468 = xor i8 %22467, 1
  store i8 %22468, i8* %21, align 1
  %22469 = xor i64 %22460, %22459
  %22470 = lshr i64 %22469, 4
  %22471 = trunc i64 %22470 to i8
  %22472 = and i8 %22471, 1
  store i8 %22472, i8* %27, align 1
  %22473 = icmp eq i64 %22460, 0
  %22474 = zext i1 %22473 to i8
  store i8 %22474, i8* %30, align 1
  %22475 = lshr i64 %22460, 63
  %22476 = trunc i64 %22475 to i8
  store i8 %22476, i8* %33, align 1
  %22477 = lshr i64 %22459, 63
  %22478 = xor i64 %22475, %22477
  %22479 = add nuw nsw i64 %22478, %22475
  %22480 = icmp eq i64 %22479, 2
  %22481 = zext i1 %22480 to i8
  store i8 %22481, i8* %39, align 1
  %22482 = add i64 %22454, -40
  %22483 = add i64 %22456, 14
  store i64 %22483, i64* %3, align 8
  %22484 = inttoptr i64 %22482 to i32*
  %22485 = load i32, i32* %22484, align 4
  %22486 = sext i32 %22485 to i64
  %22487 = mul nsw i64 %22486, 1032
  store i64 %22487, i64* %RCX.i11580, align 8
  %22488 = lshr i64 %22487, 63
  %22489 = add i64 %22487, %22460
  store i64 %22489, i64* %RAX.i11582.pre-phi, align 8
  %22490 = icmp ult i64 %22489, %22460
  %22491 = icmp ult i64 %22489, %22487
  %22492 = or i1 %22490, %22491
  %22493 = zext i1 %22492 to i8
  store i8 %22493, i8* %14, align 1
  %22494 = trunc i64 %22489 to i32
  %22495 = and i32 %22494, 255
  %22496 = tail call i32 @llvm.ctpop.i32(i32 %22495)
  %22497 = trunc i32 %22496 to i8
  %22498 = and i8 %22497, 1
  %22499 = xor i8 %22498, 1
  store i8 %22499, i8* %21, align 1
  %22500 = xor i64 %22487, %22460
  %22501 = xor i64 %22500, %22489
  %22502 = lshr i64 %22501, 4
  %22503 = trunc i64 %22502 to i8
  %22504 = and i8 %22503, 1
  store i8 %22504, i8* %27, align 1
  %22505 = icmp eq i64 %22489, 0
  %22506 = zext i1 %22505 to i8
  store i8 %22506, i8* %30, align 1
  %22507 = lshr i64 %22489, 63
  %22508 = trunc i64 %22507 to i8
  store i8 %22508, i8* %33, align 1
  %22509 = xor i64 %22507, %22475
  %22510 = xor i64 %22507, %22488
  %22511 = add nuw nsw i64 %22509, %22510
  %22512 = icmp eq i64 %22511, 2
  %22513 = zext i1 %22512 to i8
  store i8 %22513, i8* %39, align 1
  %22514 = load i64, i64* %RBP.i, align 8
  %22515 = add i64 %22514, -120
  %22516 = add i64 %22456, 28
  store i64 %22516, i64* %3, align 8
  %22517 = inttoptr i64 %22515 to i64*
  %22518 = load i64, i64* %22517, align 8
  store i64 %22518, i64* %RCX.i11580, align 8
  %22519 = add i64 %22514, -28
  %22520 = add i64 %22456, 31
  store i64 %22520, i64* %3, align 8
  %22521 = inttoptr i64 %22519 to i32*
  %22522 = load i32, i32* %22521, align 4
  %22523 = add i32 %22522, 18
  %22524 = zext i32 %22523 to i64
  store i64 %22524, i64* %576, align 8
  %22525 = icmp ugt i32 %22522, -19
  %22526 = zext i1 %22525 to i8
  store i8 %22526, i8* %14, align 1
  %22527 = and i32 %22523, 255
  %22528 = tail call i32 @llvm.ctpop.i32(i32 %22527)
  %22529 = trunc i32 %22528 to i8
  %22530 = and i8 %22529, 1
  %22531 = xor i8 %22530, 1
  store i8 %22531, i8* %21, align 1
  %22532 = xor i32 %22522, 16
  %22533 = xor i32 %22532, %22523
  %22534 = lshr i32 %22533, 4
  %22535 = trunc i32 %22534 to i8
  %22536 = and i8 %22535, 1
  store i8 %22536, i8* %27, align 1
  %22537 = icmp eq i32 %22523, 0
  %22538 = zext i1 %22537 to i8
  store i8 %22538, i8* %30, align 1
  %22539 = lshr i32 %22523, 31
  %22540 = trunc i32 %22539 to i8
  store i8 %22540, i8* %33, align 1
  %22541 = lshr i32 %22522, 31
  %22542 = xor i32 %22539, %22541
  %22543 = add nuw nsw i32 %22542, %22539
  %22544 = icmp eq i32 %22543, 2
  %22545 = zext i1 %22544 to i8
  store i8 %22545, i8* %39, align 1
  %22546 = sext i32 %22523 to i64
  store i64 %22546, i64* %RSI.i11312, align 8
  %22547 = shl nsw i64 %22546, 1
  %22548 = add i64 %22518, %22547
  %22549 = add i64 %22456, 41
  store i64 %22549, i64* %3, align 8
  %22550 = inttoptr i64 %22548 to i16*
  %22551 = load i16, i16* %22550, align 2
  %22552 = zext i16 %22551 to i64
  store i64 %22552, i64* %576, align 8
  %22553 = zext i16 %22551 to i64
  store i64 %22553, i64* %RCX.i11580, align 8
  %22554 = shl nuw nsw i64 %22553, 2
  %22555 = add i64 %22489, %22554
  %22556 = add i64 %22456, 46
  store i64 %22556, i64* %3, align 8
  %22557 = inttoptr i64 %22555 to i32*
  %22558 = load i32, i32* %22557, align 4
  %22559 = add i32 %22558, 1
  %22560 = zext i32 %22559 to i64
  store i64 %22560, i64* %576, align 8
  %22561 = icmp eq i32 %22558, -1
  %22562 = icmp eq i32 %22559, 0
  %22563 = or i1 %22561, %22562
  %22564 = zext i1 %22563 to i8
  store i8 %22564, i8* %14, align 1
  %22565 = and i32 %22559, 255
  %22566 = tail call i32 @llvm.ctpop.i32(i32 %22565)
  %22567 = trunc i32 %22566 to i8
  %22568 = and i8 %22567, 1
  %22569 = xor i8 %22568, 1
  store i8 %22569, i8* %21, align 1
  %22570 = xor i32 %22559, %22558
  %22571 = lshr i32 %22570, 4
  %22572 = trunc i32 %22571 to i8
  %22573 = and i8 %22572, 1
  store i8 %22573, i8* %27, align 1
  %22574 = zext i1 %22562 to i8
  store i8 %22574, i8* %30, align 1
  %22575 = lshr i32 %22559, 31
  %22576 = trunc i32 %22575 to i8
  store i8 %22576, i8* %33, align 1
  %22577 = lshr i32 %22558, 31
  %22578 = xor i32 %22575, %22577
  %22579 = add nuw nsw i32 %22578, %22575
  %22580 = icmp eq i32 %22579, 2
  %22581 = zext i1 %22580 to i8
  store i8 %22581, i8* %39, align 1
  %22582 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22583 = add i64 %22554, %22582
  %22584 = add i64 %22456, 52
  store i64 %22584, i64* %3, align 8
  %22585 = inttoptr i64 %22583 to i32*
  store i32 %22559, i32* %22585, align 4
  %22586 = load i64, i64* %RBP.i, align 8
  %22587 = add i64 %22586, -8
  %22588 = load i64, i64* %3, align 8
  %22589 = add i64 %22588, 4
  store i64 %22589, i64* %3, align 8
  %22590 = inttoptr i64 %22587 to i64*
  %22591 = load i64, i64* %22590, align 8
  %22592 = add i64 %22591, 45448
  store i64 %22592, i64* %RAX.i11582.pre-phi, align 8
  %22593 = icmp ugt i64 %22591, -45449
  %22594 = zext i1 %22593 to i8
  store i8 %22594, i8* %14, align 1
  %22595 = trunc i64 %22592 to i32
  %22596 = and i32 %22595, 255
  %22597 = tail call i32 @llvm.ctpop.i32(i32 %22596)
  %22598 = trunc i32 %22597 to i8
  %22599 = and i8 %22598, 1
  %22600 = xor i8 %22599, 1
  store i8 %22600, i8* %21, align 1
  %22601 = xor i64 %22592, %22591
  %22602 = lshr i64 %22601, 4
  %22603 = trunc i64 %22602 to i8
  %22604 = and i8 %22603, 1
  store i8 %22604, i8* %27, align 1
  %22605 = icmp eq i64 %22592, 0
  %22606 = zext i1 %22605 to i8
  store i8 %22606, i8* %30, align 1
  %22607 = lshr i64 %22592, 63
  %22608 = trunc i64 %22607 to i8
  store i8 %22608, i8* %33, align 1
  %22609 = lshr i64 %22591, 63
  %22610 = xor i64 %22607, %22609
  %22611 = add nuw nsw i64 %22610, %22607
  %22612 = icmp eq i64 %22611, 2
  %22613 = zext i1 %22612 to i8
  store i8 %22613, i8* %39, align 1
  %22614 = add i64 %22586, -40
  %22615 = add i64 %22588, 14
  store i64 %22615, i64* %3, align 8
  %22616 = inttoptr i64 %22614 to i32*
  %22617 = load i32, i32* %22616, align 4
  %22618 = sext i32 %22617 to i64
  %22619 = mul nsw i64 %22618, 1032
  store i64 %22619, i64* %RCX.i11580, align 8
  %22620 = lshr i64 %22619, 63
  %22621 = add i64 %22619, %22592
  store i64 %22621, i64* %RAX.i11582.pre-phi, align 8
  %22622 = icmp ult i64 %22621, %22592
  %22623 = icmp ult i64 %22621, %22619
  %22624 = or i1 %22622, %22623
  %22625 = zext i1 %22624 to i8
  store i8 %22625, i8* %14, align 1
  %22626 = trunc i64 %22621 to i32
  %22627 = and i32 %22626, 255
  %22628 = tail call i32 @llvm.ctpop.i32(i32 %22627)
  %22629 = trunc i32 %22628 to i8
  %22630 = and i8 %22629, 1
  %22631 = xor i8 %22630, 1
  store i8 %22631, i8* %21, align 1
  %22632 = xor i64 %22619, %22592
  %22633 = xor i64 %22632, %22621
  %22634 = lshr i64 %22633, 4
  %22635 = trunc i64 %22634 to i8
  %22636 = and i8 %22635, 1
  store i8 %22636, i8* %27, align 1
  %22637 = icmp eq i64 %22621, 0
  %22638 = zext i1 %22637 to i8
  store i8 %22638, i8* %30, align 1
  %22639 = lshr i64 %22621, 63
  %22640 = trunc i64 %22639 to i8
  store i8 %22640, i8* %33, align 1
  %22641 = xor i64 %22639, %22607
  %22642 = xor i64 %22639, %22620
  %22643 = add nuw nsw i64 %22641, %22642
  %22644 = icmp eq i64 %22643, 2
  %22645 = zext i1 %22644 to i8
  store i8 %22645, i8* %39, align 1
  %22646 = load i64, i64* %RBP.i, align 8
  %22647 = add i64 %22646, -120
  %22648 = add i64 %22588, 28
  store i64 %22648, i64* %3, align 8
  %22649 = inttoptr i64 %22647 to i64*
  %22650 = load i64, i64* %22649, align 8
  store i64 %22650, i64* %RCX.i11580, align 8
  %22651 = add i64 %22646, -28
  %22652 = add i64 %22588, 31
  store i64 %22652, i64* %3, align 8
  %22653 = inttoptr i64 %22651 to i32*
  %22654 = load i32, i32* %22653, align 4
  %22655 = add i32 %22654, 19
  %22656 = zext i32 %22655 to i64
  store i64 %22656, i64* %576, align 8
  %22657 = icmp ugt i32 %22654, -20
  %22658 = zext i1 %22657 to i8
  store i8 %22658, i8* %14, align 1
  %22659 = and i32 %22655, 255
  %22660 = tail call i32 @llvm.ctpop.i32(i32 %22659)
  %22661 = trunc i32 %22660 to i8
  %22662 = and i8 %22661, 1
  %22663 = xor i8 %22662, 1
  store i8 %22663, i8* %21, align 1
  %22664 = xor i32 %22654, 16
  %22665 = xor i32 %22664, %22655
  %22666 = lshr i32 %22665, 4
  %22667 = trunc i32 %22666 to i8
  %22668 = and i8 %22667, 1
  store i8 %22668, i8* %27, align 1
  %22669 = icmp eq i32 %22655, 0
  %22670 = zext i1 %22669 to i8
  store i8 %22670, i8* %30, align 1
  %22671 = lshr i32 %22655, 31
  %22672 = trunc i32 %22671 to i8
  store i8 %22672, i8* %33, align 1
  %22673 = lshr i32 %22654, 31
  %22674 = xor i32 %22671, %22673
  %22675 = add nuw nsw i32 %22674, %22671
  %22676 = icmp eq i32 %22675, 2
  %22677 = zext i1 %22676 to i8
  store i8 %22677, i8* %39, align 1
  %22678 = sext i32 %22655 to i64
  store i64 %22678, i64* %RSI.i11312, align 8
  %22679 = shl nsw i64 %22678, 1
  %22680 = add i64 %22650, %22679
  %22681 = add i64 %22588, 41
  store i64 %22681, i64* %3, align 8
  %22682 = inttoptr i64 %22680 to i16*
  %22683 = load i16, i16* %22682, align 2
  %22684 = zext i16 %22683 to i64
  store i64 %22684, i64* %576, align 8
  %22685 = zext i16 %22683 to i64
  store i64 %22685, i64* %RCX.i11580, align 8
  %22686 = shl nuw nsw i64 %22685, 2
  %22687 = add i64 %22621, %22686
  %22688 = add i64 %22588, 46
  store i64 %22688, i64* %3, align 8
  %22689 = inttoptr i64 %22687 to i32*
  %22690 = load i32, i32* %22689, align 4
  %22691 = add i32 %22690, 1
  %22692 = zext i32 %22691 to i64
  store i64 %22692, i64* %576, align 8
  %22693 = icmp eq i32 %22690, -1
  %22694 = icmp eq i32 %22691, 0
  %22695 = or i1 %22693, %22694
  %22696 = zext i1 %22695 to i8
  store i8 %22696, i8* %14, align 1
  %22697 = and i32 %22691, 255
  %22698 = tail call i32 @llvm.ctpop.i32(i32 %22697)
  %22699 = trunc i32 %22698 to i8
  %22700 = and i8 %22699, 1
  %22701 = xor i8 %22700, 1
  store i8 %22701, i8* %21, align 1
  %22702 = xor i32 %22691, %22690
  %22703 = lshr i32 %22702, 4
  %22704 = trunc i32 %22703 to i8
  %22705 = and i8 %22704, 1
  store i8 %22705, i8* %27, align 1
  %22706 = zext i1 %22694 to i8
  store i8 %22706, i8* %30, align 1
  %22707 = lshr i32 %22691, 31
  %22708 = trunc i32 %22707 to i8
  store i8 %22708, i8* %33, align 1
  %22709 = lshr i32 %22690, 31
  %22710 = xor i32 %22707, %22709
  %22711 = add nuw nsw i32 %22710, %22707
  %22712 = icmp eq i32 %22711, 2
  %22713 = zext i1 %22712 to i8
  store i8 %22713, i8* %39, align 1
  %22714 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22715 = add i64 %22686, %22714
  %22716 = add i64 %22588, 52
  store i64 %22716, i64* %3, align 8
  %22717 = inttoptr i64 %22715 to i32*
  store i32 %22691, i32* %22717, align 4
  %22718 = load i64, i64* %RBP.i, align 8
  %22719 = add i64 %22718, -8
  %22720 = load i64, i64* %3, align 8
  %22721 = add i64 %22720, 4
  store i64 %22721, i64* %3, align 8
  %22722 = inttoptr i64 %22719 to i64*
  %22723 = load i64, i64* %22722, align 8
  %22724 = add i64 %22723, 45448
  store i64 %22724, i64* %RAX.i11582.pre-phi, align 8
  %22725 = icmp ugt i64 %22723, -45449
  %22726 = zext i1 %22725 to i8
  store i8 %22726, i8* %14, align 1
  %22727 = trunc i64 %22724 to i32
  %22728 = and i32 %22727, 255
  %22729 = tail call i32 @llvm.ctpop.i32(i32 %22728)
  %22730 = trunc i32 %22729 to i8
  %22731 = and i8 %22730, 1
  %22732 = xor i8 %22731, 1
  store i8 %22732, i8* %21, align 1
  %22733 = xor i64 %22724, %22723
  %22734 = lshr i64 %22733, 4
  %22735 = trunc i64 %22734 to i8
  %22736 = and i8 %22735, 1
  store i8 %22736, i8* %27, align 1
  %22737 = icmp eq i64 %22724, 0
  %22738 = zext i1 %22737 to i8
  store i8 %22738, i8* %30, align 1
  %22739 = lshr i64 %22724, 63
  %22740 = trunc i64 %22739 to i8
  store i8 %22740, i8* %33, align 1
  %22741 = lshr i64 %22723, 63
  %22742 = xor i64 %22739, %22741
  %22743 = add nuw nsw i64 %22742, %22739
  %22744 = icmp eq i64 %22743, 2
  %22745 = zext i1 %22744 to i8
  store i8 %22745, i8* %39, align 1
  %22746 = add i64 %22718, -40
  %22747 = add i64 %22720, 14
  store i64 %22747, i64* %3, align 8
  %22748 = inttoptr i64 %22746 to i32*
  %22749 = load i32, i32* %22748, align 4
  %22750 = sext i32 %22749 to i64
  %22751 = mul nsw i64 %22750, 1032
  store i64 %22751, i64* %RCX.i11580, align 8
  %22752 = lshr i64 %22751, 63
  %22753 = add i64 %22751, %22724
  store i64 %22753, i64* %RAX.i11582.pre-phi, align 8
  %22754 = icmp ult i64 %22753, %22724
  %22755 = icmp ult i64 %22753, %22751
  %22756 = or i1 %22754, %22755
  %22757 = zext i1 %22756 to i8
  store i8 %22757, i8* %14, align 1
  %22758 = trunc i64 %22753 to i32
  %22759 = and i32 %22758, 255
  %22760 = tail call i32 @llvm.ctpop.i32(i32 %22759)
  %22761 = trunc i32 %22760 to i8
  %22762 = and i8 %22761, 1
  %22763 = xor i8 %22762, 1
  store i8 %22763, i8* %21, align 1
  %22764 = xor i64 %22751, %22724
  %22765 = xor i64 %22764, %22753
  %22766 = lshr i64 %22765, 4
  %22767 = trunc i64 %22766 to i8
  %22768 = and i8 %22767, 1
  store i8 %22768, i8* %27, align 1
  %22769 = icmp eq i64 %22753, 0
  %22770 = zext i1 %22769 to i8
  store i8 %22770, i8* %30, align 1
  %22771 = lshr i64 %22753, 63
  %22772 = trunc i64 %22771 to i8
  store i8 %22772, i8* %33, align 1
  %22773 = xor i64 %22771, %22739
  %22774 = xor i64 %22771, %22752
  %22775 = add nuw nsw i64 %22773, %22774
  %22776 = icmp eq i64 %22775, 2
  %22777 = zext i1 %22776 to i8
  store i8 %22777, i8* %39, align 1
  %22778 = load i64, i64* %RBP.i, align 8
  %22779 = add i64 %22778, -120
  %22780 = add i64 %22720, 28
  store i64 %22780, i64* %3, align 8
  %22781 = inttoptr i64 %22779 to i64*
  %22782 = load i64, i64* %22781, align 8
  store i64 %22782, i64* %RCX.i11580, align 8
  %22783 = add i64 %22778, -28
  %22784 = add i64 %22720, 31
  store i64 %22784, i64* %3, align 8
  %22785 = inttoptr i64 %22783 to i32*
  %22786 = load i32, i32* %22785, align 4
  %22787 = add i32 %22786, 20
  %22788 = zext i32 %22787 to i64
  store i64 %22788, i64* %576, align 8
  %22789 = icmp ugt i32 %22786, -21
  %22790 = zext i1 %22789 to i8
  store i8 %22790, i8* %14, align 1
  %22791 = and i32 %22787, 255
  %22792 = tail call i32 @llvm.ctpop.i32(i32 %22791)
  %22793 = trunc i32 %22792 to i8
  %22794 = and i8 %22793, 1
  %22795 = xor i8 %22794, 1
  store i8 %22795, i8* %21, align 1
  %22796 = xor i32 %22786, 16
  %22797 = xor i32 %22796, %22787
  %22798 = lshr i32 %22797, 4
  %22799 = trunc i32 %22798 to i8
  %22800 = and i8 %22799, 1
  store i8 %22800, i8* %27, align 1
  %22801 = icmp eq i32 %22787, 0
  %22802 = zext i1 %22801 to i8
  store i8 %22802, i8* %30, align 1
  %22803 = lshr i32 %22787, 31
  %22804 = trunc i32 %22803 to i8
  store i8 %22804, i8* %33, align 1
  %22805 = lshr i32 %22786, 31
  %22806 = xor i32 %22803, %22805
  %22807 = add nuw nsw i32 %22806, %22803
  %22808 = icmp eq i32 %22807, 2
  %22809 = zext i1 %22808 to i8
  store i8 %22809, i8* %39, align 1
  %22810 = sext i32 %22787 to i64
  store i64 %22810, i64* %RSI.i11312, align 8
  %22811 = shl nsw i64 %22810, 1
  %22812 = add i64 %22782, %22811
  %22813 = add i64 %22720, 41
  store i64 %22813, i64* %3, align 8
  %22814 = inttoptr i64 %22812 to i16*
  %22815 = load i16, i16* %22814, align 2
  %22816 = zext i16 %22815 to i64
  store i64 %22816, i64* %576, align 8
  %22817 = zext i16 %22815 to i64
  store i64 %22817, i64* %RCX.i11580, align 8
  %22818 = shl nuw nsw i64 %22817, 2
  %22819 = add i64 %22753, %22818
  %22820 = add i64 %22720, 46
  store i64 %22820, i64* %3, align 8
  %22821 = inttoptr i64 %22819 to i32*
  %22822 = load i32, i32* %22821, align 4
  %22823 = add i32 %22822, 1
  %22824 = zext i32 %22823 to i64
  store i64 %22824, i64* %576, align 8
  %22825 = icmp eq i32 %22822, -1
  %22826 = icmp eq i32 %22823, 0
  %22827 = or i1 %22825, %22826
  %22828 = zext i1 %22827 to i8
  store i8 %22828, i8* %14, align 1
  %22829 = and i32 %22823, 255
  %22830 = tail call i32 @llvm.ctpop.i32(i32 %22829)
  %22831 = trunc i32 %22830 to i8
  %22832 = and i8 %22831, 1
  %22833 = xor i8 %22832, 1
  store i8 %22833, i8* %21, align 1
  %22834 = xor i32 %22823, %22822
  %22835 = lshr i32 %22834, 4
  %22836 = trunc i32 %22835 to i8
  %22837 = and i8 %22836, 1
  store i8 %22837, i8* %27, align 1
  %22838 = zext i1 %22826 to i8
  store i8 %22838, i8* %30, align 1
  %22839 = lshr i32 %22823, 31
  %22840 = trunc i32 %22839 to i8
  store i8 %22840, i8* %33, align 1
  %22841 = lshr i32 %22822, 31
  %22842 = xor i32 %22839, %22841
  %22843 = add nuw nsw i32 %22842, %22839
  %22844 = icmp eq i32 %22843, 2
  %22845 = zext i1 %22844 to i8
  store i8 %22845, i8* %39, align 1
  %22846 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22847 = add i64 %22818, %22846
  %22848 = add i64 %22720, 52
  store i64 %22848, i64* %3, align 8
  %22849 = inttoptr i64 %22847 to i32*
  store i32 %22823, i32* %22849, align 4
  %22850 = load i64, i64* %RBP.i, align 8
  %22851 = add i64 %22850, -8
  %22852 = load i64, i64* %3, align 8
  %22853 = add i64 %22852, 4
  store i64 %22853, i64* %3, align 8
  %22854 = inttoptr i64 %22851 to i64*
  %22855 = load i64, i64* %22854, align 8
  %22856 = add i64 %22855, 45448
  store i64 %22856, i64* %RAX.i11582.pre-phi, align 8
  %22857 = icmp ugt i64 %22855, -45449
  %22858 = zext i1 %22857 to i8
  store i8 %22858, i8* %14, align 1
  %22859 = trunc i64 %22856 to i32
  %22860 = and i32 %22859, 255
  %22861 = tail call i32 @llvm.ctpop.i32(i32 %22860)
  %22862 = trunc i32 %22861 to i8
  %22863 = and i8 %22862, 1
  %22864 = xor i8 %22863, 1
  store i8 %22864, i8* %21, align 1
  %22865 = xor i64 %22856, %22855
  %22866 = lshr i64 %22865, 4
  %22867 = trunc i64 %22866 to i8
  %22868 = and i8 %22867, 1
  store i8 %22868, i8* %27, align 1
  %22869 = icmp eq i64 %22856, 0
  %22870 = zext i1 %22869 to i8
  store i8 %22870, i8* %30, align 1
  %22871 = lshr i64 %22856, 63
  %22872 = trunc i64 %22871 to i8
  store i8 %22872, i8* %33, align 1
  %22873 = lshr i64 %22855, 63
  %22874 = xor i64 %22871, %22873
  %22875 = add nuw nsw i64 %22874, %22871
  %22876 = icmp eq i64 %22875, 2
  %22877 = zext i1 %22876 to i8
  store i8 %22877, i8* %39, align 1
  %22878 = add i64 %22850, -40
  %22879 = add i64 %22852, 14
  store i64 %22879, i64* %3, align 8
  %22880 = inttoptr i64 %22878 to i32*
  %22881 = load i32, i32* %22880, align 4
  %22882 = sext i32 %22881 to i64
  %22883 = mul nsw i64 %22882, 1032
  store i64 %22883, i64* %RCX.i11580, align 8
  %22884 = lshr i64 %22883, 63
  %22885 = add i64 %22883, %22856
  store i64 %22885, i64* %RAX.i11582.pre-phi, align 8
  %22886 = icmp ult i64 %22885, %22856
  %22887 = icmp ult i64 %22885, %22883
  %22888 = or i1 %22886, %22887
  %22889 = zext i1 %22888 to i8
  store i8 %22889, i8* %14, align 1
  %22890 = trunc i64 %22885 to i32
  %22891 = and i32 %22890, 255
  %22892 = tail call i32 @llvm.ctpop.i32(i32 %22891)
  %22893 = trunc i32 %22892 to i8
  %22894 = and i8 %22893, 1
  %22895 = xor i8 %22894, 1
  store i8 %22895, i8* %21, align 1
  %22896 = xor i64 %22883, %22856
  %22897 = xor i64 %22896, %22885
  %22898 = lshr i64 %22897, 4
  %22899 = trunc i64 %22898 to i8
  %22900 = and i8 %22899, 1
  store i8 %22900, i8* %27, align 1
  %22901 = icmp eq i64 %22885, 0
  %22902 = zext i1 %22901 to i8
  store i8 %22902, i8* %30, align 1
  %22903 = lshr i64 %22885, 63
  %22904 = trunc i64 %22903 to i8
  store i8 %22904, i8* %33, align 1
  %22905 = xor i64 %22903, %22871
  %22906 = xor i64 %22903, %22884
  %22907 = add nuw nsw i64 %22905, %22906
  %22908 = icmp eq i64 %22907, 2
  %22909 = zext i1 %22908 to i8
  store i8 %22909, i8* %39, align 1
  %22910 = load i64, i64* %RBP.i, align 8
  %22911 = add i64 %22910, -120
  %22912 = add i64 %22852, 28
  store i64 %22912, i64* %3, align 8
  %22913 = inttoptr i64 %22911 to i64*
  %22914 = load i64, i64* %22913, align 8
  store i64 %22914, i64* %RCX.i11580, align 8
  %22915 = add i64 %22910, -28
  %22916 = add i64 %22852, 31
  store i64 %22916, i64* %3, align 8
  %22917 = inttoptr i64 %22915 to i32*
  %22918 = load i32, i32* %22917, align 4
  %22919 = add i32 %22918, 21
  %22920 = zext i32 %22919 to i64
  store i64 %22920, i64* %576, align 8
  %22921 = icmp ugt i32 %22918, -22
  %22922 = zext i1 %22921 to i8
  store i8 %22922, i8* %14, align 1
  %22923 = and i32 %22919, 255
  %22924 = tail call i32 @llvm.ctpop.i32(i32 %22923)
  %22925 = trunc i32 %22924 to i8
  %22926 = and i8 %22925, 1
  %22927 = xor i8 %22926, 1
  store i8 %22927, i8* %21, align 1
  %22928 = xor i32 %22918, 16
  %22929 = xor i32 %22928, %22919
  %22930 = lshr i32 %22929, 4
  %22931 = trunc i32 %22930 to i8
  %22932 = and i8 %22931, 1
  store i8 %22932, i8* %27, align 1
  %22933 = icmp eq i32 %22919, 0
  %22934 = zext i1 %22933 to i8
  store i8 %22934, i8* %30, align 1
  %22935 = lshr i32 %22919, 31
  %22936 = trunc i32 %22935 to i8
  store i8 %22936, i8* %33, align 1
  %22937 = lshr i32 %22918, 31
  %22938 = xor i32 %22935, %22937
  %22939 = add nuw nsw i32 %22938, %22935
  %22940 = icmp eq i32 %22939, 2
  %22941 = zext i1 %22940 to i8
  store i8 %22941, i8* %39, align 1
  %22942 = sext i32 %22919 to i64
  store i64 %22942, i64* %RSI.i11312, align 8
  %22943 = shl nsw i64 %22942, 1
  %22944 = add i64 %22914, %22943
  %22945 = add i64 %22852, 41
  store i64 %22945, i64* %3, align 8
  %22946 = inttoptr i64 %22944 to i16*
  %22947 = load i16, i16* %22946, align 2
  %22948 = zext i16 %22947 to i64
  store i64 %22948, i64* %576, align 8
  %22949 = zext i16 %22947 to i64
  store i64 %22949, i64* %RCX.i11580, align 8
  %22950 = shl nuw nsw i64 %22949, 2
  %22951 = add i64 %22885, %22950
  %22952 = add i64 %22852, 46
  store i64 %22952, i64* %3, align 8
  %22953 = inttoptr i64 %22951 to i32*
  %22954 = load i32, i32* %22953, align 4
  %22955 = add i32 %22954, 1
  %22956 = zext i32 %22955 to i64
  store i64 %22956, i64* %576, align 8
  %22957 = icmp eq i32 %22954, -1
  %22958 = icmp eq i32 %22955, 0
  %22959 = or i1 %22957, %22958
  %22960 = zext i1 %22959 to i8
  store i8 %22960, i8* %14, align 1
  %22961 = and i32 %22955, 255
  %22962 = tail call i32 @llvm.ctpop.i32(i32 %22961)
  %22963 = trunc i32 %22962 to i8
  %22964 = and i8 %22963, 1
  %22965 = xor i8 %22964, 1
  store i8 %22965, i8* %21, align 1
  %22966 = xor i32 %22955, %22954
  %22967 = lshr i32 %22966, 4
  %22968 = trunc i32 %22967 to i8
  %22969 = and i8 %22968, 1
  store i8 %22969, i8* %27, align 1
  %22970 = zext i1 %22958 to i8
  store i8 %22970, i8* %30, align 1
  %22971 = lshr i32 %22955, 31
  %22972 = trunc i32 %22971 to i8
  store i8 %22972, i8* %33, align 1
  %22973 = lshr i32 %22954, 31
  %22974 = xor i32 %22971, %22973
  %22975 = add nuw nsw i32 %22974, %22971
  %22976 = icmp eq i32 %22975, 2
  %22977 = zext i1 %22976 to i8
  store i8 %22977, i8* %39, align 1
  %22978 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %22979 = add i64 %22950, %22978
  %22980 = add i64 %22852, 52
  store i64 %22980, i64* %3, align 8
  %22981 = inttoptr i64 %22979 to i32*
  store i32 %22955, i32* %22981, align 4
  %22982 = load i64, i64* %RBP.i, align 8
  %22983 = add i64 %22982, -8
  %22984 = load i64, i64* %3, align 8
  %22985 = add i64 %22984, 4
  store i64 %22985, i64* %3, align 8
  %22986 = inttoptr i64 %22983 to i64*
  %22987 = load i64, i64* %22986, align 8
  %22988 = add i64 %22987, 45448
  store i64 %22988, i64* %RAX.i11582.pre-phi, align 8
  %22989 = icmp ugt i64 %22987, -45449
  %22990 = zext i1 %22989 to i8
  store i8 %22990, i8* %14, align 1
  %22991 = trunc i64 %22988 to i32
  %22992 = and i32 %22991, 255
  %22993 = tail call i32 @llvm.ctpop.i32(i32 %22992)
  %22994 = trunc i32 %22993 to i8
  %22995 = and i8 %22994, 1
  %22996 = xor i8 %22995, 1
  store i8 %22996, i8* %21, align 1
  %22997 = xor i64 %22988, %22987
  %22998 = lshr i64 %22997, 4
  %22999 = trunc i64 %22998 to i8
  %23000 = and i8 %22999, 1
  store i8 %23000, i8* %27, align 1
  %23001 = icmp eq i64 %22988, 0
  %23002 = zext i1 %23001 to i8
  store i8 %23002, i8* %30, align 1
  %23003 = lshr i64 %22988, 63
  %23004 = trunc i64 %23003 to i8
  store i8 %23004, i8* %33, align 1
  %23005 = lshr i64 %22987, 63
  %23006 = xor i64 %23003, %23005
  %23007 = add nuw nsw i64 %23006, %23003
  %23008 = icmp eq i64 %23007, 2
  %23009 = zext i1 %23008 to i8
  store i8 %23009, i8* %39, align 1
  %23010 = add i64 %22982, -40
  %23011 = add i64 %22984, 14
  store i64 %23011, i64* %3, align 8
  %23012 = inttoptr i64 %23010 to i32*
  %23013 = load i32, i32* %23012, align 4
  %23014 = sext i32 %23013 to i64
  %23015 = mul nsw i64 %23014, 1032
  store i64 %23015, i64* %RCX.i11580, align 8
  %23016 = lshr i64 %23015, 63
  %23017 = add i64 %23015, %22988
  store i64 %23017, i64* %RAX.i11582.pre-phi, align 8
  %23018 = icmp ult i64 %23017, %22988
  %23019 = icmp ult i64 %23017, %23015
  %23020 = or i1 %23018, %23019
  %23021 = zext i1 %23020 to i8
  store i8 %23021, i8* %14, align 1
  %23022 = trunc i64 %23017 to i32
  %23023 = and i32 %23022, 255
  %23024 = tail call i32 @llvm.ctpop.i32(i32 %23023)
  %23025 = trunc i32 %23024 to i8
  %23026 = and i8 %23025, 1
  %23027 = xor i8 %23026, 1
  store i8 %23027, i8* %21, align 1
  %23028 = xor i64 %23015, %22988
  %23029 = xor i64 %23028, %23017
  %23030 = lshr i64 %23029, 4
  %23031 = trunc i64 %23030 to i8
  %23032 = and i8 %23031, 1
  store i8 %23032, i8* %27, align 1
  %23033 = icmp eq i64 %23017, 0
  %23034 = zext i1 %23033 to i8
  store i8 %23034, i8* %30, align 1
  %23035 = lshr i64 %23017, 63
  %23036 = trunc i64 %23035 to i8
  store i8 %23036, i8* %33, align 1
  %23037 = xor i64 %23035, %23003
  %23038 = xor i64 %23035, %23016
  %23039 = add nuw nsw i64 %23037, %23038
  %23040 = icmp eq i64 %23039, 2
  %23041 = zext i1 %23040 to i8
  store i8 %23041, i8* %39, align 1
  %23042 = load i64, i64* %RBP.i, align 8
  %23043 = add i64 %23042, -120
  %23044 = add i64 %22984, 28
  store i64 %23044, i64* %3, align 8
  %23045 = inttoptr i64 %23043 to i64*
  %23046 = load i64, i64* %23045, align 8
  store i64 %23046, i64* %RCX.i11580, align 8
  %23047 = add i64 %23042, -28
  %23048 = add i64 %22984, 31
  store i64 %23048, i64* %3, align 8
  %23049 = inttoptr i64 %23047 to i32*
  %23050 = load i32, i32* %23049, align 4
  %23051 = add i32 %23050, 22
  %23052 = zext i32 %23051 to i64
  store i64 %23052, i64* %576, align 8
  %23053 = icmp ugt i32 %23050, -23
  %23054 = zext i1 %23053 to i8
  store i8 %23054, i8* %14, align 1
  %23055 = and i32 %23051, 255
  %23056 = tail call i32 @llvm.ctpop.i32(i32 %23055)
  %23057 = trunc i32 %23056 to i8
  %23058 = and i8 %23057, 1
  %23059 = xor i8 %23058, 1
  store i8 %23059, i8* %21, align 1
  %23060 = xor i32 %23050, 16
  %23061 = xor i32 %23060, %23051
  %23062 = lshr i32 %23061, 4
  %23063 = trunc i32 %23062 to i8
  %23064 = and i8 %23063, 1
  store i8 %23064, i8* %27, align 1
  %23065 = icmp eq i32 %23051, 0
  %23066 = zext i1 %23065 to i8
  store i8 %23066, i8* %30, align 1
  %23067 = lshr i32 %23051, 31
  %23068 = trunc i32 %23067 to i8
  store i8 %23068, i8* %33, align 1
  %23069 = lshr i32 %23050, 31
  %23070 = xor i32 %23067, %23069
  %23071 = add nuw nsw i32 %23070, %23067
  %23072 = icmp eq i32 %23071, 2
  %23073 = zext i1 %23072 to i8
  store i8 %23073, i8* %39, align 1
  %23074 = sext i32 %23051 to i64
  store i64 %23074, i64* %RSI.i11312, align 8
  %23075 = shl nsw i64 %23074, 1
  %23076 = add i64 %23046, %23075
  %23077 = add i64 %22984, 41
  store i64 %23077, i64* %3, align 8
  %23078 = inttoptr i64 %23076 to i16*
  %23079 = load i16, i16* %23078, align 2
  %23080 = zext i16 %23079 to i64
  store i64 %23080, i64* %576, align 8
  %23081 = zext i16 %23079 to i64
  store i64 %23081, i64* %RCX.i11580, align 8
  %23082 = shl nuw nsw i64 %23081, 2
  %23083 = add i64 %23017, %23082
  %23084 = add i64 %22984, 46
  store i64 %23084, i64* %3, align 8
  %23085 = inttoptr i64 %23083 to i32*
  %23086 = load i32, i32* %23085, align 4
  %23087 = add i32 %23086, 1
  %23088 = zext i32 %23087 to i64
  store i64 %23088, i64* %576, align 8
  %23089 = icmp eq i32 %23086, -1
  %23090 = icmp eq i32 %23087, 0
  %23091 = or i1 %23089, %23090
  %23092 = zext i1 %23091 to i8
  store i8 %23092, i8* %14, align 1
  %23093 = and i32 %23087, 255
  %23094 = tail call i32 @llvm.ctpop.i32(i32 %23093)
  %23095 = trunc i32 %23094 to i8
  %23096 = and i8 %23095, 1
  %23097 = xor i8 %23096, 1
  store i8 %23097, i8* %21, align 1
  %23098 = xor i32 %23087, %23086
  %23099 = lshr i32 %23098, 4
  %23100 = trunc i32 %23099 to i8
  %23101 = and i8 %23100, 1
  store i8 %23101, i8* %27, align 1
  %23102 = zext i1 %23090 to i8
  store i8 %23102, i8* %30, align 1
  %23103 = lshr i32 %23087, 31
  %23104 = trunc i32 %23103 to i8
  store i8 %23104, i8* %33, align 1
  %23105 = lshr i32 %23086, 31
  %23106 = xor i32 %23103, %23105
  %23107 = add nuw nsw i32 %23106, %23103
  %23108 = icmp eq i32 %23107, 2
  %23109 = zext i1 %23108 to i8
  store i8 %23109, i8* %39, align 1
  %23110 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %23111 = add i64 %23082, %23110
  %23112 = add i64 %22984, 52
  store i64 %23112, i64* %3, align 8
  %23113 = inttoptr i64 %23111 to i32*
  store i32 %23087, i32* %23113, align 4
  %23114 = load i64, i64* %RBP.i, align 8
  %23115 = add i64 %23114, -8
  %23116 = load i64, i64* %3, align 8
  %23117 = add i64 %23116, 4
  store i64 %23117, i64* %3, align 8
  %23118 = inttoptr i64 %23115 to i64*
  %23119 = load i64, i64* %23118, align 8
  %23120 = add i64 %23119, 45448
  store i64 %23120, i64* %RAX.i11582.pre-phi, align 8
  %23121 = icmp ugt i64 %23119, -45449
  %23122 = zext i1 %23121 to i8
  store i8 %23122, i8* %14, align 1
  %23123 = trunc i64 %23120 to i32
  %23124 = and i32 %23123, 255
  %23125 = tail call i32 @llvm.ctpop.i32(i32 %23124)
  %23126 = trunc i32 %23125 to i8
  %23127 = and i8 %23126, 1
  %23128 = xor i8 %23127, 1
  store i8 %23128, i8* %21, align 1
  %23129 = xor i64 %23120, %23119
  %23130 = lshr i64 %23129, 4
  %23131 = trunc i64 %23130 to i8
  %23132 = and i8 %23131, 1
  store i8 %23132, i8* %27, align 1
  %23133 = icmp eq i64 %23120, 0
  %23134 = zext i1 %23133 to i8
  store i8 %23134, i8* %30, align 1
  %23135 = lshr i64 %23120, 63
  %23136 = trunc i64 %23135 to i8
  store i8 %23136, i8* %33, align 1
  %23137 = lshr i64 %23119, 63
  %23138 = xor i64 %23135, %23137
  %23139 = add nuw nsw i64 %23138, %23135
  %23140 = icmp eq i64 %23139, 2
  %23141 = zext i1 %23140 to i8
  store i8 %23141, i8* %39, align 1
  %23142 = add i64 %23114, -40
  %23143 = add i64 %23116, 14
  store i64 %23143, i64* %3, align 8
  %23144 = inttoptr i64 %23142 to i32*
  %23145 = load i32, i32* %23144, align 4
  %23146 = sext i32 %23145 to i64
  %23147 = mul nsw i64 %23146, 1032
  store i64 %23147, i64* %RCX.i11580, align 8
  %23148 = lshr i64 %23147, 63
  %23149 = add i64 %23147, %23120
  store i64 %23149, i64* %RAX.i11582.pre-phi, align 8
  %23150 = icmp ult i64 %23149, %23120
  %23151 = icmp ult i64 %23149, %23147
  %23152 = or i1 %23150, %23151
  %23153 = zext i1 %23152 to i8
  store i8 %23153, i8* %14, align 1
  %23154 = trunc i64 %23149 to i32
  %23155 = and i32 %23154, 255
  %23156 = tail call i32 @llvm.ctpop.i32(i32 %23155)
  %23157 = trunc i32 %23156 to i8
  %23158 = and i8 %23157, 1
  %23159 = xor i8 %23158, 1
  store i8 %23159, i8* %21, align 1
  %23160 = xor i64 %23147, %23120
  %23161 = xor i64 %23160, %23149
  %23162 = lshr i64 %23161, 4
  %23163 = trunc i64 %23162 to i8
  %23164 = and i8 %23163, 1
  store i8 %23164, i8* %27, align 1
  %23165 = icmp eq i64 %23149, 0
  %23166 = zext i1 %23165 to i8
  store i8 %23166, i8* %30, align 1
  %23167 = lshr i64 %23149, 63
  %23168 = trunc i64 %23167 to i8
  store i8 %23168, i8* %33, align 1
  %23169 = xor i64 %23167, %23135
  %23170 = xor i64 %23167, %23148
  %23171 = add nuw nsw i64 %23169, %23170
  %23172 = icmp eq i64 %23171, 2
  %23173 = zext i1 %23172 to i8
  store i8 %23173, i8* %39, align 1
  %23174 = load i64, i64* %RBP.i, align 8
  %23175 = add i64 %23174, -120
  %23176 = add i64 %23116, 28
  store i64 %23176, i64* %3, align 8
  %23177 = inttoptr i64 %23175 to i64*
  %23178 = load i64, i64* %23177, align 8
  store i64 %23178, i64* %RCX.i11580, align 8
  %23179 = add i64 %23174, -28
  %23180 = add i64 %23116, 31
  store i64 %23180, i64* %3, align 8
  %23181 = inttoptr i64 %23179 to i32*
  %23182 = load i32, i32* %23181, align 4
  %23183 = add i32 %23182, 23
  %23184 = zext i32 %23183 to i64
  store i64 %23184, i64* %576, align 8
  %23185 = icmp ugt i32 %23182, -24
  %23186 = zext i1 %23185 to i8
  store i8 %23186, i8* %14, align 1
  %23187 = and i32 %23183, 255
  %23188 = tail call i32 @llvm.ctpop.i32(i32 %23187)
  %23189 = trunc i32 %23188 to i8
  %23190 = and i8 %23189, 1
  %23191 = xor i8 %23190, 1
  store i8 %23191, i8* %21, align 1
  %23192 = xor i32 %23182, 16
  %23193 = xor i32 %23192, %23183
  %23194 = lshr i32 %23193, 4
  %23195 = trunc i32 %23194 to i8
  %23196 = and i8 %23195, 1
  store i8 %23196, i8* %27, align 1
  %23197 = icmp eq i32 %23183, 0
  %23198 = zext i1 %23197 to i8
  store i8 %23198, i8* %30, align 1
  %23199 = lshr i32 %23183, 31
  %23200 = trunc i32 %23199 to i8
  store i8 %23200, i8* %33, align 1
  %23201 = lshr i32 %23182, 31
  %23202 = xor i32 %23199, %23201
  %23203 = add nuw nsw i32 %23202, %23199
  %23204 = icmp eq i32 %23203, 2
  %23205 = zext i1 %23204 to i8
  store i8 %23205, i8* %39, align 1
  %23206 = sext i32 %23183 to i64
  store i64 %23206, i64* %RSI.i11312, align 8
  %23207 = shl nsw i64 %23206, 1
  %23208 = add i64 %23178, %23207
  %23209 = add i64 %23116, 41
  store i64 %23209, i64* %3, align 8
  %23210 = inttoptr i64 %23208 to i16*
  %23211 = load i16, i16* %23210, align 2
  %23212 = zext i16 %23211 to i64
  store i64 %23212, i64* %576, align 8
  %23213 = zext i16 %23211 to i64
  store i64 %23213, i64* %RCX.i11580, align 8
  %23214 = shl nuw nsw i64 %23213, 2
  %23215 = add i64 %23149, %23214
  %23216 = add i64 %23116, 46
  store i64 %23216, i64* %3, align 8
  %23217 = inttoptr i64 %23215 to i32*
  %23218 = load i32, i32* %23217, align 4
  %23219 = add i32 %23218, 1
  %23220 = zext i32 %23219 to i64
  store i64 %23220, i64* %576, align 8
  %23221 = icmp eq i32 %23218, -1
  %23222 = icmp eq i32 %23219, 0
  %23223 = or i1 %23221, %23222
  %23224 = zext i1 %23223 to i8
  store i8 %23224, i8* %14, align 1
  %23225 = and i32 %23219, 255
  %23226 = tail call i32 @llvm.ctpop.i32(i32 %23225)
  %23227 = trunc i32 %23226 to i8
  %23228 = and i8 %23227, 1
  %23229 = xor i8 %23228, 1
  store i8 %23229, i8* %21, align 1
  %23230 = xor i32 %23219, %23218
  %23231 = lshr i32 %23230, 4
  %23232 = trunc i32 %23231 to i8
  %23233 = and i8 %23232, 1
  store i8 %23233, i8* %27, align 1
  %23234 = zext i1 %23222 to i8
  store i8 %23234, i8* %30, align 1
  %23235 = lshr i32 %23219, 31
  %23236 = trunc i32 %23235 to i8
  store i8 %23236, i8* %33, align 1
  %23237 = lshr i32 %23218, 31
  %23238 = xor i32 %23235, %23237
  %23239 = add nuw nsw i32 %23238, %23235
  %23240 = icmp eq i32 %23239, 2
  %23241 = zext i1 %23240 to i8
  store i8 %23241, i8* %39, align 1
  %23242 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %23243 = add i64 %23214, %23242
  %23244 = add i64 %23116, 52
  store i64 %23244, i64* %3, align 8
  %23245 = inttoptr i64 %23243 to i32*
  store i32 %23219, i32* %23245, align 4
  %23246 = load i64, i64* %RBP.i, align 8
  %23247 = add i64 %23246, -8
  %23248 = load i64, i64* %3, align 8
  %23249 = add i64 %23248, 4
  store i64 %23249, i64* %3, align 8
  %23250 = inttoptr i64 %23247 to i64*
  %23251 = load i64, i64* %23250, align 8
  %23252 = add i64 %23251, 45448
  store i64 %23252, i64* %RAX.i11582.pre-phi, align 8
  %23253 = icmp ugt i64 %23251, -45449
  %23254 = zext i1 %23253 to i8
  store i8 %23254, i8* %14, align 1
  %23255 = trunc i64 %23252 to i32
  %23256 = and i32 %23255, 255
  %23257 = tail call i32 @llvm.ctpop.i32(i32 %23256)
  %23258 = trunc i32 %23257 to i8
  %23259 = and i8 %23258, 1
  %23260 = xor i8 %23259, 1
  store i8 %23260, i8* %21, align 1
  %23261 = xor i64 %23252, %23251
  %23262 = lshr i64 %23261, 4
  %23263 = trunc i64 %23262 to i8
  %23264 = and i8 %23263, 1
  store i8 %23264, i8* %27, align 1
  %23265 = icmp eq i64 %23252, 0
  %23266 = zext i1 %23265 to i8
  store i8 %23266, i8* %30, align 1
  %23267 = lshr i64 %23252, 63
  %23268 = trunc i64 %23267 to i8
  store i8 %23268, i8* %33, align 1
  %23269 = lshr i64 %23251, 63
  %23270 = xor i64 %23267, %23269
  %23271 = add nuw nsw i64 %23270, %23267
  %23272 = icmp eq i64 %23271, 2
  %23273 = zext i1 %23272 to i8
  store i8 %23273, i8* %39, align 1
  %23274 = add i64 %23246, -40
  %23275 = add i64 %23248, 14
  store i64 %23275, i64* %3, align 8
  %23276 = inttoptr i64 %23274 to i32*
  %23277 = load i32, i32* %23276, align 4
  %23278 = sext i32 %23277 to i64
  %23279 = mul nsw i64 %23278, 1032
  store i64 %23279, i64* %RCX.i11580, align 8
  %23280 = lshr i64 %23279, 63
  %23281 = add i64 %23279, %23252
  store i64 %23281, i64* %RAX.i11582.pre-phi, align 8
  %23282 = icmp ult i64 %23281, %23252
  %23283 = icmp ult i64 %23281, %23279
  %23284 = or i1 %23282, %23283
  %23285 = zext i1 %23284 to i8
  store i8 %23285, i8* %14, align 1
  %23286 = trunc i64 %23281 to i32
  %23287 = and i32 %23286, 255
  %23288 = tail call i32 @llvm.ctpop.i32(i32 %23287)
  %23289 = trunc i32 %23288 to i8
  %23290 = and i8 %23289, 1
  %23291 = xor i8 %23290, 1
  store i8 %23291, i8* %21, align 1
  %23292 = xor i64 %23279, %23252
  %23293 = xor i64 %23292, %23281
  %23294 = lshr i64 %23293, 4
  %23295 = trunc i64 %23294 to i8
  %23296 = and i8 %23295, 1
  store i8 %23296, i8* %27, align 1
  %23297 = icmp eq i64 %23281, 0
  %23298 = zext i1 %23297 to i8
  store i8 %23298, i8* %30, align 1
  %23299 = lshr i64 %23281, 63
  %23300 = trunc i64 %23299 to i8
  store i8 %23300, i8* %33, align 1
  %23301 = xor i64 %23299, %23267
  %23302 = xor i64 %23299, %23280
  %23303 = add nuw nsw i64 %23301, %23302
  %23304 = icmp eq i64 %23303, 2
  %23305 = zext i1 %23304 to i8
  store i8 %23305, i8* %39, align 1
  %23306 = load i64, i64* %RBP.i, align 8
  %23307 = add i64 %23306, -120
  %23308 = add i64 %23248, 28
  store i64 %23308, i64* %3, align 8
  %23309 = inttoptr i64 %23307 to i64*
  %23310 = load i64, i64* %23309, align 8
  store i64 %23310, i64* %RCX.i11580, align 8
  %23311 = add i64 %23306, -28
  %23312 = add i64 %23248, 31
  store i64 %23312, i64* %3, align 8
  %23313 = inttoptr i64 %23311 to i32*
  %23314 = load i32, i32* %23313, align 4
  %23315 = add i32 %23314, 24
  %23316 = zext i32 %23315 to i64
  store i64 %23316, i64* %576, align 8
  %23317 = icmp ugt i32 %23314, -25
  %23318 = zext i1 %23317 to i8
  store i8 %23318, i8* %14, align 1
  %23319 = and i32 %23315, 255
  %23320 = tail call i32 @llvm.ctpop.i32(i32 %23319)
  %23321 = trunc i32 %23320 to i8
  %23322 = and i8 %23321, 1
  %23323 = xor i8 %23322, 1
  store i8 %23323, i8* %21, align 1
  %23324 = xor i32 %23314, 16
  %23325 = xor i32 %23324, %23315
  %23326 = lshr i32 %23325, 4
  %23327 = trunc i32 %23326 to i8
  %23328 = and i8 %23327, 1
  store i8 %23328, i8* %27, align 1
  %23329 = icmp eq i32 %23315, 0
  %23330 = zext i1 %23329 to i8
  store i8 %23330, i8* %30, align 1
  %23331 = lshr i32 %23315, 31
  %23332 = trunc i32 %23331 to i8
  store i8 %23332, i8* %33, align 1
  %23333 = lshr i32 %23314, 31
  %23334 = xor i32 %23331, %23333
  %23335 = add nuw nsw i32 %23334, %23331
  %23336 = icmp eq i32 %23335, 2
  %23337 = zext i1 %23336 to i8
  store i8 %23337, i8* %39, align 1
  %23338 = sext i32 %23315 to i64
  store i64 %23338, i64* %RSI.i11312, align 8
  %23339 = shl nsw i64 %23338, 1
  %23340 = add i64 %23310, %23339
  %23341 = add i64 %23248, 41
  store i64 %23341, i64* %3, align 8
  %23342 = inttoptr i64 %23340 to i16*
  %23343 = load i16, i16* %23342, align 2
  %23344 = zext i16 %23343 to i64
  store i64 %23344, i64* %576, align 8
  %23345 = zext i16 %23343 to i64
  store i64 %23345, i64* %RCX.i11580, align 8
  %23346 = shl nuw nsw i64 %23345, 2
  %23347 = add i64 %23281, %23346
  %23348 = add i64 %23248, 46
  store i64 %23348, i64* %3, align 8
  %23349 = inttoptr i64 %23347 to i32*
  %23350 = load i32, i32* %23349, align 4
  %23351 = add i32 %23350, 1
  %23352 = zext i32 %23351 to i64
  store i64 %23352, i64* %576, align 8
  %23353 = icmp eq i32 %23350, -1
  %23354 = icmp eq i32 %23351, 0
  %23355 = or i1 %23353, %23354
  %23356 = zext i1 %23355 to i8
  store i8 %23356, i8* %14, align 1
  %23357 = and i32 %23351, 255
  %23358 = tail call i32 @llvm.ctpop.i32(i32 %23357)
  %23359 = trunc i32 %23358 to i8
  %23360 = and i8 %23359, 1
  %23361 = xor i8 %23360, 1
  store i8 %23361, i8* %21, align 1
  %23362 = xor i32 %23351, %23350
  %23363 = lshr i32 %23362, 4
  %23364 = trunc i32 %23363 to i8
  %23365 = and i8 %23364, 1
  store i8 %23365, i8* %27, align 1
  %23366 = zext i1 %23354 to i8
  store i8 %23366, i8* %30, align 1
  %23367 = lshr i32 %23351, 31
  %23368 = trunc i32 %23367 to i8
  store i8 %23368, i8* %33, align 1
  %23369 = lshr i32 %23350, 31
  %23370 = xor i32 %23367, %23369
  %23371 = add nuw nsw i32 %23370, %23367
  %23372 = icmp eq i32 %23371, 2
  %23373 = zext i1 %23372 to i8
  store i8 %23373, i8* %39, align 1
  %23374 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %23375 = add i64 %23346, %23374
  %23376 = add i64 %23248, 52
  store i64 %23376, i64* %3, align 8
  %23377 = inttoptr i64 %23375 to i32*
  store i32 %23351, i32* %23377, align 4
  %23378 = load i64, i64* %RBP.i, align 8
  %23379 = add i64 %23378, -8
  %23380 = load i64, i64* %3, align 8
  %23381 = add i64 %23380, 4
  store i64 %23381, i64* %3, align 8
  %23382 = inttoptr i64 %23379 to i64*
  %23383 = load i64, i64* %23382, align 8
  %23384 = add i64 %23383, 45448
  store i64 %23384, i64* %RAX.i11582.pre-phi, align 8
  %23385 = icmp ugt i64 %23383, -45449
  %23386 = zext i1 %23385 to i8
  store i8 %23386, i8* %14, align 1
  %23387 = trunc i64 %23384 to i32
  %23388 = and i32 %23387, 255
  %23389 = tail call i32 @llvm.ctpop.i32(i32 %23388)
  %23390 = trunc i32 %23389 to i8
  %23391 = and i8 %23390, 1
  %23392 = xor i8 %23391, 1
  store i8 %23392, i8* %21, align 1
  %23393 = xor i64 %23384, %23383
  %23394 = lshr i64 %23393, 4
  %23395 = trunc i64 %23394 to i8
  %23396 = and i8 %23395, 1
  store i8 %23396, i8* %27, align 1
  %23397 = icmp eq i64 %23384, 0
  %23398 = zext i1 %23397 to i8
  store i8 %23398, i8* %30, align 1
  %23399 = lshr i64 %23384, 63
  %23400 = trunc i64 %23399 to i8
  store i8 %23400, i8* %33, align 1
  %23401 = lshr i64 %23383, 63
  %23402 = xor i64 %23399, %23401
  %23403 = add nuw nsw i64 %23402, %23399
  %23404 = icmp eq i64 %23403, 2
  %23405 = zext i1 %23404 to i8
  store i8 %23405, i8* %39, align 1
  %23406 = add i64 %23378, -40
  %23407 = add i64 %23380, 14
  store i64 %23407, i64* %3, align 8
  %23408 = inttoptr i64 %23406 to i32*
  %23409 = load i32, i32* %23408, align 4
  %23410 = sext i32 %23409 to i64
  %23411 = mul nsw i64 %23410, 1032
  store i64 %23411, i64* %RCX.i11580, align 8
  %23412 = lshr i64 %23411, 63
  %23413 = add i64 %23411, %23384
  store i64 %23413, i64* %RAX.i11582.pre-phi, align 8
  %23414 = icmp ult i64 %23413, %23384
  %23415 = icmp ult i64 %23413, %23411
  %23416 = or i1 %23414, %23415
  %23417 = zext i1 %23416 to i8
  store i8 %23417, i8* %14, align 1
  %23418 = trunc i64 %23413 to i32
  %23419 = and i32 %23418, 255
  %23420 = tail call i32 @llvm.ctpop.i32(i32 %23419)
  %23421 = trunc i32 %23420 to i8
  %23422 = and i8 %23421, 1
  %23423 = xor i8 %23422, 1
  store i8 %23423, i8* %21, align 1
  %23424 = xor i64 %23411, %23384
  %23425 = xor i64 %23424, %23413
  %23426 = lshr i64 %23425, 4
  %23427 = trunc i64 %23426 to i8
  %23428 = and i8 %23427, 1
  store i8 %23428, i8* %27, align 1
  %23429 = icmp eq i64 %23413, 0
  %23430 = zext i1 %23429 to i8
  store i8 %23430, i8* %30, align 1
  %23431 = lshr i64 %23413, 63
  %23432 = trunc i64 %23431 to i8
  store i8 %23432, i8* %33, align 1
  %23433 = xor i64 %23431, %23399
  %23434 = xor i64 %23431, %23412
  %23435 = add nuw nsw i64 %23433, %23434
  %23436 = icmp eq i64 %23435, 2
  %23437 = zext i1 %23436 to i8
  store i8 %23437, i8* %39, align 1
  %23438 = load i64, i64* %RBP.i, align 8
  %23439 = add i64 %23438, -120
  %23440 = add i64 %23380, 28
  store i64 %23440, i64* %3, align 8
  %23441 = inttoptr i64 %23439 to i64*
  %23442 = load i64, i64* %23441, align 8
  store i64 %23442, i64* %RCX.i11580, align 8
  %23443 = add i64 %23438, -28
  %23444 = add i64 %23380, 31
  store i64 %23444, i64* %3, align 8
  %23445 = inttoptr i64 %23443 to i32*
  %23446 = load i32, i32* %23445, align 4
  %23447 = add i32 %23446, 25
  %23448 = zext i32 %23447 to i64
  store i64 %23448, i64* %576, align 8
  %23449 = icmp ugt i32 %23446, -26
  %23450 = zext i1 %23449 to i8
  store i8 %23450, i8* %14, align 1
  %23451 = and i32 %23447, 255
  %23452 = tail call i32 @llvm.ctpop.i32(i32 %23451)
  %23453 = trunc i32 %23452 to i8
  %23454 = and i8 %23453, 1
  %23455 = xor i8 %23454, 1
  store i8 %23455, i8* %21, align 1
  %23456 = xor i32 %23446, 16
  %23457 = xor i32 %23456, %23447
  %23458 = lshr i32 %23457, 4
  %23459 = trunc i32 %23458 to i8
  %23460 = and i8 %23459, 1
  store i8 %23460, i8* %27, align 1
  %23461 = icmp eq i32 %23447, 0
  %23462 = zext i1 %23461 to i8
  store i8 %23462, i8* %30, align 1
  %23463 = lshr i32 %23447, 31
  %23464 = trunc i32 %23463 to i8
  store i8 %23464, i8* %33, align 1
  %23465 = lshr i32 %23446, 31
  %23466 = xor i32 %23463, %23465
  %23467 = add nuw nsw i32 %23466, %23463
  %23468 = icmp eq i32 %23467, 2
  %23469 = zext i1 %23468 to i8
  store i8 %23469, i8* %39, align 1
  %23470 = sext i32 %23447 to i64
  store i64 %23470, i64* %RSI.i11312, align 8
  %23471 = shl nsw i64 %23470, 1
  %23472 = add i64 %23442, %23471
  %23473 = add i64 %23380, 41
  store i64 %23473, i64* %3, align 8
  %23474 = inttoptr i64 %23472 to i16*
  %23475 = load i16, i16* %23474, align 2
  %23476 = zext i16 %23475 to i64
  store i64 %23476, i64* %576, align 8
  %23477 = zext i16 %23475 to i64
  store i64 %23477, i64* %RCX.i11580, align 8
  %23478 = shl nuw nsw i64 %23477, 2
  %23479 = add i64 %23413, %23478
  %23480 = add i64 %23380, 46
  store i64 %23480, i64* %3, align 8
  %23481 = inttoptr i64 %23479 to i32*
  %23482 = load i32, i32* %23481, align 4
  %23483 = add i32 %23482, 1
  %23484 = zext i32 %23483 to i64
  store i64 %23484, i64* %576, align 8
  %23485 = icmp eq i32 %23482, -1
  %23486 = icmp eq i32 %23483, 0
  %23487 = or i1 %23485, %23486
  %23488 = zext i1 %23487 to i8
  store i8 %23488, i8* %14, align 1
  %23489 = and i32 %23483, 255
  %23490 = tail call i32 @llvm.ctpop.i32(i32 %23489)
  %23491 = trunc i32 %23490 to i8
  %23492 = and i8 %23491, 1
  %23493 = xor i8 %23492, 1
  store i8 %23493, i8* %21, align 1
  %23494 = xor i32 %23483, %23482
  %23495 = lshr i32 %23494, 4
  %23496 = trunc i32 %23495 to i8
  %23497 = and i8 %23496, 1
  store i8 %23497, i8* %27, align 1
  %23498 = zext i1 %23486 to i8
  store i8 %23498, i8* %30, align 1
  %23499 = lshr i32 %23483, 31
  %23500 = trunc i32 %23499 to i8
  store i8 %23500, i8* %33, align 1
  %23501 = lshr i32 %23482, 31
  %23502 = xor i32 %23499, %23501
  %23503 = add nuw nsw i32 %23502, %23499
  %23504 = icmp eq i32 %23503, 2
  %23505 = zext i1 %23504 to i8
  store i8 %23505, i8* %39, align 1
  %23506 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %23507 = add i64 %23478, %23506
  %23508 = add i64 %23380, 52
  store i64 %23508, i64* %3, align 8
  %23509 = inttoptr i64 %23507 to i32*
  store i32 %23483, i32* %23509, align 4
  %23510 = load i64, i64* %RBP.i, align 8
  %23511 = add i64 %23510, -8
  %23512 = load i64, i64* %3, align 8
  %23513 = add i64 %23512, 4
  store i64 %23513, i64* %3, align 8
  %23514 = inttoptr i64 %23511 to i64*
  %23515 = load i64, i64* %23514, align 8
  %23516 = add i64 %23515, 45448
  store i64 %23516, i64* %RAX.i11582.pre-phi, align 8
  %23517 = icmp ugt i64 %23515, -45449
  %23518 = zext i1 %23517 to i8
  store i8 %23518, i8* %14, align 1
  %23519 = trunc i64 %23516 to i32
  %23520 = and i32 %23519, 255
  %23521 = tail call i32 @llvm.ctpop.i32(i32 %23520)
  %23522 = trunc i32 %23521 to i8
  %23523 = and i8 %23522, 1
  %23524 = xor i8 %23523, 1
  store i8 %23524, i8* %21, align 1
  %23525 = xor i64 %23516, %23515
  %23526 = lshr i64 %23525, 4
  %23527 = trunc i64 %23526 to i8
  %23528 = and i8 %23527, 1
  store i8 %23528, i8* %27, align 1
  %23529 = icmp eq i64 %23516, 0
  %23530 = zext i1 %23529 to i8
  store i8 %23530, i8* %30, align 1
  %23531 = lshr i64 %23516, 63
  %23532 = trunc i64 %23531 to i8
  store i8 %23532, i8* %33, align 1
  %23533 = lshr i64 %23515, 63
  %23534 = xor i64 %23531, %23533
  %23535 = add nuw nsw i64 %23534, %23531
  %23536 = icmp eq i64 %23535, 2
  %23537 = zext i1 %23536 to i8
  store i8 %23537, i8* %39, align 1
  %23538 = add i64 %23510, -40
  %23539 = add i64 %23512, 14
  store i64 %23539, i64* %3, align 8
  %23540 = inttoptr i64 %23538 to i32*
  %23541 = load i32, i32* %23540, align 4
  %23542 = sext i32 %23541 to i64
  %23543 = mul nsw i64 %23542, 1032
  store i64 %23543, i64* %RCX.i11580, align 8
  %23544 = lshr i64 %23543, 63
  %23545 = add i64 %23543, %23516
  store i64 %23545, i64* %RAX.i11582.pre-phi, align 8
  %23546 = icmp ult i64 %23545, %23516
  %23547 = icmp ult i64 %23545, %23543
  %23548 = or i1 %23546, %23547
  %23549 = zext i1 %23548 to i8
  store i8 %23549, i8* %14, align 1
  %23550 = trunc i64 %23545 to i32
  %23551 = and i32 %23550, 255
  %23552 = tail call i32 @llvm.ctpop.i32(i32 %23551)
  %23553 = trunc i32 %23552 to i8
  %23554 = and i8 %23553, 1
  %23555 = xor i8 %23554, 1
  store i8 %23555, i8* %21, align 1
  %23556 = xor i64 %23543, %23516
  %23557 = xor i64 %23556, %23545
  %23558 = lshr i64 %23557, 4
  %23559 = trunc i64 %23558 to i8
  %23560 = and i8 %23559, 1
  store i8 %23560, i8* %27, align 1
  %23561 = icmp eq i64 %23545, 0
  %23562 = zext i1 %23561 to i8
  store i8 %23562, i8* %30, align 1
  %23563 = lshr i64 %23545, 63
  %23564 = trunc i64 %23563 to i8
  store i8 %23564, i8* %33, align 1
  %23565 = xor i64 %23563, %23531
  %23566 = xor i64 %23563, %23544
  %23567 = add nuw nsw i64 %23565, %23566
  %23568 = icmp eq i64 %23567, 2
  %23569 = zext i1 %23568 to i8
  store i8 %23569, i8* %39, align 1
  %23570 = load i64, i64* %RBP.i, align 8
  %23571 = add i64 %23570, -120
  %23572 = add i64 %23512, 28
  store i64 %23572, i64* %3, align 8
  %23573 = inttoptr i64 %23571 to i64*
  %23574 = load i64, i64* %23573, align 8
  store i64 %23574, i64* %RCX.i11580, align 8
  %23575 = add i64 %23570, -28
  %23576 = add i64 %23512, 31
  store i64 %23576, i64* %3, align 8
  %23577 = inttoptr i64 %23575 to i32*
  %23578 = load i32, i32* %23577, align 4
  %23579 = add i32 %23578, 26
  %23580 = zext i32 %23579 to i64
  store i64 %23580, i64* %576, align 8
  %23581 = icmp ugt i32 %23578, -27
  %23582 = zext i1 %23581 to i8
  store i8 %23582, i8* %14, align 1
  %23583 = and i32 %23579, 255
  %23584 = tail call i32 @llvm.ctpop.i32(i32 %23583)
  %23585 = trunc i32 %23584 to i8
  %23586 = and i8 %23585, 1
  %23587 = xor i8 %23586, 1
  store i8 %23587, i8* %21, align 1
  %23588 = xor i32 %23578, 16
  %23589 = xor i32 %23588, %23579
  %23590 = lshr i32 %23589, 4
  %23591 = trunc i32 %23590 to i8
  %23592 = and i8 %23591, 1
  store i8 %23592, i8* %27, align 1
  %23593 = icmp eq i32 %23579, 0
  %23594 = zext i1 %23593 to i8
  store i8 %23594, i8* %30, align 1
  %23595 = lshr i32 %23579, 31
  %23596 = trunc i32 %23595 to i8
  store i8 %23596, i8* %33, align 1
  %23597 = lshr i32 %23578, 31
  %23598 = xor i32 %23595, %23597
  %23599 = add nuw nsw i32 %23598, %23595
  %23600 = icmp eq i32 %23599, 2
  %23601 = zext i1 %23600 to i8
  store i8 %23601, i8* %39, align 1
  %23602 = sext i32 %23579 to i64
  store i64 %23602, i64* %RSI.i11312, align 8
  %23603 = shl nsw i64 %23602, 1
  %23604 = add i64 %23574, %23603
  %23605 = add i64 %23512, 41
  store i64 %23605, i64* %3, align 8
  %23606 = inttoptr i64 %23604 to i16*
  %23607 = load i16, i16* %23606, align 2
  %23608 = zext i16 %23607 to i64
  store i64 %23608, i64* %576, align 8
  %23609 = zext i16 %23607 to i64
  store i64 %23609, i64* %RCX.i11580, align 8
  %23610 = shl nuw nsw i64 %23609, 2
  %23611 = add i64 %23545, %23610
  %23612 = add i64 %23512, 46
  store i64 %23612, i64* %3, align 8
  %23613 = inttoptr i64 %23611 to i32*
  %23614 = load i32, i32* %23613, align 4
  %23615 = add i32 %23614, 1
  %23616 = zext i32 %23615 to i64
  store i64 %23616, i64* %576, align 8
  %23617 = icmp eq i32 %23614, -1
  %23618 = icmp eq i32 %23615, 0
  %23619 = or i1 %23617, %23618
  %23620 = zext i1 %23619 to i8
  store i8 %23620, i8* %14, align 1
  %23621 = and i32 %23615, 255
  %23622 = tail call i32 @llvm.ctpop.i32(i32 %23621)
  %23623 = trunc i32 %23622 to i8
  %23624 = and i8 %23623, 1
  %23625 = xor i8 %23624, 1
  store i8 %23625, i8* %21, align 1
  %23626 = xor i32 %23615, %23614
  %23627 = lshr i32 %23626, 4
  %23628 = trunc i32 %23627 to i8
  %23629 = and i8 %23628, 1
  store i8 %23629, i8* %27, align 1
  %23630 = zext i1 %23618 to i8
  store i8 %23630, i8* %30, align 1
  %23631 = lshr i32 %23615, 31
  %23632 = trunc i32 %23631 to i8
  store i8 %23632, i8* %33, align 1
  %23633 = lshr i32 %23614, 31
  %23634 = xor i32 %23631, %23633
  %23635 = add nuw nsw i32 %23634, %23631
  %23636 = icmp eq i32 %23635, 2
  %23637 = zext i1 %23636 to i8
  store i8 %23637, i8* %39, align 1
  %23638 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %23639 = add i64 %23610, %23638
  %23640 = add i64 %23512, 52
  store i64 %23640, i64* %3, align 8
  %23641 = inttoptr i64 %23639 to i32*
  store i32 %23615, i32* %23641, align 4
  %23642 = load i64, i64* %RBP.i, align 8
  %23643 = add i64 %23642, -8
  %23644 = load i64, i64* %3, align 8
  %23645 = add i64 %23644, 4
  store i64 %23645, i64* %3, align 8
  %23646 = inttoptr i64 %23643 to i64*
  %23647 = load i64, i64* %23646, align 8
  %23648 = add i64 %23647, 45448
  store i64 %23648, i64* %RAX.i11582.pre-phi, align 8
  %23649 = icmp ugt i64 %23647, -45449
  %23650 = zext i1 %23649 to i8
  store i8 %23650, i8* %14, align 1
  %23651 = trunc i64 %23648 to i32
  %23652 = and i32 %23651, 255
  %23653 = tail call i32 @llvm.ctpop.i32(i32 %23652)
  %23654 = trunc i32 %23653 to i8
  %23655 = and i8 %23654, 1
  %23656 = xor i8 %23655, 1
  store i8 %23656, i8* %21, align 1
  %23657 = xor i64 %23648, %23647
  %23658 = lshr i64 %23657, 4
  %23659 = trunc i64 %23658 to i8
  %23660 = and i8 %23659, 1
  store i8 %23660, i8* %27, align 1
  %23661 = icmp eq i64 %23648, 0
  %23662 = zext i1 %23661 to i8
  store i8 %23662, i8* %30, align 1
  %23663 = lshr i64 %23648, 63
  %23664 = trunc i64 %23663 to i8
  store i8 %23664, i8* %33, align 1
  %23665 = lshr i64 %23647, 63
  %23666 = xor i64 %23663, %23665
  %23667 = add nuw nsw i64 %23666, %23663
  %23668 = icmp eq i64 %23667, 2
  %23669 = zext i1 %23668 to i8
  store i8 %23669, i8* %39, align 1
  %23670 = add i64 %23642, -40
  %23671 = add i64 %23644, 14
  store i64 %23671, i64* %3, align 8
  %23672 = inttoptr i64 %23670 to i32*
  %23673 = load i32, i32* %23672, align 4
  %23674 = sext i32 %23673 to i64
  %23675 = mul nsw i64 %23674, 1032
  store i64 %23675, i64* %RCX.i11580, align 8
  %23676 = lshr i64 %23675, 63
  %23677 = add i64 %23675, %23648
  store i64 %23677, i64* %RAX.i11582.pre-phi, align 8
  %23678 = icmp ult i64 %23677, %23648
  %23679 = icmp ult i64 %23677, %23675
  %23680 = or i1 %23678, %23679
  %23681 = zext i1 %23680 to i8
  store i8 %23681, i8* %14, align 1
  %23682 = trunc i64 %23677 to i32
  %23683 = and i32 %23682, 255
  %23684 = tail call i32 @llvm.ctpop.i32(i32 %23683)
  %23685 = trunc i32 %23684 to i8
  %23686 = and i8 %23685, 1
  %23687 = xor i8 %23686, 1
  store i8 %23687, i8* %21, align 1
  %23688 = xor i64 %23675, %23648
  %23689 = xor i64 %23688, %23677
  %23690 = lshr i64 %23689, 4
  %23691 = trunc i64 %23690 to i8
  %23692 = and i8 %23691, 1
  store i8 %23692, i8* %27, align 1
  %23693 = icmp eq i64 %23677, 0
  %23694 = zext i1 %23693 to i8
  store i8 %23694, i8* %30, align 1
  %23695 = lshr i64 %23677, 63
  %23696 = trunc i64 %23695 to i8
  store i8 %23696, i8* %33, align 1
  %23697 = xor i64 %23695, %23663
  %23698 = xor i64 %23695, %23676
  %23699 = add nuw nsw i64 %23697, %23698
  %23700 = icmp eq i64 %23699, 2
  %23701 = zext i1 %23700 to i8
  store i8 %23701, i8* %39, align 1
  %23702 = load i64, i64* %RBP.i, align 8
  %23703 = add i64 %23702, -120
  %23704 = add i64 %23644, 28
  store i64 %23704, i64* %3, align 8
  %23705 = inttoptr i64 %23703 to i64*
  %23706 = load i64, i64* %23705, align 8
  store i64 %23706, i64* %RCX.i11580, align 8
  %23707 = add i64 %23702, -28
  %23708 = add i64 %23644, 31
  store i64 %23708, i64* %3, align 8
  %23709 = inttoptr i64 %23707 to i32*
  %23710 = load i32, i32* %23709, align 4
  %23711 = add i32 %23710, 27
  %23712 = zext i32 %23711 to i64
  store i64 %23712, i64* %576, align 8
  %23713 = icmp ugt i32 %23710, -28
  %23714 = zext i1 %23713 to i8
  store i8 %23714, i8* %14, align 1
  %23715 = and i32 %23711, 255
  %23716 = tail call i32 @llvm.ctpop.i32(i32 %23715)
  %23717 = trunc i32 %23716 to i8
  %23718 = and i8 %23717, 1
  %23719 = xor i8 %23718, 1
  store i8 %23719, i8* %21, align 1
  %23720 = xor i32 %23710, 16
  %23721 = xor i32 %23720, %23711
  %23722 = lshr i32 %23721, 4
  %23723 = trunc i32 %23722 to i8
  %23724 = and i8 %23723, 1
  store i8 %23724, i8* %27, align 1
  %23725 = icmp eq i32 %23711, 0
  %23726 = zext i1 %23725 to i8
  store i8 %23726, i8* %30, align 1
  %23727 = lshr i32 %23711, 31
  %23728 = trunc i32 %23727 to i8
  store i8 %23728, i8* %33, align 1
  %23729 = lshr i32 %23710, 31
  %23730 = xor i32 %23727, %23729
  %23731 = add nuw nsw i32 %23730, %23727
  %23732 = icmp eq i32 %23731, 2
  %23733 = zext i1 %23732 to i8
  store i8 %23733, i8* %39, align 1
  %23734 = sext i32 %23711 to i64
  store i64 %23734, i64* %RSI.i11312, align 8
  %23735 = shl nsw i64 %23734, 1
  %23736 = add i64 %23706, %23735
  %23737 = add i64 %23644, 41
  store i64 %23737, i64* %3, align 8
  %23738 = inttoptr i64 %23736 to i16*
  %23739 = load i16, i16* %23738, align 2
  %23740 = zext i16 %23739 to i64
  store i64 %23740, i64* %576, align 8
  %23741 = zext i16 %23739 to i64
  store i64 %23741, i64* %RCX.i11580, align 8
  %23742 = shl nuw nsw i64 %23741, 2
  %23743 = add i64 %23677, %23742
  %23744 = add i64 %23644, 46
  store i64 %23744, i64* %3, align 8
  %23745 = inttoptr i64 %23743 to i32*
  %23746 = load i32, i32* %23745, align 4
  %23747 = add i32 %23746, 1
  %23748 = zext i32 %23747 to i64
  store i64 %23748, i64* %576, align 8
  %23749 = icmp eq i32 %23746, -1
  %23750 = icmp eq i32 %23747, 0
  %23751 = or i1 %23749, %23750
  %23752 = zext i1 %23751 to i8
  store i8 %23752, i8* %14, align 1
  %23753 = and i32 %23747, 255
  %23754 = tail call i32 @llvm.ctpop.i32(i32 %23753)
  %23755 = trunc i32 %23754 to i8
  %23756 = and i8 %23755, 1
  %23757 = xor i8 %23756, 1
  store i8 %23757, i8* %21, align 1
  %23758 = xor i32 %23747, %23746
  %23759 = lshr i32 %23758, 4
  %23760 = trunc i32 %23759 to i8
  %23761 = and i8 %23760, 1
  store i8 %23761, i8* %27, align 1
  %23762 = zext i1 %23750 to i8
  store i8 %23762, i8* %30, align 1
  %23763 = lshr i32 %23747, 31
  %23764 = trunc i32 %23763 to i8
  store i8 %23764, i8* %33, align 1
  %23765 = lshr i32 %23746, 31
  %23766 = xor i32 %23763, %23765
  %23767 = add nuw nsw i32 %23766, %23763
  %23768 = icmp eq i32 %23767, 2
  %23769 = zext i1 %23768 to i8
  store i8 %23769, i8* %39, align 1
  %23770 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %23771 = add i64 %23742, %23770
  %23772 = add i64 %23644, 52
  store i64 %23772, i64* %3, align 8
  %23773 = inttoptr i64 %23771 to i32*
  store i32 %23747, i32* %23773, align 4
  %23774 = load i64, i64* %RBP.i, align 8
  %23775 = add i64 %23774, -8
  %23776 = load i64, i64* %3, align 8
  %23777 = add i64 %23776, 4
  store i64 %23777, i64* %3, align 8
  %23778 = inttoptr i64 %23775 to i64*
  %23779 = load i64, i64* %23778, align 8
  %23780 = add i64 %23779, 45448
  store i64 %23780, i64* %RAX.i11582.pre-phi, align 8
  %23781 = icmp ugt i64 %23779, -45449
  %23782 = zext i1 %23781 to i8
  store i8 %23782, i8* %14, align 1
  %23783 = trunc i64 %23780 to i32
  %23784 = and i32 %23783, 255
  %23785 = tail call i32 @llvm.ctpop.i32(i32 %23784)
  %23786 = trunc i32 %23785 to i8
  %23787 = and i8 %23786, 1
  %23788 = xor i8 %23787, 1
  store i8 %23788, i8* %21, align 1
  %23789 = xor i64 %23780, %23779
  %23790 = lshr i64 %23789, 4
  %23791 = trunc i64 %23790 to i8
  %23792 = and i8 %23791, 1
  store i8 %23792, i8* %27, align 1
  %23793 = icmp eq i64 %23780, 0
  %23794 = zext i1 %23793 to i8
  store i8 %23794, i8* %30, align 1
  %23795 = lshr i64 %23780, 63
  %23796 = trunc i64 %23795 to i8
  store i8 %23796, i8* %33, align 1
  %23797 = lshr i64 %23779, 63
  %23798 = xor i64 %23795, %23797
  %23799 = add nuw nsw i64 %23798, %23795
  %23800 = icmp eq i64 %23799, 2
  %23801 = zext i1 %23800 to i8
  store i8 %23801, i8* %39, align 1
  %23802 = add i64 %23774, -40
  %23803 = add i64 %23776, 14
  store i64 %23803, i64* %3, align 8
  %23804 = inttoptr i64 %23802 to i32*
  %23805 = load i32, i32* %23804, align 4
  %23806 = sext i32 %23805 to i64
  %23807 = mul nsw i64 %23806, 1032
  store i64 %23807, i64* %RCX.i11580, align 8
  %23808 = lshr i64 %23807, 63
  %23809 = add i64 %23807, %23780
  store i64 %23809, i64* %RAX.i11582.pre-phi, align 8
  %23810 = icmp ult i64 %23809, %23780
  %23811 = icmp ult i64 %23809, %23807
  %23812 = or i1 %23810, %23811
  %23813 = zext i1 %23812 to i8
  store i8 %23813, i8* %14, align 1
  %23814 = trunc i64 %23809 to i32
  %23815 = and i32 %23814, 255
  %23816 = tail call i32 @llvm.ctpop.i32(i32 %23815)
  %23817 = trunc i32 %23816 to i8
  %23818 = and i8 %23817, 1
  %23819 = xor i8 %23818, 1
  store i8 %23819, i8* %21, align 1
  %23820 = xor i64 %23807, %23780
  %23821 = xor i64 %23820, %23809
  %23822 = lshr i64 %23821, 4
  %23823 = trunc i64 %23822 to i8
  %23824 = and i8 %23823, 1
  store i8 %23824, i8* %27, align 1
  %23825 = icmp eq i64 %23809, 0
  %23826 = zext i1 %23825 to i8
  store i8 %23826, i8* %30, align 1
  %23827 = lshr i64 %23809, 63
  %23828 = trunc i64 %23827 to i8
  store i8 %23828, i8* %33, align 1
  %23829 = xor i64 %23827, %23795
  %23830 = xor i64 %23827, %23808
  %23831 = add nuw nsw i64 %23829, %23830
  %23832 = icmp eq i64 %23831, 2
  %23833 = zext i1 %23832 to i8
  store i8 %23833, i8* %39, align 1
  %23834 = load i64, i64* %RBP.i, align 8
  %23835 = add i64 %23834, -120
  %23836 = add i64 %23776, 28
  store i64 %23836, i64* %3, align 8
  %23837 = inttoptr i64 %23835 to i64*
  %23838 = load i64, i64* %23837, align 8
  store i64 %23838, i64* %RCX.i11580, align 8
  %23839 = add i64 %23834, -28
  %23840 = add i64 %23776, 31
  store i64 %23840, i64* %3, align 8
  %23841 = inttoptr i64 %23839 to i32*
  %23842 = load i32, i32* %23841, align 4
  %23843 = add i32 %23842, 28
  %23844 = zext i32 %23843 to i64
  store i64 %23844, i64* %576, align 8
  %23845 = icmp ugt i32 %23842, -29
  %23846 = zext i1 %23845 to i8
  store i8 %23846, i8* %14, align 1
  %23847 = and i32 %23843, 255
  %23848 = tail call i32 @llvm.ctpop.i32(i32 %23847)
  %23849 = trunc i32 %23848 to i8
  %23850 = and i8 %23849, 1
  %23851 = xor i8 %23850, 1
  store i8 %23851, i8* %21, align 1
  %23852 = xor i32 %23842, 16
  %23853 = xor i32 %23852, %23843
  %23854 = lshr i32 %23853, 4
  %23855 = trunc i32 %23854 to i8
  %23856 = and i8 %23855, 1
  store i8 %23856, i8* %27, align 1
  %23857 = icmp eq i32 %23843, 0
  %23858 = zext i1 %23857 to i8
  store i8 %23858, i8* %30, align 1
  %23859 = lshr i32 %23843, 31
  %23860 = trunc i32 %23859 to i8
  store i8 %23860, i8* %33, align 1
  %23861 = lshr i32 %23842, 31
  %23862 = xor i32 %23859, %23861
  %23863 = add nuw nsw i32 %23862, %23859
  %23864 = icmp eq i32 %23863, 2
  %23865 = zext i1 %23864 to i8
  store i8 %23865, i8* %39, align 1
  %23866 = sext i32 %23843 to i64
  store i64 %23866, i64* %RSI.i11312, align 8
  %23867 = shl nsw i64 %23866, 1
  %23868 = add i64 %23838, %23867
  %23869 = add i64 %23776, 41
  store i64 %23869, i64* %3, align 8
  %23870 = inttoptr i64 %23868 to i16*
  %23871 = load i16, i16* %23870, align 2
  %23872 = zext i16 %23871 to i64
  store i64 %23872, i64* %576, align 8
  %23873 = zext i16 %23871 to i64
  store i64 %23873, i64* %RCX.i11580, align 8
  %23874 = shl nuw nsw i64 %23873, 2
  %23875 = add i64 %23809, %23874
  %23876 = add i64 %23776, 46
  store i64 %23876, i64* %3, align 8
  %23877 = inttoptr i64 %23875 to i32*
  %23878 = load i32, i32* %23877, align 4
  %23879 = add i32 %23878, 1
  %23880 = zext i32 %23879 to i64
  store i64 %23880, i64* %576, align 8
  %23881 = icmp eq i32 %23878, -1
  %23882 = icmp eq i32 %23879, 0
  %23883 = or i1 %23881, %23882
  %23884 = zext i1 %23883 to i8
  store i8 %23884, i8* %14, align 1
  %23885 = and i32 %23879, 255
  %23886 = tail call i32 @llvm.ctpop.i32(i32 %23885)
  %23887 = trunc i32 %23886 to i8
  %23888 = and i8 %23887, 1
  %23889 = xor i8 %23888, 1
  store i8 %23889, i8* %21, align 1
  %23890 = xor i32 %23879, %23878
  %23891 = lshr i32 %23890, 4
  %23892 = trunc i32 %23891 to i8
  %23893 = and i8 %23892, 1
  store i8 %23893, i8* %27, align 1
  %23894 = zext i1 %23882 to i8
  store i8 %23894, i8* %30, align 1
  %23895 = lshr i32 %23879, 31
  %23896 = trunc i32 %23895 to i8
  store i8 %23896, i8* %33, align 1
  %23897 = lshr i32 %23878, 31
  %23898 = xor i32 %23895, %23897
  %23899 = add nuw nsw i32 %23898, %23895
  %23900 = icmp eq i32 %23899, 2
  %23901 = zext i1 %23900 to i8
  store i8 %23901, i8* %39, align 1
  %23902 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %23903 = add i64 %23874, %23902
  %23904 = add i64 %23776, 52
  store i64 %23904, i64* %3, align 8
  %23905 = inttoptr i64 %23903 to i32*
  store i32 %23879, i32* %23905, align 4
  %23906 = load i64, i64* %RBP.i, align 8
  %23907 = add i64 %23906, -8
  %23908 = load i64, i64* %3, align 8
  %23909 = add i64 %23908, 4
  store i64 %23909, i64* %3, align 8
  %23910 = inttoptr i64 %23907 to i64*
  %23911 = load i64, i64* %23910, align 8
  %23912 = add i64 %23911, 45448
  store i64 %23912, i64* %RAX.i11582.pre-phi, align 8
  %23913 = icmp ugt i64 %23911, -45449
  %23914 = zext i1 %23913 to i8
  store i8 %23914, i8* %14, align 1
  %23915 = trunc i64 %23912 to i32
  %23916 = and i32 %23915, 255
  %23917 = tail call i32 @llvm.ctpop.i32(i32 %23916)
  %23918 = trunc i32 %23917 to i8
  %23919 = and i8 %23918, 1
  %23920 = xor i8 %23919, 1
  store i8 %23920, i8* %21, align 1
  %23921 = xor i64 %23912, %23911
  %23922 = lshr i64 %23921, 4
  %23923 = trunc i64 %23922 to i8
  %23924 = and i8 %23923, 1
  store i8 %23924, i8* %27, align 1
  %23925 = icmp eq i64 %23912, 0
  %23926 = zext i1 %23925 to i8
  store i8 %23926, i8* %30, align 1
  %23927 = lshr i64 %23912, 63
  %23928 = trunc i64 %23927 to i8
  store i8 %23928, i8* %33, align 1
  %23929 = lshr i64 %23911, 63
  %23930 = xor i64 %23927, %23929
  %23931 = add nuw nsw i64 %23930, %23927
  %23932 = icmp eq i64 %23931, 2
  %23933 = zext i1 %23932 to i8
  store i8 %23933, i8* %39, align 1
  %23934 = add i64 %23906, -40
  %23935 = add i64 %23908, 14
  store i64 %23935, i64* %3, align 8
  %23936 = inttoptr i64 %23934 to i32*
  %23937 = load i32, i32* %23936, align 4
  %23938 = sext i32 %23937 to i64
  %23939 = mul nsw i64 %23938, 1032
  store i64 %23939, i64* %RCX.i11580, align 8
  %23940 = lshr i64 %23939, 63
  %23941 = add i64 %23939, %23912
  store i64 %23941, i64* %RAX.i11582.pre-phi, align 8
  %23942 = icmp ult i64 %23941, %23912
  %23943 = icmp ult i64 %23941, %23939
  %23944 = or i1 %23942, %23943
  %23945 = zext i1 %23944 to i8
  store i8 %23945, i8* %14, align 1
  %23946 = trunc i64 %23941 to i32
  %23947 = and i32 %23946, 255
  %23948 = tail call i32 @llvm.ctpop.i32(i32 %23947)
  %23949 = trunc i32 %23948 to i8
  %23950 = and i8 %23949, 1
  %23951 = xor i8 %23950, 1
  store i8 %23951, i8* %21, align 1
  %23952 = xor i64 %23939, %23912
  %23953 = xor i64 %23952, %23941
  %23954 = lshr i64 %23953, 4
  %23955 = trunc i64 %23954 to i8
  %23956 = and i8 %23955, 1
  store i8 %23956, i8* %27, align 1
  %23957 = icmp eq i64 %23941, 0
  %23958 = zext i1 %23957 to i8
  store i8 %23958, i8* %30, align 1
  %23959 = lshr i64 %23941, 63
  %23960 = trunc i64 %23959 to i8
  store i8 %23960, i8* %33, align 1
  %23961 = xor i64 %23959, %23927
  %23962 = xor i64 %23959, %23940
  %23963 = add nuw nsw i64 %23961, %23962
  %23964 = icmp eq i64 %23963, 2
  %23965 = zext i1 %23964 to i8
  store i8 %23965, i8* %39, align 1
  %23966 = load i64, i64* %RBP.i, align 8
  %23967 = add i64 %23966, -120
  %23968 = add i64 %23908, 28
  store i64 %23968, i64* %3, align 8
  %23969 = inttoptr i64 %23967 to i64*
  %23970 = load i64, i64* %23969, align 8
  store i64 %23970, i64* %RCX.i11580, align 8
  %23971 = add i64 %23966, -28
  %23972 = add i64 %23908, 31
  store i64 %23972, i64* %3, align 8
  %23973 = inttoptr i64 %23971 to i32*
  %23974 = load i32, i32* %23973, align 4
  %23975 = add i32 %23974, 29
  %23976 = zext i32 %23975 to i64
  store i64 %23976, i64* %576, align 8
  %23977 = icmp ugt i32 %23974, -30
  %23978 = zext i1 %23977 to i8
  store i8 %23978, i8* %14, align 1
  %23979 = and i32 %23975, 255
  %23980 = tail call i32 @llvm.ctpop.i32(i32 %23979)
  %23981 = trunc i32 %23980 to i8
  %23982 = and i8 %23981, 1
  %23983 = xor i8 %23982, 1
  store i8 %23983, i8* %21, align 1
  %23984 = xor i32 %23974, 16
  %23985 = xor i32 %23984, %23975
  %23986 = lshr i32 %23985, 4
  %23987 = trunc i32 %23986 to i8
  %23988 = and i8 %23987, 1
  store i8 %23988, i8* %27, align 1
  %23989 = icmp eq i32 %23975, 0
  %23990 = zext i1 %23989 to i8
  store i8 %23990, i8* %30, align 1
  %23991 = lshr i32 %23975, 31
  %23992 = trunc i32 %23991 to i8
  store i8 %23992, i8* %33, align 1
  %23993 = lshr i32 %23974, 31
  %23994 = xor i32 %23991, %23993
  %23995 = add nuw nsw i32 %23994, %23991
  %23996 = icmp eq i32 %23995, 2
  %23997 = zext i1 %23996 to i8
  store i8 %23997, i8* %39, align 1
  %23998 = sext i32 %23975 to i64
  store i64 %23998, i64* %RSI.i11312, align 8
  %23999 = shl nsw i64 %23998, 1
  %24000 = add i64 %23970, %23999
  %24001 = add i64 %23908, 41
  store i64 %24001, i64* %3, align 8
  %24002 = inttoptr i64 %24000 to i16*
  %24003 = load i16, i16* %24002, align 2
  %24004 = zext i16 %24003 to i64
  store i64 %24004, i64* %576, align 8
  %24005 = zext i16 %24003 to i64
  store i64 %24005, i64* %RCX.i11580, align 8
  %24006 = shl nuw nsw i64 %24005, 2
  %24007 = add i64 %23941, %24006
  %24008 = add i64 %23908, 46
  store i64 %24008, i64* %3, align 8
  %24009 = inttoptr i64 %24007 to i32*
  %24010 = load i32, i32* %24009, align 4
  %24011 = add i32 %24010, 1
  %24012 = zext i32 %24011 to i64
  store i64 %24012, i64* %576, align 8
  %24013 = icmp eq i32 %24010, -1
  %24014 = icmp eq i32 %24011, 0
  %24015 = or i1 %24013, %24014
  %24016 = zext i1 %24015 to i8
  store i8 %24016, i8* %14, align 1
  %24017 = and i32 %24011, 255
  %24018 = tail call i32 @llvm.ctpop.i32(i32 %24017)
  %24019 = trunc i32 %24018 to i8
  %24020 = and i8 %24019, 1
  %24021 = xor i8 %24020, 1
  store i8 %24021, i8* %21, align 1
  %24022 = xor i32 %24011, %24010
  %24023 = lshr i32 %24022, 4
  %24024 = trunc i32 %24023 to i8
  %24025 = and i8 %24024, 1
  store i8 %24025, i8* %27, align 1
  %24026 = zext i1 %24014 to i8
  store i8 %24026, i8* %30, align 1
  %24027 = lshr i32 %24011, 31
  %24028 = trunc i32 %24027 to i8
  store i8 %24028, i8* %33, align 1
  %24029 = lshr i32 %24010, 31
  %24030 = xor i32 %24027, %24029
  %24031 = add nuw nsw i32 %24030, %24027
  %24032 = icmp eq i32 %24031, 2
  %24033 = zext i1 %24032 to i8
  store i8 %24033, i8* %39, align 1
  %24034 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24035 = add i64 %24006, %24034
  %24036 = add i64 %23908, 52
  store i64 %24036, i64* %3, align 8
  %24037 = inttoptr i64 %24035 to i32*
  store i32 %24011, i32* %24037, align 4
  %24038 = load i64, i64* %RBP.i, align 8
  %24039 = add i64 %24038, -8
  %24040 = load i64, i64* %3, align 8
  %24041 = add i64 %24040, 4
  store i64 %24041, i64* %3, align 8
  %24042 = inttoptr i64 %24039 to i64*
  %24043 = load i64, i64* %24042, align 8
  %24044 = add i64 %24043, 45448
  store i64 %24044, i64* %RAX.i11582.pre-phi, align 8
  %24045 = icmp ugt i64 %24043, -45449
  %24046 = zext i1 %24045 to i8
  store i8 %24046, i8* %14, align 1
  %24047 = trunc i64 %24044 to i32
  %24048 = and i32 %24047, 255
  %24049 = tail call i32 @llvm.ctpop.i32(i32 %24048)
  %24050 = trunc i32 %24049 to i8
  %24051 = and i8 %24050, 1
  %24052 = xor i8 %24051, 1
  store i8 %24052, i8* %21, align 1
  %24053 = xor i64 %24044, %24043
  %24054 = lshr i64 %24053, 4
  %24055 = trunc i64 %24054 to i8
  %24056 = and i8 %24055, 1
  store i8 %24056, i8* %27, align 1
  %24057 = icmp eq i64 %24044, 0
  %24058 = zext i1 %24057 to i8
  store i8 %24058, i8* %30, align 1
  %24059 = lshr i64 %24044, 63
  %24060 = trunc i64 %24059 to i8
  store i8 %24060, i8* %33, align 1
  %24061 = lshr i64 %24043, 63
  %24062 = xor i64 %24059, %24061
  %24063 = add nuw nsw i64 %24062, %24059
  %24064 = icmp eq i64 %24063, 2
  %24065 = zext i1 %24064 to i8
  store i8 %24065, i8* %39, align 1
  %24066 = add i64 %24038, -40
  %24067 = add i64 %24040, 14
  store i64 %24067, i64* %3, align 8
  %24068 = inttoptr i64 %24066 to i32*
  %24069 = load i32, i32* %24068, align 4
  %24070 = sext i32 %24069 to i64
  %24071 = mul nsw i64 %24070, 1032
  store i64 %24071, i64* %RCX.i11580, align 8
  %24072 = lshr i64 %24071, 63
  %24073 = add i64 %24071, %24044
  store i64 %24073, i64* %RAX.i11582.pre-phi, align 8
  %24074 = icmp ult i64 %24073, %24044
  %24075 = icmp ult i64 %24073, %24071
  %24076 = or i1 %24074, %24075
  %24077 = zext i1 %24076 to i8
  store i8 %24077, i8* %14, align 1
  %24078 = trunc i64 %24073 to i32
  %24079 = and i32 %24078, 255
  %24080 = tail call i32 @llvm.ctpop.i32(i32 %24079)
  %24081 = trunc i32 %24080 to i8
  %24082 = and i8 %24081, 1
  %24083 = xor i8 %24082, 1
  store i8 %24083, i8* %21, align 1
  %24084 = xor i64 %24071, %24044
  %24085 = xor i64 %24084, %24073
  %24086 = lshr i64 %24085, 4
  %24087 = trunc i64 %24086 to i8
  %24088 = and i8 %24087, 1
  store i8 %24088, i8* %27, align 1
  %24089 = icmp eq i64 %24073, 0
  %24090 = zext i1 %24089 to i8
  store i8 %24090, i8* %30, align 1
  %24091 = lshr i64 %24073, 63
  %24092 = trunc i64 %24091 to i8
  store i8 %24092, i8* %33, align 1
  %24093 = xor i64 %24091, %24059
  %24094 = xor i64 %24091, %24072
  %24095 = add nuw nsw i64 %24093, %24094
  %24096 = icmp eq i64 %24095, 2
  %24097 = zext i1 %24096 to i8
  store i8 %24097, i8* %39, align 1
  %24098 = load i64, i64* %RBP.i, align 8
  %24099 = add i64 %24098, -120
  %24100 = add i64 %24040, 28
  store i64 %24100, i64* %3, align 8
  %24101 = inttoptr i64 %24099 to i64*
  %24102 = load i64, i64* %24101, align 8
  store i64 %24102, i64* %RCX.i11580, align 8
  %24103 = add i64 %24098, -28
  %24104 = add i64 %24040, 31
  store i64 %24104, i64* %3, align 8
  %24105 = inttoptr i64 %24103 to i32*
  %24106 = load i32, i32* %24105, align 4
  %24107 = add i32 %24106, 30
  %24108 = zext i32 %24107 to i64
  store i64 %24108, i64* %576, align 8
  %24109 = icmp ugt i32 %24106, -31
  %24110 = zext i1 %24109 to i8
  store i8 %24110, i8* %14, align 1
  %24111 = and i32 %24107, 255
  %24112 = tail call i32 @llvm.ctpop.i32(i32 %24111)
  %24113 = trunc i32 %24112 to i8
  %24114 = and i8 %24113, 1
  %24115 = xor i8 %24114, 1
  store i8 %24115, i8* %21, align 1
  %24116 = xor i32 %24106, 16
  %24117 = xor i32 %24116, %24107
  %24118 = lshr i32 %24117, 4
  %24119 = trunc i32 %24118 to i8
  %24120 = and i8 %24119, 1
  store i8 %24120, i8* %27, align 1
  %24121 = icmp eq i32 %24107, 0
  %24122 = zext i1 %24121 to i8
  store i8 %24122, i8* %30, align 1
  %24123 = lshr i32 %24107, 31
  %24124 = trunc i32 %24123 to i8
  store i8 %24124, i8* %33, align 1
  %24125 = lshr i32 %24106, 31
  %24126 = xor i32 %24123, %24125
  %24127 = add nuw nsw i32 %24126, %24123
  %24128 = icmp eq i32 %24127, 2
  %24129 = zext i1 %24128 to i8
  store i8 %24129, i8* %39, align 1
  %24130 = sext i32 %24107 to i64
  store i64 %24130, i64* %RSI.i11312, align 8
  %24131 = shl nsw i64 %24130, 1
  %24132 = add i64 %24102, %24131
  %24133 = add i64 %24040, 41
  store i64 %24133, i64* %3, align 8
  %24134 = inttoptr i64 %24132 to i16*
  %24135 = load i16, i16* %24134, align 2
  %24136 = zext i16 %24135 to i64
  store i64 %24136, i64* %576, align 8
  %24137 = zext i16 %24135 to i64
  store i64 %24137, i64* %RCX.i11580, align 8
  %24138 = shl nuw nsw i64 %24137, 2
  %24139 = add i64 %24073, %24138
  %24140 = add i64 %24040, 46
  store i64 %24140, i64* %3, align 8
  %24141 = inttoptr i64 %24139 to i32*
  %24142 = load i32, i32* %24141, align 4
  %24143 = add i32 %24142, 1
  %24144 = zext i32 %24143 to i64
  store i64 %24144, i64* %576, align 8
  %24145 = icmp eq i32 %24142, -1
  %24146 = icmp eq i32 %24143, 0
  %24147 = or i1 %24145, %24146
  %24148 = zext i1 %24147 to i8
  store i8 %24148, i8* %14, align 1
  %24149 = and i32 %24143, 255
  %24150 = tail call i32 @llvm.ctpop.i32(i32 %24149)
  %24151 = trunc i32 %24150 to i8
  %24152 = and i8 %24151, 1
  %24153 = xor i8 %24152, 1
  store i8 %24153, i8* %21, align 1
  %24154 = xor i32 %24143, %24142
  %24155 = lshr i32 %24154, 4
  %24156 = trunc i32 %24155 to i8
  %24157 = and i8 %24156, 1
  store i8 %24157, i8* %27, align 1
  %24158 = zext i1 %24146 to i8
  store i8 %24158, i8* %30, align 1
  %24159 = lshr i32 %24143, 31
  %24160 = trunc i32 %24159 to i8
  store i8 %24160, i8* %33, align 1
  %24161 = lshr i32 %24142, 31
  %24162 = xor i32 %24159, %24161
  %24163 = add nuw nsw i32 %24162, %24159
  %24164 = icmp eq i32 %24163, 2
  %24165 = zext i1 %24164 to i8
  store i8 %24165, i8* %39, align 1
  %24166 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24167 = add i64 %24138, %24166
  %24168 = add i64 %24040, 52
  store i64 %24168, i64* %3, align 8
  %24169 = inttoptr i64 %24167 to i32*
  store i32 %24143, i32* %24169, align 4
  %24170 = load i64, i64* %RBP.i, align 8
  %24171 = add i64 %24170, -8
  %24172 = load i64, i64* %3, align 8
  %24173 = add i64 %24172, 4
  store i64 %24173, i64* %3, align 8
  %24174 = inttoptr i64 %24171 to i64*
  %24175 = load i64, i64* %24174, align 8
  %24176 = add i64 %24175, 45448
  store i64 %24176, i64* %RAX.i11582.pre-phi, align 8
  %24177 = icmp ugt i64 %24175, -45449
  %24178 = zext i1 %24177 to i8
  store i8 %24178, i8* %14, align 1
  %24179 = trunc i64 %24176 to i32
  %24180 = and i32 %24179, 255
  %24181 = tail call i32 @llvm.ctpop.i32(i32 %24180)
  %24182 = trunc i32 %24181 to i8
  %24183 = and i8 %24182, 1
  %24184 = xor i8 %24183, 1
  store i8 %24184, i8* %21, align 1
  %24185 = xor i64 %24176, %24175
  %24186 = lshr i64 %24185, 4
  %24187 = trunc i64 %24186 to i8
  %24188 = and i8 %24187, 1
  store i8 %24188, i8* %27, align 1
  %24189 = icmp eq i64 %24176, 0
  %24190 = zext i1 %24189 to i8
  store i8 %24190, i8* %30, align 1
  %24191 = lshr i64 %24176, 63
  %24192 = trunc i64 %24191 to i8
  store i8 %24192, i8* %33, align 1
  %24193 = lshr i64 %24175, 63
  %24194 = xor i64 %24191, %24193
  %24195 = add nuw nsw i64 %24194, %24191
  %24196 = icmp eq i64 %24195, 2
  %24197 = zext i1 %24196 to i8
  store i8 %24197, i8* %39, align 1
  %24198 = add i64 %24170, -40
  %24199 = add i64 %24172, 14
  store i64 %24199, i64* %3, align 8
  %24200 = inttoptr i64 %24198 to i32*
  %24201 = load i32, i32* %24200, align 4
  %24202 = sext i32 %24201 to i64
  %24203 = mul nsw i64 %24202, 1032
  store i64 %24203, i64* %RCX.i11580, align 8
  %24204 = lshr i64 %24203, 63
  %24205 = add i64 %24203, %24176
  store i64 %24205, i64* %RAX.i11582.pre-phi, align 8
  %24206 = icmp ult i64 %24205, %24176
  %24207 = icmp ult i64 %24205, %24203
  %24208 = or i1 %24206, %24207
  %24209 = zext i1 %24208 to i8
  store i8 %24209, i8* %14, align 1
  %24210 = trunc i64 %24205 to i32
  %24211 = and i32 %24210, 255
  %24212 = tail call i32 @llvm.ctpop.i32(i32 %24211)
  %24213 = trunc i32 %24212 to i8
  %24214 = and i8 %24213, 1
  %24215 = xor i8 %24214, 1
  store i8 %24215, i8* %21, align 1
  %24216 = xor i64 %24203, %24176
  %24217 = xor i64 %24216, %24205
  %24218 = lshr i64 %24217, 4
  %24219 = trunc i64 %24218 to i8
  %24220 = and i8 %24219, 1
  store i8 %24220, i8* %27, align 1
  %24221 = icmp eq i64 %24205, 0
  %24222 = zext i1 %24221 to i8
  store i8 %24222, i8* %30, align 1
  %24223 = lshr i64 %24205, 63
  %24224 = trunc i64 %24223 to i8
  store i8 %24224, i8* %33, align 1
  %24225 = xor i64 %24223, %24191
  %24226 = xor i64 %24223, %24204
  %24227 = add nuw nsw i64 %24225, %24226
  %24228 = icmp eq i64 %24227, 2
  %24229 = zext i1 %24228 to i8
  store i8 %24229, i8* %39, align 1
  %24230 = load i64, i64* %RBP.i, align 8
  %24231 = add i64 %24230, -120
  %24232 = add i64 %24172, 28
  store i64 %24232, i64* %3, align 8
  %24233 = inttoptr i64 %24231 to i64*
  %24234 = load i64, i64* %24233, align 8
  store i64 %24234, i64* %RCX.i11580, align 8
  %24235 = add i64 %24230, -28
  %24236 = add i64 %24172, 31
  store i64 %24236, i64* %3, align 8
  %24237 = inttoptr i64 %24235 to i32*
  %24238 = load i32, i32* %24237, align 4
  %24239 = add i32 %24238, 31
  %24240 = zext i32 %24239 to i64
  store i64 %24240, i64* %576, align 8
  %24241 = icmp ugt i32 %24238, -32
  %24242 = zext i1 %24241 to i8
  store i8 %24242, i8* %14, align 1
  %24243 = and i32 %24239, 255
  %24244 = tail call i32 @llvm.ctpop.i32(i32 %24243)
  %24245 = trunc i32 %24244 to i8
  %24246 = and i8 %24245, 1
  %24247 = xor i8 %24246, 1
  store i8 %24247, i8* %21, align 1
  %24248 = xor i32 %24238, 16
  %24249 = xor i32 %24248, %24239
  %24250 = lshr i32 %24249, 4
  %24251 = trunc i32 %24250 to i8
  %24252 = and i8 %24251, 1
  store i8 %24252, i8* %27, align 1
  %24253 = icmp eq i32 %24239, 0
  %24254 = zext i1 %24253 to i8
  store i8 %24254, i8* %30, align 1
  %24255 = lshr i32 %24239, 31
  %24256 = trunc i32 %24255 to i8
  store i8 %24256, i8* %33, align 1
  %24257 = lshr i32 %24238, 31
  %24258 = xor i32 %24255, %24257
  %24259 = add nuw nsw i32 %24258, %24255
  %24260 = icmp eq i32 %24259, 2
  %24261 = zext i1 %24260 to i8
  store i8 %24261, i8* %39, align 1
  %24262 = sext i32 %24239 to i64
  store i64 %24262, i64* %RSI.i11312, align 8
  %24263 = shl nsw i64 %24262, 1
  %24264 = add i64 %24234, %24263
  %24265 = add i64 %24172, 41
  store i64 %24265, i64* %3, align 8
  %24266 = inttoptr i64 %24264 to i16*
  %24267 = load i16, i16* %24266, align 2
  %24268 = zext i16 %24267 to i64
  store i64 %24268, i64* %576, align 8
  %24269 = zext i16 %24267 to i64
  store i64 %24269, i64* %RCX.i11580, align 8
  %24270 = shl nuw nsw i64 %24269, 2
  %24271 = add i64 %24205, %24270
  %24272 = add i64 %24172, 46
  store i64 %24272, i64* %3, align 8
  %24273 = inttoptr i64 %24271 to i32*
  %24274 = load i32, i32* %24273, align 4
  %24275 = add i32 %24274, 1
  %24276 = zext i32 %24275 to i64
  store i64 %24276, i64* %576, align 8
  %24277 = icmp eq i32 %24274, -1
  %24278 = icmp eq i32 %24275, 0
  %24279 = or i1 %24277, %24278
  %24280 = zext i1 %24279 to i8
  store i8 %24280, i8* %14, align 1
  %24281 = and i32 %24275, 255
  %24282 = tail call i32 @llvm.ctpop.i32(i32 %24281)
  %24283 = trunc i32 %24282 to i8
  %24284 = and i8 %24283, 1
  %24285 = xor i8 %24284, 1
  store i8 %24285, i8* %21, align 1
  %24286 = xor i32 %24275, %24274
  %24287 = lshr i32 %24286, 4
  %24288 = trunc i32 %24287 to i8
  %24289 = and i8 %24288, 1
  store i8 %24289, i8* %27, align 1
  %24290 = zext i1 %24278 to i8
  store i8 %24290, i8* %30, align 1
  %24291 = lshr i32 %24275, 31
  %24292 = trunc i32 %24291 to i8
  store i8 %24292, i8* %33, align 1
  %24293 = lshr i32 %24274, 31
  %24294 = xor i32 %24291, %24293
  %24295 = add nuw nsw i32 %24294, %24291
  %24296 = icmp eq i32 %24295, 2
  %24297 = zext i1 %24296 to i8
  store i8 %24297, i8* %39, align 1
  %24298 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24299 = add i64 %24270, %24298
  %24300 = add i64 %24172, 52
  store i64 %24300, i64* %3, align 8
  %24301 = inttoptr i64 %24299 to i32*
  store i32 %24275, i32* %24301, align 4
  %24302 = load i64, i64* %RBP.i, align 8
  %24303 = add i64 %24302, -8
  %24304 = load i64, i64* %3, align 8
  %24305 = add i64 %24304, 4
  store i64 %24305, i64* %3, align 8
  %24306 = inttoptr i64 %24303 to i64*
  %24307 = load i64, i64* %24306, align 8
  %24308 = add i64 %24307, 45448
  store i64 %24308, i64* %RAX.i11582.pre-phi, align 8
  %24309 = icmp ugt i64 %24307, -45449
  %24310 = zext i1 %24309 to i8
  store i8 %24310, i8* %14, align 1
  %24311 = trunc i64 %24308 to i32
  %24312 = and i32 %24311, 255
  %24313 = tail call i32 @llvm.ctpop.i32(i32 %24312)
  %24314 = trunc i32 %24313 to i8
  %24315 = and i8 %24314, 1
  %24316 = xor i8 %24315, 1
  store i8 %24316, i8* %21, align 1
  %24317 = xor i64 %24308, %24307
  %24318 = lshr i64 %24317, 4
  %24319 = trunc i64 %24318 to i8
  %24320 = and i8 %24319, 1
  store i8 %24320, i8* %27, align 1
  %24321 = icmp eq i64 %24308, 0
  %24322 = zext i1 %24321 to i8
  store i8 %24322, i8* %30, align 1
  %24323 = lshr i64 %24308, 63
  %24324 = trunc i64 %24323 to i8
  store i8 %24324, i8* %33, align 1
  %24325 = lshr i64 %24307, 63
  %24326 = xor i64 %24323, %24325
  %24327 = add nuw nsw i64 %24326, %24323
  %24328 = icmp eq i64 %24327, 2
  %24329 = zext i1 %24328 to i8
  store i8 %24329, i8* %39, align 1
  %24330 = add i64 %24302, -40
  %24331 = add i64 %24304, 14
  store i64 %24331, i64* %3, align 8
  %24332 = inttoptr i64 %24330 to i32*
  %24333 = load i32, i32* %24332, align 4
  %24334 = sext i32 %24333 to i64
  %24335 = mul nsw i64 %24334, 1032
  store i64 %24335, i64* %RCX.i11580, align 8
  %24336 = lshr i64 %24335, 63
  %24337 = add i64 %24335, %24308
  store i64 %24337, i64* %RAX.i11582.pre-phi, align 8
  %24338 = icmp ult i64 %24337, %24308
  %24339 = icmp ult i64 %24337, %24335
  %24340 = or i1 %24338, %24339
  %24341 = zext i1 %24340 to i8
  store i8 %24341, i8* %14, align 1
  %24342 = trunc i64 %24337 to i32
  %24343 = and i32 %24342, 255
  %24344 = tail call i32 @llvm.ctpop.i32(i32 %24343)
  %24345 = trunc i32 %24344 to i8
  %24346 = and i8 %24345, 1
  %24347 = xor i8 %24346, 1
  store i8 %24347, i8* %21, align 1
  %24348 = xor i64 %24335, %24308
  %24349 = xor i64 %24348, %24337
  %24350 = lshr i64 %24349, 4
  %24351 = trunc i64 %24350 to i8
  %24352 = and i8 %24351, 1
  store i8 %24352, i8* %27, align 1
  %24353 = icmp eq i64 %24337, 0
  %24354 = zext i1 %24353 to i8
  store i8 %24354, i8* %30, align 1
  %24355 = lshr i64 %24337, 63
  %24356 = trunc i64 %24355 to i8
  store i8 %24356, i8* %33, align 1
  %24357 = xor i64 %24355, %24323
  %24358 = xor i64 %24355, %24336
  %24359 = add nuw nsw i64 %24357, %24358
  %24360 = icmp eq i64 %24359, 2
  %24361 = zext i1 %24360 to i8
  store i8 %24361, i8* %39, align 1
  %24362 = load i64, i64* %RBP.i, align 8
  %24363 = add i64 %24362, -120
  %24364 = add i64 %24304, 28
  store i64 %24364, i64* %3, align 8
  %24365 = inttoptr i64 %24363 to i64*
  %24366 = load i64, i64* %24365, align 8
  store i64 %24366, i64* %RCX.i11580, align 8
  %24367 = add i64 %24362, -28
  %24368 = add i64 %24304, 31
  store i64 %24368, i64* %3, align 8
  %24369 = inttoptr i64 %24367 to i32*
  %24370 = load i32, i32* %24369, align 4
  %24371 = add i32 %24370, 32
  %24372 = zext i32 %24371 to i64
  store i64 %24372, i64* %576, align 8
  %24373 = icmp ugt i32 %24370, -33
  %24374 = zext i1 %24373 to i8
  store i8 %24374, i8* %14, align 1
  %24375 = and i32 %24371, 255
  %24376 = tail call i32 @llvm.ctpop.i32(i32 %24375)
  %24377 = trunc i32 %24376 to i8
  %24378 = and i8 %24377, 1
  %24379 = xor i8 %24378, 1
  store i8 %24379, i8* %21, align 1
  %24380 = xor i32 %24371, %24370
  %24381 = lshr i32 %24380, 4
  %24382 = trunc i32 %24381 to i8
  %24383 = and i8 %24382, 1
  store i8 %24383, i8* %27, align 1
  %24384 = icmp eq i32 %24371, 0
  %24385 = zext i1 %24384 to i8
  store i8 %24385, i8* %30, align 1
  %24386 = lshr i32 %24371, 31
  %24387 = trunc i32 %24386 to i8
  store i8 %24387, i8* %33, align 1
  %24388 = lshr i32 %24370, 31
  %24389 = xor i32 %24386, %24388
  %24390 = add nuw nsw i32 %24389, %24386
  %24391 = icmp eq i32 %24390, 2
  %24392 = zext i1 %24391 to i8
  store i8 %24392, i8* %39, align 1
  %24393 = sext i32 %24371 to i64
  store i64 %24393, i64* %RSI.i11312, align 8
  %24394 = shl nsw i64 %24393, 1
  %24395 = add i64 %24366, %24394
  %24396 = add i64 %24304, 41
  store i64 %24396, i64* %3, align 8
  %24397 = inttoptr i64 %24395 to i16*
  %24398 = load i16, i16* %24397, align 2
  %24399 = zext i16 %24398 to i64
  store i64 %24399, i64* %576, align 8
  %24400 = zext i16 %24398 to i64
  store i64 %24400, i64* %RCX.i11580, align 8
  %24401 = shl nuw nsw i64 %24400, 2
  %24402 = add i64 %24337, %24401
  %24403 = add i64 %24304, 46
  store i64 %24403, i64* %3, align 8
  %24404 = inttoptr i64 %24402 to i32*
  %24405 = load i32, i32* %24404, align 4
  %24406 = add i32 %24405, 1
  %24407 = zext i32 %24406 to i64
  store i64 %24407, i64* %576, align 8
  %24408 = icmp eq i32 %24405, -1
  %24409 = icmp eq i32 %24406, 0
  %24410 = or i1 %24408, %24409
  %24411 = zext i1 %24410 to i8
  store i8 %24411, i8* %14, align 1
  %24412 = and i32 %24406, 255
  %24413 = tail call i32 @llvm.ctpop.i32(i32 %24412)
  %24414 = trunc i32 %24413 to i8
  %24415 = and i8 %24414, 1
  %24416 = xor i8 %24415, 1
  store i8 %24416, i8* %21, align 1
  %24417 = xor i32 %24406, %24405
  %24418 = lshr i32 %24417, 4
  %24419 = trunc i32 %24418 to i8
  %24420 = and i8 %24419, 1
  store i8 %24420, i8* %27, align 1
  %24421 = zext i1 %24409 to i8
  store i8 %24421, i8* %30, align 1
  %24422 = lshr i32 %24406, 31
  %24423 = trunc i32 %24422 to i8
  store i8 %24423, i8* %33, align 1
  %24424 = lshr i32 %24405, 31
  %24425 = xor i32 %24422, %24424
  %24426 = add nuw nsw i32 %24425, %24422
  %24427 = icmp eq i32 %24426, 2
  %24428 = zext i1 %24427 to i8
  store i8 %24428, i8* %39, align 1
  %24429 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24430 = add i64 %24401, %24429
  %24431 = add i64 %24304, 52
  store i64 %24431, i64* %3, align 8
  %24432 = inttoptr i64 %24430 to i32*
  store i32 %24406, i32* %24432, align 4
  %24433 = load i64, i64* %RBP.i, align 8
  %24434 = add i64 %24433, -8
  %24435 = load i64, i64* %3, align 8
  %24436 = add i64 %24435, 4
  store i64 %24436, i64* %3, align 8
  %24437 = inttoptr i64 %24434 to i64*
  %24438 = load i64, i64* %24437, align 8
  %24439 = add i64 %24438, 45448
  store i64 %24439, i64* %RAX.i11582.pre-phi, align 8
  %24440 = icmp ugt i64 %24438, -45449
  %24441 = zext i1 %24440 to i8
  store i8 %24441, i8* %14, align 1
  %24442 = trunc i64 %24439 to i32
  %24443 = and i32 %24442, 255
  %24444 = tail call i32 @llvm.ctpop.i32(i32 %24443)
  %24445 = trunc i32 %24444 to i8
  %24446 = and i8 %24445, 1
  %24447 = xor i8 %24446, 1
  store i8 %24447, i8* %21, align 1
  %24448 = xor i64 %24439, %24438
  %24449 = lshr i64 %24448, 4
  %24450 = trunc i64 %24449 to i8
  %24451 = and i8 %24450, 1
  store i8 %24451, i8* %27, align 1
  %24452 = icmp eq i64 %24439, 0
  %24453 = zext i1 %24452 to i8
  store i8 %24453, i8* %30, align 1
  %24454 = lshr i64 %24439, 63
  %24455 = trunc i64 %24454 to i8
  store i8 %24455, i8* %33, align 1
  %24456 = lshr i64 %24438, 63
  %24457 = xor i64 %24454, %24456
  %24458 = add nuw nsw i64 %24457, %24454
  %24459 = icmp eq i64 %24458, 2
  %24460 = zext i1 %24459 to i8
  store i8 %24460, i8* %39, align 1
  %24461 = add i64 %24433, -40
  %24462 = add i64 %24435, 14
  store i64 %24462, i64* %3, align 8
  %24463 = inttoptr i64 %24461 to i32*
  %24464 = load i32, i32* %24463, align 4
  %24465 = sext i32 %24464 to i64
  %24466 = mul nsw i64 %24465, 1032
  store i64 %24466, i64* %RCX.i11580, align 8
  %24467 = lshr i64 %24466, 63
  %24468 = add i64 %24466, %24439
  store i64 %24468, i64* %RAX.i11582.pre-phi, align 8
  %24469 = icmp ult i64 %24468, %24439
  %24470 = icmp ult i64 %24468, %24466
  %24471 = or i1 %24469, %24470
  %24472 = zext i1 %24471 to i8
  store i8 %24472, i8* %14, align 1
  %24473 = trunc i64 %24468 to i32
  %24474 = and i32 %24473, 255
  %24475 = tail call i32 @llvm.ctpop.i32(i32 %24474)
  %24476 = trunc i32 %24475 to i8
  %24477 = and i8 %24476, 1
  %24478 = xor i8 %24477, 1
  store i8 %24478, i8* %21, align 1
  %24479 = xor i64 %24466, %24439
  %24480 = xor i64 %24479, %24468
  %24481 = lshr i64 %24480, 4
  %24482 = trunc i64 %24481 to i8
  %24483 = and i8 %24482, 1
  store i8 %24483, i8* %27, align 1
  %24484 = icmp eq i64 %24468, 0
  %24485 = zext i1 %24484 to i8
  store i8 %24485, i8* %30, align 1
  %24486 = lshr i64 %24468, 63
  %24487 = trunc i64 %24486 to i8
  store i8 %24487, i8* %33, align 1
  %24488 = xor i64 %24486, %24454
  %24489 = xor i64 %24486, %24467
  %24490 = add nuw nsw i64 %24488, %24489
  %24491 = icmp eq i64 %24490, 2
  %24492 = zext i1 %24491 to i8
  store i8 %24492, i8* %39, align 1
  %24493 = load i64, i64* %RBP.i, align 8
  %24494 = add i64 %24493, -120
  %24495 = add i64 %24435, 28
  store i64 %24495, i64* %3, align 8
  %24496 = inttoptr i64 %24494 to i64*
  %24497 = load i64, i64* %24496, align 8
  store i64 %24497, i64* %RCX.i11580, align 8
  %24498 = add i64 %24493, -28
  %24499 = add i64 %24435, 31
  store i64 %24499, i64* %3, align 8
  %24500 = inttoptr i64 %24498 to i32*
  %24501 = load i32, i32* %24500, align 4
  %24502 = add i32 %24501, 33
  %24503 = zext i32 %24502 to i64
  store i64 %24503, i64* %576, align 8
  %24504 = icmp ugt i32 %24501, -34
  %24505 = zext i1 %24504 to i8
  store i8 %24505, i8* %14, align 1
  %24506 = and i32 %24502, 255
  %24507 = tail call i32 @llvm.ctpop.i32(i32 %24506)
  %24508 = trunc i32 %24507 to i8
  %24509 = and i8 %24508, 1
  %24510 = xor i8 %24509, 1
  store i8 %24510, i8* %21, align 1
  %24511 = xor i32 %24502, %24501
  %24512 = lshr i32 %24511, 4
  %24513 = trunc i32 %24512 to i8
  %24514 = and i8 %24513, 1
  store i8 %24514, i8* %27, align 1
  %24515 = icmp eq i32 %24502, 0
  %24516 = zext i1 %24515 to i8
  store i8 %24516, i8* %30, align 1
  %24517 = lshr i32 %24502, 31
  %24518 = trunc i32 %24517 to i8
  store i8 %24518, i8* %33, align 1
  %24519 = lshr i32 %24501, 31
  %24520 = xor i32 %24517, %24519
  %24521 = add nuw nsw i32 %24520, %24517
  %24522 = icmp eq i32 %24521, 2
  %24523 = zext i1 %24522 to i8
  store i8 %24523, i8* %39, align 1
  %24524 = sext i32 %24502 to i64
  store i64 %24524, i64* %RSI.i11312, align 8
  %24525 = shl nsw i64 %24524, 1
  %24526 = add i64 %24497, %24525
  %24527 = add i64 %24435, 41
  store i64 %24527, i64* %3, align 8
  %24528 = inttoptr i64 %24526 to i16*
  %24529 = load i16, i16* %24528, align 2
  %24530 = zext i16 %24529 to i64
  store i64 %24530, i64* %576, align 8
  %24531 = zext i16 %24529 to i64
  store i64 %24531, i64* %RCX.i11580, align 8
  %24532 = shl nuw nsw i64 %24531, 2
  %24533 = add i64 %24468, %24532
  %24534 = add i64 %24435, 46
  store i64 %24534, i64* %3, align 8
  %24535 = inttoptr i64 %24533 to i32*
  %24536 = load i32, i32* %24535, align 4
  %24537 = add i32 %24536, 1
  %24538 = zext i32 %24537 to i64
  store i64 %24538, i64* %576, align 8
  %24539 = icmp eq i32 %24536, -1
  %24540 = icmp eq i32 %24537, 0
  %24541 = or i1 %24539, %24540
  %24542 = zext i1 %24541 to i8
  store i8 %24542, i8* %14, align 1
  %24543 = and i32 %24537, 255
  %24544 = tail call i32 @llvm.ctpop.i32(i32 %24543)
  %24545 = trunc i32 %24544 to i8
  %24546 = and i8 %24545, 1
  %24547 = xor i8 %24546, 1
  store i8 %24547, i8* %21, align 1
  %24548 = xor i32 %24537, %24536
  %24549 = lshr i32 %24548, 4
  %24550 = trunc i32 %24549 to i8
  %24551 = and i8 %24550, 1
  store i8 %24551, i8* %27, align 1
  %24552 = zext i1 %24540 to i8
  store i8 %24552, i8* %30, align 1
  %24553 = lshr i32 %24537, 31
  %24554 = trunc i32 %24553 to i8
  store i8 %24554, i8* %33, align 1
  %24555 = lshr i32 %24536, 31
  %24556 = xor i32 %24553, %24555
  %24557 = add nuw nsw i32 %24556, %24553
  %24558 = icmp eq i32 %24557, 2
  %24559 = zext i1 %24558 to i8
  store i8 %24559, i8* %39, align 1
  %24560 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24561 = add i64 %24532, %24560
  %24562 = add i64 %24435, 52
  store i64 %24562, i64* %3, align 8
  %24563 = inttoptr i64 %24561 to i32*
  store i32 %24537, i32* %24563, align 4
  %24564 = load i64, i64* %RBP.i, align 8
  %24565 = add i64 %24564, -8
  %24566 = load i64, i64* %3, align 8
  %24567 = add i64 %24566, 4
  store i64 %24567, i64* %3, align 8
  %24568 = inttoptr i64 %24565 to i64*
  %24569 = load i64, i64* %24568, align 8
  %24570 = add i64 %24569, 45448
  store i64 %24570, i64* %RAX.i11582.pre-phi, align 8
  %24571 = icmp ugt i64 %24569, -45449
  %24572 = zext i1 %24571 to i8
  store i8 %24572, i8* %14, align 1
  %24573 = trunc i64 %24570 to i32
  %24574 = and i32 %24573, 255
  %24575 = tail call i32 @llvm.ctpop.i32(i32 %24574)
  %24576 = trunc i32 %24575 to i8
  %24577 = and i8 %24576, 1
  %24578 = xor i8 %24577, 1
  store i8 %24578, i8* %21, align 1
  %24579 = xor i64 %24570, %24569
  %24580 = lshr i64 %24579, 4
  %24581 = trunc i64 %24580 to i8
  %24582 = and i8 %24581, 1
  store i8 %24582, i8* %27, align 1
  %24583 = icmp eq i64 %24570, 0
  %24584 = zext i1 %24583 to i8
  store i8 %24584, i8* %30, align 1
  %24585 = lshr i64 %24570, 63
  %24586 = trunc i64 %24585 to i8
  store i8 %24586, i8* %33, align 1
  %24587 = lshr i64 %24569, 63
  %24588 = xor i64 %24585, %24587
  %24589 = add nuw nsw i64 %24588, %24585
  %24590 = icmp eq i64 %24589, 2
  %24591 = zext i1 %24590 to i8
  store i8 %24591, i8* %39, align 1
  %24592 = add i64 %24564, -40
  %24593 = add i64 %24566, 14
  store i64 %24593, i64* %3, align 8
  %24594 = inttoptr i64 %24592 to i32*
  %24595 = load i32, i32* %24594, align 4
  %24596 = sext i32 %24595 to i64
  %24597 = mul nsw i64 %24596, 1032
  store i64 %24597, i64* %RCX.i11580, align 8
  %24598 = lshr i64 %24597, 63
  %24599 = add i64 %24597, %24570
  store i64 %24599, i64* %RAX.i11582.pre-phi, align 8
  %24600 = icmp ult i64 %24599, %24570
  %24601 = icmp ult i64 %24599, %24597
  %24602 = or i1 %24600, %24601
  %24603 = zext i1 %24602 to i8
  store i8 %24603, i8* %14, align 1
  %24604 = trunc i64 %24599 to i32
  %24605 = and i32 %24604, 255
  %24606 = tail call i32 @llvm.ctpop.i32(i32 %24605)
  %24607 = trunc i32 %24606 to i8
  %24608 = and i8 %24607, 1
  %24609 = xor i8 %24608, 1
  store i8 %24609, i8* %21, align 1
  %24610 = xor i64 %24597, %24570
  %24611 = xor i64 %24610, %24599
  %24612 = lshr i64 %24611, 4
  %24613 = trunc i64 %24612 to i8
  %24614 = and i8 %24613, 1
  store i8 %24614, i8* %27, align 1
  %24615 = icmp eq i64 %24599, 0
  %24616 = zext i1 %24615 to i8
  store i8 %24616, i8* %30, align 1
  %24617 = lshr i64 %24599, 63
  %24618 = trunc i64 %24617 to i8
  store i8 %24618, i8* %33, align 1
  %24619 = xor i64 %24617, %24585
  %24620 = xor i64 %24617, %24598
  %24621 = add nuw nsw i64 %24619, %24620
  %24622 = icmp eq i64 %24621, 2
  %24623 = zext i1 %24622 to i8
  store i8 %24623, i8* %39, align 1
  %24624 = load i64, i64* %RBP.i, align 8
  %24625 = add i64 %24624, -120
  %24626 = add i64 %24566, 28
  store i64 %24626, i64* %3, align 8
  %24627 = inttoptr i64 %24625 to i64*
  %24628 = load i64, i64* %24627, align 8
  store i64 %24628, i64* %RCX.i11580, align 8
  %24629 = add i64 %24624, -28
  %24630 = add i64 %24566, 31
  store i64 %24630, i64* %3, align 8
  %24631 = inttoptr i64 %24629 to i32*
  %24632 = load i32, i32* %24631, align 4
  %24633 = add i32 %24632, 34
  %24634 = zext i32 %24633 to i64
  store i64 %24634, i64* %576, align 8
  %24635 = icmp ugt i32 %24632, -35
  %24636 = zext i1 %24635 to i8
  store i8 %24636, i8* %14, align 1
  %24637 = and i32 %24633, 255
  %24638 = tail call i32 @llvm.ctpop.i32(i32 %24637)
  %24639 = trunc i32 %24638 to i8
  %24640 = and i8 %24639, 1
  %24641 = xor i8 %24640, 1
  store i8 %24641, i8* %21, align 1
  %24642 = xor i32 %24633, %24632
  %24643 = lshr i32 %24642, 4
  %24644 = trunc i32 %24643 to i8
  %24645 = and i8 %24644, 1
  store i8 %24645, i8* %27, align 1
  %24646 = icmp eq i32 %24633, 0
  %24647 = zext i1 %24646 to i8
  store i8 %24647, i8* %30, align 1
  %24648 = lshr i32 %24633, 31
  %24649 = trunc i32 %24648 to i8
  store i8 %24649, i8* %33, align 1
  %24650 = lshr i32 %24632, 31
  %24651 = xor i32 %24648, %24650
  %24652 = add nuw nsw i32 %24651, %24648
  %24653 = icmp eq i32 %24652, 2
  %24654 = zext i1 %24653 to i8
  store i8 %24654, i8* %39, align 1
  %24655 = sext i32 %24633 to i64
  store i64 %24655, i64* %RSI.i11312, align 8
  %24656 = shl nsw i64 %24655, 1
  %24657 = add i64 %24628, %24656
  %24658 = add i64 %24566, 41
  store i64 %24658, i64* %3, align 8
  %24659 = inttoptr i64 %24657 to i16*
  %24660 = load i16, i16* %24659, align 2
  %24661 = zext i16 %24660 to i64
  store i64 %24661, i64* %576, align 8
  %24662 = zext i16 %24660 to i64
  store i64 %24662, i64* %RCX.i11580, align 8
  %24663 = shl nuw nsw i64 %24662, 2
  %24664 = add i64 %24599, %24663
  %24665 = add i64 %24566, 46
  store i64 %24665, i64* %3, align 8
  %24666 = inttoptr i64 %24664 to i32*
  %24667 = load i32, i32* %24666, align 4
  %24668 = add i32 %24667, 1
  %24669 = zext i32 %24668 to i64
  store i64 %24669, i64* %576, align 8
  %24670 = icmp eq i32 %24667, -1
  %24671 = icmp eq i32 %24668, 0
  %24672 = or i1 %24670, %24671
  %24673 = zext i1 %24672 to i8
  store i8 %24673, i8* %14, align 1
  %24674 = and i32 %24668, 255
  %24675 = tail call i32 @llvm.ctpop.i32(i32 %24674)
  %24676 = trunc i32 %24675 to i8
  %24677 = and i8 %24676, 1
  %24678 = xor i8 %24677, 1
  store i8 %24678, i8* %21, align 1
  %24679 = xor i32 %24668, %24667
  %24680 = lshr i32 %24679, 4
  %24681 = trunc i32 %24680 to i8
  %24682 = and i8 %24681, 1
  store i8 %24682, i8* %27, align 1
  %24683 = zext i1 %24671 to i8
  store i8 %24683, i8* %30, align 1
  %24684 = lshr i32 %24668, 31
  %24685 = trunc i32 %24684 to i8
  store i8 %24685, i8* %33, align 1
  %24686 = lshr i32 %24667, 31
  %24687 = xor i32 %24684, %24686
  %24688 = add nuw nsw i32 %24687, %24684
  %24689 = icmp eq i32 %24688, 2
  %24690 = zext i1 %24689 to i8
  store i8 %24690, i8* %39, align 1
  %24691 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24692 = add i64 %24663, %24691
  %24693 = add i64 %24566, 52
  store i64 %24693, i64* %3, align 8
  %24694 = inttoptr i64 %24692 to i32*
  store i32 %24668, i32* %24694, align 4
  %24695 = load i64, i64* %RBP.i, align 8
  %24696 = add i64 %24695, -8
  %24697 = load i64, i64* %3, align 8
  %24698 = add i64 %24697, 4
  store i64 %24698, i64* %3, align 8
  %24699 = inttoptr i64 %24696 to i64*
  %24700 = load i64, i64* %24699, align 8
  %24701 = add i64 %24700, 45448
  store i64 %24701, i64* %RAX.i11582.pre-phi, align 8
  %24702 = icmp ugt i64 %24700, -45449
  %24703 = zext i1 %24702 to i8
  store i8 %24703, i8* %14, align 1
  %24704 = trunc i64 %24701 to i32
  %24705 = and i32 %24704, 255
  %24706 = tail call i32 @llvm.ctpop.i32(i32 %24705)
  %24707 = trunc i32 %24706 to i8
  %24708 = and i8 %24707, 1
  %24709 = xor i8 %24708, 1
  store i8 %24709, i8* %21, align 1
  %24710 = xor i64 %24701, %24700
  %24711 = lshr i64 %24710, 4
  %24712 = trunc i64 %24711 to i8
  %24713 = and i8 %24712, 1
  store i8 %24713, i8* %27, align 1
  %24714 = icmp eq i64 %24701, 0
  %24715 = zext i1 %24714 to i8
  store i8 %24715, i8* %30, align 1
  %24716 = lshr i64 %24701, 63
  %24717 = trunc i64 %24716 to i8
  store i8 %24717, i8* %33, align 1
  %24718 = lshr i64 %24700, 63
  %24719 = xor i64 %24716, %24718
  %24720 = add nuw nsw i64 %24719, %24716
  %24721 = icmp eq i64 %24720, 2
  %24722 = zext i1 %24721 to i8
  store i8 %24722, i8* %39, align 1
  %24723 = add i64 %24695, -40
  %24724 = add i64 %24697, 14
  store i64 %24724, i64* %3, align 8
  %24725 = inttoptr i64 %24723 to i32*
  %24726 = load i32, i32* %24725, align 4
  %24727 = sext i32 %24726 to i64
  %24728 = mul nsw i64 %24727, 1032
  store i64 %24728, i64* %RCX.i11580, align 8
  %24729 = lshr i64 %24728, 63
  %24730 = add i64 %24728, %24701
  store i64 %24730, i64* %RAX.i11582.pre-phi, align 8
  %24731 = icmp ult i64 %24730, %24701
  %24732 = icmp ult i64 %24730, %24728
  %24733 = or i1 %24731, %24732
  %24734 = zext i1 %24733 to i8
  store i8 %24734, i8* %14, align 1
  %24735 = trunc i64 %24730 to i32
  %24736 = and i32 %24735, 255
  %24737 = tail call i32 @llvm.ctpop.i32(i32 %24736)
  %24738 = trunc i32 %24737 to i8
  %24739 = and i8 %24738, 1
  %24740 = xor i8 %24739, 1
  store i8 %24740, i8* %21, align 1
  %24741 = xor i64 %24728, %24701
  %24742 = xor i64 %24741, %24730
  %24743 = lshr i64 %24742, 4
  %24744 = trunc i64 %24743 to i8
  %24745 = and i8 %24744, 1
  store i8 %24745, i8* %27, align 1
  %24746 = icmp eq i64 %24730, 0
  %24747 = zext i1 %24746 to i8
  store i8 %24747, i8* %30, align 1
  %24748 = lshr i64 %24730, 63
  %24749 = trunc i64 %24748 to i8
  store i8 %24749, i8* %33, align 1
  %24750 = xor i64 %24748, %24716
  %24751 = xor i64 %24748, %24729
  %24752 = add nuw nsw i64 %24750, %24751
  %24753 = icmp eq i64 %24752, 2
  %24754 = zext i1 %24753 to i8
  store i8 %24754, i8* %39, align 1
  %24755 = load i64, i64* %RBP.i, align 8
  %24756 = add i64 %24755, -120
  %24757 = add i64 %24697, 28
  store i64 %24757, i64* %3, align 8
  %24758 = inttoptr i64 %24756 to i64*
  %24759 = load i64, i64* %24758, align 8
  store i64 %24759, i64* %RCX.i11580, align 8
  %24760 = add i64 %24755, -28
  %24761 = add i64 %24697, 31
  store i64 %24761, i64* %3, align 8
  %24762 = inttoptr i64 %24760 to i32*
  %24763 = load i32, i32* %24762, align 4
  %24764 = add i32 %24763, 35
  %24765 = zext i32 %24764 to i64
  store i64 %24765, i64* %576, align 8
  %24766 = icmp ugt i32 %24763, -36
  %24767 = zext i1 %24766 to i8
  store i8 %24767, i8* %14, align 1
  %24768 = and i32 %24764, 255
  %24769 = tail call i32 @llvm.ctpop.i32(i32 %24768)
  %24770 = trunc i32 %24769 to i8
  %24771 = and i8 %24770, 1
  %24772 = xor i8 %24771, 1
  store i8 %24772, i8* %21, align 1
  %24773 = xor i32 %24764, %24763
  %24774 = lshr i32 %24773, 4
  %24775 = trunc i32 %24774 to i8
  %24776 = and i8 %24775, 1
  store i8 %24776, i8* %27, align 1
  %24777 = icmp eq i32 %24764, 0
  %24778 = zext i1 %24777 to i8
  store i8 %24778, i8* %30, align 1
  %24779 = lshr i32 %24764, 31
  %24780 = trunc i32 %24779 to i8
  store i8 %24780, i8* %33, align 1
  %24781 = lshr i32 %24763, 31
  %24782 = xor i32 %24779, %24781
  %24783 = add nuw nsw i32 %24782, %24779
  %24784 = icmp eq i32 %24783, 2
  %24785 = zext i1 %24784 to i8
  store i8 %24785, i8* %39, align 1
  %24786 = sext i32 %24764 to i64
  store i64 %24786, i64* %RSI.i11312, align 8
  %24787 = shl nsw i64 %24786, 1
  %24788 = add i64 %24759, %24787
  %24789 = add i64 %24697, 41
  store i64 %24789, i64* %3, align 8
  %24790 = inttoptr i64 %24788 to i16*
  %24791 = load i16, i16* %24790, align 2
  %24792 = zext i16 %24791 to i64
  store i64 %24792, i64* %576, align 8
  %24793 = zext i16 %24791 to i64
  store i64 %24793, i64* %RCX.i11580, align 8
  %24794 = shl nuw nsw i64 %24793, 2
  %24795 = add i64 %24730, %24794
  %24796 = add i64 %24697, 46
  store i64 %24796, i64* %3, align 8
  %24797 = inttoptr i64 %24795 to i32*
  %24798 = load i32, i32* %24797, align 4
  %24799 = add i32 %24798, 1
  %24800 = zext i32 %24799 to i64
  store i64 %24800, i64* %576, align 8
  %24801 = icmp eq i32 %24798, -1
  %24802 = icmp eq i32 %24799, 0
  %24803 = or i1 %24801, %24802
  %24804 = zext i1 %24803 to i8
  store i8 %24804, i8* %14, align 1
  %24805 = and i32 %24799, 255
  %24806 = tail call i32 @llvm.ctpop.i32(i32 %24805)
  %24807 = trunc i32 %24806 to i8
  %24808 = and i8 %24807, 1
  %24809 = xor i8 %24808, 1
  store i8 %24809, i8* %21, align 1
  %24810 = xor i32 %24799, %24798
  %24811 = lshr i32 %24810, 4
  %24812 = trunc i32 %24811 to i8
  %24813 = and i8 %24812, 1
  store i8 %24813, i8* %27, align 1
  %24814 = zext i1 %24802 to i8
  store i8 %24814, i8* %30, align 1
  %24815 = lshr i32 %24799, 31
  %24816 = trunc i32 %24815 to i8
  store i8 %24816, i8* %33, align 1
  %24817 = lshr i32 %24798, 31
  %24818 = xor i32 %24815, %24817
  %24819 = add nuw nsw i32 %24818, %24815
  %24820 = icmp eq i32 %24819, 2
  %24821 = zext i1 %24820 to i8
  store i8 %24821, i8* %39, align 1
  %24822 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24823 = add i64 %24794, %24822
  %24824 = add i64 %24697, 52
  store i64 %24824, i64* %3, align 8
  %24825 = inttoptr i64 %24823 to i32*
  store i32 %24799, i32* %24825, align 4
  %24826 = load i64, i64* %RBP.i, align 8
  %24827 = add i64 %24826, -8
  %24828 = load i64, i64* %3, align 8
  %24829 = add i64 %24828, 4
  store i64 %24829, i64* %3, align 8
  %24830 = inttoptr i64 %24827 to i64*
  %24831 = load i64, i64* %24830, align 8
  %24832 = add i64 %24831, 45448
  store i64 %24832, i64* %RAX.i11582.pre-phi, align 8
  %24833 = icmp ugt i64 %24831, -45449
  %24834 = zext i1 %24833 to i8
  store i8 %24834, i8* %14, align 1
  %24835 = trunc i64 %24832 to i32
  %24836 = and i32 %24835, 255
  %24837 = tail call i32 @llvm.ctpop.i32(i32 %24836)
  %24838 = trunc i32 %24837 to i8
  %24839 = and i8 %24838, 1
  %24840 = xor i8 %24839, 1
  store i8 %24840, i8* %21, align 1
  %24841 = xor i64 %24832, %24831
  %24842 = lshr i64 %24841, 4
  %24843 = trunc i64 %24842 to i8
  %24844 = and i8 %24843, 1
  store i8 %24844, i8* %27, align 1
  %24845 = icmp eq i64 %24832, 0
  %24846 = zext i1 %24845 to i8
  store i8 %24846, i8* %30, align 1
  %24847 = lshr i64 %24832, 63
  %24848 = trunc i64 %24847 to i8
  store i8 %24848, i8* %33, align 1
  %24849 = lshr i64 %24831, 63
  %24850 = xor i64 %24847, %24849
  %24851 = add nuw nsw i64 %24850, %24847
  %24852 = icmp eq i64 %24851, 2
  %24853 = zext i1 %24852 to i8
  store i8 %24853, i8* %39, align 1
  %24854 = add i64 %24826, -40
  %24855 = add i64 %24828, 14
  store i64 %24855, i64* %3, align 8
  %24856 = inttoptr i64 %24854 to i32*
  %24857 = load i32, i32* %24856, align 4
  %24858 = sext i32 %24857 to i64
  %24859 = mul nsw i64 %24858, 1032
  store i64 %24859, i64* %RCX.i11580, align 8
  %24860 = lshr i64 %24859, 63
  %24861 = add i64 %24859, %24832
  store i64 %24861, i64* %RAX.i11582.pre-phi, align 8
  %24862 = icmp ult i64 %24861, %24832
  %24863 = icmp ult i64 %24861, %24859
  %24864 = or i1 %24862, %24863
  %24865 = zext i1 %24864 to i8
  store i8 %24865, i8* %14, align 1
  %24866 = trunc i64 %24861 to i32
  %24867 = and i32 %24866, 255
  %24868 = tail call i32 @llvm.ctpop.i32(i32 %24867)
  %24869 = trunc i32 %24868 to i8
  %24870 = and i8 %24869, 1
  %24871 = xor i8 %24870, 1
  store i8 %24871, i8* %21, align 1
  %24872 = xor i64 %24859, %24832
  %24873 = xor i64 %24872, %24861
  %24874 = lshr i64 %24873, 4
  %24875 = trunc i64 %24874 to i8
  %24876 = and i8 %24875, 1
  store i8 %24876, i8* %27, align 1
  %24877 = icmp eq i64 %24861, 0
  %24878 = zext i1 %24877 to i8
  store i8 %24878, i8* %30, align 1
  %24879 = lshr i64 %24861, 63
  %24880 = trunc i64 %24879 to i8
  store i8 %24880, i8* %33, align 1
  %24881 = xor i64 %24879, %24847
  %24882 = xor i64 %24879, %24860
  %24883 = add nuw nsw i64 %24881, %24882
  %24884 = icmp eq i64 %24883, 2
  %24885 = zext i1 %24884 to i8
  store i8 %24885, i8* %39, align 1
  %24886 = load i64, i64* %RBP.i, align 8
  %24887 = add i64 %24886, -120
  %24888 = add i64 %24828, 28
  store i64 %24888, i64* %3, align 8
  %24889 = inttoptr i64 %24887 to i64*
  %24890 = load i64, i64* %24889, align 8
  store i64 %24890, i64* %RCX.i11580, align 8
  %24891 = add i64 %24886, -28
  %24892 = add i64 %24828, 31
  store i64 %24892, i64* %3, align 8
  %24893 = inttoptr i64 %24891 to i32*
  %24894 = load i32, i32* %24893, align 4
  %24895 = add i32 %24894, 36
  %24896 = zext i32 %24895 to i64
  store i64 %24896, i64* %576, align 8
  %24897 = icmp ugt i32 %24894, -37
  %24898 = zext i1 %24897 to i8
  store i8 %24898, i8* %14, align 1
  %24899 = and i32 %24895, 255
  %24900 = tail call i32 @llvm.ctpop.i32(i32 %24899)
  %24901 = trunc i32 %24900 to i8
  %24902 = and i8 %24901, 1
  %24903 = xor i8 %24902, 1
  store i8 %24903, i8* %21, align 1
  %24904 = xor i32 %24895, %24894
  %24905 = lshr i32 %24904, 4
  %24906 = trunc i32 %24905 to i8
  %24907 = and i8 %24906, 1
  store i8 %24907, i8* %27, align 1
  %24908 = icmp eq i32 %24895, 0
  %24909 = zext i1 %24908 to i8
  store i8 %24909, i8* %30, align 1
  %24910 = lshr i32 %24895, 31
  %24911 = trunc i32 %24910 to i8
  store i8 %24911, i8* %33, align 1
  %24912 = lshr i32 %24894, 31
  %24913 = xor i32 %24910, %24912
  %24914 = add nuw nsw i32 %24913, %24910
  %24915 = icmp eq i32 %24914, 2
  %24916 = zext i1 %24915 to i8
  store i8 %24916, i8* %39, align 1
  %24917 = sext i32 %24895 to i64
  store i64 %24917, i64* %RSI.i11312, align 8
  %24918 = shl nsw i64 %24917, 1
  %24919 = add i64 %24890, %24918
  %24920 = add i64 %24828, 41
  store i64 %24920, i64* %3, align 8
  %24921 = inttoptr i64 %24919 to i16*
  %24922 = load i16, i16* %24921, align 2
  %24923 = zext i16 %24922 to i64
  store i64 %24923, i64* %576, align 8
  %24924 = zext i16 %24922 to i64
  store i64 %24924, i64* %RCX.i11580, align 8
  %24925 = shl nuw nsw i64 %24924, 2
  %24926 = add i64 %24861, %24925
  %24927 = add i64 %24828, 46
  store i64 %24927, i64* %3, align 8
  %24928 = inttoptr i64 %24926 to i32*
  %24929 = load i32, i32* %24928, align 4
  %24930 = add i32 %24929, 1
  %24931 = zext i32 %24930 to i64
  store i64 %24931, i64* %576, align 8
  %24932 = icmp eq i32 %24929, -1
  %24933 = icmp eq i32 %24930, 0
  %24934 = or i1 %24932, %24933
  %24935 = zext i1 %24934 to i8
  store i8 %24935, i8* %14, align 1
  %24936 = and i32 %24930, 255
  %24937 = tail call i32 @llvm.ctpop.i32(i32 %24936)
  %24938 = trunc i32 %24937 to i8
  %24939 = and i8 %24938, 1
  %24940 = xor i8 %24939, 1
  store i8 %24940, i8* %21, align 1
  %24941 = xor i32 %24930, %24929
  %24942 = lshr i32 %24941, 4
  %24943 = trunc i32 %24942 to i8
  %24944 = and i8 %24943, 1
  store i8 %24944, i8* %27, align 1
  %24945 = zext i1 %24933 to i8
  store i8 %24945, i8* %30, align 1
  %24946 = lshr i32 %24930, 31
  %24947 = trunc i32 %24946 to i8
  store i8 %24947, i8* %33, align 1
  %24948 = lshr i32 %24929, 31
  %24949 = xor i32 %24946, %24948
  %24950 = add nuw nsw i32 %24949, %24946
  %24951 = icmp eq i32 %24950, 2
  %24952 = zext i1 %24951 to i8
  store i8 %24952, i8* %39, align 1
  %24953 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %24954 = add i64 %24925, %24953
  %24955 = add i64 %24828, 52
  store i64 %24955, i64* %3, align 8
  %24956 = inttoptr i64 %24954 to i32*
  store i32 %24930, i32* %24956, align 4
  %24957 = load i64, i64* %RBP.i, align 8
  %24958 = add i64 %24957, -8
  %24959 = load i64, i64* %3, align 8
  %24960 = add i64 %24959, 4
  store i64 %24960, i64* %3, align 8
  %24961 = inttoptr i64 %24958 to i64*
  %24962 = load i64, i64* %24961, align 8
  %24963 = add i64 %24962, 45448
  store i64 %24963, i64* %RAX.i11582.pre-phi, align 8
  %24964 = icmp ugt i64 %24962, -45449
  %24965 = zext i1 %24964 to i8
  store i8 %24965, i8* %14, align 1
  %24966 = trunc i64 %24963 to i32
  %24967 = and i32 %24966, 255
  %24968 = tail call i32 @llvm.ctpop.i32(i32 %24967)
  %24969 = trunc i32 %24968 to i8
  %24970 = and i8 %24969, 1
  %24971 = xor i8 %24970, 1
  store i8 %24971, i8* %21, align 1
  %24972 = xor i64 %24963, %24962
  %24973 = lshr i64 %24972, 4
  %24974 = trunc i64 %24973 to i8
  %24975 = and i8 %24974, 1
  store i8 %24975, i8* %27, align 1
  %24976 = icmp eq i64 %24963, 0
  %24977 = zext i1 %24976 to i8
  store i8 %24977, i8* %30, align 1
  %24978 = lshr i64 %24963, 63
  %24979 = trunc i64 %24978 to i8
  store i8 %24979, i8* %33, align 1
  %24980 = lshr i64 %24962, 63
  %24981 = xor i64 %24978, %24980
  %24982 = add nuw nsw i64 %24981, %24978
  %24983 = icmp eq i64 %24982, 2
  %24984 = zext i1 %24983 to i8
  store i8 %24984, i8* %39, align 1
  %24985 = add i64 %24957, -40
  %24986 = add i64 %24959, 14
  store i64 %24986, i64* %3, align 8
  %24987 = inttoptr i64 %24985 to i32*
  %24988 = load i32, i32* %24987, align 4
  %24989 = sext i32 %24988 to i64
  %24990 = mul nsw i64 %24989, 1032
  store i64 %24990, i64* %RCX.i11580, align 8
  %24991 = lshr i64 %24990, 63
  %24992 = add i64 %24990, %24963
  store i64 %24992, i64* %RAX.i11582.pre-phi, align 8
  %24993 = icmp ult i64 %24992, %24963
  %24994 = icmp ult i64 %24992, %24990
  %24995 = or i1 %24993, %24994
  %24996 = zext i1 %24995 to i8
  store i8 %24996, i8* %14, align 1
  %24997 = trunc i64 %24992 to i32
  %24998 = and i32 %24997, 255
  %24999 = tail call i32 @llvm.ctpop.i32(i32 %24998)
  %25000 = trunc i32 %24999 to i8
  %25001 = and i8 %25000, 1
  %25002 = xor i8 %25001, 1
  store i8 %25002, i8* %21, align 1
  %25003 = xor i64 %24990, %24963
  %25004 = xor i64 %25003, %24992
  %25005 = lshr i64 %25004, 4
  %25006 = trunc i64 %25005 to i8
  %25007 = and i8 %25006, 1
  store i8 %25007, i8* %27, align 1
  %25008 = icmp eq i64 %24992, 0
  %25009 = zext i1 %25008 to i8
  store i8 %25009, i8* %30, align 1
  %25010 = lshr i64 %24992, 63
  %25011 = trunc i64 %25010 to i8
  store i8 %25011, i8* %33, align 1
  %25012 = xor i64 %25010, %24978
  %25013 = xor i64 %25010, %24991
  %25014 = add nuw nsw i64 %25012, %25013
  %25015 = icmp eq i64 %25014, 2
  %25016 = zext i1 %25015 to i8
  store i8 %25016, i8* %39, align 1
  %25017 = load i64, i64* %RBP.i, align 8
  %25018 = add i64 %25017, -120
  %25019 = add i64 %24959, 28
  store i64 %25019, i64* %3, align 8
  %25020 = inttoptr i64 %25018 to i64*
  %25021 = load i64, i64* %25020, align 8
  store i64 %25021, i64* %RCX.i11580, align 8
  %25022 = add i64 %25017, -28
  %25023 = add i64 %24959, 31
  store i64 %25023, i64* %3, align 8
  %25024 = inttoptr i64 %25022 to i32*
  %25025 = load i32, i32* %25024, align 4
  %25026 = add i32 %25025, 37
  %25027 = zext i32 %25026 to i64
  store i64 %25027, i64* %576, align 8
  %25028 = icmp ugt i32 %25025, -38
  %25029 = zext i1 %25028 to i8
  store i8 %25029, i8* %14, align 1
  %25030 = and i32 %25026, 255
  %25031 = tail call i32 @llvm.ctpop.i32(i32 %25030)
  %25032 = trunc i32 %25031 to i8
  %25033 = and i8 %25032, 1
  %25034 = xor i8 %25033, 1
  store i8 %25034, i8* %21, align 1
  %25035 = xor i32 %25026, %25025
  %25036 = lshr i32 %25035, 4
  %25037 = trunc i32 %25036 to i8
  %25038 = and i8 %25037, 1
  store i8 %25038, i8* %27, align 1
  %25039 = icmp eq i32 %25026, 0
  %25040 = zext i1 %25039 to i8
  store i8 %25040, i8* %30, align 1
  %25041 = lshr i32 %25026, 31
  %25042 = trunc i32 %25041 to i8
  store i8 %25042, i8* %33, align 1
  %25043 = lshr i32 %25025, 31
  %25044 = xor i32 %25041, %25043
  %25045 = add nuw nsw i32 %25044, %25041
  %25046 = icmp eq i32 %25045, 2
  %25047 = zext i1 %25046 to i8
  store i8 %25047, i8* %39, align 1
  %25048 = sext i32 %25026 to i64
  store i64 %25048, i64* %RSI.i11312, align 8
  %25049 = shl nsw i64 %25048, 1
  %25050 = add i64 %25021, %25049
  %25051 = add i64 %24959, 41
  store i64 %25051, i64* %3, align 8
  %25052 = inttoptr i64 %25050 to i16*
  %25053 = load i16, i16* %25052, align 2
  %25054 = zext i16 %25053 to i64
  store i64 %25054, i64* %576, align 8
  %25055 = zext i16 %25053 to i64
  store i64 %25055, i64* %RCX.i11580, align 8
  %25056 = shl nuw nsw i64 %25055, 2
  %25057 = add i64 %24992, %25056
  %25058 = add i64 %24959, 46
  store i64 %25058, i64* %3, align 8
  %25059 = inttoptr i64 %25057 to i32*
  %25060 = load i32, i32* %25059, align 4
  %25061 = add i32 %25060, 1
  %25062 = zext i32 %25061 to i64
  store i64 %25062, i64* %576, align 8
  %25063 = icmp eq i32 %25060, -1
  %25064 = icmp eq i32 %25061, 0
  %25065 = or i1 %25063, %25064
  %25066 = zext i1 %25065 to i8
  store i8 %25066, i8* %14, align 1
  %25067 = and i32 %25061, 255
  %25068 = tail call i32 @llvm.ctpop.i32(i32 %25067)
  %25069 = trunc i32 %25068 to i8
  %25070 = and i8 %25069, 1
  %25071 = xor i8 %25070, 1
  store i8 %25071, i8* %21, align 1
  %25072 = xor i32 %25061, %25060
  %25073 = lshr i32 %25072, 4
  %25074 = trunc i32 %25073 to i8
  %25075 = and i8 %25074, 1
  store i8 %25075, i8* %27, align 1
  %25076 = zext i1 %25064 to i8
  store i8 %25076, i8* %30, align 1
  %25077 = lshr i32 %25061, 31
  %25078 = trunc i32 %25077 to i8
  store i8 %25078, i8* %33, align 1
  %25079 = lshr i32 %25060, 31
  %25080 = xor i32 %25077, %25079
  %25081 = add nuw nsw i32 %25080, %25077
  %25082 = icmp eq i32 %25081, 2
  %25083 = zext i1 %25082 to i8
  store i8 %25083, i8* %39, align 1
  %25084 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %25085 = add i64 %25056, %25084
  %25086 = add i64 %24959, 52
  store i64 %25086, i64* %3, align 8
  %25087 = inttoptr i64 %25085 to i32*
  store i32 %25061, i32* %25087, align 4
  %25088 = load i64, i64* %RBP.i, align 8
  %25089 = add i64 %25088, -8
  %25090 = load i64, i64* %3, align 8
  %25091 = add i64 %25090, 4
  store i64 %25091, i64* %3, align 8
  %25092 = inttoptr i64 %25089 to i64*
  %25093 = load i64, i64* %25092, align 8
  %25094 = add i64 %25093, 45448
  store i64 %25094, i64* %RAX.i11582.pre-phi, align 8
  %25095 = icmp ugt i64 %25093, -45449
  %25096 = zext i1 %25095 to i8
  store i8 %25096, i8* %14, align 1
  %25097 = trunc i64 %25094 to i32
  %25098 = and i32 %25097, 255
  %25099 = tail call i32 @llvm.ctpop.i32(i32 %25098)
  %25100 = trunc i32 %25099 to i8
  %25101 = and i8 %25100, 1
  %25102 = xor i8 %25101, 1
  store i8 %25102, i8* %21, align 1
  %25103 = xor i64 %25094, %25093
  %25104 = lshr i64 %25103, 4
  %25105 = trunc i64 %25104 to i8
  %25106 = and i8 %25105, 1
  store i8 %25106, i8* %27, align 1
  %25107 = icmp eq i64 %25094, 0
  %25108 = zext i1 %25107 to i8
  store i8 %25108, i8* %30, align 1
  %25109 = lshr i64 %25094, 63
  %25110 = trunc i64 %25109 to i8
  store i8 %25110, i8* %33, align 1
  %25111 = lshr i64 %25093, 63
  %25112 = xor i64 %25109, %25111
  %25113 = add nuw nsw i64 %25112, %25109
  %25114 = icmp eq i64 %25113, 2
  %25115 = zext i1 %25114 to i8
  store i8 %25115, i8* %39, align 1
  %25116 = add i64 %25088, -40
  %25117 = add i64 %25090, 14
  store i64 %25117, i64* %3, align 8
  %25118 = inttoptr i64 %25116 to i32*
  %25119 = load i32, i32* %25118, align 4
  %25120 = sext i32 %25119 to i64
  %25121 = mul nsw i64 %25120, 1032
  store i64 %25121, i64* %RCX.i11580, align 8
  %25122 = lshr i64 %25121, 63
  %25123 = add i64 %25121, %25094
  store i64 %25123, i64* %RAX.i11582.pre-phi, align 8
  %25124 = icmp ult i64 %25123, %25094
  %25125 = icmp ult i64 %25123, %25121
  %25126 = or i1 %25124, %25125
  %25127 = zext i1 %25126 to i8
  store i8 %25127, i8* %14, align 1
  %25128 = trunc i64 %25123 to i32
  %25129 = and i32 %25128, 255
  %25130 = tail call i32 @llvm.ctpop.i32(i32 %25129)
  %25131 = trunc i32 %25130 to i8
  %25132 = and i8 %25131, 1
  %25133 = xor i8 %25132, 1
  store i8 %25133, i8* %21, align 1
  %25134 = xor i64 %25121, %25094
  %25135 = xor i64 %25134, %25123
  %25136 = lshr i64 %25135, 4
  %25137 = trunc i64 %25136 to i8
  %25138 = and i8 %25137, 1
  store i8 %25138, i8* %27, align 1
  %25139 = icmp eq i64 %25123, 0
  %25140 = zext i1 %25139 to i8
  store i8 %25140, i8* %30, align 1
  %25141 = lshr i64 %25123, 63
  %25142 = trunc i64 %25141 to i8
  store i8 %25142, i8* %33, align 1
  %25143 = xor i64 %25141, %25109
  %25144 = xor i64 %25141, %25122
  %25145 = add nuw nsw i64 %25143, %25144
  %25146 = icmp eq i64 %25145, 2
  %25147 = zext i1 %25146 to i8
  store i8 %25147, i8* %39, align 1
  %25148 = load i64, i64* %RBP.i, align 8
  %25149 = add i64 %25148, -120
  %25150 = add i64 %25090, 28
  store i64 %25150, i64* %3, align 8
  %25151 = inttoptr i64 %25149 to i64*
  %25152 = load i64, i64* %25151, align 8
  store i64 %25152, i64* %RCX.i11580, align 8
  %25153 = add i64 %25148, -28
  %25154 = add i64 %25090, 31
  store i64 %25154, i64* %3, align 8
  %25155 = inttoptr i64 %25153 to i32*
  %25156 = load i32, i32* %25155, align 4
  %25157 = add i32 %25156, 38
  %25158 = zext i32 %25157 to i64
  store i64 %25158, i64* %576, align 8
  %25159 = icmp ugt i32 %25156, -39
  %25160 = zext i1 %25159 to i8
  store i8 %25160, i8* %14, align 1
  %25161 = and i32 %25157, 255
  %25162 = tail call i32 @llvm.ctpop.i32(i32 %25161)
  %25163 = trunc i32 %25162 to i8
  %25164 = and i8 %25163, 1
  %25165 = xor i8 %25164, 1
  store i8 %25165, i8* %21, align 1
  %25166 = xor i32 %25157, %25156
  %25167 = lshr i32 %25166, 4
  %25168 = trunc i32 %25167 to i8
  %25169 = and i8 %25168, 1
  store i8 %25169, i8* %27, align 1
  %25170 = icmp eq i32 %25157, 0
  %25171 = zext i1 %25170 to i8
  store i8 %25171, i8* %30, align 1
  %25172 = lshr i32 %25157, 31
  %25173 = trunc i32 %25172 to i8
  store i8 %25173, i8* %33, align 1
  %25174 = lshr i32 %25156, 31
  %25175 = xor i32 %25172, %25174
  %25176 = add nuw nsw i32 %25175, %25172
  %25177 = icmp eq i32 %25176, 2
  %25178 = zext i1 %25177 to i8
  store i8 %25178, i8* %39, align 1
  %25179 = sext i32 %25157 to i64
  store i64 %25179, i64* %RSI.i11312, align 8
  %25180 = shl nsw i64 %25179, 1
  %25181 = add i64 %25152, %25180
  %25182 = add i64 %25090, 41
  store i64 %25182, i64* %3, align 8
  %25183 = inttoptr i64 %25181 to i16*
  %25184 = load i16, i16* %25183, align 2
  %25185 = zext i16 %25184 to i64
  store i64 %25185, i64* %576, align 8
  %25186 = zext i16 %25184 to i64
  store i64 %25186, i64* %RCX.i11580, align 8
  %25187 = shl nuw nsw i64 %25186, 2
  %25188 = add i64 %25123, %25187
  %25189 = add i64 %25090, 46
  store i64 %25189, i64* %3, align 8
  %25190 = inttoptr i64 %25188 to i32*
  %25191 = load i32, i32* %25190, align 4
  %25192 = add i32 %25191, 1
  %25193 = zext i32 %25192 to i64
  store i64 %25193, i64* %576, align 8
  %25194 = icmp eq i32 %25191, -1
  %25195 = icmp eq i32 %25192, 0
  %25196 = or i1 %25194, %25195
  %25197 = zext i1 %25196 to i8
  store i8 %25197, i8* %14, align 1
  %25198 = and i32 %25192, 255
  %25199 = tail call i32 @llvm.ctpop.i32(i32 %25198)
  %25200 = trunc i32 %25199 to i8
  %25201 = and i8 %25200, 1
  %25202 = xor i8 %25201, 1
  store i8 %25202, i8* %21, align 1
  %25203 = xor i32 %25192, %25191
  %25204 = lshr i32 %25203, 4
  %25205 = trunc i32 %25204 to i8
  %25206 = and i8 %25205, 1
  store i8 %25206, i8* %27, align 1
  %25207 = zext i1 %25195 to i8
  store i8 %25207, i8* %30, align 1
  %25208 = lshr i32 %25192, 31
  %25209 = trunc i32 %25208 to i8
  store i8 %25209, i8* %33, align 1
  %25210 = lshr i32 %25191, 31
  %25211 = xor i32 %25208, %25210
  %25212 = add nuw nsw i32 %25211, %25208
  %25213 = icmp eq i32 %25212, 2
  %25214 = zext i1 %25213 to i8
  store i8 %25214, i8* %39, align 1
  %25215 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %25216 = add i64 %25187, %25215
  %25217 = add i64 %25090, 52
  store i64 %25217, i64* %3, align 8
  %25218 = inttoptr i64 %25216 to i32*
  store i32 %25192, i32* %25218, align 4
  %25219 = load i64, i64* %RBP.i, align 8
  %25220 = add i64 %25219, -8
  %25221 = load i64, i64* %3, align 8
  %25222 = add i64 %25221, 4
  store i64 %25222, i64* %3, align 8
  %25223 = inttoptr i64 %25220 to i64*
  %25224 = load i64, i64* %25223, align 8
  %25225 = add i64 %25224, 45448
  store i64 %25225, i64* %RAX.i11582.pre-phi, align 8
  %25226 = icmp ugt i64 %25224, -45449
  %25227 = zext i1 %25226 to i8
  store i8 %25227, i8* %14, align 1
  %25228 = trunc i64 %25225 to i32
  %25229 = and i32 %25228, 255
  %25230 = tail call i32 @llvm.ctpop.i32(i32 %25229)
  %25231 = trunc i32 %25230 to i8
  %25232 = and i8 %25231, 1
  %25233 = xor i8 %25232, 1
  store i8 %25233, i8* %21, align 1
  %25234 = xor i64 %25225, %25224
  %25235 = lshr i64 %25234, 4
  %25236 = trunc i64 %25235 to i8
  %25237 = and i8 %25236, 1
  store i8 %25237, i8* %27, align 1
  %25238 = icmp eq i64 %25225, 0
  %25239 = zext i1 %25238 to i8
  store i8 %25239, i8* %30, align 1
  %25240 = lshr i64 %25225, 63
  %25241 = trunc i64 %25240 to i8
  store i8 %25241, i8* %33, align 1
  %25242 = lshr i64 %25224, 63
  %25243 = xor i64 %25240, %25242
  %25244 = add nuw nsw i64 %25243, %25240
  %25245 = icmp eq i64 %25244, 2
  %25246 = zext i1 %25245 to i8
  store i8 %25246, i8* %39, align 1
  %25247 = add i64 %25219, -40
  %25248 = add i64 %25221, 14
  store i64 %25248, i64* %3, align 8
  %25249 = inttoptr i64 %25247 to i32*
  %25250 = load i32, i32* %25249, align 4
  %25251 = sext i32 %25250 to i64
  %25252 = mul nsw i64 %25251, 1032
  store i64 %25252, i64* %RCX.i11580, align 8
  %25253 = lshr i64 %25252, 63
  %25254 = add i64 %25252, %25225
  store i64 %25254, i64* %RAX.i11582.pre-phi, align 8
  %25255 = icmp ult i64 %25254, %25225
  %25256 = icmp ult i64 %25254, %25252
  %25257 = or i1 %25255, %25256
  %25258 = zext i1 %25257 to i8
  store i8 %25258, i8* %14, align 1
  %25259 = trunc i64 %25254 to i32
  %25260 = and i32 %25259, 255
  %25261 = tail call i32 @llvm.ctpop.i32(i32 %25260)
  %25262 = trunc i32 %25261 to i8
  %25263 = and i8 %25262, 1
  %25264 = xor i8 %25263, 1
  store i8 %25264, i8* %21, align 1
  %25265 = xor i64 %25252, %25225
  %25266 = xor i64 %25265, %25254
  %25267 = lshr i64 %25266, 4
  %25268 = trunc i64 %25267 to i8
  %25269 = and i8 %25268, 1
  store i8 %25269, i8* %27, align 1
  %25270 = icmp eq i64 %25254, 0
  %25271 = zext i1 %25270 to i8
  store i8 %25271, i8* %30, align 1
  %25272 = lshr i64 %25254, 63
  %25273 = trunc i64 %25272 to i8
  store i8 %25273, i8* %33, align 1
  %25274 = xor i64 %25272, %25240
  %25275 = xor i64 %25272, %25253
  %25276 = add nuw nsw i64 %25274, %25275
  %25277 = icmp eq i64 %25276, 2
  %25278 = zext i1 %25277 to i8
  store i8 %25278, i8* %39, align 1
  %25279 = load i64, i64* %RBP.i, align 8
  %25280 = add i64 %25279, -120
  %25281 = add i64 %25221, 28
  store i64 %25281, i64* %3, align 8
  %25282 = inttoptr i64 %25280 to i64*
  %25283 = load i64, i64* %25282, align 8
  store i64 %25283, i64* %RCX.i11580, align 8
  %25284 = add i64 %25279, -28
  %25285 = add i64 %25221, 31
  store i64 %25285, i64* %3, align 8
  %25286 = inttoptr i64 %25284 to i32*
  %25287 = load i32, i32* %25286, align 4
  %25288 = add i32 %25287, 39
  %25289 = zext i32 %25288 to i64
  store i64 %25289, i64* %576, align 8
  %25290 = icmp ugt i32 %25287, -40
  %25291 = zext i1 %25290 to i8
  store i8 %25291, i8* %14, align 1
  %25292 = and i32 %25288, 255
  %25293 = tail call i32 @llvm.ctpop.i32(i32 %25292)
  %25294 = trunc i32 %25293 to i8
  %25295 = and i8 %25294, 1
  %25296 = xor i8 %25295, 1
  store i8 %25296, i8* %21, align 1
  %25297 = xor i32 %25288, %25287
  %25298 = lshr i32 %25297, 4
  %25299 = trunc i32 %25298 to i8
  %25300 = and i8 %25299, 1
  store i8 %25300, i8* %27, align 1
  %25301 = icmp eq i32 %25288, 0
  %25302 = zext i1 %25301 to i8
  store i8 %25302, i8* %30, align 1
  %25303 = lshr i32 %25288, 31
  %25304 = trunc i32 %25303 to i8
  store i8 %25304, i8* %33, align 1
  %25305 = lshr i32 %25287, 31
  %25306 = xor i32 %25303, %25305
  %25307 = add nuw nsw i32 %25306, %25303
  %25308 = icmp eq i32 %25307, 2
  %25309 = zext i1 %25308 to i8
  store i8 %25309, i8* %39, align 1
  %25310 = sext i32 %25288 to i64
  store i64 %25310, i64* %RSI.i11312, align 8
  %25311 = shl nsw i64 %25310, 1
  %25312 = add i64 %25283, %25311
  %25313 = add i64 %25221, 41
  store i64 %25313, i64* %3, align 8
  %25314 = inttoptr i64 %25312 to i16*
  %25315 = load i16, i16* %25314, align 2
  %25316 = zext i16 %25315 to i64
  store i64 %25316, i64* %576, align 8
  %25317 = zext i16 %25315 to i64
  store i64 %25317, i64* %RCX.i11580, align 8
  %25318 = shl nuw nsw i64 %25317, 2
  %25319 = add i64 %25254, %25318
  %25320 = add i64 %25221, 46
  store i64 %25320, i64* %3, align 8
  %25321 = inttoptr i64 %25319 to i32*
  %25322 = load i32, i32* %25321, align 4
  %25323 = add i32 %25322, 1
  %25324 = zext i32 %25323 to i64
  store i64 %25324, i64* %576, align 8
  %25325 = icmp eq i32 %25322, -1
  %25326 = icmp eq i32 %25323, 0
  %25327 = or i1 %25325, %25326
  %25328 = zext i1 %25327 to i8
  store i8 %25328, i8* %14, align 1
  %25329 = and i32 %25323, 255
  %25330 = tail call i32 @llvm.ctpop.i32(i32 %25329)
  %25331 = trunc i32 %25330 to i8
  %25332 = and i8 %25331, 1
  %25333 = xor i8 %25332, 1
  store i8 %25333, i8* %21, align 1
  %25334 = xor i32 %25323, %25322
  %25335 = lshr i32 %25334, 4
  %25336 = trunc i32 %25335 to i8
  %25337 = and i8 %25336, 1
  store i8 %25337, i8* %27, align 1
  %25338 = zext i1 %25326 to i8
  store i8 %25338, i8* %30, align 1
  %25339 = lshr i32 %25323, 31
  %25340 = trunc i32 %25339 to i8
  store i8 %25340, i8* %33, align 1
  %25341 = lshr i32 %25322, 31
  %25342 = xor i32 %25339, %25341
  %25343 = add nuw nsw i32 %25342, %25339
  %25344 = icmp eq i32 %25343, 2
  %25345 = zext i1 %25344 to i8
  store i8 %25345, i8* %39, align 1
  %25346 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %25347 = add i64 %25318, %25346
  %25348 = add i64 %25221, 52
  store i64 %25348, i64* %3, align 8
  %25349 = inttoptr i64 %25347 to i32*
  store i32 %25323, i32* %25349, align 4
  %25350 = load i64, i64* %RBP.i, align 8
  %25351 = add i64 %25350, -8
  %25352 = load i64, i64* %3, align 8
  %25353 = add i64 %25352, 4
  store i64 %25353, i64* %3, align 8
  %25354 = inttoptr i64 %25351 to i64*
  %25355 = load i64, i64* %25354, align 8
  %25356 = add i64 %25355, 45448
  store i64 %25356, i64* %RAX.i11582.pre-phi, align 8
  %25357 = icmp ugt i64 %25355, -45449
  %25358 = zext i1 %25357 to i8
  store i8 %25358, i8* %14, align 1
  %25359 = trunc i64 %25356 to i32
  %25360 = and i32 %25359, 255
  %25361 = tail call i32 @llvm.ctpop.i32(i32 %25360)
  %25362 = trunc i32 %25361 to i8
  %25363 = and i8 %25362, 1
  %25364 = xor i8 %25363, 1
  store i8 %25364, i8* %21, align 1
  %25365 = xor i64 %25356, %25355
  %25366 = lshr i64 %25365, 4
  %25367 = trunc i64 %25366 to i8
  %25368 = and i8 %25367, 1
  store i8 %25368, i8* %27, align 1
  %25369 = icmp eq i64 %25356, 0
  %25370 = zext i1 %25369 to i8
  store i8 %25370, i8* %30, align 1
  %25371 = lshr i64 %25356, 63
  %25372 = trunc i64 %25371 to i8
  store i8 %25372, i8* %33, align 1
  %25373 = lshr i64 %25355, 63
  %25374 = xor i64 %25371, %25373
  %25375 = add nuw nsw i64 %25374, %25371
  %25376 = icmp eq i64 %25375, 2
  %25377 = zext i1 %25376 to i8
  store i8 %25377, i8* %39, align 1
  %25378 = add i64 %25350, -40
  %25379 = add i64 %25352, 14
  store i64 %25379, i64* %3, align 8
  %25380 = inttoptr i64 %25378 to i32*
  %25381 = load i32, i32* %25380, align 4
  %25382 = sext i32 %25381 to i64
  %25383 = mul nsw i64 %25382, 1032
  store i64 %25383, i64* %RCX.i11580, align 8
  %25384 = lshr i64 %25383, 63
  %25385 = add i64 %25383, %25356
  store i64 %25385, i64* %RAX.i11582.pre-phi, align 8
  %25386 = icmp ult i64 %25385, %25356
  %25387 = icmp ult i64 %25385, %25383
  %25388 = or i1 %25386, %25387
  %25389 = zext i1 %25388 to i8
  store i8 %25389, i8* %14, align 1
  %25390 = trunc i64 %25385 to i32
  %25391 = and i32 %25390, 255
  %25392 = tail call i32 @llvm.ctpop.i32(i32 %25391)
  %25393 = trunc i32 %25392 to i8
  %25394 = and i8 %25393, 1
  %25395 = xor i8 %25394, 1
  store i8 %25395, i8* %21, align 1
  %25396 = xor i64 %25383, %25356
  %25397 = xor i64 %25396, %25385
  %25398 = lshr i64 %25397, 4
  %25399 = trunc i64 %25398 to i8
  %25400 = and i8 %25399, 1
  store i8 %25400, i8* %27, align 1
  %25401 = icmp eq i64 %25385, 0
  %25402 = zext i1 %25401 to i8
  store i8 %25402, i8* %30, align 1
  %25403 = lshr i64 %25385, 63
  %25404 = trunc i64 %25403 to i8
  store i8 %25404, i8* %33, align 1
  %25405 = xor i64 %25403, %25371
  %25406 = xor i64 %25403, %25384
  %25407 = add nuw nsw i64 %25405, %25406
  %25408 = icmp eq i64 %25407, 2
  %25409 = zext i1 %25408 to i8
  store i8 %25409, i8* %39, align 1
  %25410 = load i64, i64* %RBP.i, align 8
  %25411 = add i64 %25410, -120
  %25412 = add i64 %25352, 28
  store i64 %25412, i64* %3, align 8
  %25413 = inttoptr i64 %25411 to i64*
  %25414 = load i64, i64* %25413, align 8
  store i64 %25414, i64* %RCX.i11580, align 8
  %25415 = add i64 %25410, -28
  %25416 = add i64 %25352, 31
  store i64 %25416, i64* %3, align 8
  %25417 = inttoptr i64 %25415 to i32*
  %25418 = load i32, i32* %25417, align 4
  %25419 = add i32 %25418, 40
  %25420 = zext i32 %25419 to i64
  store i64 %25420, i64* %576, align 8
  %25421 = icmp ugt i32 %25418, -41
  %25422 = zext i1 %25421 to i8
  store i8 %25422, i8* %14, align 1
  %25423 = and i32 %25419, 255
  %25424 = tail call i32 @llvm.ctpop.i32(i32 %25423)
  %25425 = trunc i32 %25424 to i8
  %25426 = and i8 %25425, 1
  %25427 = xor i8 %25426, 1
  store i8 %25427, i8* %21, align 1
  %25428 = xor i32 %25419, %25418
  %25429 = lshr i32 %25428, 4
  %25430 = trunc i32 %25429 to i8
  %25431 = and i8 %25430, 1
  store i8 %25431, i8* %27, align 1
  %25432 = icmp eq i32 %25419, 0
  %25433 = zext i1 %25432 to i8
  store i8 %25433, i8* %30, align 1
  %25434 = lshr i32 %25419, 31
  %25435 = trunc i32 %25434 to i8
  store i8 %25435, i8* %33, align 1
  %25436 = lshr i32 %25418, 31
  %25437 = xor i32 %25434, %25436
  %25438 = add nuw nsw i32 %25437, %25434
  %25439 = icmp eq i32 %25438, 2
  %25440 = zext i1 %25439 to i8
  store i8 %25440, i8* %39, align 1
  %25441 = sext i32 %25419 to i64
  store i64 %25441, i64* %RSI.i11312, align 8
  %25442 = shl nsw i64 %25441, 1
  %25443 = add i64 %25414, %25442
  %25444 = add i64 %25352, 41
  store i64 %25444, i64* %3, align 8
  %25445 = inttoptr i64 %25443 to i16*
  %25446 = load i16, i16* %25445, align 2
  %25447 = zext i16 %25446 to i64
  store i64 %25447, i64* %576, align 8
  %25448 = zext i16 %25446 to i64
  store i64 %25448, i64* %RCX.i11580, align 8
  %25449 = shl nuw nsw i64 %25448, 2
  %25450 = add i64 %25385, %25449
  %25451 = add i64 %25352, 46
  store i64 %25451, i64* %3, align 8
  %25452 = inttoptr i64 %25450 to i32*
  %25453 = load i32, i32* %25452, align 4
  %25454 = add i32 %25453, 1
  %25455 = zext i32 %25454 to i64
  store i64 %25455, i64* %576, align 8
  %25456 = icmp eq i32 %25453, -1
  %25457 = icmp eq i32 %25454, 0
  %25458 = or i1 %25456, %25457
  %25459 = zext i1 %25458 to i8
  store i8 %25459, i8* %14, align 1
  %25460 = and i32 %25454, 255
  %25461 = tail call i32 @llvm.ctpop.i32(i32 %25460)
  %25462 = trunc i32 %25461 to i8
  %25463 = and i8 %25462, 1
  %25464 = xor i8 %25463, 1
  store i8 %25464, i8* %21, align 1
  %25465 = xor i32 %25454, %25453
  %25466 = lshr i32 %25465, 4
  %25467 = trunc i32 %25466 to i8
  %25468 = and i8 %25467, 1
  store i8 %25468, i8* %27, align 1
  %25469 = zext i1 %25457 to i8
  store i8 %25469, i8* %30, align 1
  %25470 = lshr i32 %25454, 31
  %25471 = trunc i32 %25470 to i8
  store i8 %25471, i8* %33, align 1
  %25472 = lshr i32 %25453, 31
  %25473 = xor i32 %25470, %25472
  %25474 = add nuw nsw i32 %25473, %25470
  %25475 = icmp eq i32 %25474, 2
  %25476 = zext i1 %25475 to i8
  store i8 %25476, i8* %39, align 1
  %25477 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %25478 = add i64 %25449, %25477
  %25479 = add i64 %25352, 52
  store i64 %25479, i64* %3, align 8
  %25480 = inttoptr i64 %25478 to i32*
  store i32 %25454, i32* %25480, align 4
  %25481 = load i64, i64* %RBP.i, align 8
  %25482 = add i64 %25481, -8
  %25483 = load i64, i64* %3, align 8
  %25484 = add i64 %25483, 4
  store i64 %25484, i64* %3, align 8
  %25485 = inttoptr i64 %25482 to i64*
  %25486 = load i64, i64* %25485, align 8
  %25487 = add i64 %25486, 45448
  store i64 %25487, i64* %RAX.i11582.pre-phi, align 8
  %25488 = icmp ugt i64 %25486, -45449
  %25489 = zext i1 %25488 to i8
  store i8 %25489, i8* %14, align 1
  %25490 = trunc i64 %25487 to i32
  %25491 = and i32 %25490, 255
  %25492 = tail call i32 @llvm.ctpop.i32(i32 %25491)
  %25493 = trunc i32 %25492 to i8
  %25494 = and i8 %25493, 1
  %25495 = xor i8 %25494, 1
  store i8 %25495, i8* %21, align 1
  %25496 = xor i64 %25487, %25486
  %25497 = lshr i64 %25496, 4
  %25498 = trunc i64 %25497 to i8
  %25499 = and i8 %25498, 1
  store i8 %25499, i8* %27, align 1
  %25500 = icmp eq i64 %25487, 0
  %25501 = zext i1 %25500 to i8
  store i8 %25501, i8* %30, align 1
  %25502 = lshr i64 %25487, 63
  %25503 = trunc i64 %25502 to i8
  store i8 %25503, i8* %33, align 1
  %25504 = lshr i64 %25486, 63
  %25505 = xor i64 %25502, %25504
  %25506 = add nuw nsw i64 %25505, %25502
  %25507 = icmp eq i64 %25506, 2
  %25508 = zext i1 %25507 to i8
  store i8 %25508, i8* %39, align 1
  %25509 = add i64 %25481, -40
  %25510 = add i64 %25483, 14
  store i64 %25510, i64* %3, align 8
  %25511 = inttoptr i64 %25509 to i32*
  %25512 = load i32, i32* %25511, align 4
  %25513 = sext i32 %25512 to i64
  %25514 = mul nsw i64 %25513, 1032
  store i64 %25514, i64* %RCX.i11580, align 8
  %25515 = lshr i64 %25514, 63
  %25516 = add i64 %25514, %25487
  store i64 %25516, i64* %RAX.i11582.pre-phi, align 8
  %25517 = icmp ult i64 %25516, %25487
  %25518 = icmp ult i64 %25516, %25514
  %25519 = or i1 %25517, %25518
  %25520 = zext i1 %25519 to i8
  store i8 %25520, i8* %14, align 1
  %25521 = trunc i64 %25516 to i32
  %25522 = and i32 %25521, 255
  %25523 = tail call i32 @llvm.ctpop.i32(i32 %25522)
  %25524 = trunc i32 %25523 to i8
  %25525 = and i8 %25524, 1
  %25526 = xor i8 %25525, 1
  store i8 %25526, i8* %21, align 1
  %25527 = xor i64 %25514, %25487
  %25528 = xor i64 %25527, %25516
  %25529 = lshr i64 %25528, 4
  %25530 = trunc i64 %25529 to i8
  %25531 = and i8 %25530, 1
  store i8 %25531, i8* %27, align 1
  %25532 = icmp eq i64 %25516, 0
  %25533 = zext i1 %25532 to i8
  store i8 %25533, i8* %30, align 1
  %25534 = lshr i64 %25516, 63
  %25535 = trunc i64 %25534 to i8
  store i8 %25535, i8* %33, align 1
  %25536 = xor i64 %25534, %25502
  %25537 = xor i64 %25534, %25515
  %25538 = add nuw nsw i64 %25536, %25537
  %25539 = icmp eq i64 %25538, 2
  %25540 = zext i1 %25539 to i8
  store i8 %25540, i8* %39, align 1
  %25541 = load i64, i64* %RBP.i, align 8
  %25542 = add i64 %25541, -120
  %25543 = add i64 %25483, 28
  store i64 %25543, i64* %3, align 8
  %25544 = inttoptr i64 %25542 to i64*
  %25545 = load i64, i64* %25544, align 8
  store i64 %25545, i64* %RCX.i11580, align 8
  %25546 = add i64 %25541, -28
  %25547 = add i64 %25483, 31
  store i64 %25547, i64* %3, align 8
  %25548 = inttoptr i64 %25546 to i32*
  %25549 = load i32, i32* %25548, align 4
  %25550 = add i32 %25549, 41
  %25551 = zext i32 %25550 to i64
  store i64 %25551, i64* %576, align 8
  %25552 = icmp ugt i32 %25549, -42
  %25553 = zext i1 %25552 to i8
  store i8 %25553, i8* %14, align 1
  %25554 = and i32 %25550, 255
  %25555 = tail call i32 @llvm.ctpop.i32(i32 %25554)
  %25556 = trunc i32 %25555 to i8
  %25557 = and i8 %25556, 1
  %25558 = xor i8 %25557, 1
  store i8 %25558, i8* %21, align 1
  %25559 = xor i32 %25550, %25549
  %25560 = lshr i32 %25559, 4
  %25561 = trunc i32 %25560 to i8
  %25562 = and i8 %25561, 1
  store i8 %25562, i8* %27, align 1
  %25563 = icmp eq i32 %25550, 0
  %25564 = zext i1 %25563 to i8
  store i8 %25564, i8* %30, align 1
  %25565 = lshr i32 %25550, 31
  %25566 = trunc i32 %25565 to i8
  store i8 %25566, i8* %33, align 1
  %25567 = lshr i32 %25549, 31
  %25568 = xor i32 %25565, %25567
  %25569 = add nuw nsw i32 %25568, %25565
  %25570 = icmp eq i32 %25569, 2
  %25571 = zext i1 %25570 to i8
  store i8 %25571, i8* %39, align 1
  %25572 = sext i32 %25550 to i64
  store i64 %25572, i64* %RSI.i11312, align 8
  %25573 = shl nsw i64 %25572, 1
  %25574 = add i64 %25545, %25573
  %25575 = add i64 %25483, 41
  store i64 %25575, i64* %3, align 8
  %25576 = inttoptr i64 %25574 to i16*
  %25577 = load i16, i16* %25576, align 2
  %25578 = zext i16 %25577 to i64
  store i64 %25578, i64* %576, align 8
  %25579 = zext i16 %25577 to i64
  store i64 %25579, i64* %RCX.i11580, align 8
  %25580 = shl nuw nsw i64 %25579, 2
  %25581 = add i64 %25516, %25580
  %25582 = add i64 %25483, 46
  store i64 %25582, i64* %3, align 8
  %25583 = inttoptr i64 %25581 to i32*
  %25584 = load i32, i32* %25583, align 4
  %25585 = add i32 %25584, 1
  %25586 = zext i32 %25585 to i64
  store i64 %25586, i64* %576, align 8
  %25587 = icmp eq i32 %25584, -1
  %25588 = icmp eq i32 %25585, 0
  %25589 = or i1 %25587, %25588
  %25590 = zext i1 %25589 to i8
  store i8 %25590, i8* %14, align 1
  %25591 = and i32 %25585, 255
  %25592 = tail call i32 @llvm.ctpop.i32(i32 %25591)
  %25593 = trunc i32 %25592 to i8
  %25594 = and i8 %25593, 1
  %25595 = xor i8 %25594, 1
  store i8 %25595, i8* %21, align 1
  %25596 = xor i32 %25585, %25584
  %25597 = lshr i32 %25596, 4
  %25598 = trunc i32 %25597 to i8
  %25599 = and i8 %25598, 1
  store i8 %25599, i8* %27, align 1
  %25600 = zext i1 %25588 to i8
  store i8 %25600, i8* %30, align 1
  %25601 = lshr i32 %25585, 31
  %25602 = trunc i32 %25601 to i8
  store i8 %25602, i8* %33, align 1
  %25603 = lshr i32 %25584, 31
  %25604 = xor i32 %25601, %25603
  %25605 = add nuw nsw i32 %25604, %25601
  %25606 = icmp eq i32 %25605, 2
  %25607 = zext i1 %25606 to i8
  store i8 %25607, i8* %39, align 1
  %25608 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %25609 = add i64 %25580, %25608
  %25610 = add i64 %25483, 52
  store i64 %25610, i64* %3, align 8
  %25611 = inttoptr i64 %25609 to i32*
  store i32 %25585, i32* %25611, align 4
  %25612 = load i64, i64* %RBP.i, align 8
  %25613 = add i64 %25612, -8
  %25614 = load i64, i64* %3, align 8
  %25615 = add i64 %25614, 4
  store i64 %25615, i64* %3, align 8
  %25616 = inttoptr i64 %25613 to i64*
  %25617 = load i64, i64* %25616, align 8
  %25618 = add i64 %25617, 45448
  store i64 %25618, i64* %RAX.i11582.pre-phi, align 8
  %25619 = icmp ugt i64 %25617, -45449
  %25620 = zext i1 %25619 to i8
  store i8 %25620, i8* %14, align 1
  %25621 = trunc i64 %25618 to i32
  %25622 = and i32 %25621, 255
  %25623 = tail call i32 @llvm.ctpop.i32(i32 %25622)
  %25624 = trunc i32 %25623 to i8
  %25625 = and i8 %25624, 1
  %25626 = xor i8 %25625, 1
  store i8 %25626, i8* %21, align 1
  %25627 = xor i64 %25618, %25617
  %25628 = lshr i64 %25627, 4
  %25629 = trunc i64 %25628 to i8
  %25630 = and i8 %25629, 1
  store i8 %25630, i8* %27, align 1
  %25631 = icmp eq i64 %25618, 0
  %25632 = zext i1 %25631 to i8
  store i8 %25632, i8* %30, align 1
  %25633 = lshr i64 %25618, 63
  %25634 = trunc i64 %25633 to i8
  store i8 %25634, i8* %33, align 1
  %25635 = lshr i64 %25617, 63
  %25636 = xor i64 %25633, %25635
  %25637 = add nuw nsw i64 %25636, %25633
  %25638 = icmp eq i64 %25637, 2
  %25639 = zext i1 %25638 to i8
  store i8 %25639, i8* %39, align 1
  %25640 = add i64 %25612, -40
  %25641 = add i64 %25614, 14
  store i64 %25641, i64* %3, align 8
  %25642 = inttoptr i64 %25640 to i32*
  %25643 = load i32, i32* %25642, align 4
  %25644 = sext i32 %25643 to i64
  %25645 = mul nsw i64 %25644, 1032
  store i64 %25645, i64* %RCX.i11580, align 8
  %25646 = lshr i64 %25645, 63
  %25647 = add i64 %25645, %25618
  store i64 %25647, i64* %RAX.i11582.pre-phi, align 8
  %25648 = icmp ult i64 %25647, %25618
  %25649 = icmp ult i64 %25647, %25645
  %25650 = or i1 %25648, %25649
  %25651 = zext i1 %25650 to i8
  store i8 %25651, i8* %14, align 1
  %25652 = trunc i64 %25647 to i32
  %25653 = and i32 %25652, 255
  %25654 = tail call i32 @llvm.ctpop.i32(i32 %25653)
  %25655 = trunc i32 %25654 to i8
  %25656 = and i8 %25655, 1
  %25657 = xor i8 %25656, 1
  store i8 %25657, i8* %21, align 1
  %25658 = xor i64 %25645, %25618
  %25659 = xor i64 %25658, %25647
  %25660 = lshr i64 %25659, 4
  %25661 = trunc i64 %25660 to i8
  %25662 = and i8 %25661, 1
  store i8 %25662, i8* %27, align 1
  %25663 = icmp eq i64 %25647, 0
  %25664 = zext i1 %25663 to i8
  store i8 %25664, i8* %30, align 1
  %25665 = lshr i64 %25647, 63
  %25666 = trunc i64 %25665 to i8
  store i8 %25666, i8* %33, align 1
  %25667 = xor i64 %25665, %25633
  %25668 = xor i64 %25665, %25646
  %25669 = add nuw nsw i64 %25667, %25668
  %25670 = icmp eq i64 %25669, 2
  %25671 = zext i1 %25670 to i8
  store i8 %25671, i8* %39, align 1
  %25672 = load i64, i64* %RBP.i, align 8
  %25673 = add i64 %25672, -120
  %25674 = add i64 %25614, 28
  store i64 %25674, i64* %3, align 8
  %25675 = inttoptr i64 %25673 to i64*
  %25676 = load i64, i64* %25675, align 8
  store i64 %25676, i64* %RCX.i11580, align 8
  %25677 = add i64 %25672, -28
  %25678 = add i64 %25614, 31
  store i64 %25678, i64* %3, align 8
  %25679 = inttoptr i64 %25677 to i32*
  %25680 = load i32, i32* %25679, align 4
  %25681 = add i32 %25680, 42
  %25682 = zext i32 %25681 to i64
  store i64 %25682, i64* %576, align 8
  %25683 = icmp ugt i32 %25680, -43
  %25684 = zext i1 %25683 to i8
  store i8 %25684, i8* %14, align 1
  %25685 = and i32 %25681, 255
  %25686 = tail call i32 @llvm.ctpop.i32(i32 %25685)
  %25687 = trunc i32 %25686 to i8
  %25688 = and i8 %25687, 1
  %25689 = xor i8 %25688, 1
  store i8 %25689, i8* %21, align 1
  %25690 = xor i32 %25681, %25680
  %25691 = lshr i32 %25690, 4
  %25692 = trunc i32 %25691 to i8
  %25693 = and i8 %25692, 1
  store i8 %25693, i8* %27, align 1
  %25694 = icmp eq i32 %25681, 0
  %25695 = zext i1 %25694 to i8
  store i8 %25695, i8* %30, align 1
  %25696 = lshr i32 %25681, 31
  %25697 = trunc i32 %25696 to i8
  store i8 %25697, i8* %33, align 1
  %25698 = lshr i32 %25680, 31
  %25699 = xor i32 %25696, %25698
  %25700 = add nuw nsw i32 %25699, %25696
  %25701 = icmp eq i32 %25700, 2
  %25702 = zext i1 %25701 to i8
  store i8 %25702, i8* %39, align 1
  %25703 = sext i32 %25681 to i64
  store i64 %25703, i64* %RSI.i11312, align 8
  %25704 = shl nsw i64 %25703, 1
  %25705 = add i64 %25676, %25704
  %25706 = add i64 %25614, 41
  store i64 %25706, i64* %3, align 8
  %25707 = inttoptr i64 %25705 to i16*
  %25708 = load i16, i16* %25707, align 2
  %25709 = zext i16 %25708 to i64
  store i64 %25709, i64* %576, align 8
  %25710 = zext i16 %25708 to i64
  store i64 %25710, i64* %RCX.i11580, align 8
  %25711 = shl nuw nsw i64 %25710, 2
  %25712 = add i64 %25647, %25711
  %25713 = add i64 %25614, 46
  store i64 %25713, i64* %3, align 8
  %25714 = inttoptr i64 %25712 to i32*
  %25715 = load i32, i32* %25714, align 4
  %25716 = add i32 %25715, 1
  %25717 = zext i32 %25716 to i64
  store i64 %25717, i64* %576, align 8
  %25718 = icmp eq i32 %25715, -1
  %25719 = icmp eq i32 %25716, 0
  %25720 = or i1 %25718, %25719
  %25721 = zext i1 %25720 to i8
  store i8 %25721, i8* %14, align 1
  %25722 = and i32 %25716, 255
  %25723 = tail call i32 @llvm.ctpop.i32(i32 %25722)
  %25724 = trunc i32 %25723 to i8
  %25725 = and i8 %25724, 1
  %25726 = xor i8 %25725, 1
  store i8 %25726, i8* %21, align 1
  %25727 = xor i32 %25716, %25715
  %25728 = lshr i32 %25727, 4
  %25729 = trunc i32 %25728 to i8
  %25730 = and i8 %25729, 1
  store i8 %25730, i8* %27, align 1
  %25731 = zext i1 %25719 to i8
  store i8 %25731, i8* %30, align 1
  %25732 = lshr i32 %25716, 31
  %25733 = trunc i32 %25732 to i8
  store i8 %25733, i8* %33, align 1
  %25734 = lshr i32 %25715, 31
  %25735 = xor i32 %25732, %25734
  %25736 = add nuw nsw i32 %25735, %25732
  %25737 = icmp eq i32 %25736, 2
  %25738 = zext i1 %25737 to i8
  store i8 %25738, i8* %39, align 1
  %25739 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %25740 = add i64 %25711, %25739
  %25741 = add i64 %25614, 52
  store i64 %25741, i64* %3, align 8
  %25742 = inttoptr i64 %25740 to i32*
  store i32 %25716, i32* %25742, align 4
  %25743 = load i64, i64* %RBP.i, align 8
  %25744 = add i64 %25743, -8
  %25745 = load i64, i64* %3, align 8
  %25746 = add i64 %25745, 4
  store i64 %25746, i64* %3, align 8
  %25747 = inttoptr i64 %25744 to i64*
  %25748 = load i64, i64* %25747, align 8
  %25749 = add i64 %25748, 45448
  store i64 %25749, i64* %RAX.i11582.pre-phi, align 8
  %25750 = icmp ugt i64 %25748, -45449
  %25751 = zext i1 %25750 to i8
  store i8 %25751, i8* %14, align 1
  %25752 = trunc i64 %25749 to i32
  %25753 = and i32 %25752, 255
  %25754 = tail call i32 @llvm.ctpop.i32(i32 %25753)
  %25755 = trunc i32 %25754 to i8
  %25756 = and i8 %25755, 1
  %25757 = xor i8 %25756, 1
  store i8 %25757, i8* %21, align 1
  %25758 = xor i64 %25749, %25748
  %25759 = lshr i64 %25758, 4
  %25760 = trunc i64 %25759 to i8
  %25761 = and i8 %25760, 1
  store i8 %25761, i8* %27, align 1
  %25762 = icmp eq i64 %25749, 0
  %25763 = zext i1 %25762 to i8
  store i8 %25763, i8* %30, align 1
  %25764 = lshr i64 %25749, 63
  %25765 = trunc i64 %25764 to i8
  store i8 %25765, i8* %33, align 1
  %25766 = lshr i64 %25748, 63
  %25767 = xor i64 %25764, %25766
  %25768 = add nuw nsw i64 %25767, %25764
  %25769 = icmp eq i64 %25768, 2
  %25770 = zext i1 %25769 to i8
  store i8 %25770, i8* %39, align 1
  %25771 = add i64 %25743, -40
  %25772 = add i64 %25745, 14
  store i64 %25772, i64* %3, align 8
  %25773 = inttoptr i64 %25771 to i32*
  %25774 = load i32, i32* %25773, align 4
  %25775 = sext i32 %25774 to i64
  %25776 = mul nsw i64 %25775, 1032
  store i64 %25776, i64* %RCX.i11580, align 8
  %25777 = lshr i64 %25776, 63
  %25778 = add i64 %25776, %25749
  store i64 %25778, i64* %RAX.i11582.pre-phi, align 8
  %25779 = icmp ult i64 %25778, %25749
  %25780 = icmp ult i64 %25778, %25776
  %25781 = or i1 %25779, %25780
  %25782 = zext i1 %25781 to i8
  store i8 %25782, i8* %14, align 1
  %25783 = trunc i64 %25778 to i32
  %25784 = and i32 %25783, 255
  %25785 = tail call i32 @llvm.ctpop.i32(i32 %25784)
  %25786 = trunc i32 %25785 to i8
  %25787 = and i8 %25786, 1
  %25788 = xor i8 %25787, 1
  store i8 %25788, i8* %21, align 1
  %25789 = xor i64 %25776, %25749
  %25790 = xor i64 %25789, %25778
  %25791 = lshr i64 %25790, 4
  %25792 = trunc i64 %25791 to i8
  %25793 = and i8 %25792, 1
  store i8 %25793, i8* %27, align 1
  %25794 = icmp eq i64 %25778, 0
  %25795 = zext i1 %25794 to i8
  store i8 %25795, i8* %30, align 1
  %25796 = lshr i64 %25778, 63
  %25797 = trunc i64 %25796 to i8
  store i8 %25797, i8* %33, align 1
  %25798 = xor i64 %25796, %25764
  %25799 = xor i64 %25796, %25777
  %25800 = add nuw nsw i64 %25798, %25799
  %25801 = icmp eq i64 %25800, 2
  %25802 = zext i1 %25801 to i8
  store i8 %25802, i8* %39, align 1
  %25803 = load i64, i64* %RBP.i, align 8
  %25804 = add i64 %25803, -120
  %25805 = add i64 %25745, 28
  store i64 %25805, i64* %3, align 8
  %25806 = inttoptr i64 %25804 to i64*
  %25807 = load i64, i64* %25806, align 8
  store i64 %25807, i64* %RCX.i11580, align 8
  %25808 = add i64 %25803, -28
  %25809 = add i64 %25745, 31
  store i64 %25809, i64* %3, align 8
  %25810 = inttoptr i64 %25808 to i32*
  %25811 = load i32, i32* %25810, align 4
  %25812 = add i32 %25811, 43
  %25813 = zext i32 %25812 to i64
  store i64 %25813, i64* %576, align 8
  %25814 = icmp ugt i32 %25811, -44
  %25815 = zext i1 %25814 to i8
  store i8 %25815, i8* %14, align 1
  %25816 = and i32 %25812, 255
  %25817 = tail call i32 @llvm.ctpop.i32(i32 %25816)
  %25818 = trunc i32 %25817 to i8
  %25819 = and i8 %25818, 1
  %25820 = xor i8 %25819, 1
  store i8 %25820, i8* %21, align 1
  %25821 = xor i32 %25812, %25811
  %25822 = lshr i32 %25821, 4
  %25823 = trunc i32 %25822 to i8
  %25824 = and i8 %25823, 1
  store i8 %25824, i8* %27, align 1
  %25825 = icmp eq i32 %25812, 0
  %25826 = zext i1 %25825 to i8
  store i8 %25826, i8* %30, align 1
  %25827 = lshr i32 %25812, 31
  %25828 = trunc i32 %25827 to i8
  store i8 %25828, i8* %33, align 1
  %25829 = lshr i32 %25811, 31
  %25830 = xor i32 %25827, %25829
  %25831 = add nuw nsw i32 %25830, %25827
  %25832 = icmp eq i32 %25831, 2
  %25833 = zext i1 %25832 to i8
  store i8 %25833, i8* %39, align 1
  %25834 = sext i32 %25812 to i64
  store i64 %25834, i64* %RSI.i11312, align 8
  %25835 = shl nsw i64 %25834, 1
  %25836 = add i64 %25807, %25835
  %25837 = add i64 %25745, 41
  store i64 %25837, i64* %3, align 8
  %25838 = inttoptr i64 %25836 to i16*
  %25839 = load i16, i16* %25838, align 2
  %25840 = zext i16 %25839 to i64
  store i64 %25840, i64* %576, align 8
  %25841 = zext i16 %25839 to i64
  store i64 %25841, i64* %RCX.i11580, align 8
  %25842 = shl nuw nsw i64 %25841, 2
  %25843 = add i64 %25778, %25842
  %25844 = add i64 %25745, 46
  store i64 %25844, i64* %3, align 8
  %25845 = inttoptr i64 %25843 to i32*
  %25846 = load i32, i32* %25845, align 4
  %25847 = add i32 %25846, 1
  %25848 = zext i32 %25847 to i64
  store i64 %25848, i64* %576, align 8
  %25849 = icmp eq i32 %25846, -1
  %25850 = icmp eq i32 %25847, 0
  %25851 = or i1 %25849, %25850
  %25852 = zext i1 %25851 to i8
  store i8 %25852, i8* %14, align 1
  %25853 = and i32 %25847, 255
  %25854 = tail call i32 @llvm.ctpop.i32(i32 %25853)
  %25855 = trunc i32 %25854 to i8
  %25856 = and i8 %25855, 1
  %25857 = xor i8 %25856, 1
  store i8 %25857, i8* %21, align 1
  %25858 = xor i32 %25847, %25846
  %25859 = lshr i32 %25858, 4
  %25860 = trunc i32 %25859 to i8
  %25861 = and i8 %25860, 1
  store i8 %25861, i8* %27, align 1
  %25862 = zext i1 %25850 to i8
  store i8 %25862, i8* %30, align 1
  %25863 = lshr i32 %25847, 31
  %25864 = trunc i32 %25863 to i8
  store i8 %25864, i8* %33, align 1
  %25865 = lshr i32 %25846, 31
  %25866 = xor i32 %25863, %25865
  %25867 = add nuw nsw i32 %25866, %25863
  %25868 = icmp eq i32 %25867, 2
  %25869 = zext i1 %25868 to i8
  store i8 %25869, i8* %39, align 1
  %25870 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %25871 = add i64 %25842, %25870
  %25872 = add i64 %25745, 52
  store i64 %25872, i64* %3, align 8
  %25873 = inttoptr i64 %25871 to i32*
  store i32 %25847, i32* %25873, align 4
  %25874 = load i64, i64* %RBP.i, align 8
  %25875 = add i64 %25874, -8
  %25876 = load i64, i64* %3, align 8
  %25877 = add i64 %25876, 4
  store i64 %25877, i64* %3, align 8
  %25878 = inttoptr i64 %25875 to i64*
  %25879 = load i64, i64* %25878, align 8
  %25880 = add i64 %25879, 45448
  store i64 %25880, i64* %RAX.i11582.pre-phi, align 8
  %25881 = icmp ugt i64 %25879, -45449
  %25882 = zext i1 %25881 to i8
  store i8 %25882, i8* %14, align 1
  %25883 = trunc i64 %25880 to i32
  %25884 = and i32 %25883, 255
  %25885 = tail call i32 @llvm.ctpop.i32(i32 %25884)
  %25886 = trunc i32 %25885 to i8
  %25887 = and i8 %25886, 1
  %25888 = xor i8 %25887, 1
  store i8 %25888, i8* %21, align 1
  %25889 = xor i64 %25880, %25879
  %25890 = lshr i64 %25889, 4
  %25891 = trunc i64 %25890 to i8
  %25892 = and i8 %25891, 1
  store i8 %25892, i8* %27, align 1
  %25893 = icmp eq i64 %25880, 0
  %25894 = zext i1 %25893 to i8
  store i8 %25894, i8* %30, align 1
  %25895 = lshr i64 %25880, 63
  %25896 = trunc i64 %25895 to i8
  store i8 %25896, i8* %33, align 1
  %25897 = lshr i64 %25879, 63
  %25898 = xor i64 %25895, %25897
  %25899 = add nuw nsw i64 %25898, %25895
  %25900 = icmp eq i64 %25899, 2
  %25901 = zext i1 %25900 to i8
  store i8 %25901, i8* %39, align 1
  %25902 = add i64 %25874, -40
  %25903 = add i64 %25876, 14
  store i64 %25903, i64* %3, align 8
  %25904 = inttoptr i64 %25902 to i32*
  %25905 = load i32, i32* %25904, align 4
  %25906 = sext i32 %25905 to i64
  %25907 = mul nsw i64 %25906, 1032
  store i64 %25907, i64* %RCX.i11580, align 8
  %25908 = lshr i64 %25907, 63
  %25909 = add i64 %25907, %25880
  store i64 %25909, i64* %RAX.i11582.pre-phi, align 8
  %25910 = icmp ult i64 %25909, %25880
  %25911 = icmp ult i64 %25909, %25907
  %25912 = or i1 %25910, %25911
  %25913 = zext i1 %25912 to i8
  store i8 %25913, i8* %14, align 1
  %25914 = trunc i64 %25909 to i32
  %25915 = and i32 %25914, 255
  %25916 = tail call i32 @llvm.ctpop.i32(i32 %25915)
  %25917 = trunc i32 %25916 to i8
  %25918 = and i8 %25917, 1
  %25919 = xor i8 %25918, 1
  store i8 %25919, i8* %21, align 1
  %25920 = xor i64 %25907, %25880
  %25921 = xor i64 %25920, %25909
  %25922 = lshr i64 %25921, 4
  %25923 = trunc i64 %25922 to i8
  %25924 = and i8 %25923, 1
  store i8 %25924, i8* %27, align 1
  %25925 = icmp eq i64 %25909, 0
  %25926 = zext i1 %25925 to i8
  store i8 %25926, i8* %30, align 1
  %25927 = lshr i64 %25909, 63
  %25928 = trunc i64 %25927 to i8
  store i8 %25928, i8* %33, align 1
  %25929 = xor i64 %25927, %25895
  %25930 = xor i64 %25927, %25908
  %25931 = add nuw nsw i64 %25929, %25930
  %25932 = icmp eq i64 %25931, 2
  %25933 = zext i1 %25932 to i8
  store i8 %25933, i8* %39, align 1
  %25934 = load i64, i64* %RBP.i, align 8
  %25935 = add i64 %25934, -120
  %25936 = add i64 %25876, 28
  store i64 %25936, i64* %3, align 8
  %25937 = inttoptr i64 %25935 to i64*
  %25938 = load i64, i64* %25937, align 8
  store i64 %25938, i64* %RCX.i11580, align 8
  %25939 = add i64 %25934, -28
  %25940 = add i64 %25876, 31
  store i64 %25940, i64* %3, align 8
  %25941 = inttoptr i64 %25939 to i32*
  %25942 = load i32, i32* %25941, align 4
  %25943 = add i32 %25942, 44
  %25944 = zext i32 %25943 to i64
  store i64 %25944, i64* %576, align 8
  %25945 = icmp ugt i32 %25942, -45
  %25946 = zext i1 %25945 to i8
  store i8 %25946, i8* %14, align 1
  %25947 = and i32 %25943, 255
  %25948 = tail call i32 @llvm.ctpop.i32(i32 %25947)
  %25949 = trunc i32 %25948 to i8
  %25950 = and i8 %25949, 1
  %25951 = xor i8 %25950, 1
  store i8 %25951, i8* %21, align 1
  %25952 = xor i32 %25943, %25942
  %25953 = lshr i32 %25952, 4
  %25954 = trunc i32 %25953 to i8
  %25955 = and i8 %25954, 1
  store i8 %25955, i8* %27, align 1
  %25956 = icmp eq i32 %25943, 0
  %25957 = zext i1 %25956 to i8
  store i8 %25957, i8* %30, align 1
  %25958 = lshr i32 %25943, 31
  %25959 = trunc i32 %25958 to i8
  store i8 %25959, i8* %33, align 1
  %25960 = lshr i32 %25942, 31
  %25961 = xor i32 %25958, %25960
  %25962 = add nuw nsw i32 %25961, %25958
  %25963 = icmp eq i32 %25962, 2
  %25964 = zext i1 %25963 to i8
  store i8 %25964, i8* %39, align 1
  %25965 = sext i32 %25943 to i64
  store i64 %25965, i64* %RSI.i11312, align 8
  %25966 = shl nsw i64 %25965, 1
  %25967 = add i64 %25938, %25966
  %25968 = add i64 %25876, 41
  store i64 %25968, i64* %3, align 8
  %25969 = inttoptr i64 %25967 to i16*
  %25970 = load i16, i16* %25969, align 2
  %25971 = zext i16 %25970 to i64
  store i64 %25971, i64* %576, align 8
  %25972 = zext i16 %25970 to i64
  store i64 %25972, i64* %RCX.i11580, align 8
  %25973 = shl nuw nsw i64 %25972, 2
  %25974 = add i64 %25909, %25973
  %25975 = add i64 %25876, 46
  store i64 %25975, i64* %3, align 8
  %25976 = inttoptr i64 %25974 to i32*
  %25977 = load i32, i32* %25976, align 4
  %25978 = add i32 %25977, 1
  %25979 = zext i32 %25978 to i64
  store i64 %25979, i64* %576, align 8
  %25980 = icmp eq i32 %25977, -1
  %25981 = icmp eq i32 %25978, 0
  %25982 = or i1 %25980, %25981
  %25983 = zext i1 %25982 to i8
  store i8 %25983, i8* %14, align 1
  %25984 = and i32 %25978, 255
  %25985 = tail call i32 @llvm.ctpop.i32(i32 %25984)
  %25986 = trunc i32 %25985 to i8
  %25987 = and i8 %25986, 1
  %25988 = xor i8 %25987, 1
  store i8 %25988, i8* %21, align 1
  %25989 = xor i32 %25978, %25977
  %25990 = lshr i32 %25989, 4
  %25991 = trunc i32 %25990 to i8
  %25992 = and i8 %25991, 1
  store i8 %25992, i8* %27, align 1
  %25993 = zext i1 %25981 to i8
  store i8 %25993, i8* %30, align 1
  %25994 = lshr i32 %25978, 31
  %25995 = trunc i32 %25994 to i8
  store i8 %25995, i8* %33, align 1
  %25996 = lshr i32 %25977, 31
  %25997 = xor i32 %25994, %25996
  %25998 = add nuw nsw i32 %25997, %25994
  %25999 = icmp eq i32 %25998, 2
  %26000 = zext i1 %25999 to i8
  store i8 %26000, i8* %39, align 1
  %26001 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %26002 = add i64 %25973, %26001
  %26003 = add i64 %25876, 52
  store i64 %26003, i64* %3, align 8
  %26004 = inttoptr i64 %26002 to i32*
  store i32 %25978, i32* %26004, align 4
  %26005 = load i64, i64* %RBP.i, align 8
  %26006 = add i64 %26005, -8
  %26007 = load i64, i64* %3, align 8
  %26008 = add i64 %26007, 4
  store i64 %26008, i64* %3, align 8
  %26009 = inttoptr i64 %26006 to i64*
  %26010 = load i64, i64* %26009, align 8
  %26011 = add i64 %26010, 45448
  store i64 %26011, i64* %RAX.i11582.pre-phi, align 8
  %26012 = icmp ugt i64 %26010, -45449
  %26013 = zext i1 %26012 to i8
  store i8 %26013, i8* %14, align 1
  %26014 = trunc i64 %26011 to i32
  %26015 = and i32 %26014, 255
  %26016 = tail call i32 @llvm.ctpop.i32(i32 %26015)
  %26017 = trunc i32 %26016 to i8
  %26018 = and i8 %26017, 1
  %26019 = xor i8 %26018, 1
  store i8 %26019, i8* %21, align 1
  %26020 = xor i64 %26011, %26010
  %26021 = lshr i64 %26020, 4
  %26022 = trunc i64 %26021 to i8
  %26023 = and i8 %26022, 1
  store i8 %26023, i8* %27, align 1
  %26024 = icmp eq i64 %26011, 0
  %26025 = zext i1 %26024 to i8
  store i8 %26025, i8* %30, align 1
  %26026 = lshr i64 %26011, 63
  %26027 = trunc i64 %26026 to i8
  store i8 %26027, i8* %33, align 1
  %26028 = lshr i64 %26010, 63
  %26029 = xor i64 %26026, %26028
  %26030 = add nuw nsw i64 %26029, %26026
  %26031 = icmp eq i64 %26030, 2
  %26032 = zext i1 %26031 to i8
  store i8 %26032, i8* %39, align 1
  %26033 = add i64 %26005, -40
  %26034 = add i64 %26007, 14
  store i64 %26034, i64* %3, align 8
  %26035 = inttoptr i64 %26033 to i32*
  %26036 = load i32, i32* %26035, align 4
  %26037 = sext i32 %26036 to i64
  %26038 = mul nsw i64 %26037, 1032
  store i64 %26038, i64* %RCX.i11580, align 8
  %26039 = lshr i64 %26038, 63
  %26040 = add i64 %26038, %26011
  store i64 %26040, i64* %RAX.i11582.pre-phi, align 8
  %26041 = icmp ult i64 %26040, %26011
  %26042 = icmp ult i64 %26040, %26038
  %26043 = or i1 %26041, %26042
  %26044 = zext i1 %26043 to i8
  store i8 %26044, i8* %14, align 1
  %26045 = trunc i64 %26040 to i32
  %26046 = and i32 %26045, 255
  %26047 = tail call i32 @llvm.ctpop.i32(i32 %26046)
  %26048 = trunc i32 %26047 to i8
  %26049 = and i8 %26048, 1
  %26050 = xor i8 %26049, 1
  store i8 %26050, i8* %21, align 1
  %26051 = xor i64 %26038, %26011
  %26052 = xor i64 %26051, %26040
  %26053 = lshr i64 %26052, 4
  %26054 = trunc i64 %26053 to i8
  %26055 = and i8 %26054, 1
  store i8 %26055, i8* %27, align 1
  %26056 = icmp eq i64 %26040, 0
  %26057 = zext i1 %26056 to i8
  store i8 %26057, i8* %30, align 1
  %26058 = lshr i64 %26040, 63
  %26059 = trunc i64 %26058 to i8
  store i8 %26059, i8* %33, align 1
  %26060 = xor i64 %26058, %26026
  %26061 = xor i64 %26058, %26039
  %26062 = add nuw nsw i64 %26060, %26061
  %26063 = icmp eq i64 %26062, 2
  %26064 = zext i1 %26063 to i8
  store i8 %26064, i8* %39, align 1
  %26065 = load i64, i64* %RBP.i, align 8
  %26066 = add i64 %26065, -120
  %26067 = add i64 %26007, 28
  store i64 %26067, i64* %3, align 8
  %26068 = inttoptr i64 %26066 to i64*
  %26069 = load i64, i64* %26068, align 8
  store i64 %26069, i64* %RCX.i11580, align 8
  %26070 = add i64 %26065, -28
  %26071 = add i64 %26007, 31
  store i64 %26071, i64* %3, align 8
  %26072 = inttoptr i64 %26070 to i32*
  %26073 = load i32, i32* %26072, align 4
  %26074 = add i32 %26073, 45
  %26075 = zext i32 %26074 to i64
  store i64 %26075, i64* %576, align 8
  %26076 = icmp ugt i32 %26073, -46
  %26077 = zext i1 %26076 to i8
  store i8 %26077, i8* %14, align 1
  %26078 = and i32 %26074, 255
  %26079 = tail call i32 @llvm.ctpop.i32(i32 %26078)
  %26080 = trunc i32 %26079 to i8
  %26081 = and i8 %26080, 1
  %26082 = xor i8 %26081, 1
  store i8 %26082, i8* %21, align 1
  %26083 = xor i32 %26074, %26073
  %26084 = lshr i32 %26083, 4
  %26085 = trunc i32 %26084 to i8
  %26086 = and i8 %26085, 1
  store i8 %26086, i8* %27, align 1
  %26087 = icmp eq i32 %26074, 0
  %26088 = zext i1 %26087 to i8
  store i8 %26088, i8* %30, align 1
  %26089 = lshr i32 %26074, 31
  %26090 = trunc i32 %26089 to i8
  store i8 %26090, i8* %33, align 1
  %26091 = lshr i32 %26073, 31
  %26092 = xor i32 %26089, %26091
  %26093 = add nuw nsw i32 %26092, %26089
  %26094 = icmp eq i32 %26093, 2
  %26095 = zext i1 %26094 to i8
  store i8 %26095, i8* %39, align 1
  %26096 = sext i32 %26074 to i64
  store i64 %26096, i64* %RSI.i11312, align 8
  %26097 = shl nsw i64 %26096, 1
  %26098 = add i64 %26069, %26097
  %26099 = add i64 %26007, 41
  store i64 %26099, i64* %3, align 8
  %26100 = inttoptr i64 %26098 to i16*
  %26101 = load i16, i16* %26100, align 2
  %26102 = zext i16 %26101 to i64
  store i64 %26102, i64* %576, align 8
  %26103 = zext i16 %26101 to i64
  store i64 %26103, i64* %RCX.i11580, align 8
  %26104 = shl nuw nsw i64 %26103, 2
  %26105 = add i64 %26040, %26104
  %26106 = add i64 %26007, 46
  store i64 %26106, i64* %3, align 8
  %26107 = inttoptr i64 %26105 to i32*
  %26108 = load i32, i32* %26107, align 4
  %26109 = add i32 %26108, 1
  %26110 = zext i32 %26109 to i64
  store i64 %26110, i64* %576, align 8
  %26111 = icmp eq i32 %26108, -1
  %26112 = icmp eq i32 %26109, 0
  %26113 = or i1 %26111, %26112
  %26114 = zext i1 %26113 to i8
  store i8 %26114, i8* %14, align 1
  %26115 = and i32 %26109, 255
  %26116 = tail call i32 @llvm.ctpop.i32(i32 %26115)
  %26117 = trunc i32 %26116 to i8
  %26118 = and i8 %26117, 1
  %26119 = xor i8 %26118, 1
  store i8 %26119, i8* %21, align 1
  %26120 = xor i32 %26109, %26108
  %26121 = lshr i32 %26120, 4
  %26122 = trunc i32 %26121 to i8
  %26123 = and i8 %26122, 1
  store i8 %26123, i8* %27, align 1
  %26124 = zext i1 %26112 to i8
  store i8 %26124, i8* %30, align 1
  %26125 = lshr i32 %26109, 31
  %26126 = trunc i32 %26125 to i8
  store i8 %26126, i8* %33, align 1
  %26127 = lshr i32 %26108, 31
  %26128 = xor i32 %26125, %26127
  %26129 = add nuw nsw i32 %26128, %26125
  %26130 = icmp eq i32 %26129, 2
  %26131 = zext i1 %26130 to i8
  store i8 %26131, i8* %39, align 1
  %26132 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %26133 = add i64 %26104, %26132
  %26134 = add i64 %26007, 52
  store i64 %26134, i64* %3, align 8
  %26135 = inttoptr i64 %26133 to i32*
  store i32 %26109, i32* %26135, align 4
  %26136 = load i64, i64* %RBP.i, align 8
  %26137 = add i64 %26136, -8
  %26138 = load i64, i64* %3, align 8
  %26139 = add i64 %26138, 4
  store i64 %26139, i64* %3, align 8
  %26140 = inttoptr i64 %26137 to i64*
  %26141 = load i64, i64* %26140, align 8
  %26142 = add i64 %26141, 45448
  store i64 %26142, i64* %RAX.i11582.pre-phi, align 8
  %26143 = icmp ugt i64 %26141, -45449
  %26144 = zext i1 %26143 to i8
  store i8 %26144, i8* %14, align 1
  %26145 = trunc i64 %26142 to i32
  %26146 = and i32 %26145, 255
  %26147 = tail call i32 @llvm.ctpop.i32(i32 %26146)
  %26148 = trunc i32 %26147 to i8
  %26149 = and i8 %26148, 1
  %26150 = xor i8 %26149, 1
  store i8 %26150, i8* %21, align 1
  %26151 = xor i64 %26142, %26141
  %26152 = lshr i64 %26151, 4
  %26153 = trunc i64 %26152 to i8
  %26154 = and i8 %26153, 1
  store i8 %26154, i8* %27, align 1
  %26155 = icmp eq i64 %26142, 0
  %26156 = zext i1 %26155 to i8
  store i8 %26156, i8* %30, align 1
  %26157 = lshr i64 %26142, 63
  %26158 = trunc i64 %26157 to i8
  store i8 %26158, i8* %33, align 1
  %26159 = lshr i64 %26141, 63
  %26160 = xor i64 %26157, %26159
  %26161 = add nuw nsw i64 %26160, %26157
  %26162 = icmp eq i64 %26161, 2
  %26163 = zext i1 %26162 to i8
  store i8 %26163, i8* %39, align 1
  %26164 = add i64 %26136, -40
  %26165 = add i64 %26138, 14
  store i64 %26165, i64* %3, align 8
  %26166 = inttoptr i64 %26164 to i32*
  %26167 = load i32, i32* %26166, align 4
  %26168 = sext i32 %26167 to i64
  %26169 = mul nsw i64 %26168, 1032
  store i64 %26169, i64* %RCX.i11580, align 8
  %26170 = lshr i64 %26169, 63
  %26171 = add i64 %26169, %26142
  store i64 %26171, i64* %RAX.i11582.pre-phi, align 8
  %26172 = icmp ult i64 %26171, %26142
  %26173 = icmp ult i64 %26171, %26169
  %26174 = or i1 %26172, %26173
  %26175 = zext i1 %26174 to i8
  store i8 %26175, i8* %14, align 1
  %26176 = trunc i64 %26171 to i32
  %26177 = and i32 %26176, 255
  %26178 = tail call i32 @llvm.ctpop.i32(i32 %26177)
  %26179 = trunc i32 %26178 to i8
  %26180 = and i8 %26179, 1
  %26181 = xor i8 %26180, 1
  store i8 %26181, i8* %21, align 1
  %26182 = xor i64 %26169, %26142
  %26183 = xor i64 %26182, %26171
  %26184 = lshr i64 %26183, 4
  %26185 = trunc i64 %26184 to i8
  %26186 = and i8 %26185, 1
  store i8 %26186, i8* %27, align 1
  %26187 = icmp eq i64 %26171, 0
  %26188 = zext i1 %26187 to i8
  store i8 %26188, i8* %30, align 1
  %26189 = lshr i64 %26171, 63
  %26190 = trunc i64 %26189 to i8
  store i8 %26190, i8* %33, align 1
  %26191 = xor i64 %26189, %26157
  %26192 = xor i64 %26189, %26170
  %26193 = add nuw nsw i64 %26191, %26192
  %26194 = icmp eq i64 %26193, 2
  %26195 = zext i1 %26194 to i8
  store i8 %26195, i8* %39, align 1
  %26196 = load i64, i64* %RBP.i, align 8
  %26197 = add i64 %26196, -120
  %26198 = add i64 %26138, 28
  store i64 %26198, i64* %3, align 8
  %26199 = inttoptr i64 %26197 to i64*
  %26200 = load i64, i64* %26199, align 8
  store i64 %26200, i64* %RCX.i11580, align 8
  %26201 = add i64 %26196, -28
  %26202 = add i64 %26138, 31
  store i64 %26202, i64* %3, align 8
  %26203 = inttoptr i64 %26201 to i32*
  %26204 = load i32, i32* %26203, align 4
  %26205 = add i32 %26204, 46
  %26206 = zext i32 %26205 to i64
  store i64 %26206, i64* %576, align 8
  %26207 = icmp ugt i32 %26204, -47
  %26208 = zext i1 %26207 to i8
  store i8 %26208, i8* %14, align 1
  %26209 = and i32 %26205, 255
  %26210 = tail call i32 @llvm.ctpop.i32(i32 %26209)
  %26211 = trunc i32 %26210 to i8
  %26212 = and i8 %26211, 1
  %26213 = xor i8 %26212, 1
  store i8 %26213, i8* %21, align 1
  %26214 = xor i32 %26205, %26204
  %26215 = lshr i32 %26214, 4
  %26216 = trunc i32 %26215 to i8
  %26217 = and i8 %26216, 1
  store i8 %26217, i8* %27, align 1
  %26218 = icmp eq i32 %26205, 0
  %26219 = zext i1 %26218 to i8
  store i8 %26219, i8* %30, align 1
  %26220 = lshr i32 %26205, 31
  %26221 = trunc i32 %26220 to i8
  store i8 %26221, i8* %33, align 1
  %26222 = lshr i32 %26204, 31
  %26223 = xor i32 %26220, %26222
  %26224 = add nuw nsw i32 %26223, %26220
  %26225 = icmp eq i32 %26224, 2
  %26226 = zext i1 %26225 to i8
  store i8 %26226, i8* %39, align 1
  %26227 = sext i32 %26205 to i64
  store i64 %26227, i64* %RSI.i11312, align 8
  %26228 = shl nsw i64 %26227, 1
  %26229 = add i64 %26200, %26228
  %26230 = add i64 %26138, 41
  store i64 %26230, i64* %3, align 8
  %26231 = inttoptr i64 %26229 to i16*
  %26232 = load i16, i16* %26231, align 2
  %26233 = zext i16 %26232 to i64
  store i64 %26233, i64* %576, align 8
  %26234 = zext i16 %26232 to i64
  store i64 %26234, i64* %RCX.i11580, align 8
  %26235 = shl nuw nsw i64 %26234, 2
  %26236 = add i64 %26171, %26235
  %26237 = add i64 %26138, 46
  store i64 %26237, i64* %3, align 8
  %26238 = inttoptr i64 %26236 to i32*
  %26239 = load i32, i32* %26238, align 4
  %26240 = add i32 %26239, 1
  %26241 = zext i32 %26240 to i64
  store i64 %26241, i64* %576, align 8
  %26242 = icmp eq i32 %26239, -1
  %26243 = icmp eq i32 %26240, 0
  %26244 = or i1 %26242, %26243
  %26245 = zext i1 %26244 to i8
  store i8 %26245, i8* %14, align 1
  %26246 = and i32 %26240, 255
  %26247 = tail call i32 @llvm.ctpop.i32(i32 %26246)
  %26248 = trunc i32 %26247 to i8
  %26249 = and i8 %26248, 1
  %26250 = xor i8 %26249, 1
  store i8 %26250, i8* %21, align 1
  %26251 = xor i32 %26240, %26239
  %26252 = lshr i32 %26251, 4
  %26253 = trunc i32 %26252 to i8
  %26254 = and i8 %26253, 1
  store i8 %26254, i8* %27, align 1
  %26255 = zext i1 %26243 to i8
  store i8 %26255, i8* %30, align 1
  %26256 = lshr i32 %26240, 31
  %26257 = trunc i32 %26256 to i8
  store i8 %26257, i8* %33, align 1
  %26258 = lshr i32 %26239, 31
  %26259 = xor i32 %26256, %26258
  %26260 = add nuw nsw i32 %26259, %26256
  %26261 = icmp eq i32 %26260, 2
  %26262 = zext i1 %26261 to i8
  store i8 %26262, i8* %39, align 1
  %26263 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %26264 = add i64 %26235, %26263
  %26265 = add i64 %26138, 52
  store i64 %26265, i64* %3, align 8
  %26266 = inttoptr i64 %26264 to i32*
  store i32 %26240, i32* %26266, align 4
  %26267 = load i64, i64* %RBP.i, align 8
  %26268 = add i64 %26267, -8
  %26269 = load i64, i64* %3, align 8
  %26270 = add i64 %26269, 4
  store i64 %26270, i64* %3, align 8
  %26271 = inttoptr i64 %26268 to i64*
  %26272 = load i64, i64* %26271, align 8
  %26273 = add i64 %26272, 45448
  store i64 %26273, i64* %RAX.i11582.pre-phi, align 8
  %26274 = icmp ugt i64 %26272, -45449
  %26275 = zext i1 %26274 to i8
  store i8 %26275, i8* %14, align 1
  %26276 = trunc i64 %26273 to i32
  %26277 = and i32 %26276, 255
  %26278 = tail call i32 @llvm.ctpop.i32(i32 %26277)
  %26279 = trunc i32 %26278 to i8
  %26280 = and i8 %26279, 1
  %26281 = xor i8 %26280, 1
  store i8 %26281, i8* %21, align 1
  %26282 = xor i64 %26273, %26272
  %26283 = lshr i64 %26282, 4
  %26284 = trunc i64 %26283 to i8
  %26285 = and i8 %26284, 1
  store i8 %26285, i8* %27, align 1
  %26286 = icmp eq i64 %26273, 0
  %26287 = zext i1 %26286 to i8
  store i8 %26287, i8* %30, align 1
  %26288 = lshr i64 %26273, 63
  %26289 = trunc i64 %26288 to i8
  store i8 %26289, i8* %33, align 1
  %26290 = lshr i64 %26272, 63
  %26291 = xor i64 %26288, %26290
  %26292 = add nuw nsw i64 %26291, %26288
  %26293 = icmp eq i64 %26292, 2
  %26294 = zext i1 %26293 to i8
  store i8 %26294, i8* %39, align 1
  %26295 = add i64 %26267, -40
  %26296 = add i64 %26269, 14
  store i64 %26296, i64* %3, align 8
  %26297 = inttoptr i64 %26295 to i32*
  %26298 = load i32, i32* %26297, align 4
  %26299 = sext i32 %26298 to i64
  %26300 = mul nsw i64 %26299, 1032
  store i64 %26300, i64* %RCX.i11580, align 8
  %26301 = lshr i64 %26300, 63
  %26302 = add i64 %26300, %26273
  store i64 %26302, i64* %RAX.i11582.pre-phi, align 8
  %26303 = icmp ult i64 %26302, %26273
  %26304 = icmp ult i64 %26302, %26300
  %26305 = or i1 %26303, %26304
  %26306 = zext i1 %26305 to i8
  store i8 %26306, i8* %14, align 1
  %26307 = trunc i64 %26302 to i32
  %26308 = and i32 %26307, 255
  %26309 = tail call i32 @llvm.ctpop.i32(i32 %26308)
  %26310 = trunc i32 %26309 to i8
  %26311 = and i8 %26310, 1
  %26312 = xor i8 %26311, 1
  store i8 %26312, i8* %21, align 1
  %26313 = xor i64 %26300, %26273
  %26314 = xor i64 %26313, %26302
  %26315 = lshr i64 %26314, 4
  %26316 = trunc i64 %26315 to i8
  %26317 = and i8 %26316, 1
  store i8 %26317, i8* %27, align 1
  %26318 = icmp eq i64 %26302, 0
  %26319 = zext i1 %26318 to i8
  store i8 %26319, i8* %30, align 1
  %26320 = lshr i64 %26302, 63
  %26321 = trunc i64 %26320 to i8
  store i8 %26321, i8* %33, align 1
  %26322 = xor i64 %26320, %26288
  %26323 = xor i64 %26320, %26301
  %26324 = add nuw nsw i64 %26322, %26323
  %26325 = icmp eq i64 %26324, 2
  %26326 = zext i1 %26325 to i8
  store i8 %26326, i8* %39, align 1
  %26327 = load i64, i64* %RBP.i, align 8
  %26328 = add i64 %26327, -120
  %26329 = add i64 %26269, 28
  store i64 %26329, i64* %3, align 8
  %26330 = inttoptr i64 %26328 to i64*
  %26331 = load i64, i64* %26330, align 8
  store i64 %26331, i64* %RCX.i11580, align 8
  %26332 = add i64 %26327, -28
  %26333 = add i64 %26269, 31
  store i64 %26333, i64* %3, align 8
  %26334 = inttoptr i64 %26332 to i32*
  %26335 = load i32, i32* %26334, align 4
  %26336 = add i32 %26335, 47
  %26337 = zext i32 %26336 to i64
  store i64 %26337, i64* %576, align 8
  %26338 = icmp ugt i32 %26335, -48
  %26339 = zext i1 %26338 to i8
  store i8 %26339, i8* %14, align 1
  %26340 = and i32 %26336, 255
  %26341 = tail call i32 @llvm.ctpop.i32(i32 %26340)
  %26342 = trunc i32 %26341 to i8
  %26343 = and i8 %26342, 1
  %26344 = xor i8 %26343, 1
  store i8 %26344, i8* %21, align 1
  %26345 = xor i32 %26336, %26335
  %26346 = lshr i32 %26345, 4
  %26347 = trunc i32 %26346 to i8
  %26348 = and i8 %26347, 1
  store i8 %26348, i8* %27, align 1
  %26349 = icmp eq i32 %26336, 0
  %26350 = zext i1 %26349 to i8
  store i8 %26350, i8* %30, align 1
  %26351 = lshr i32 %26336, 31
  %26352 = trunc i32 %26351 to i8
  store i8 %26352, i8* %33, align 1
  %26353 = lshr i32 %26335, 31
  %26354 = xor i32 %26351, %26353
  %26355 = add nuw nsw i32 %26354, %26351
  %26356 = icmp eq i32 %26355, 2
  %26357 = zext i1 %26356 to i8
  store i8 %26357, i8* %39, align 1
  %26358 = sext i32 %26336 to i64
  store i64 %26358, i64* %RSI.i11312, align 8
  %26359 = shl nsw i64 %26358, 1
  %26360 = add i64 %26331, %26359
  %26361 = add i64 %26269, 41
  store i64 %26361, i64* %3, align 8
  %26362 = inttoptr i64 %26360 to i16*
  %26363 = load i16, i16* %26362, align 2
  %26364 = zext i16 %26363 to i64
  store i64 %26364, i64* %576, align 8
  %26365 = zext i16 %26363 to i64
  store i64 %26365, i64* %RCX.i11580, align 8
  %26366 = shl nuw nsw i64 %26365, 2
  %26367 = add i64 %26302, %26366
  %26368 = add i64 %26269, 46
  store i64 %26368, i64* %3, align 8
  %26369 = inttoptr i64 %26367 to i32*
  %26370 = load i32, i32* %26369, align 4
  %26371 = add i32 %26370, 1
  %26372 = zext i32 %26371 to i64
  store i64 %26372, i64* %576, align 8
  %26373 = icmp eq i32 %26370, -1
  %26374 = icmp eq i32 %26371, 0
  %26375 = or i1 %26373, %26374
  %26376 = zext i1 %26375 to i8
  store i8 %26376, i8* %14, align 1
  %26377 = and i32 %26371, 255
  %26378 = tail call i32 @llvm.ctpop.i32(i32 %26377)
  %26379 = trunc i32 %26378 to i8
  %26380 = and i8 %26379, 1
  %26381 = xor i8 %26380, 1
  store i8 %26381, i8* %21, align 1
  %26382 = xor i32 %26371, %26370
  %26383 = lshr i32 %26382, 4
  %26384 = trunc i32 %26383 to i8
  %26385 = and i8 %26384, 1
  store i8 %26385, i8* %27, align 1
  %26386 = zext i1 %26374 to i8
  store i8 %26386, i8* %30, align 1
  %26387 = lshr i32 %26371, 31
  %26388 = trunc i32 %26387 to i8
  store i8 %26388, i8* %33, align 1
  %26389 = lshr i32 %26370, 31
  %26390 = xor i32 %26387, %26389
  %26391 = add nuw nsw i32 %26390, %26387
  %26392 = icmp eq i32 %26391, 2
  %26393 = zext i1 %26392 to i8
  store i8 %26393, i8* %39, align 1
  %26394 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %26395 = add i64 %26366, %26394
  %26396 = add i64 %26269, 52
  store i64 %26396, i64* %3, align 8
  %26397 = inttoptr i64 %26395 to i32*
  store i32 %26371, i32* %26397, align 4
  %26398 = load i64, i64* %RBP.i, align 8
  %26399 = add i64 %26398, -8
  %26400 = load i64, i64* %3, align 8
  %26401 = add i64 %26400, 4
  store i64 %26401, i64* %3, align 8
  %26402 = inttoptr i64 %26399 to i64*
  %26403 = load i64, i64* %26402, align 8
  %26404 = add i64 %26403, 45448
  store i64 %26404, i64* %RAX.i11582.pre-phi, align 8
  %26405 = icmp ugt i64 %26403, -45449
  %26406 = zext i1 %26405 to i8
  store i8 %26406, i8* %14, align 1
  %26407 = trunc i64 %26404 to i32
  %26408 = and i32 %26407, 255
  %26409 = tail call i32 @llvm.ctpop.i32(i32 %26408)
  %26410 = trunc i32 %26409 to i8
  %26411 = and i8 %26410, 1
  %26412 = xor i8 %26411, 1
  store i8 %26412, i8* %21, align 1
  %26413 = xor i64 %26404, %26403
  %26414 = lshr i64 %26413, 4
  %26415 = trunc i64 %26414 to i8
  %26416 = and i8 %26415, 1
  store i8 %26416, i8* %27, align 1
  %26417 = icmp eq i64 %26404, 0
  %26418 = zext i1 %26417 to i8
  store i8 %26418, i8* %30, align 1
  %26419 = lshr i64 %26404, 63
  %26420 = trunc i64 %26419 to i8
  store i8 %26420, i8* %33, align 1
  %26421 = lshr i64 %26403, 63
  %26422 = xor i64 %26419, %26421
  %26423 = add nuw nsw i64 %26422, %26419
  %26424 = icmp eq i64 %26423, 2
  %26425 = zext i1 %26424 to i8
  store i8 %26425, i8* %39, align 1
  %26426 = add i64 %26398, -40
  %26427 = add i64 %26400, 14
  store i64 %26427, i64* %3, align 8
  %26428 = inttoptr i64 %26426 to i32*
  %26429 = load i32, i32* %26428, align 4
  %26430 = sext i32 %26429 to i64
  %26431 = mul nsw i64 %26430, 1032
  store i64 %26431, i64* %RCX.i11580, align 8
  %26432 = lshr i64 %26431, 63
  %26433 = add i64 %26431, %26404
  store i64 %26433, i64* %RAX.i11582.pre-phi, align 8
  %26434 = icmp ult i64 %26433, %26404
  %26435 = icmp ult i64 %26433, %26431
  %26436 = or i1 %26434, %26435
  %26437 = zext i1 %26436 to i8
  store i8 %26437, i8* %14, align 1
  %26438 = trunc i64 %26433 to i32
  %26439 = and i32 %26438, 255
  %26440 = tail call i32 @llvm.ctpop.i32(i32 %26439)
  %26441 = trunc i32 %26440 to i8
  %26442 = and i8 %26441, 1
  %26443 = xor i8 %26442, 1
  store i8 %26443, i8* %21, align 1
  %26444 = xor i64 %26431, %26404
  %26445 = xor i64 %26444, %26433
  %26446 = lshr i64 %26445, 4
  %26447 = trunc i64 %26446 to i8
  %26448 = and i8 %26447, 1
  store i8 %26448, i8* %27, align 1
  %26449 = icmp eq i64 %26433, 0
  %26450 = zext i1 %26449 to i8
  store i8 %26450, i8* %30, align 1
  %26451 = lshr i64 %26433, 63
  %26452 = trunc i64 %26451 to i8
  store i8 %26452, i8* %33, align 1
  %26453 = xor i64 %26451, %26419
  %26454 = xor i64 %26451, %26432
  %26455 = add nuw nsw i64 %26453, %26454
  %26456 = icmp eq i64 %26455, 2
  %26457 = zext i1 %26456 to i8
  store i8 %26457, i8* %39, align 1
  %26458 = load i64, i64* %RBP.i, align 8
  %26459 = add i64 %26458, -120
  %26460 = add i64 %26400, 28
  store i64 %26460, i64* %3, align 8
  %26461 = inttoptr i64 %26459 to i64*
  %26462 = load i64, i64* %26461, align 8
  store i64 %26462, i64* %RCX.i11580, align 8
  %26463 = add i64 %26458, -28
  %26464 = add i64 %26400, 31
  store i64 %26464, i64* %3, align 8
  %26465 = inttoptr i64 %26463 to i32*
  %26466 = load i32, i32* %26465, align 4
  %26467 = add i32 %26466, 48
  %26468 = zext i32 %26467 to i64
  store i64 %26468, i64* %576, align 8
  %26469 = icmp ugt i32 %26466, -49
  %26470 = zext i1 %26469 to i8
  store i8 %26470, i8* %14, align 1
  %26471 = and i32 %26467, 255
  %26472 = tail call i32 @llvm.ctpop.i32(i32 %26471)
  %26473 = trunc i32 %26472 to i8
  %26474 = and i8 %26473, 1
  %26475 = xor i8 %26474, 1
  store i8 %26475, i8* %21, align 1
  %26476 = xor i32 %26466, 16
  %26477 = xor i32 %26476, %26467
  %26478 = lshr i32 %26477, 4
  %26479 = trunc i32 %26478 to i8
  %26480 = and i8 %26479, 1
  store i8 %26480, i8* %27, align 1
  %26481 = icmp eq i32 %26467, 0
  %26482 = zext i1 %26481 to i8
  store i8 %26482, i8* %30, align 1
  %26483 = lshr i32 %26467, 31
  %26484 = trunc i32 %26483 to i8
  store i8 %26484, i8* %33, align 1
  %26485 = lshr i32 %26466, 31
  %26486 = xor i32 %26483, %26485
  %26487 = add nuw nsw i32 %26486, %26483
  %26488 = icmp eq i32 %26487, 2
  %26489 = zext i1 %26488 to i8
  store i8 %26489, i8* %39, align 1
  %26490 = sext i32 %26467 to i64
  store i64 %26490, i64* %RSI.i11312, align 8
  %26491 = shl nsw i64 %26490, 1
  %26492 = add i64 %26462, %26491
  %26493 = add i64 %26400, 41
  store i64 %26493, i64* %3, align 8
  %26494 = inttoptr i64 %26492 to i16*
  %26495 = load i16, i16* %26494, align 2
  %26496 = zext i16 %26495 to i64
  store i64 %26496, i64* %576, align 8
  %26497 = zext i16 %26495 to i64
  store i64 %26497, i64* %RCX.i11580, align 8
  %26498 = shl nuw nsw i64 %26497, 2
  %26499 = add i64 %26433, %26498
  %26500 = add i64 %26400, 46
  store i64 %26500, i64* %3, align 8
  %26501 = inttoptr i64 %26499 to i32*
  %26502 = load i32, i32* %26501, align 4
  %26503 = add i32 %26502, 1
  %26504 = zext i32 %26503 to i64
  store i64 %26504, i64* %576, align 8
  %26505 = icmp eq i32 %26502, -1
  %26506 = icmp eq i32 %26503, 0
  %26507 = or i1 %26505, %26506
  %26508 = zext i1 %26507 to i8
  store i8 %26508, i8* %14, align 1
  %26509 = and i32 %26503, 255
  %26510 = tail call i32 @llvm.ctpop.i32(i32 %26509)
  %26511 = trunc i32 %26510 to i8
  %26512 = and i8 %26511, 1
  %26513 = xor i8 %26512, 1
  store i8 %26513, i8* %21, align 1
  %26514 = xor i32 %26503, %26502
  %26515 = lshr i32 %26514, 4
  %26516 = trunc i32 %26515 to i8
  %26517 = and i8 %26516, 1
  store i8 %26517, i8* %27, align 1
  %26518 = zext i1 %26506 to i8
  store i8 %26518, i8* %30, align 1
  %26519 = lshr i32 %26503, 31
  %26520 = trunc i32 %26519 to i8
  store i8 %26520, i8* %33, align 1
  %26521 = lshr i32 %26502, 31
  %26522 = xor i32 %26519, %26521
  %26523 = add nuw nsw i32 %26522, %26519
  %26524 = icmp eq i32 %26523, 2
  %26525 = zext i1 %26524 to i8
  store i8 %26525, i8* %39, align 1
  %26526 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %26527 = add i64 %26498, %26526
  %26528 = add i64 %26400, 52
  store i64 %26528, i64* %3, align 8
  %26529 = inttoptr i64 %26527 to i32*
  store i32 %26503, i32* %26529, align 4
  %26530 = load i64, i64* %RBP.i, align 8
  %26531 = add i64 %26530, -8
  %26532 = load i64, i64* %3, align 8
  %26533 = add i64 %26532, 4
  store i64 %26533, i64* %3, align 8
  %26534 = inttoptr i64 %26531 to i64*
  %26535 = load i64, i64* %26534, align 8
  %26536 = add i64 %26535, 45448
  store i64 %26536, i64* %RAX.i11582.pre-phi, align 8
  %26537 = icmp ugt i64 %26535, -45449
  %26538 = zext i1 %26537 to i8
  store i8 %26538, i8* %14, align 1
  %26539 = trunc i64 %26536 to i32
  %26540 = and i32 %26539, 255
  %26541 = tail call i32 @llvm.ctpop.i32(i32 %26540)
  %26542 = trunc i32 %26541 to i8
  %26543 = and i8 %26542, 1
  %26544 = xor i8 %26543, 1
  store i8 %26544, i8* %21, align 1
  %26545 = xor i64 %26536, %26535
  %26546 = lshr i64 %26545, 4
  %26547 = trunc i64 %26546 to i8
  %26548 = and i8 %26547, 1
  store i8 %26548, i8* %27, align 1
  %26549 = icmp eq i64 %26536, 0
  %26550 = zext i1 %26549 to i8
  store i8 %26550, i8* %30, align 1
  %26551 = lshr i64 %26536, 63
  %26552 = trunc i64 %26551 to i8
  store i8 %26552, i8* %33, align 1
  %26553 = lshr i64 %26535, 63
  %26554 = xor i64 %26551, %26553
  %26555 = add nuw nsw i64 %26554, %26551
  %26556 = icmp eq i64 %26555, 2
  %26557 = zext i1 %26556 to i8
  store i8 %26557, i8* %39, align 1
  %26558 = add i64 %26530, -40
  %26559 = add i64 %26532, 14
  store i64 %26559, i64* %3, align 8
  %26560 = inttoptr i64 %26558 to i32*
  %26561 = load i32, i32* %26560, align 4
  %26562 = sext i32 %26561 to i64
  %26563 = mul nsw i64 %26562, 1032
  store i64 %26563, i64* %RCX.i11580, align 8
  %26564 = lshr i64 %26563, 63
  %26565 = add i64 %26563, %26536
  store i64 %26565, i64* %RAX.i11582.pre-phi, align 8
  %26566 = icmp ult i64 %26565, %26536
  %26567 = icmp ult i64 %26565, %26563
  %26568 = or i1 %26566, %26567
  %26569 = zext i1 %26568 to i8
  store i8 %26569, i8* %14, align 1
  %26570 = trunc i64 %26565 to i32
  %26571 = and i32 %26570, 255
  %26572 = tail call i32 @llvm.ctpop.i32(i32 %26571)
  %26573 = trunc i32 %26572 to i8
  %26574 = and i8 %26573, 1
  %26575 = xor i8 %26574, 1
  store i8 %26575, i8* %21, align 1
  %26576 = xor i64 %26563, %26536
  %26577 = xor i64 %26576, %26565
  %26578 = lshr i64 %26577, 4
  %26579 = trunc i64 %26578 to i8
  %26580 = and i8 %26579, 1
  store i8 %26580, i8* %27, align 1
  %26581 = icmp eq i64 %26565, 0
  %26582 = zext i1 %26581 to i8
  store i8 %26582, i8* %30, align 1
  %26583 = lshr i64 %26565, 63
  %26584 = trunc i64 %26583 to i8
  store i8 %26584, i8* %33, align 1
  %26585 = xor i64 %26583, %26551
  %26586 = xor i64 %26583, %26564
  %26587 = add nuw nsw i64 %26585, %26586
  %26588 = icmp eq i64 %26587, 2
  %26589 = zext i1 %26588 to i8
  store i8 %26589, i8* %39, align 1
  %26590 = load i64, i64* %RBP.i, align 8
  %26591 = add i64 %26590, -120
  %26592 = add i64 %26532, 28
  store i64 %26592, i64* %3, align 8
  %26593 = inttoptr i64 %26591 to i64*
  %26594 = load i64, i64* %26593, align 8
  store i64 %26594, i64* %RCX.i11580, align 8
  %26595 = add i64 %26590, -28
  %26596 = add i64 %26532, 31
  store i64 %26596, i64* %3, align 8
  %26597 = inttoptr i64 %26595 to i32*
  %26598 = load i32, i32* %26597, align 4
  %26599 = add i32 %26598, 49
  %26600 = zext i32 %26599 to i64
  store i64 %26600, i64* %576, align 8
  %26601 = icmp ugt i32 %26598, -50
  %26602 = zext i1 %26601 to i8
  store i8 %26602, i8* %14, align 1
  %26603 = and i32 %26599, 255
  %26604 = tail call i32 @llvm.ctpop.i32(i32 %26603)
  %26605 = trunc i32 %26604 to i8
  %26606 = and i8 %26605, 1
  %26607 = xor i8 %26606, 1
  store i8 %26607, i8* %21, align 1
  %26608 = xor i32 %26598, 16
  %26609 = xor i32 %26608, %26599
  %26610 = lshr i32 %26609, 4
  %26611 = trunc i32 %26610 to i8
  %26612 = and i8 %26611, 1
  store i8 %26612, i8* %27, align 1
  %26613 = icmp eq i32 %26599, 0
  %26614 = zext i1 %26613 to i8
  store i8 %26614, i8* %30, align 1
  %26615 = lshr i32 %26599, 31
  %26616 = trunc i32 %26615 to i8
  store i8 %26616, i8* %33, align 1
  %26617 = lshr i32 %26598, 31
  %26618 = xor i32 %26615, %26617
  %26619 = add nuw nsw i32 %26618, %26615
  %26620 = icmp eq i32 %26619, 2
  %26621 = zext i1 %26620 to i8
  store i8 %26621, i8* %39, align 1
  %26622 = sext i32 %26599 to i64
  store i64 %26622, i64* %RSI.i11312, align 8
  %26623 = shl nsw i64 %26622, 1
  %26624 = add i64 %26594, %26623
  %26625 = add i64 %26532, 41
  store i64 %26625, i64* %3, align 8
  %26626 = inttoptr i64 %26624 to i16*
  %26627 = load i16, i16* %26626, align 2
  %26628 = zext i16 %26627 to i64
  store i64 %26628, i64* %576, align 8
  %26629 = zext i16 %26627 to i64
  store i64 %26629, i64* %RCX.i11580, align 8
  %26630 = shl nuw nsw i64 %26629, 2
  %26631 = add i64 %26565, %26630
  %26632 = add i64 %26532, 46
  store i64 %26632, i64* %3, align 8
  %26633 = inttoptr i64 %26631 to i32*
  %26634 = load i32, i32* %26633, align 4
  %26635 = add i32 %26634, 1
  %26636 = zext i32 %26635 to i64
  store i64 %26636, i64* %576, align 8
  %26637 = icmp eq i32 %26634, -1
  %26638 = icmp eq i32 %26635, 0
  %26639 = or i1 %26637, %26638
  %26640 = zext i1 %26639 to i8
  store i8 %26640, i8* %14, align 1
  %26641 = and i32 %26635, 255
  %26642 = tail call i32 @llvm.ctpop.i32(i32 %26641)
  %26643 = trunc i32 %26642 to i8
  %26644 = and i8 %26643, 1
  %26645 = xor i8 %26644, 1
  store i8 %26645, i8* %21, align 1
  %26646 = xor i32 %26635, %26634
  %26647 = lshr i32 %26646, 4
  %26648 = trunc i32 %26647 to i8
  %26649 = and i8 %26648, 1
  store i8 %26649, i8* %27, align 1
  %26650 = zext i1 %26638 to i8
  store i8 %26650, i8* %30, align 1
  %26651 = lshr i32 %26635, 31
  %26652 = trunc i32 %26651 to i8
  store i8 %26652, i8* %33, align 1
  %26653 = lshr i32 %26634, 31
  %26654 = xor i32 %26651, %26653
  %26655 = add nuw nsw i32 %26654, %26651
  %26656 = icmp eq i32 %26655, 2
  %26657 = zext i1 %26656 to i8
  store i8 %26657, i8* %39, align 1
  %26658 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %26659 = add i64 %26630, %26658
  %26660 = add i64 %26532, 52
  store i64 %26660, i64* %3, align 8
  %26661 = inttoptr i64 %26659 to i32*
  store i32 %26635, i32* %26661, align 4
  %26662 = load i64, i64* %3, align 8
  %26663 = add i64 %26662, 89
  %.pre275 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40d11d

block_.L_40d0c9:                                  ; preds = %block_40c686, %block_.L_40c647
  %26664 = phi i64 [ %20071, %block_.L_40c647 ], [ %20105, %block_40c686 ]
  %26665 = phi i64 [ %20044, %block_.L_40c647 ], [ %20106, %block_40c686 ]
  %26666 = add i64 %26665, -28
  %26667 = add i64 %26664, 3
  store i64 %26667, i64* %3, align 8
  %26668 = inttoptr i64 %26666 to i32*
  %26669 = load i32, i32* %26668, align 4
  %26670 = zext i32 %26669 to i64
  store i64 %26670, i64* %RAX.i11582.pre-phi, align 8
  %26671 = add i64 %26665, -20
  %26672 = add i64 %26664, 6
  store i64 %26672, i64* %3, align 8
  %26673 = inttoptr i64 %26671 to i32*
  store i32 %26669, i32* %26673, align 4
  %.pre274 = load i64, i64* %3, align 8
  br label %block_.L_40d0cf

block_.L_40d0cf:                                  ; preds = %block_40d0db, %block_.L_40d0c9
  %26674 = phi i64 [ %26847, %block_40d0db ], [ %.pre274, %block_.L_40d0c9 ]
  %26675 = load i64, i64* %RBP.i, align 8
  %26676 = add i64 %26675, -20
  %26677 = add i64 %26674, 3
  store i64 %26677, i64* %3, align 8
  %26678 = inttoptr i64 %26676 to i32*
  %26679 = load i32, i32* %26678, align 4
  %26680 = zext i32 %26679 to i64
  store i64 %26680, i64* %RAX.i11582.pre-phi, align 8
  %26681 = add i64 %26675, -32
  %26682 = add i64 %26674, 6
  store i64 %26682, i64* %3, align 8
  %26683 = inttoptr i64 %26681 to i32*
  %26684 = load i32, i32* %26683, align 4
  %26685 = sub i32 %26679, %26684
  %26686 = icmp ult i32 %26679, %26684
  %26687 = zext i1 %26686 to i8
  store i8 %26687, i8* %14, align 1
  %26688 = and i32 %26685, 255
  %26689 = tail call i32 @llvm.ctpop.i32(i32 %26688)
  %26690 = trunc i32 %26689 to i8
  %26691 = and i8 %26690, 1
  %26692 = xor i8 %26691, 1
  store i8 %26692, i8* %21, align 1
  %26693 = xor i32 %26684, %26679
  %26694 = xor i32 %26693, %26685
  %26695 = lshr i32 %26694, 4
  %26696 = trunc i32 %26695 to i8
  %26697 = and i8 %26696, 1
  store i8 %26697, i8* %27, align 1
  %26698 = icmp eq i32 %26685, 0
  %26699 = zext i1 %26698 to i8
  store i8 %26699, i8* %30, align 1
  %26700 = lshr i32 %26685, 31
  %26701 = trunc i32 %26700 to i8
  store i8 %26701, i8* %33, align 1
  %26702 = lshr i32 %26679, 31
  %26703 = lshr i32 %26684, 31
  %26704 = xor i32 %26703, %26702
  %26705 = xor i32 %26700, %26702
  %26706 = add nuw nsw i32 %26705, %26704
  %26707 = icmp eq i32 %26706, 2
  %26708 = zext i1 %26707 to i8
  store i8 %26708, i8* %39, align 1
  %26709 = icmp ne i8 %26701, 0
  %26710 = xor i1 %26709, %26707
  %.demorgan291 = or i1 %26698, %26710
  %.v357 = select i1 %.demorgan291, i64 12, i64 73
  %26711 = add i64 %26674, %.v357
  store i64 %26711, i64* %3, align 8
  br i1 %.demorgan291, label %block_40d0db, label %block_.L_40d118

block_40d0db:                                     ; preds = %block_.L_40d0cf
  %26712 = add i64 %26675, -8
  %26713 = add i64 %26711, 4
  store i64 %26713, i64* %3, align 8
  %26714 = inttoptr i64 %26712 to i64*
  %26715 = load i64, i64* %26714, align 8
  %26716 = add i64 %26715, 45448
  store i64 %26716, i64* %RAX.i11582.pre-phi, align 8
  %26717 = icmp ugt i64 %26715, -45449
  %26718 = zext i1 %26717 to i8
  store i8 %26718, i8* %14, align 1
  %26719 = trunc i64 %26716 to i32
  %26720 = and i32 %26719, 255
  %26721 = tail call i32 @llvm.ctpop.i32(i32 %26720)
  %26722 = trunc i32 %26721 to i8
  %26723 = and i8 %26722, 1
  %26724 = xor i8 %26723, 1
  store i8 %26724, i8* %21, align 1
  %26725 = xor i64 %26716, %26715
  %26726 = lshr i64 %26725, 4
  %26727 = trunc i64 %26726 to i8
  %26728 = and i8 %26727, 1
  store i8 %26728, i8* %27, align 1
  %26729 = icmp eq i64 %26716, 0
  %26730 = zext i1 %26729 to i8
  store i8 %26730, i8* %30, align 1
  %26731 = lshr i64 %26716, 63
  %26732 = trunc i64 %26731 to i8
  store i8 %26732, i8* %33, align 1
  %26733 = lshr i64 %26715, 63
  %26734 = xor i64 %26731, %26733
  %26735 = add nuw nsw i64 %26734, %26731
  %26736 = icmp eq i64 %26735, 2
  %26737 = zext i1 %26736 to i8
  store i8 %26737, i8* %39, align 1
  %26738 = add i64 %26675, -40
  %26739 = add i64 %26711, 14
  store i64 %26739, i64* %3, align 8
  %26740 = inttoptr i64 %26738 to i32*
  %26741 = load i32, i32* %26740, align 4
  %26742 = sext i32 %26741 to i64
  %26743 = mul nsw i64 %26742, 1032
  store i64 %26743, i64* %RCX.i11580, align 8
  %26744 = lshr i64 %26743, 63
  %26745 = add i64 %26743, %26716
  store i64 %26745, i64* %RAX.i11582.pre-phi, align 8
  %26746 = icmp ult i64 %26745, %26716
  %26747 = icmp ult i64 %26745, %26743
  %26748 = or i1 %26746, %26747
  %26749 = zext i1 %26748 to i8
  store i8 %26749, i8* %14, align 1
  %26750 = trunc i64 %26745 to i32
  %26751 = and i32 %26750, 255
  %26752 = tail call i32 @llvm.ctpop.i32(i32 %26751)
  %26753 = trunc i32 %26752 to i8
  %26754 = and i8 %26753, 1
  %26755 = xor i8 %26754, 1
  store i8 %26755, i8* %21, align 1
  %26756 = xor i64 %26743, %26716
  %26757 = xor i64 %26756, %26745
  %26758 = lshr i64 %26757, 4
  %26759 = trunc i64 %26758 to i8
  %26760 = and i8 %26759, 1
  store i8 %26760, i8* %27, align 1
  %26761 = icmp eq i64 %26745, 0
  %26762 = zext i1 %26761 to i8
  store i8 %26762, i8* %30, align 1
  %26763 = lshr i64 %26745, 63
  %26764 = trunc i64 %26763 to i8
  store i8 %26764, i8* %33, align 1
  %26765 = xor i64 %26763, %26731
  %26766 = xor i64 %26763, %26744
  %26767 = add nuw nsw i64 %26765, %26766
  %26768 = icmp eq i64 %26767, 2
  %26769 = zext i1 %26768 to i8
  store i8 %26769, i8* %39, align 1
  %26770 = load i64, i64* %RBP.i, align 8
  %26771 = add i64 %26770, -120
  %26772 = add i64 %26711, 28
  store i64 %26772, i64* %3, align 8
  %26773 = inttoptr i64 %26771 to i64*
  %26774 = load i64, i64* %26773, align 8
  store i64 %26774, i64* %RCX.i11580, align 8
  %26775 = add i64 %26770, -20
  %26776 = add i64 %26711, 32
  store i64 %26776, i64* %3, align 8
  %26777 = inttoptr i64 %26775 to i32*
  %26778 = load i32, i32* %26777, align 4
  %26779 = sext i32 %26778 to i64
  store i64 %26779, i64* %576, align 8
  %26780 = shl nsw i64 %26779, 1
  %26781 = add i64 %26780, %26774
  %26782 = add i64 %26711, 36
  store i64 %26782, i64* %3, align 8
  %26783 = inttoptr i64 %26781 to i16*
  %26784 = load i16, i16* %26783, align 2
  %26785 = zext i16 %26784 to i64
  store i64 %26785, i64* %RSI.i11312, align 8
  %26786 = zext i16 %26784 to i64
  store i64 %26786, i64* %RCX.i11580, align 8
  %26787 = shl nuw nsw i64 %26786, 2
  %26788 = add i64 %26745, %26787
  %26789 = add i64 %26711, 41
  store i64 %26789, i64* %3, align 8
  %26790 = inttoptr i64 %26788 to i32*
  %26791 = load i32, i32* %26790, align 4
  %26792 = add i32 %26791, 1
  %26793 = zext i32 %26792 to i64
  store i64 %26793, i64* %RSI.i11312, align 8
  %26794 = icmp eq i32 %26791, -1
  %26795 = icmp eq i32 %26792, 0
  %26796 = or i1 %26794, %26795
  %26797 = zext i1 %26796 to i8
  store i8 %26797, i8* %14, align 1
  %26798 = and i32 %26792, 255
  %26799 = tail call i32 @llvm.ctpop.i32(i32 %26798)
  %26800 = trunc i32 %26799 to i8
  %26801 = and i8 %26800, 1
  %26802 = xor i8 %26801, 1
  store i8 %26802, i8* %21, align 1
  %26803 = xor i32 %26792, %26791
  %26804 = lshr i32 %26803, 4
  %26805 = trunc i32 %26804 to i8
  %26806 = and i8 %26805, 1
  store i8 %26806, i8* %27, align 1
  %26807 = zext i1 %26795 to i8
  store i8 %26807, i8* %30, align 1
  %26808 = lshr i32 %26792, 31
  %26809 = trunc i32 %26808 to i8
  store i8 %26809, i8* %33, align 1
  %26810 = lshr i32 %26791, 31
  %26811 = xor i32 %26808, %26810
  %26812 = add nuw nsw i32 %26811, %26808
  %26813 = icmp eq i32 %26812, 2
  %26814 = zext i1 %26813 to i8
  store i8 %26814, i8* %39, align 1
  %26815 = add i64 %26711, 47
  store i64 %26815, i64* %3, align 8
  store i32 %26792, i32* %26790, align 4
  %26816 = load i64, i64* %RBP.i, align 8
  %26817 = add i64 %26816, -20
  %26818 = load i64, i64* %3, align 8
  %26819 = add i64 %26818, 3
  store i64 %26819, i64* %3, align 8
  %26820 = inttoptr i64 %26817 to i32*
  %26821 = load i32, i32* %26820, align 4
  %26822 = add i32 %26821, 1
  %26823 = zext i32 %26822 to i64
  store i64 %26823, i64* %RAX.i11582.pre-phi, align 8
  %26824 = icmp eq i32 %26821, -1
  %26825 = icmp eq i32 %26822, 0
  %26826 = or i1 %26824, %26825
  %26827 = zext i1 %26826 to i8
  store i8 %26827, i8* %14, align 1
  %26828 = and i32 %26822, 255
  %26829 = tail call i32 @llvm.ctpop.i32(i32 %26828)
  %26830 = trunc i32 %26829 to i8
  %26831 = and i8 %26830, 1
  %26832 = xor i8 %26831, 1
  store i8 %26832, i8* %21, align 1
  %26833 = xor i32 %26822, %26821
  %26834 = lshr i32 %26833, 4
  %26835 = trunc i32 %26834 to i8
  %26836 = and i8 %26835, 1
  store i8 %26836, i8* %27, align 1
  %26837 = zext i1 %26825 to i8
  store i8 %26837, i8* %30, align 1
  %26838 = lshr i32 %26822, 31
  %26839 = trunc i32 %26838 to i8
  store i8 %26839, i8* %33, align 1
  %26840 = lshr i32 %26821, 31
  %26841 = xor i32 %26838, %26840
  %26842 = add nuw nsw i32 %26841, %26838
  %26843 = icmp eq i32 %26842, 2
  %26844 = zext i1 %26843 to i8
  store i8 %26844, i8* %39, align 1
  %26845 = add i64 %26818, 9
  store i64 %26845, i64* %3, align 8
  store i32 %26822, i32* %26820, align 4
  %26846 = load i64, i64* %3, align 8
  %26847 = add i64 %26846, -68
  store i64 %26847, i64* %3, align 8
  br label %block_.L_40d0cf

block_.L_40d118:                                  ; preds = %block_.L_40d0cf
  %26848 = add i64 %26711, 5
  store i64 %26848, i64* %3, align 8
  br label %block_.L_40d11d

block_.L_40d11d:                                  ; preds = %block_.L_40d118, %block_40c69c
  %26849 = phi i64 [ %.pre275, %block_40c69c ], [ %26675, %block_.L_40d118 ]
  %storemerge83 = phi i64 [ %26663, %block_40c69c ], [ %26848, %block_.L_40d118 ]
  %26850 = add i64 %26849, -32
  %26851 = add i64 %storemerge83, 3
  store i64 %26851, i64* %3, align 8
  %26852 = inttoptr i64 %26850 to i32*
  %26853 = load i32, i32* %26852, align 4
  %26854 = add i32 %26853, 1
  %26855 = zext i32 %26854 to i64
  store i64 %26855, i64* %RAX.i11582.pre-phi, align 8
  %26856 = icmp eq i32 %26853, -1
  %26857 = icmp eq i32 %26854, 0
  %26858 = or i1 %26856, %26857
  %26859 = zext i1 %26858 to i8
  store i8 %26859, i8* %14, align 1
  %26860 = and i32 %26854, 255
  %26861 = tail call i32 @llvm.ctpop.i32(i32 %26860)
  %26862 = trunc i32 %26861 to i8
  %26863 = and i8 %26862, 1
  %26864 = xor i8 %26863, 1
  store i8 %26864, i8* %21, align 1
  %26865 = xor i32 %26854, %26853
  %26866 = lshr i32 %26865, 4
  %26867 = trunc i32 %26866 to i8
  %26868 = and i8 %26867, 1
  store i8 %26868, i8* %27, align 1
  %26869 = zext i1 %26857 to i8
  store i8 %26869, i8* %30, align 1
  %26870 = lshr i32 %26854, 31
  %26871 = trunc i32 %26870 to i8
  store i8 %26871, i8* %33, align 1
  %26872 = lshr i32 %26853, 31
  %26873 = xor i32 %26870, %26872
  %26874 = add nuw nsw i32 %26873, %26870
  %26875 = icmp eq i32 %26874, 2
  %26876 = zext i1 %26875 to i8
  store i8 %26876, i8* %39, align 1
  %26877 = add i64 %26849, -28
  %26878 = add i64 %storemerge83, 9
  store i64 %26878, i64* %3, align 8
  %26879 = inttoptr i64 %26877 to i32*
  store i32 %26854, i32* %26879, align 4
  %26880 = load i64, i64* %3, align 8
  %26881 = add i64 %26880, -10608
  store i64 %26881, i64* %3, align 8
  br label %block_.L_40a7b6

block_40d13c:                                     ; preds = %block_40a7c9
  store i64 ptrtoint (%G__0x416594_type* @G__0x416594 to i64), i64* %RSI.i11290, align 8
  store i64 8, i64* %RAX.i11582.pre-phi, align 8
  %26882 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %26882, i64* %RDI.i2910, align 8
  %26883 = add i64 %2351, -48
  %26884 = add i64 %2421, 26
  store i64 %26884, i64* %3, align 8
  %26885 = inttoptr i64 %26883 to i32*
  %26886 = load i32, i32* %26885, align 4
  %26887 = add i32 %26886, 1
  %26888 = zext i32 %26887 to i64
  store i64 %26888, i64* %RCX.i11580, align 8
  %26889 = icmp eq i32 %26886, -1
  %26890 = icmp eq i32 %26887, 0
  %26891 = or i1 %26889, %26890
  %26892 = zext i1 %26891 to i8
  store i8 %26892, i8* %14, align 1
  %26893 = and i32 %26887, 255
  %26894 = tail call i32 @llvm.ctpop.i32(i32 %26893)
  %26895 = trunc i32 %26894 to i8
  %26896 = and i8 %26895, 1
  %26897 = xor i8 %26896, 1
  store i8 %26897, i8* %21, align 1
  %26898 = xor i32 %26887, %26886
  %26899 = lshr i32 %26898, 4
  %26900 = trunc i32 %26899 to i8
  %26901 = and i8 %26900, 1
  store i8 %26901, i8* %27, align 1
  %26902 = zext i1 %26890 to i8
  store i8 %26902, i8* %30, align 1
  %26903 = lshr i32 %26887, 31
  %26904 = trunc i32 %26903 to i8
  store i8 %26904, i8* %33, align 1
  %26905 = lshr i32 %26886, 31
  %26906 = xor i32 %26903, %26905
  %26907 = add nuw nsw i32 %26906, %26903
  %26908 = icmp eq i32 %26907, 2
  %26909 = zext i1 %26908 to i8
  store i8 %26909, i8* %39, align 1
  %26910 = add i64 %2351, -36
  %26911 = add i64 %2421, 32
  store i64 %26911, i64* %3, align 8
  %26912 = inttoptr i64 %26910 to i32*
  %26913 = load i32, i32* %26912, align 4
  %26914 = zext i32 %26913 to i64
  store i64 %26914, i64* %576, align 8
  %26915 = add i64 %2351, -236
  %26916 = add i64 %2421, 38
  store i64 %26916, i64* %3, align 8
  %26917 = inttoptr i64 %26915 to i32*
  store i32 8, i32* %26917, align 4
  %26918 = load i32, i32* %575, align 4
  %26919 = zext i32 %26918 to i64
  %26920 = load i64, i64* %3, align 8
  store i64 %26919, i64* %RAX.i11582.pre-phi, align 8
  %26921 = sext i32 %26918 to i64
  %26922 = lshr i64 %26921, 32
  store i64 %26922, i64* %573, align 8
  %26923 = load i64, i64* %RBP.i, align 8
  %26924 = add i64 %26923, -236
  %26925 = add i64 %26920, 10
  store i64 %26925, i64* %3, align 8
  %26926 = inttoptr i64 %26924 to i32*
  %26927 = load i32, i32* %26926, align 4
  %26928 = zext i32 %26927 to i64
  store i64 %26928, i64* %1608, align 8
  %26929 = add i64 %26920, 13
  store i64 %26929, i64* %3, align 8
  %26930 = sext i32 %26927 to i64
  %26931 = shl nuw i64 %26922, 32
  %26932 = or i64 %26931, %26919
  %26933 = sdiv i64 %26932, %26930
  %26934 = shl i64 %26933, 32
  %26935 = ashr exact i64 %26934, 32
  %26936 = icmp eq i64 %26933, %26935
  br i1 %26936, label %26939, label %26937

; <label>:26937:                                  ; preds = %block_40d13c
  %26938 = tail call %struct.Memory* @__remill_error(%struct.State* nonnull dereferenceable(3376) %0, i64 %26929, %struct.Memory* %MEMORY.16)
  %.pre259 = load i64, i64* %3, align 8
  %.pre260 = load i32, i32* %EAX.i11561.pre-phi, align 4
  br label %routine_idivl__r8d.exit

; <label>:26939:                                  ; preds = %block_40d13c
  %26940 = srem i64 %26932, %26930
  %26941 = and i64 %26933, 4294967295
  store i64 %26941, i64* %RAX.i11582.pre-phi, align 8
  %26942 = and i64 %26940, 4294967295
  store i64 %26942, i64* %576, align 8
  store i8 0, i8* %14, align 1
  store i8 0, i8* %21, align 1
  store i8 0, i8* %27, align 1
  store i8 0, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %26943 = trunc i64 %26933 to i32
  br label %routine_idivl__r8d.exit

routine_idivl__r8d.exit:                          ; preds = %26939, %26937
  %26944 = phi i32 [ %.pre260, %26937 ], [ %26943, %26939 ]
  %26945 = phi i64 [ %.pre259, %26937 ], [ %26929, %26939 ]
  %26946 = phi %struct.Memory* [ %26938, %26937 ], [ %MEMORY.16, %26939 ]
  %26947 = load i32, i32* %ECX.i11574, align 4
  %26948 = zext i32 %26947 to i64
  store i64 %26948, i64* %576, align 8
  %26949 = zext i32 %26944 to i64
  store i64 %26949, i64* %RCX.i11580, align 8
  store i8 0, i8* %AL.i11425, align 1
  %26950 = add i64 %26945, -51263
  %26951 = add i64 %26945, 11
  %26952 = load i64, i64* %6, align 8
  %26953 = add i64 %26952, -8
  %26954 = inttoptr i64 %26953 to i64*
  store i64 %26951, i64* %26954, align 8
  store i64 %26953, i64* %6, align 8
  store i64 %26950, i64* %3, align 8
  %26955 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %26946)
  %26956 = load i64, i64* %RBP.i, align 8
  %26957 = add i64 %26956, -16
  %26958 = load i64, i64* %3, align 8
  %26959 = add i64 %26958, 7
  store i64 %26959, i64* %3, align 8
  %26960 = inttoptr i64 %26957 to i32*
  store i32 0, i32* %26960, align 4
  %26961 = load i64, i64* %RBP.i, align 8
  %26962 = add i64 %26961, -240
  %26963 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %26964 = load i64, i64* %3, align 8
  %26965 = add i64 %26964, 6
  store i64 %26965, i64* %3, align 8
  %26966 = inttoptr i64 %26962 to i32*
  store i32 %26963, i32* %26966, align 4
  %.pre261 = load i64, i64* %3, align 8
  br label %block_.L_40d187

block_.L_40d187:                                  ; preds = %block_40d193, %routine_idivl__r8d.exit
  %26967 = phi i64 [ %.pre261, %routine_idivl__r8d.exit ], [ %27060, %block_40d193 ]
  %MEMORY.34 = phi %struct.Memory* [ %26955, %routine_idivl__r8d.exit ], [ %27022, %block_40d193 ]
  %26968 = load i64, i64* %RBP.i, align 8
  %26969 = add i64 %26968, -16
  %26970 = add i64 %26967, 3
  store i64 %26970, i64* %3, align 8
  %26971 = inttoptr i64 %26969 to i32*
  %26972 = load i32, i32* %26971, align 4
  %26973 = zext i32 %26972 to i64
  store i64 %26973, i64* %RAX.i11582.pre-phi, align 8
  %26974 = add i64 %26968, -72
  %26975 = add i64 %26967, 6
  store i64 %26975, i64* %3, align 8
  %26976 = inttoptr i64 %26974 to i32*
  %26977 = load i32, i32* %26976, align 4
  %26978 = sub i32 %26972, %26977
  %26979 = icmp ult i32 %26972, %26977
  %26980 = zext i1 %26979 to i8
  store i8 %26980, i8* %14, align 1
  %26981 = and i32 %26978, 255
  %26982 = tail call i32 @llvm.ctpop.i32(i32 %26981)
  %26983 = trunc i32 %26982 to i8
  %26984 = and i8 %26983, 1
  %26985 = xor i8 %26984, 1
  store i8 %26985, i8* %21, align 1
  %26986 = xor i32 %26977, %26972
  %26987 = xor i32 %26986, %26978
  %26988 = lshr i32 %26987, 4
  %26989 = trunc i32 %26988 to i8
  %26990 = and i8 %26989, 1
  store i8 %26990, i8* %27, align 1
  %26991 = icmp eq i32 %26978, 0
  %26992 = zext i1 %26991 to i8
  store i8 %26992, i8* %30, align 1
  %26993 = lshr i32 %26978, 31
  %26994 = trunc i32 %26993 to i8
  store i8 %26994, i8* %33, align 1
  %26995 = lshr i32 %26972, 31
  %26996 = lshr i32 %26977, 31
  %26997 = xor i32 %26996, %26995
  %26998 = xor i32 %26993, %26995
  %26999 = add nuw nsw i32 %26998, %26997
  %27000 = icmp eq i32 %26999, 2
  %27001 = zext i1 %27000 to i8
  store i8 %27001, i8* %39, align 1
  %27002 = icmp ne i8 %26994, 0
  %27003 = xor i1 %27002, %27000
  %.v349 = select i1 %27003, i64 12, i64 65
  %27004 = add i64 %26967, %.v349
  %27005 = add i64 %27004, 10
  store i64 %27005, i64* %3, align 8
  br i1 %27003, label %block_40d193, label %block_.L_40d1c8

block_40d193:                                     ; preds = %block_.L_40d187
  store i64 ptrtoint (%G__0x4165bd_type* @G__0x4165bd to i64), i64* %RSI.i11290, align 8
  %27006 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %27006, i64* %RDI.i2910, align 8
  %27007 = add i64 %27004, 22
  store i64 %27007, i64* %3, align 8
  %27008 = load i32, i32* %26971, align 4
  %27009 = sext i32 %27008 to i64
  store i64 %27009, i64* %RAX.i11582.pre-phi, align 8
  %27010 = shl nsw i64 %27009, 2
  %27011 = add i64 %26968, -112
  %27012 = add i64 %27011, %27010
  %27013 = add i64 %27004, 26
  store i64 %27013, i64* %3, align 8
  %27014 = inttoptr i64 %27012 to i32*
  %27015 = load i32, i32* %27014, align 4
  %27016 = zext i32 %27015 to i64
  store i64 %27016, i64* %576, align 8
  store i8 0, i8* %AL.i11425, align 1
  %27017 = add i64 %27004, -51299
  %27018 = add i64 %27004, 33
  %27019 = load i64, i64* %6, align 8
  %27020 = add i64 %27019, -8
  %27021 = inttoptr i64 %27020 to i64*
  store i64 %27018, i64* %27021, align 8
  store i64 %27020, i64* %6, align 8
  store i64 %27017, i64* %3, align 8
  %27022 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %MEMORY.34)
  %27023 = load i64, i64* %RBP.i, align 8
  %27024 = add i64 %27023, -244
  %27025 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %27026 = load i64, i64* %3, align 8
  %27027 = add i64 %27026, 6
  store i64 %27027, i64* %3, align 8
  %27028 = inttoptr i64 %27024 to i32*
  store i32 %27025, i32* %27028, align 4
  %27029 = load i64, i64* %RBP.i, align 8
  %27030 = add i64 %27029, -16
  %27031 = load i64, i64* %3, align 8
  %27032 = add i64 %27031, 3
  store i64 %27032, i64* %3, align 8
  %27033 = inttoptr i64 %27030 to i32*
  %27034 = load i32, i32* %27033, align 4
  %27035 = add i32 %27034, 1
  %27036 = zext i32 %27035 to i64
  store i64 %27036, i64* %RAX.i11582.pre-phi, align 8
  %27037 = icmp eq i32 %27034, -1
  %27038 = icmp eq i32 %27035, 0
  %27039 = or i1 %27037, %27038
  %27040 = zext i1 %27039 to i8
  store i8 %27040, i8* %14, align 1
  %27041 = and i32 %27035, 255
  %27042 = tail call i32 @llvm.ctpop.i32(i32 %27041)
  %27043 = trunc i32 %27042 to i8
  %27044 = and i8 %27043, 1
  %27045 = xor i8 %27044, 1
  store i8 %27045, i8* %21, align 1
  %27046 = xor i32 %27035, %27034
  %27047 = lshr i32 %27046, 4
  %27048 = trunc i32 %27047 to i8
  %27049 = and i8 %27048, 1
  store i8 %27049, i8* %27, align 1
  %27050 = zext i1 %27038 to i8
  store i8 %27050, i8* %30, align 1
  %27051 = lshr i32 %27035, 31
  %27052 = trunc i32 %27051 to i8
  store i8 %27052, i8* %33, align 1
  %27053 = lshr i32 %27034, 31
  %27054 = xor i32 %27051, %27053
  %27055 = add nuw nsw i32 %27054, %27051
  %27056 = icmp eq i32 %27055, 2
  %27057 = zext i1 %27056 to i8
  store i8 %27057, i8* %39, align 1
  %27058 = add i64 %27031, 9
  store i64 %27058, i64* %3, align 8
  store i32 %27035, i32* %27033, align 4
  %27059 = load i64, i64* %3, align 8
  %27060 = add i64 %27059, -60
  store i64 %27060, i64* %3, align 8
  br label %block_.L_40d187

block_.L_40d1c8:                                  ; preds = %block_.L_40d187
  store i64 ptrtoint (%G__0x415fb8_type* @G__0x415fb8 to i64), i64* %RSI.i11290, align 8
  %27061 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %27061, i64* %RDI.i2910, align 8
  store i8 0, i8* %AL.i11425, align 1
  %27062 = add i64 %27004, -51352
  %27063 = add i64 %27004, 25
  %27064 = load i64, i64* %6, align 8
  %27065 = add i64 %27064, -8
  %27066 = inttoptr i64 %27065 to i64*
  store i64 %27063, i64* %27066, align 8
  store i64 %27065, i64* %6, align 8
  store i64 %27062, i64* %3, align 8
  %27067 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %MEMORY.34)
  %27068 = load i64, i64* %RBP.i, align 8
  %27069 = add i64 %27068, -248
  %27070 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %27071 = load i64, i64* %3, align 8
  %27072 = add i64 %27071, 6
  store i64 %27072, i64* %3, align 8
  %27073 = inttoptr i64 %27069 to i32*
  store i32 %27070, i32* %27073, align 4
  %.pre262 = load i64, i64* %RBP.i, align 8
  %.pre263 = load i64, i64* %3, align 8
  br label %block_.L_40d1e7

block_.L_40d1e7:                                  ; preds = %block_.L_40d1c8, %block_40a7c9
  %27074 = phi i64 [ %2421, %block_40a7c9 ], [ %.pre263, %block_.L_40d1c8 ]
  %27075 = phi i64 [ %2351, %block_40a7c9 ], [ %.pre262, %block_.L_40d1c8 ]
  %MEMORY.35 = phi %struct.Memory* [ %MEMORY.16, %block_40a7c9 ], [ %27067, %block_.L_40d1c8 ]
  %27076 = add i64 %27075, -16
  %27077 = add i64 %27074, 7
  store i64 %27077, i64* %3, align 8
  %27078 = inttoptr i64 %27076 to i32*
  store i32 0, i32* %27078, align 4
  %.pre264 = load i64, i64* %3, align 8
  br label %block_.L_40d1ee

block_.L_40d1ee:                                  ; preds = %block_40d1fa, %block_.L_40d1e7
  %27079 = phi i64 [ %27286, %block_40d1fa ], [ %.pre264, %block_.L_40d1e7 ]
  %27080 = load i64, i64* %RBP.i, align 8
  %27081 = add i64 %27080, -16
  %27082 = add i64 %27079, 3
  store i64 %27082, i64* %3, align 8
  %27083 = inttoptr i64 %27081 to i32*
  %27084 = load i32, i32* %27083, align 4
  %27085 = zext i32 %27084 to i64
  store i64 %27085, i64* %RAX.i11582.pre-phi, align 8
  %27086 = add i64 %27080, -72
  %27087 = add i64 %27079, 6
  store i64 %27087, i64* %3, align 8
  %27088 = inttoptr i64 %27086 to i32*
  %27089 = load i32, i32* %27088, align 4
  %27090 = sub i32 %27084, %27089
  %27091 = icmp ult i32 %27084, %27089
  %27092 = zext i1 %27091 to i8
  store i8 %27092, i8* %14, align 1
  %27093 = and i32 %27090, 255
  %27094 = tail call i32 @llvm.ctpop.i32(i32 %27093)
  %27095 = trunc i32 %27094 to i8
  %27096 = and i8 %27095, 1
  %27097 = xor i8 %27096, 1
  store i8 %27097, i8* %21, align 1
  %27098 = xor i32 %27089, %27084
  %27099 = xor i32 %27098, %27090
  %27100 = lshr i32 %27099, 4
  %27101 = trunc i32 %27100 to i8
  %27102 = and i8 %27101, 1
  store i8 %27102, i8* %27, align 1
  %27103 = icmp eq i32 %27090, 0
  %27104 = zext i1 %27103 to i8
  store i8 %27104, i8* %30, align 1
  %27105 = lshr i32 %27090, 31
  %27106 = trunc i32 %27105 to i8
  store i8 %27106, i8* %33, align 1
  %27107 = lshr i32 %27084, 31
  %27108 = lshr i32 %27089, 31
  %27109 = xor i32 %27108, %27107
  %27110 = xor i32 %27105, %27107
  %27111 = add nuw nsw i32 %27110, %27109
  %27112 = icmp eq i32 %27111, 2
  %27113 = zext i1 %27112 to i8
  store i8 %27113, i8* %39, align 1
  %27114 = icmp ne i8 %27106, 0
  %27115 = xor i1 %27114, %27112
  %.v350 = select i1 %27115, i64 12, i64 106
  %27116 = add i64 %27079, %.v350
  %27117 = add i64 %27116, 5
  store i64 %27117, i64* %3, align 8
  br i1 %27115, label %block_40d1fa, label %block_.L_40d258

block_40d1fa:                                     ; preds = %block_.L_40d1ee
  store i64 17, i64* %RCX.i11580, align 8
  %27118 = add i64 %27080, -8
  %27119 = add i64 %27116, 9
  store i64 %27119, i64* %3, align 8
  %27120 = inttoptr i64 %27118 to i64*
  %27121 = load i64, i64* %27120, align 8
  %27122 = add i64 %27121, 37708
  store i64 %27122, i64* %RAX.i11582.pre-phi, align 8
  %27123 = icmp ugt i64 %27121, -37709
  %27124 = zext i1 %27123 to i8
  store i8 %27124, i8* %14, align 1
  %27125 = trunc i64 %27122 to i32
  %27126 = and i32 %27125, 255
  %27127 = tail call i32 @llvm.ctpop.i32(i32 %27126)
  %27128 = trunc i32 %27127 to i8
  %27129 = and i8 %27128, 1
  %27130 = xor i8 %27129, 1
  store i8 %27130, i8* %21, align 1
  %27131 = xor i64 %27122, %27121
  %27132 = lshr i64 %27131, 4
  %27133 = trunc i64 %27132 to i8
  %27134 = and i8 %27133, 1
  store i8 %27134, i8* %27, align 1
  %27135 = icmp eq i64 %27122, 0
  %27136 = zext i1 %27135 to i8
  store i8 %27136, i8* %30, align 1
  %27137 = lshr i64 %27122, 63
  %27138 = trunc i64 %27137 to i8
  store i8 %27138, i8* %33, align 1
  %27139 = lshr i64 %27121, 63
  %27140 = xor i64 %27137, %27139
  %27141 = add nuw nsw i64 %27140, %27137
  %27142 = icmp eq i64 %27141, 2
  %27143 = zext i1 %27142 to i8
  store i8 %27143, i8* %39, align 1
  %27144 = add i64 %27116, 19
  store i64 %27144, i64* %3, align 8
  %27145 = load i32, i32* %27083, align 4
  %27146 = sext i32 %27145 to i64
  %27147 = mul nsw i64 %27146, 258
  store i64 %27147, i64* %573, align 8
  %27148 = lshr i64 %27147, 63
  %27149 = add i64 %27147, %27122
  store i64 %27149, i64* %RAX.i11582.pre-phi, align 8
  %27150 = icmp ult i64 %27149, %27122
  %27151 = icmp ult i64 %27149, %27147
  %27152 = or i1 %27150, %27151
  %27153 = zext i1 %27152 to i8
  store i8 %27153, i8* %14, align 1
  %27154 = trunc i64 %27149 to i32
  %27155 = and i32 %27154, 255
  %27156 = tail call i32 @llvm.ctpop.i32(i32 %27155)
  %27157 = trunc i32 %27156 to i8
  %27158 = and i8 %27157, 1
  %27159 = xor i8 %27158, 1
  store i8 %27159, i8* %21, align 1
  %27160 = xor i64 %27147, %27122
  %27161 = xor i64 %27160, %27149
  %27162 = lshr i64 %27161, 4
  %27163 = trunc i64 %27162 to i8
  %27164 = and i8 %27163, 1
  store i8 %27164, i8* %27, align 1
  %27165 = icmp eq i64 %27149, 0
  %27166 = zext i1 %27165 to i8
  store i8 %27166, i8* %30, align 1
  %27167 = lshr i64 %27149, 63
  %27168 = trunc i64 %27167 to i8
  store i8 %27168, i8* %33, align 1
  %27169 = xor i64 %27167, %27137
  %27170 = xor i64 %27167, %27148
  %27171 = add nuw nsw i64 %27169, %27170
  %27172 = icmp eq i64 %27171, 2
  %27173 = zext i1 %27172 to i8
  store i8 %27173, i8* %39, align 1
  %27174 = load i64, i64* %RBP.i, align 8
  %27175 = add i64 %27174, -8
  %27176 = add i64 %27116, 33
  store i64 %27176, i64* %3, align 8
  %27177 = inttoptr i64 %27175 to i64*
  %27178 = load i64, i64* %27177, align 8
  %27179 = add i64 %27178, 45448
  store i64 %27179, i64* %573, align 8
  %27180 = icmp ugt i64 %27178, -45449
  %27181 = zext i1 %27180 to i8
  store i8 %27181, i8* %14, align 1
  %27182 = trunc i64 %27179 to i32
  %27183 = and i32 %27182, 255
  %27184 = tail call i32 @llvm.ctpop.i32(i32 %27183)
  %27185 = trunc i32 %27184 to i8
  %27186 = and i8 %27185, 1
  %27187 = xor i8 %27186, 1
  store i8 %27187, i8* %21, align 1
  %27188 = xor i64 %27179, %27178
  %27189 = lshr i64 %27188, 4
  %27190 = trunc i64 %27189 to i8
  %27191 = and i8 %27190, 1
  store i8 %27191, i8* %27, align 1
  %27192 = icmp eq i64 %27179, 0
  %27193 = zext i1 %27192 to i8
  store i8 %27193, i8* %30, align 1
  %27194 = lshr i64 %27179, 63
  %27195 = trunc i64 %27194 to i8
  store i8 %27195, i8* %33, align 1
  %27196 = lshr i64 %27178, 63
  %27197 = xor i64 %27194, %27196
  %27198 = add nuw nsw i64 %27197, %27194
  %27199 = icmp eq i64 %27198, 2
  %27200 = zext i1 %27199 to i8
  store i8 %27200, i8* %39, align 1
  %27201 = add i64 %27174, -16
  %27202 = add i64 %27116, 44
  store i64 %27202, i64* %3, align 8
  %27203 = inttoptr i64 %27201 to i32*
  %27204 = load i32, i32* %27203, align 4
  %27205 = sext i32 %27204 to i64
  %27206 = mul nsw i64 %27205, 1032
  store i64 %27206, i64* %RSI.i11290, align 8
  %27207 = lshr i64 %27206, 63
  %27208 = add i64 %27206, %27179
  store i64 %27208, i64* %573, align 8
  %27209 = icmp ult i64 %27208, %27179
  %27210 = icmp ult i64 %27208, %27206
  %27211 = or i1 %27209, %27210
  %27212 = zext i1 %27211 to i8
  store i8 %27212, i8* %14, align 1
  %27213 = trunc i64 %27208 to i32
  %27214 = and i32 %27213, 255
  %27215 = tail call i32 @llvm.ctpop.i32(i32 %27214)
  %27216 = trunc i32 %27215 to i8
  %27217 = and i8 %27216, 1
  %27218 = xor i8 %27217, 1
  store i8 %27218, i8* %21, align 1
  %27219 = xor i64 %27206, %27179
  %27220 = xor i64 %27219, %27208
  %27221 = lshr i64 %27220, 4
  %27222 = trunc i64 %27221 to i8
  %27223 = and i8 %27222, 1
  store i8 %27223, i8* %27, align 1
  %27224 = icmp eq i64 %27208, 0
  %27225 = zext i1 %27224 to i8
  store i8 %27225, i8* %30, align 1
  %27226 = lshr i64 %27208, 63
  %27227 = trunc i64 %27226 to i8
  store i8 %27227, i8* %33, align 1
  %27228 = xor i64 %27226, %27194
  %27229 = xor i64 %27226, %27207
  %27230 = add nuw nsw i64 %27228, %27229
  %27231 = icmp eq i64 %27230, 2
  %27232 = zext i1 %27231 to i8
  store i8 %27232, i8* %39, align 1
  %27233 = load i64, i64* %RBP.i, align 8
  %27234 = add i64 %27233, -56
  %27235 = add i64 %27116, 57
  store i64 %27235, i64* %3, align 8
  %27236 = inttoptr i64 %27234 to i32*
  %27237 = load i32, i32* %27236, align 4
  %27238 = add i64 %27233, -252
  %27239 = add i64 %27116, 63
  store i64 %27239, i64* %3, align 8
  %27240 = inttoptr i64 %27238 to i32*
  store i32 %27237, i32* %27240, align 4
  %27241 = load i64, i64* %RAX.i11582.pre-phi, align 8
  %27242 = load i64, i64* %3, align 8
  store i64 %27241, i64* %RDI.i2910, align 8
  %27243 = load i64, i64* %573, align 8
  store i64 %27243, i64* %RSI.i11290, align 8
  %27244 = load i64, i64* %RBP.i, align 8
  %27245 = add i64 %27244, -252
  %27246 = add i64 %27242, 12
  store i64 %27246, i64* %3, align 8
  %27247 = inttoptr i64 %27245 to i32*
  %27248 = load i32, i32* %27247, align 4
  %27249 = zext i32 %27248 to i64
  store i64 %27249, i64* %573, align 8
  %27250 = add i64 %27242, 25975
  %27251 = add i64 %27242, 17
  %27252 = load i64, i64* %6, align 8
  %27253 = add i64 %27252, -8
  %27254 = inttoptr i64 %27253 to i64*
  store i64 %27251, i64* %27254, align 8
  store i64 %27253, i64* %6, align 8
  store i64 %27250, i64* %3, align 8
  %call2_40d245 = tail call %struct.Memory* @sub_4137b0.BZ2_hbMakeCodeLengths(%struct.State* nonnull %0, i64 %27250, %struct.Memory* %MEMORY.35)
  %27255 = load i64, i64* %RBP.i, align 8
  %27256 = add i64 %27255, -16
  %27257 = load i64, i64* %3, align 8
  %27258 = add i64 %27257, 3
  store i64 %27258, i64* %3, align 8
  %27259 = inttoptr i64 %27256 to i32*
  %27260 = load i32, i32* %27259, align 4
  %27261 = add i32 %27260, 1
  %27262 = zext i32 %27261 to i64
  store i64 %27262, i64* %RAX.i11582.pre-phi, align 8
  %27263 = icmp eq i32 %27260, -1
  %27264 = icmp eq i32 %27261, 0
  %27265 = or i1 %27263, %27264
  %27266 = zext i1 %27265 to i8
  store i8 %27266, i8* %14, align 1
  %27267 = and i32 %27261, 255
  %27268 = tail call i32 @llvm.ctpop.i32(i32 %27267)
  %27269 = trunc i32 %27268 to i8
  %27270 = and i8 %27269, 1
  %27271 = xor i8 %27270, 1
  store i8 %27271, i8* %21, align 1
  %27272 = xor i32 %27261, %27260
  %27273 = lshr i32 %27272, 4
  %27274 = trunc i32 %27273 to i8
  %27275 = and i8 %27274, 1
  store i8 %27275, i8* %27, align 1
  %27276 = zext i1 %27264 to i8
  store i8 %27276, i8* %30, align 1
  %27277 = lshr i32 %27261, 31
  %27278 = trunc i32 %27277 to i8
  store i8 %27278, i8* %33, align 1
  %27279 = lshr i32 %27260, 31
  %27280 = xor i32 %27277, %27279
  %27281 = add nuw nsw i32 %27280, %27277
  %27282 = icmp eq i32 %27281, 2
  %27283 = zext i1 %27282 to i8
  store i8 %27283, i8* %39, align 1
  %27284 = add i64 %27257, 9
  store i64 %27284, i64* %3, align 8
  store i32 %27261, i32* %27259, align 4
  %27285 = load i64, i64* %3, align 8
  %27286 = add i64 %27285, -101
  store i64 %27286, i64* %3, align 8
  br label %block_.L_40d1ee

block_.L_40d258:                                  ; preds = %block_.L_40d1ee
  %27287 = add i64 %27080, -48
  %27288 = add i64 %27116, 8
  store i64 %27288, i64* %3, align 8
  %27289 = inttoptr i64 %27287 to i32*
  %27290 = load i32, i32* %27289, align 4
  %27291 = add i32 %27290, 1
  %27292 = zext i32 %27291 to i64
  store i64 %27292, i64* %RAX.i11582.pre-phi, align 8
  %27293 = icmp eq i32 %27290, -1
  %27294 = icmp eq i32 %27291, 0
  %27295 = or i1 %27293, %27294
  %27296 = zext i1 %27295 to i8
  store i8 %27296, i8* %14, align 1
  %27297 = and i32 %27291, 255
  %27298 = tail call i32 @llvm.ctpop.i32(i32 %27297)
  %27299 = trunc i32 %27298 to i8
  %27300 = and i8 %27299, 1
  %27301 = xor i8 %27300, 1
  store i8 %27301, i8* %21, align 1
  %27302 = xor i32 %27291, %27290
  %27303 = lshr i32 %27302, 4
  %27304 = trunc i32 %27303 to i8
  %27305 = and i8 %27304, 1
  store i8 %27305, i8* %27, align 1
  %27306 = zext i1 %27294 to i8
  store i8 %27306, i8* %30, align 1
  %27307 = lshr i32 %27291, 31
  %27308 = trunc i32 %27307 to i8
  store i8 %27308, i8* %33, align 1
  %27309 = lshr i32 %27290, 31
  %27310 = xor i32 %27307, %27309
  %27311 = add nuw nsw i32 %27310, %27307
  %27312 = icmp eq i32 %27311, 2
  %27313 = zext i1 %27312 to i8
  store i8 %27313, i8* %39, align 1
  %27314 = add i64 %27116, 14
  store i64 %27314, i64* %3, align 8
  store i32 %27291, i32* %27289, align 4
  %27315 = load i64, i64* %3, align 8
  %27316 = add i64 %27315, -11340
  store i64 %27316, i64* %3, align 8
  br label %block_.L_40a61a

block_.L_40d26b:                                  ; preds = %block_.L_40a61a
  %27317 = add i64 %1610, -72
  %27318 = add i64 %1638, 4
  store i64 %27318, i64* %3, align 8
  %27319 = inttoptr i64 %27317 to i32*
  %27320 = load i32, i32* %27319, align 4
  %27321 = add i32 %27320, -8
  %27322 = icmp ult i32 %27320, 8
  %27323 = zext i1 %27322 to i8
  store i8 %27323, i8* %14, align 1
  %27324 = and i32 %27321, 255
  %27325 = tail call i32 @llvm.ctpop.i32(i32 %27324)
  %27326 = trunc i32 %27325 to i8
  %27327 = and i8 %27326, 1
  %27328 = xor i8 %27327, 1
  store i8 %27328, i8* %21, align 1
  %27329 = xor i32 %27321, %27320
  %27330 = lshr i32 %27329, 4
  %27331 = trunc i32 %27330 to i8
  %27332 = and i8 %27331, 1
  store i8 %27332, i8* %27, align 1
  %27333 = icmp eq i32 %27321, 0
  %27334 = zext i1 %27333 to i8
  store i8 %27334, i8* %30, align 1
  %27335 = lshr i32 %27321, 31
  %27336 = trunc i32 %27335 to i8
  store i8 %27336, i8* %33, align 1
  %27337 = lshr i32 %27320, 31
  %27338 = xor i32 %27335, %27337
  %27339 = add nuw nsw i32 %27338, %27337
  %27340 = icmp eq i32 %27339, 2
  %27341 = zext i1 %27340 to i8
  store i8 %27341, i8* %39, align 1
  %27342 = icmp ne i8 %27336, 0
  %27343 = xor i1 %27342, %27340
  %.v309 = select i1 %27343, i64 20, i64 10
  %27344 = add i64 %1638, %.v309
  store i64 %27344, i64* %3, align 8
  br i1 %27343, label %block_.L_40d27f, label %block_40d275

block_40d275:                                     ; preds = %block_.L_40d26b
  store i64 3002, i64* %RDI.i2910, align 8
  %27345 = add i64 %27344, -33621
  %27346 = add i64 %27344, 10
  %27347 = load i64, i64* %6, align 8
  %27348 = add i64 %27347, -8
  %27349 = inttoptr i64 %27348 to i64*
  store i64 %27346, i64* %27349, align 8
  store i64 %27348, i64* %6, align 8
  store i64 %27345, i64* %3, align 8
  %call2_40d27a = tail call %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* nonnull %0, i64 %27345, %struct.Memory* %MEMORY.16)
  %.pre207 = load i64, i64* %RBP.i, align 8
  %.pre208 = load i64, i64* %3, align 8
  br label %block_.L_40d27f

block_.L_40d27f:                                  ; preds = %block_40d275, %block_.L_40d26b
  %27350 = phi i64 [ %27344, %block_.L_40d26b ], [ %.pre208, %block_40d275 ]
  %27351 = phi i64 [ %1610, %block_.L_40d26b ], [ %.pre207, %block_40d275 ]
  %MEMORY.37 = phi %struct.Memory* [ %MEMORY.16, %block_.L_40d26b ], [ %call2_40d27a, %block_40d275 ]
  %27352 = add i64 %27351, -52
  %27353 = add i64 %27350, 7
  store i64 %27353, i64* %3, align 8
  %27354 = inttoptr i64 %27352 to i32*
  %27355 = load i32, i32* %27354, align 4
  %27356 = add i32 %27355, -32768
  %27357 = icmp ult i32 %27355, 32768
  %27358 = zext i1 %27357 to i8
  store i8 %27358, i8* %14, align 1
  %27359 = and i32 %27356, 255
  %27360 = tail call i32 @llvm.ctpop.i32(i32 %27359)
  %27361 = trunc i32 %27360 to i8
  %27362 = and i8 %27361, 1
  %27363 = xor i8 %27362, 1
  store i8 %27363, i8* %21, align 1
  %27364 = xor i32 %27356, %27355
  %27365 = lshr i32 %27364, 4
  %27366 = trunc i32 %27365 to i8
  %27367 = and i8 %27366, 1
  store i8 %27367, i8* %27, align 1
  %27368 = icmp eq i32 %27356, 0
  %27369 = zext i1 %27368 to i8
  store i8 %27369, i8* %30, align 1
  %27370 = lshr i32 %27356, 31
  %27371 = trunc i32 %27370 to i8
  store i8 %27371, i8* %33, align 1
  %27372 = lshr i32 %27355, 31
  %27373 = xor i32 %27370, %27372
  %27374 = add nuw nsw i32 %27373, %27372
  %27375 = icmp eq i32 %27374, 2
  %27376 = zext i1 %27375 to i8
  store i8 %27376, i8* %39, align 1
  %27377 = icmp ne i8 %27371, 0
  %27378 = xor i1 %27377, %27375
  %.v310 = select i1 %27378, i64 13, i64 26
  %27379 = add i64 %27350, %.v310
  store i64 %27379, i64* %3, align 8
  br i1 %27378, label %block_40d28c, label %block_.L_40d299

block_40d28c:                                     ; preds = %block_.L_40d27f
  %27380 = add i64 %27379, 7
  store i64 %27380, i64* %3, align 8
  %27381 = load i32, i32* %27354, align 4
  %27382 = add i32 %27381, -18002
  %27383 = icmp ult i32 %27381, 18002
  %27384 = zext i1 %27383 to i8
  store i8 %27384, i8* %14, align 1
  %27385 = and i32 %27382, 255
  %27386 = tail call i32 @llvm.ctpop.i32(i32 %27385)
  %27387 = trunc i32 %27386 to i8
  %27388 = and i8 %27387, 1
  %27389 = xor i8 %27388, 1
  store i8 %27389, i8* %21, align 1
  %27390 = xor i32 %27381, 16
  %27391 = xor i32 %27390, %27382
  %27392 = lshr i32 %27391, 4
  %27393 = trunc i32 %27392 to i8
  %27394 = and i8 %27393, 1
  store i8 %27394, i8* %27, align 1
  %27395 = icmp eq i32 %27382, 0
  %27396 = zext i1 %27395 to i8
  store i8 %27396, i8* %30, align 1
  %27397 = lshr i32 %27382, 31
  %27398 = trunc i32 %27397 to i8
  store i8 %27398, i8* %33, align 1
  %27399 = lshr i32 %27381, 31
  %27400 = xor i32 %27397, %27399
  %27401 = add nuw nsw i32 %27400, %27399
  %27402 = icmp eq i32 %27401, 2
  %27403 = zext i1 %27402 to i8
  store i8 %27403, i8* %39, align 1
  %27404 = icmp ne i8 %27398, 0
  %27405 = xor i1 %27404, %27402
  %27406 = or i1 %27395, %27405
  %.v344 = select i1 %27406, i64 23, i64 13
  %27407 = add i64 %27379, %.v344
  store i64 %27407, i64* %3, align 8
  br i1 %27406, label %block_.L_40d2a3, label %block_.L_40d299

block_.L_40d299:                                  ; preds = %block_.L_40d27f, %block_40d28c
  %27408 = phi i64 [ %27379, %block_.L_40d27f ], [ %27407, %block_40d28c ]
  store i64 3003, i64* %RDI.i2910, align 8
  %27409 = add i64 %27408, -33657
  %27410 = add i64 %27408, 10
  %27411 = load i64, i64* %6, align 8
  %27412 = add i64 %27411, -8
  %27413 = inttoptr i64 %27412 to i64*
  store i64 %27410, i64* %27413, align 8
  store i64 %27412, i64* %6, align 8
  store i64 %27409, i64* %3, align 8
  %call2_40d29e = tail call %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* nonnull %0, i64 %27409, %struct.Memory* %MEMORY.37)
  %.pre209 = load i64, i64* %RBP.i, align 8
  %.pre210 = load i64, i64* %3, align 8
  br label %block_.L_40d2a3

block_.L_40d2a3:                                  ; preds = %block_.L_40d299, %block_40d28c
  %27414 = phi i64 [ %.pre210, %block_.L_40d299 ], [ %27407, %block_40d28c ]
  %27415 = phi i64 [ %.pre209, %block_.L_40d299 ], [ %27351, %block_40d28c ]
  %MEMORY.39 = phi %struct.Memory* [ %call2_40d29e, %block_.L_40d299 ], [ %MEMORY.37, %block_40d28c ]
  %27416 = add i64 %27415, -20
  %27417 = add i64 %27414, 7
  store i64 %27417, i64* %3, align 8
  %27418 = inttoptr i64 %27416 to i32*
  store i32 0, i32* %27418, align 4
  %.pre211 = load i64, i64* %3, align 8
  br label %block_.L_40d2aa

block_.L_40d2aa:                                  ; preds = %block_40d2b6, %block_.L_40d2a3
  %27419 = phi i64 [ %27499, %block_40d2b6 ], [ %.pre211, %block_.L_40d2a3 ]
  %27420 = load i64, i64* %RBP.i, align 8
  %27421 = add i64 %27420, -20
  %27422 = add i64 %27419, 3
  store i64 %27422, i64* %3, align 8
  %27423 = inttoptr i64 %27421 to i32*
  %27424 = load i32, i32* %27423, align 4
  %27425 = zext i32 %27424 to i64
  store i64 %27425, i64* %RAX.i11582.pre-phi, align 8
  %27426 = add i64 %27420, -72
  %27427 = add i64 %27419, 6
  store i64 %27427, i64* %3, align 8
  %27428 = inttoptr i64 %27426 to i32*
  %27429 = load i32, i32* %27428, align 4
  %27430 = sub i32 %27424, %27429
  %27431 = icmp ult i32 %27424, %27429
  %27432 = zext i1 %27431 to i8
  store i8 %27432, i8* %14, align 1
  %27433 = and i32 %27430, 255
  %27434 = tail call i32 @llvm.ctpop.i32(i32 %27433)
  %27435 = trunc i32 %27434 to i8
  %27436 = and i8 %27435, 1
  %27437 = xor i8 %27436, 1
  store i8 %27437, i8* %21, align 1
  %27438 = xor i32 %27429, %27424
  %27439 = xor i32 %27438, %27430
  %27440 = lshr i32 %27439, 4
  %27441 = trunc i32 %27440 to i8
  %27442 = and i8 %27441, 1
  store i8 %27442, i8* %27, align 1
  %27443 = icmp eq i32 %27430, 0
  %27444 = zext i1 %27443 to i8
  store i8 %27444, i8* %30, align 1
  %27445 = lshr i32 %27430, 31
  %27446 = trunc i32 %27445 to i8
  store i8 %27446, i8* %33, align 1
  %27447 = lshr i32 %27424, 31
  %27448 = lshr i32 %27429, 31
  %27449 = xor i32 %27448, %27447
  %27450 = xor i32 %27445, %27447
  %27451 = add nuw nsw i32 %27450, %27449
  %27452 = icmp eq i32 %27451, 2
  %27453 = zext i1 %27452 to i8
  store i8 %27453, i8* %39, align 1
  %27454 = icmp ne i8 %27446, 0
  %27455 = xor i1 %27454, %27452
  %.v311 = select i1 %27455, i64 12, i64 42
  %27456 = add i64 %27419, %.v311
  store i64 %27456, i64* %3, align 8
  br i1 %27455, label %block_40d2b6, label %block_.L_40d2d4

block_40d2b6:                                     ; preds = %block_.L_40d2aa
  %27457 = add i64 %27456, 3
  store i64 %27457, i64* %3, align 8
  %27458 = load i32, i32* %27423, align 4
  %27459 = zext i32 %27458 to i64
  store i64 %27459, i64* %RAX.i11582.pre-phi, align 8
  %27460 = trunc i32 %27458 to i8
  store i8 %27460, i8* %CL.i11426, align 1
  %27461 = add i64 %27456, 9
  store i64 %27461, i64* %3, align 8
  %27462 = load i32, i32* %27423, align 4
  %27463 = sext i32 %27462 to i64
  store i64 %27463, i64* %573, align 8
  %27464 = add nsw i64 %27463, -158
  %27465 = add i64 %27464, %27420
  %27466 = add i64 %27456, 16
  store i64 %27466, i64* %3, align 8
  %27467 = inttoptr i64 %27465 to i8*
  store i8 %27460, i8* %27467, align 1
  %27468 = load i64, i64* %RBP.i, align 8
  %27469 = add i64 %27468, -20
  %27470 = load i64, i64* %3, align 8
  %27471 = add i64 %27470, 3
  store i64 %27471, i64* %3, align 8
  %27472 = inttoptr i64 %27469 to i32*
  %27473 = load i32, i32* %27472, align 4
  %27474 = add i32 %27473, 1
  %27475 = zext i32 %27474 to i64
  store i64 %27475, i64* %RAX.i11582.pre-phi, align 8
  %27476 = icmp eq i32 %27473, -1
  %27477 = icmp eq i32 %27474, 0
  %27478 = or i1 %27476, %27477
  %27479 = zext i1 %27478 to i8
  store i8 %27479, i8* %14, align 1
  %27480 = and i32 %27474, 255
  %27481 = tail call i32 @llvm.ctpop.i32(i32 %27480)
  %27482 = trunc i32 %27481 to i8
  %27483 = and i8 %27482, 1
  %27484 = xor i8 %27483, 1
  store i8 %27484, i8* %21, align 1
  %27485 = xor i32 %27474, %27473
  %27486 = lshr i32 %27485, 4
  %27487 = trunc i32 %27486 to i8
  %27488 = and i8 %27487, 1
  store i8 %27488, i8* %27, align 1
  %27489 = zext i1 %27477 to i8
  store i8 %27489, i8* %30, align 1
  %27490 = lshr i32 %27474, 31
  %27491 = trunc i32 %27490 to i8
  store i8 %27491, i8* %33, align 1
  %27492 = lshr i32 %27473, 31
  %27493 = xor i32 %27490, %27492
  %27494 = add nuw nsw i32 %27493, %27490
  %27495 = icmp eq i32 %27494, 2
  %27496 = zext i1 %27495 to i8
  store i8 %27496, i8* %39, align 1
  %27497 = add i64 %27470, 9
  store i64 %27497, i64* %3, align 8
  store i32 %27474, i32* %27472, align 4
  %27498 = load i64, i64* %3, align 8
  %27499 = add i64 %27498, -37
  store i64 %27499, i64* %3, align 8
  br label %block_.L_40d2aa

block_.L_40d2d4:                                  ; preds = %block_.L_40d2aa
  %27500 = add i64 %27456, 7
  store i64 %27500, i64* %3, align 8
  store i32 0, i32* %27423, align 4
  %.pre212 = load i64, i64* %3, align 8
  br label %block_.L_40d2db

block_.L_40d2db:                                  ; preds = %block_.L_40d366, %block_.L_40d2d4
  %27501 = phi i64 [ %27733, %block_.L_40d366 ], [ %.pre212, %block_.L_40d2d4 ]
  %27502 = load i64, i64* %RBP.i, align 8
  %27503 = add i64 %27502, -20
  %27504 = add i64 %27501, 3
  store i64 %27504, i64* %3, align 8
  %27505 = inttoptr i64 %27503 to i32*
  %27506 = load i32, i32* %27505, align 4
  %27507 = zext i32 %27506 to i64
  store i64 %27507, i64* %RAX.i11582.pre-phi, align 8
  %27508 = add i64 %27502, -52
  %27509 = add i64 %27501, 6
  store i64 %27509, i64* %3, align 8
  %27510 = inttoptr i64 %27508 to i32*
  %27511 = load i32, i32* %27510, align 4
  %27512 = sub i32 %27506, %27511
  %27513 = icmp ult i32 %27506, %27511
  %27514 = zext i1 %27513 to i8
  store i8 %27514, i8* %14, align 1
  %27515 = and i32 %27512, 255
  %27516 = tail call i32 @llvm.ctpop.i32(i32 %27515)
  %27517 = trunc i32 %27516 to i8
  %27518 = and i8 %27517, 1
  %27519 = xor i8 %27518, 1
  store i8 %27519, i8* %21, align 1
  %27520 = xor i32 %27511, %27506
  %27521 = xor i32 %27520, %27512
  %27522 = lshr i32 %27521, 4
  %27523 = trunc i32 %27522 to i8
  %27524 = and i8 %27523, 1
  store i8 %27524, i8* %27, align 1
  %27525 = icmp eq i32 %27512, 0
  %27526 = zext i1 %27525 to i8
  store i8 %27526, i8* %30, align 1
  %27527 = lshr i32 %27512, 31
  %27528 = trunc i32 %27527 to i8
  store i8 %27528, i8* %33, align 1
  %27529 = lshr i32 %27506, 31
  %27530 = lshr i32 %27511, 31
  %27531 = xor i32 %27530, %27529
  %27532 = xor i32 %27527, %27529
  %27533 = add nuw nsw i32 %27532, %27531
  %27534 = icmp eq i32 %27533, 2
  %27535 = zext i1 %27534 to i8
  store i8 %27535, i8* %39, align 1
  %27536 = icmp ne i8 %27528, 0
  %27537 = xor i1 %27536, %27534
  %.v312 = select i1 %27537, i64 12, i64 185
  %27538 = add i64 %27501, %.v312
  store i64 %27538, i64* %3, align 8
  br i1 %27537, label %block_40d2e7, label %block_.L_40d394

block_40d2e7:                                     ; preds = %block_.L_40d2db
  %27539 = add i64 %27502, -8
  %27540 = add i64 %27538, 4
  store i64 %27540, i64* %3, align 8
  %27541 = inttoptr i64 %27539 to i64*
  %27542 = load i64, i64* %27541, align 8
  store i64 %27542, i64* %RAX.i11582.pre-phi, align 8
  %27543 = add i64 %27538, 8
  store i64 %27543, i64* %3, align 8
  %27544 = load i32, i32* %27505, align 4
  %27545 = sext i32 %27544 to i64
  store i64 %27545, i64* %RCX.i11580, align 8
  %27546 = add nsw i64 %27545, 1704
  %27547 = add i64 %27546, %27542
  %27548 = add i64 %27538, 15
  store i64 %27548, i64* %3, align 8
  %27549 = inttoptr i64 %27547 to i8*
  %27550 = load i8, i8* %27549, align 1
  store i8 %27550, i8* %DL.i11402, align 1
  %27551 = add i64 %27502, -159
  %27552 = add i64 %27538, 21
  store i64 %27552, i64* %3, align 8
  %27553 = inttoptr i64 %27551 to i8*
  store i8 %27550, i8* %27553, align 1
  %27554 = load i64, i64* %RBP.i, align 8
  %27555 = add i64 %27554, -24
  %27556 = load i64, i64* %3, align 8
  %27557 = add i64 %27556, 7
  store i64 %27557, i64* %3, align 8
  %27558 = inttoptr i64 %27555 to i32*
  store i32 0, i32* %27558, align 4
  %27559 = load i64, i64* %RBP.i, align 8
  %27560 = add i64 %27559, -24
  %27561 = load i64, i64* %3, align 8
  %27562 = add i64 %27561, 4
  store i64 %27562, i64* %3, align 8
  %27563 = inttoptr i64 %27560 to i32*
  %27564 = load i32, i32* %27563, align 4
  %27565 = sext i32 %27564 to i64
  store i64 %27565, i64* %RAX.i11582.pre-phi, align 8
  %27566 = add nsw i64 %27565, -158
  %27567 = add i64 %27566, %27559
  %27568 = add i64 %27561, 11
  store i64 %27568, i64* %3, align 8
  %27569 = inttoptr i64 %27567 to i8*
  %27570 = load i8, i8* %27569, align 1
  store i8 %27570, i8* %DL.i11402, align 1
  %27571 = add i64 %27559, -161
  %27572 = add i64 %27561, 17
  store i64 %27572, i64* %3, align 8
  %27573 = inttoptr i64 %27571 to i8*
  store i8 %27570, i8* %27573, align 1
  %.pre254 = load i64, i64* %3, align 8
  br label %block_.L_40d314

block_.L_40d314:                                  ; preds = %block_40d32a, %block_40d2e7
  %27574 = phi i64 [ %27675, %block_40d32a ], [ %.pre254, %block_40d2e7 ]
  %27575 = load i64, i64* %RBP.i, align 8
  %27576 = add i64 %27575, -159
  %27577 = add i64 %27574, 7
  store i64 %27577, i64* %3, align 8
  %27578 = inttoptr i64 %27576 to i8*
  %27579 = load i8, i8* %27578, align 1
  %27580 = zext i8 %27579 to i64
  store i64 %27580, i64* %RAX.i11582.pre-phi, align 8
  %27581 = add i64 %27575, -161
  %27582 = add i64 %27574, 14
  store i64 %27582, i64* %3, align 8
  %27583 = inttoptr i64 %27581 to i8*
  %27584 = load i8, i8* %27583, align 1
  %27585 = zext i8 %27584 to i64
  store i64 %27585, i64* %RCX.i11580, align 8
  %27586 = zext i8 %27579 to i32
  %27587 = zext i8 %27584 to i32
  %27588 = sub nsw i32 %27586, %27587
  %27589 = icmp ult i8 %27579, %27584
  %27590 = zext i1 %27589 to i8
  store i8 %27590, i8* %14, align 1
  %27591 = and i32 %27588, 255
  %27592 = tail call i32 @llvm.ctpop.i32(i32 %27591)
  %27593 = trunc i32 %27592 to i8
  %27594 = and i8 %27593, 1
  %27595 = xor i8 %27594, 1
  store i8 %27595, i8* %21, align 1
  %27596 = xor i8 %27584, %27579
  %27597 = zext i8 %27596 to i32
  %27598 = xor i32 %27597, %27588
  %27599 = lshr i32 %27598, 4
  %27600 = trunc i32 %27599 to i8
  %27601 = and i8 %27600, 1
  store i8 %27601, i8* %27, align 1
  %27602 = icmp eq i32 %27588, 0
  %27603 = zext i1 %27602 to i8
  store i8 %27603, i8* %30, align 1
  %27604 = lshr i32 %27588, 31
  %27605 = trunc i32 %27604 to i8
  store i8 %27605, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %.v343 = select i1 %27602, i64 82, i64 22
  %27606 = add i64 %27574, %.v343
  store i64 %27606, i64* %3, align 8
  br i1 %27602, label %block_.L_40d366, label %block_40d32a

block_40d32a:                                     ; preds = %block_.L_40d314
  %27607 = add i64 %27575, -24
  %27608 = add i64 %27606, 3
  store i64 %27608, i64* %3, align 8
  %27609 = inttoptr i64 %27607 to i32*
  %27610 = load i32, i32* %27609, align 4
  %27611 = add i32 %27610, 1
  %27612 = zext i32 %27611 to i64
  store i64 %27612, i64* %RAX.i11582.pre-phi, align 8
  %27613 = icmp eq i32 %27610, -1
  %27614 = icmp eq i32 %27611, 0
  %27615 = or i1 %27613, %27614
  %27616 = zext i1 %27615 to i8
  store i8 %27616, i8* %14, align 1
  %27617 = and i32 %27611, 255
  %27618 = tail call i32 @llvm.ctpop.i32(i32 %27617)
  %27619 = trunc i32 %27618 to i8
  %27620 = and i8 %27619, 1
  %27621 = xor i8 %27620, 1
  store i8 %27621, i8* %21, align 1
  %27622 = xor i32 %27611, %27610
  %27623 = lshr i32 %27622, 4
  %27624 = trunc i32 %27623 to i8
  %27625 = and i8 %27624, 1
  store i8 %27625, i8* %27, align 1
  %27626 = zext i1 %27614 to i8
  store i8 %27626, i8* %30, align 1
  %27627 = lshr i32 %27611, 31
  %27628 = trunc i32 %27627 to i8
  store i8 %27628, i8* %33, align 1
  %27629 = lshr i32 %27610, 31
  %27630 = xor i32 %27627, %27629
  %27631 = add nuw nsw i32 %27630, %27627
  %27632 = icmp eq i32 %27631, 2
  %27633 = zext i1 %27632 to i8
  store i8 %27633, i8* %39, align 1
  %27634 = add i64 %27606, 9
  store i64 %27634, i64* %3, align 8
  store i32 %27611, i32* %27609, align 4
  %27635 = load i64, i64* %RBP.i, align 8
  %27636 = add i64 %27635, -161
  %27637 = load i64, i64* %3, align 8
  %27638 = add i64 %27637, 6
  store i64 %27638, i64* %3, align 8
  %27639 = inttoptr i64 %27636 to i8*
  %27640 = load i8, i8* %27639, align 1
  store i8 %27640, i8* %CL.i11426, align 1
  %27641 = add i64 %27635, -160
  %27642 = add i64 %27637, 12
  store i64 %27642, i64* %3, align 8
  %27643 = inttoptr i64 %27641 to i8*
  store i8 %27640, i8* %27643, align 1
  %27644 = load i64, i64* %RBP.i, align 8
  %27645 = add i64 %27644, -24
  %27646 = load i64, i64* %3, align 8
  %27647 = add i64 %27646, 4
  store i64 %27647, i64* %3, align 8
  %27648 = inttoptr i64 %27645 to i32*
  %27649 = load i32, i32* %27648, align 4
  %27650 = sext i32 %27649 to i64
  store i64 %27650, i64* %576, align 8
  %27651 = add nsw i64 %27650, -158
  %27652 = add i64 %27651, %27644
  %27653 = add i64 %27646, 11
  store i64 %27653, i64* %3, align 8
  %27654 = inttoptr i64 %27652 to i8*
  %27655 = load i8, i8* %27654, align 1
  store i8 %27655, i8* %CL.i11426, align 1
  %27656 = add i64 %27644, -161
  %27657 = add i64 %27646, 17
  store i64 %27657, i64* %3, align 8
  %27658 = inttoptr i64 %27656 to i8*
  store i8 %27655, i8* %27658, align 1
  %27659 = load i64, i64* %RBP.i, align 8
  %27660 = add i64 %27659, -160
  %27661 = load i64, i64* %3, align 8
  %27662 = add i64 %27661, 6
  store i64 %27662, i64* %3, align 8
  %27663 = inttoptr i64 %27660 to i8*
  %27664 = load i8, i8* %27663, align 1
  store i8 %27664, i8* %CL.i11426, align 1
  %27665 = add i64 %27659, -24
  %27666 = add i64 %27661, 10
  store i64 %27666, i64* %3, align 8
  %27667 = inttoptr i64 %27665 to i32*
  %27668 = load i32, i32* %27667, align 4
  %27669 = sext i32 %27668 to i64
  store i64 %27669, i64* %576, align 8
  %27670 = add nsw i64 %27669, -158
  %27671 = add i64 %27670, %27659
  %27672 = add i64 %27661, 17
  store i64 %27672, i64* %3, align 8
  %27673 = inttoptr i64 %27671 to i8*
  store i8 %27664, i8* %27673, align 1
  %27674 = load i64, i64* %3, align 8
  %27675 = add i64 %27674, -77
  store i64 %27675, i64* %3, align 8
  br label %block_.L_40d314

block_.L_40d366:                                  ; preds = %block_.L_40d314
  %27676 = add i64 %27606, 6
  store i64 %27676, i64* %3, align 8
  %27677 = load i8, i8* %27583, align 1
  store i8 %27677, i8* %AL.i11425, align 1
  %27678 = add i64 %27575, -158
  %27679 = add i64 %27606, 12
  store i64 %27679, i64* %3, align 8
  %27680 = inttoptr i64 %27678 to i8*
  store i8 %27677, i8* %27680, align 1
  %27681 = load i64, i64* %RBP.i, align 8
  %27682 = add i64 %27681, -24
  %27683 = load i64, i64* %3, align 8
  %27684 = add i64 %27683, 3
  store i64 %27684, i64* %3, align 8
  %27685 = inttoptr i64 %27682 to i32*
  %27686 = load i32, i32* %27685, align 4
  %27687 = zext i32 %27686 to i64
  store i64 %27687, i64* %RCX.i11580, align 8
  %27688 = trunc i32 %27686 to i8
  store i8 %27688, i8* %AL.i11425, align 1
  %27689 = add i64 %27681, -8
  %27690 = add i64 %27683, 9
  store i64 %27690, i64* %3, align 8
  %27691 = inttoptr i64 %27689 to i64*
  %27692 = load i64, i64* %27691, align 8
  store i64 %27692, i64* %576, align 8
  %27693 = add i64 %27681, -20
  %27694 = add i64 %27683, 13
  store i64 %27694, i64* %3, align 8
  %27695 = inttoptr i64 %27693 to i32*
  %27696 = load i32, i32* %27695, align 4
  %27697 = sext i32 %27696 to i64
  store i64 %27697, i64* %RSI.i11290, align 8
  %27698 = add nsw i64 %27697, 19706
  %27699 = add i64 %27698, %27692
  %27700 = add i64 %27683, 20
  store i64 %27700, i64* %3, align 8
  %27701 = inttoptr i64 %27699 to i8*
  store i8 %27688, i8* %27701, align 1
  %27702 = load i64, i64* %RBP.i, align 8
  %27703 = add i64 %27702, -20
  %27704 = load i64, i64* %3, align 8
  %27705 = add i64 %27704, 3
  store i64 %27705, i64* %3, align 8
  %27706 = inttoptr i64 %27703 to i32*
  %27707 = load i32, i32* %27706, align 4
  %27708 = add i32 %27707, 1
  %27709 = zext i32 %27708 to i64
  store i64 %27709, i64* %RAX.i11582.pre-phi, align 8
  %27710 = icmp eq i32 %27707, -1
  %27711 = icmp eq i32 %27708, 0
  %27712 = or i1 %27710, %27711
  %27713 = zext i1 %27712 to i8
  store i8 %27713, i8* %14, align 1
  %27714 = and i32 %27708, 255
  %27715 = tail call i32 @llvm.ctpop.i32(i32 %27714)
  %27716 = trunc i32 %27715 to i8
  %27717 = and i8 %27716, 1
  %27718 = xor i8 %27717, 1
  store i8 %27718, i8* %21, align 1
  %27719 = xor i32 %27708, %27707
  %27720 = lshr i32 %27719, 4
  %27721 = trunc i32 %27720 to i8
  %27722 = and i8 %27721, 1
  store i8 %27722, i8* %27, align 1
  %27723 = zext i1 %27711 to i8
  store i8 %27723, i8* %30, align 1
  %27724 = lshr i32 %27708, 31
  %27725 = trunc i32 %27724 to i8
  store i8 %27725, i8* %33, align 1
  %27726 = lshr i32 %27707, 31
  %27727 = xor i32 %27724, %27726
  %27728 = add nuw nsw i32 %27727, %27724
  %27729 = icmp eq i32 %27728, 2
  %27730 = zext i1 %27729 to i8
  store i8 %27730, i8* %39, align 1
  %27731 = add i64 %27704, 9
  store i64 %27731, i64* %3, align 8
  store i32 %27708, i32* %27706, align 4
  %27732 = load i64, i64* %3, align 8
  %27733 = add i64 %27732, -180
  store i64 %27733, i64* %3, align 8
  br label %block_.L_40d2db

block_.L_40d394:                                  ; preds = %block_.L_40d2db
  %27734 = add i64 %27502, -16
  %27735 = add i64 %27538, 7
  store i64 %27735, i64* %3, align 8
  %27736 = inttoptr i64 %27734 to i32*
  store i32 0, i32* %27736, align 4
  %.pre213 = load i64, i64* %3, align 8
  br label %block_.L_40d39b

block_.L_40d39b:                                  ; preds = %block_.L_40d49b, %block_.L_40d394
  %27737 = phi i64 [ %.pre213, %block_.L_40d394 ], [ %28457, %block_.L_40d49b ]
  %MEMORY.43 = phi %struct.Memory* [ %MEMORY.39, %block_.L_40d394 ], [ %call2_40d4e8, %block_.L_40d49b ]
  %27738 = load i64, i64* %RBP.i, align 8
  %27739 = add i64 %27738, -16
  %27740 = add i64 %27737, 3
  store i64 %27740, i64* %3, align 8
  %27741 = inttoptr i64 %27739 to i32*
  %27742 = load i32, i32* %27741, align 4
  %27743 = zext i32 %27742 to i64
  store i64 %27743, i64* %RAX.i11582.pre-phi, align 8
  %27744 = add i64 %27738, -72
  %27745 = add i64 %27737, 6
  store i64 %27745, i64* %3, align 8
  %27746 = inttoptr i64 %27744 to i32*
  %27747 = load i32, i32* %27746, align 4
  %27748 = sub i32 %27742, %27747
  %27749 = icmp ult i32 %27742, %27747
  %27750 = zext i1 %27749 to i8
  store i8 %27750, i8* %14, align 1
  %27751 = and i32 %27748, 255
  %27752 = tail call i32 @llvm.ctpop.i32(i32 %27751)
  %27753 = trunc i32 %27752 to i8
  %27754 = and i8 %27753, 1
  %27755 = xor i8 %27754, 1
  store i8 %27755, i8* %21, align 1
  %27756 = xor i32 %27747, %27742
  %27757 = xor i32 %27756, %27748
  %27758 = lshr i32 %27757, 4
  %27759 = trunc i32 %27758 to i8
  %27760 = and i8 %27759, 1
  store i8 %27760, i8* %27, align 1
  %27761 = icmp eq i32 %27748, 0
  %27762 = zext i1 %27761 to i8
  store i8 %27762, i8* %30, align 1
  %27763 = lshr i32 %27748, 31
  %27764 = trunc i32 %27763 to i8
  store i8 %27764, i8* %33, align 1
  %27765 = lshr i32 %27742, 31
  %27766 = lshr i32 %27747, 31
  %27767 = xor i32 %27766, %27765
  %27768 = xor i32 %27763, %27765
  %27769 = add nuw nsw i32 %27768, %27767
  %27770 = icmp eq i32 %27769, 2
  %27771 = zext i1 %27770 to i8
  store i8 %27771, i8* %39, align 1
  %27772 = icmp ne i8 %27764, 0
  %27773 = xor i1 %27772, %27770
  %.v313 = select i1 %27773, i64 12, i64 352
  %27774 = add i64 %27737, %.v313
  store i64 %27774, i64* %3, align 8
  br i1 %27773, label %block_40d3a7, label %block_.L_40d4fb

block_40d3a7:                                     ; preds = %block_.L_40d39b
  %27775 = add i64 %27738, -60
  %27776 = add i64 %27774, 7
  store i64 %27776, i64* %3, align 8
  %27777 = inttoptr i64 %27775 to i32*
  store i32 32, i32* %27777, align 4
  %27778 = load i64, i64* %RBP.i, align 8
  %27779 = add i64 %27778, -64
  %27780 = load i64, i64* %3, align 8
  %27781 = add i64 %27780, 7
  store i64 %27781, i64* %3, align 8
  %27782 = inttoptr i64 %27779 to i32*
  store i32 0, i32* %27782, align 4
  %27783 = load i64, i64* %RBP.i, align 8
  %27784 = add i64 %27783, -20
  %27785 = load i64, i64* %3, align 8
  %27786 = add i64 %27785, 7
  store i64 %27786, i64* %3, align 8
  %27787 = inttoptr i64 %27784 to i32*
  store i32 0, i32* %27787, align 4
  %.pre245 = load i64, i64* %3, align 8
  br label %block_.L_40d3bc

block_.L_40d3bc:                                  ; preds = %block_.L_40d460, %block_40d3a7
  %27788 = phi i64 [ %28202, %block_.L_40d460 ], [ %.pre245, %block_40d3a7 ]
  %27789 = load i64, i64* %RBP.i, align 8
  %27790 = add i64 %27789, -20
  %27791 = add i64 %27788, 3
  store i64 %27791, i64* %3, align 8
  %27792 = inttoptr i64 %27790 to i32*
  %27793 = load i32, i32* %27792, align 4
  %27794 = zext i32 %27793 to i64
  store i64 %27794, i64* %RAX.i11582.pre-phi, align 8
  %27795 = add i64 %27789, -56
  %27796 = add i64 %27788, 6
  store i64 %27796, i64* %3, align 8
  %27797 = inttoptr i64 %27795 to i32*
  %27798 = load i32, i32* %27797, align 4
  %27799 = sub i32 %27793, %27798
  %27800 = icmp ult i32 %27793, %27798
  %27801 = zext i1 %27800 to i8
  store i8 %27801, i8* %14, align 1
  %27802 = and i32 %27799, 255
  %27803 = tail call i32 @llvm.ctpop.i32(i32 %27802)
  %27804 = trunc i32 %27803 to i8
  %27805 = and i8 %27804, 1
  %27806 = xor i8 %27805, 1
  store i8 %27806, i8* %21, align 1
  %27807 = xor i32 %27798, %27793
  %27808 = xor i32 %27807, %27799
  %27809 = lshr i32 %27808, 4
  %27810 = trunc i32 %27809 to i8
  %27811 = and i8 %27810, 1
  store i8 %27811, i8* %27, align 1
  %27812 = icmp eq i32 %27799, 0
  %27813 = zext i1 %27812 to i8
  store i8 %27813, i8* %30, align 1
  %27814 = lshr i32 %27799, 31
  %27815 = trunc i32 %27814 to i8
  store i8 %27815, i8* %33, align 1
  %27816 = lshr i32 %27793, 31
  %27817 = lshr i32 %27798, 31
  %27818 = xor i32 %27817, %27816
  %27819 = xor i32 %27814, %27816
  %27820 = add nuw nsw i32 %27819, %27818
  %27821 = icmp eq i32 %27820, 2
  %27822 = zext i1 %27821 to i8
  store i8 %27822, i8* %39, align 1
  %27823 = icmp ne i8 %27815, 0
  %27824 = xor i1 %27823, %27821
  %.v338 = select i1 %27824, i64 12, i64 183
  %27825 = add i64 %27788, %.v338
  store i64 %27825, i64* %3, align 8
  br i1 %27824, label %block_40d3c8, label %block_.L_40d473

block_40d3c8:                                     ; preds = %block_.L_40d3bc
  %27826 = add i64 %27789, -8
  %27827 = add i64 %27825, 4
  store i64 %27827, i64* %3, align 8
  %27828 = inttoptr i64 %27826 to i64*
  %27829 = load i64, i64* %27828, align 8
  %27830 = add i64 %27829, 37708
  store i64 %27830, i64* %RAX.i11582.pre-phi, align 8
  %27831 = icmp ugt i64 %27829, -37709
  %27832 = zext i1 %27831 to i8
  store i8 %27832, i8* %14, align 1
  %27833 = trunc i64 %27830 to i32
  %27834 = and i32 %27833, 255
  %27835 = tail call i32 @llvm.ctpop.i32(i32 %27834)
  %27836 = trunc i32 %27835 to i8
  %27837 = and i8 %27836, 1
  %27838 = xor i8 %27837, 1
  store i8 %27838, i8* %21, align 1
  %27839 = xor i64 %27830, %27829
  %27840 = lshr i64 %27839, 4
  %27841 = trunc i64 %27840 to i8
  %27842 = and i8 %27841, 1
  store i8 %27842, i8* %27, align 1
  %27843 = icmp eq i64 %27830, 0
  %27844 = zext i1 %27843 to i8
  store i8 %27844, i8* %30, align 1
  %27845 = lshr i64 %27830, 63
  %27846 = trunc i64 %27845 to i8
  store i8 %27846, i8* %33, align 1
  %27847 = lshr i64 %27829, 63
  %27848 = xor i64 %27845, %27847
  %27849 = add nuw nsw i64 %27848, %27845
  %27850 = icmp eq i64 %27849, 2
  %27851 = zext i1 %27850 to i8
  store i8 %27851, i8* %39, align 1
  %27852 = add i64 %27789, -16
  %27853 = add i64 %27825, 14
  store i64 %27853, i64* %3, align 8
  %27854 = inttoptr i64 %27852 to i32*
  %27855 = load i32, i32* %27854, align 4
  %27856 = sext i32 %27855 to i64
  %27857 = mul nsw i64 %27856, 258
  store i64 %27857, i64* %RCX.i11580, align 8
  %27858 = lshr i64 %27857, 63
  %27859 = add i64 %27857, %27830
  store i64 %27859, i64* %RAX.i11582.pre-phi, align 8
  %27860 = icmp ult i64 %27859, %27830
  %27861 = icmp ult i64 %27859, %27857
  %27862 = or i1 %27860, %27861
  %27863 = zext i1 %27862 to i8
  store i8 %27863, i8* %14, align 1
  %27864 = trunc i64 %27859 to i32
  %27865 = and i32 %27864, 255
  %27866 = tail call i32 @llvm.ctpop.i32(i32 %27865)
  %27867 = trunc i32 %27866 to i8
  %27868 = and i8 %27867, 1
  %27869 = xor i8 %27868, 1
  store i8 %27869, i8* %21, align 1
  %27870 = xor i64 %27857, %27830
  %27871 = xor i64 %27870, %27859
  %27872 = lshr i64 %27871, 4
  %27873 = trunc i64 %27872 to i8
  %27874 = and i8 %27873, 1
  store i8 %27874, i8* %27, align 1
  %27875 = icmp eq i64 %27859, 0
  %27876 = zext i1 %27875 to i8
  store i8 %27876, i8* %30, align 1
  %27877 = lshr i64 %27859, 63
  %27878 = trunc i64 %27877 to i8
  store i8 %27878, i8* %33, align 1
  %27879 = xor i64 %27877, %27845
  %27880 = xor i64 %27877, %27858
  %27881 = add nuw nsw i64 %27879, %27880
  %27882 = icmp eq i64 %27881, 2
  %27883 = zext i1 %27882 to i8
  store i8 %27883, i8* %39, align 1
  %27884 = load i64, i64* %RBP.i, align 8
  %27885 = add i64 %27884, -20
  %27886 = add i64 %27825, 28
  store i64 %27886, i64* %3, align 8
  %27887 = inttoptr i64 %27885 to i32*
  %27888 = load i32, i32* %27887, align 4
  %27889 = sext i32 %27888 to i64
  store i64 %27889, i64* %RCX.i11580, align 8
  %27890 = add i64 %27859, %27889
  %27891 = add i64 %27825, 32
  store i64 %27891, i64* %3, align 8
  %27892 = inttoptr i64 %27890 to i8*
  %27893 = load i8, i8* %27892, align 1
  %27894 = zext i8 %27893 to i64
  store i64 %27894, i64* %576, align 8
  %27895 = zext i8 %27893 to i32
  %27896 = add i64 %27884, -64
  %27897 = add i64 %27825, 35
  store i64 %27897, i64* %3, align 8
  %27898 = inttoptr i64 %27896 to i32*
  %27899 = load i32, i32* %27898, align 4
  %27900 = sub i32 %27895, %27899
  %27901 = icmp ult i32 %27895, %27899
  %27902 = zext i1 %27901 to i8
  store i8 %27902, i8* %14, align 1
  %27903 = and i32 %27900, 255
  %27904 = tail call i32 @llvm.ctpop.i32(i32 %27903)
  %27905 = trunc i32 %27904 to i8
  %27906 = and i8 %27905, 1
  %27907 = xor i8 %27906, 1
  store i8 %27907, i8* %21, align 1
  %27908 = xor i32 %27899, %27895
  %27909 = xor i32 %27908, %27900
  %27910 = lshr i32 %27909, 4
  %27911 = trunc i32 %27910 to i8
  %27912 = and i8 %27911, 1
  store i8 %27912, i8* %27, align 1
  %27913 = icmp eq i32 %27900, 0
  %27914 = zext i1 %27913 to i8
  store i8 %27914, i8* %30, align 1
  %27915 = lshr i32 %27900, 31
  %27916 = trunc i32 %27915 to i8
  store i8 %27916, i8* %33, align 1
  %27917 = lshr i32 %27899, 31
  %27918 = add nuw nsw i32 %27915, %27917
  %27919 = icmp eq i32 %27918, 2
  %27920 = zext i1 %27919 to i8
  store i8 %27920, i8* %39, align 1
  %27921 = icmp ne i8 %27916, 0
  %27922 = xor i1 %27921, %27919
  %27923 = or i1 %27913, %27922
  %.v341 = select i1 %27923, i64 76, i64 41
  %27924 = add i64 %27825, %.v341
  store i64 %27924, i64* %3, align 8
  br i1 %27923, label %block_.L_40d414, label %block_40d3f1

block_40d3f1:                                     ; preds = %block_40d3c8
  %27925 = add i64 %27884, -8
  %27926 = add i64 %27924, 4
  store i64 %27926, i64* %3, align 8
  %27927 = inttoptr i64 %27925 to i64*
  %27928 = load i64, i64* %27927, align 8
  %27929 = add i64 %27928, 37708
  store i64 %27929, i64* %RAX.i11582.pre-phi, align 8
  %27930 = icmp ugt i64 %27928, -37709
  %27931 = zext i1 %27930 to i8
  store i8 %27931, i8* %14, align 1
  %27932 = trunc i64 %27929 to i32
  %27933 = and i32 %27932, 255
  %27934 = tail call i32 @llvm.ctpop.i32(i32 %27933)
  %27935 = trunc i32 %27934 to i8
  %27936 = and i8 %27935, 1
  %27937 = xor i8 %27936, 1
  store i8 %27937, i8* %21, align 1
  %27938 = xor i64 %27929, %27928
  %27939 = lshr i64 %27938, 4
  %27940 = trunc i64 %27939 to i8
  %27941 = and i8 %27940, 1
  store i8 %27941, i8* %27, align 1
  %27942 = icmp eq i64 %27929, 0
  %27943 = zext i1 %27942 to i8
  store i8 %27943, i8* %30, align 1
  %27944 = lshr i64 %27929, 63
  %27945 = trunc i64 %27944 to i8
  store i8 %27945, i8* %33, align 1
  %27946 = lshr i64 %27928, 63
  %27947 = xor i64 %27944, %27946
  %27948 = add nuw nsw i64 %27947, %27944
  %27949 = icmp eq i64 %27948, 2
  %27950 = zext i1 %27949 to i8
  store i8 %27950, i8* %39, align 1
  %27951 = add i64 %27884, -16
  %27952 = add i64 %27924, 14
  store i64 %27952, i64* %3, align 8
  %27953 = inttoptr i64 %27951 to i32*
  %27954 = load i32, i32* %27953, align 4
  %27955 = sext i32 %27954 to i64
  %27956 = mul nsw i64 %27955, 258
  store i64 %27956, i64* %RCX.i11580, align 8
  %27957 = lshr i64 %27956, 63
  %27958 = add i64 %27956, %27929
  store i64 %27958, i64* %RAX.i11582.pre-phi, align 8
  %27959 = icmp ult i64 %27958, %27929
  %27960 = icmp ult i64 %27958, %27956
  %27961 = or i1 %27959, %27960
  %27962 = zext i1 %27961 to i8
  store i8 %27962, i8* %14, align 1
  %27963 = trunc i64 %27958 to i32
  %27964 = and i32 %27963, 255
  %27965 = tail call i32 @llvm.ctpop.i32(i32 %27964)
  %27966 = trunc i32 %27965 to i8
  %27967 = and i8 %27966, 1
  %27968 = xor i8 %27967, 1
  store i8 %27968, i8* %21, align 1
  %27969 = xor i64 %27956, %27929
  %27970 = xor i64 %27969, %27958
  %27971 = lshr i64 %27970, 4
  %27972 = trunc i64 %27971 to i8
  %27973 = and i8 %27972, 1
  store i8 %27973, i8* %27, align 1
  %27974 = icmp eq i64 %27958, 0
  %27975 = zext i1 %27974 to i8
  store i8 %27975, i8* %30, align 1
  %27976 = lshr i64 %27958, 63
  %27977 = trunc i64 %27976 to i8
  store i8 %27977, i8* %33, align 1
  %27978 = xor i64 %27976, %27944
  %27979 = xor i64 %27976, %27957
  %27980 = add nuw nsw i64 %27978, %27979
  %27981 = icmp eq i64 %27980, 2
  %27982 = zext i1 %27981 to i8
  store i8 %27982, i8* %39, align 1
  %27983 = load i64, i64* %RBP.i, align 8
  %27984 = add i64 %27983, -20
  %27985 = add i64 %27924, 28
  store i64 %27985, i64* %3, align 8
  %27986 = inttoptr i64 %27984 to i32*
  %27987 = load i32, i32* %27986, align 4
  %27988 = sext i32 %27987 to i64
  store i64 %27988, i64* %RCX.i11580, align 8
  %27989 = add i64 %27958, %27988
  %27990 = add i64 %27924, 32
  store i64 %27990, i64* %3, align 8
  %27991 = inttoptr i64 %27989 to i8*
  %27992 = load i8, i8* %27991, align 1
  %27993 = zext i8 %27992 to i64
  store i64 %27993, i64* %576, align 8
  %27994 = add i64 %27983, -64
  %27995 = zext i8 %27992 to i32
  %27996 = add i64 %27924, 35
  store i64 %27996, i64* %3, align 8
  %27997 = inttoptr i64 %27994 to i32*
  store i32 %27995, i32* %27997, align 4
  %.pre250 = load i64, i64* %RBP.i, align 8
  %.pre251 = load i64, i64* %3, align 8
  br label %block_.L_40d414

block_.L_40d414:                                  ; preds = %block_40d3f1, %block_40d3c8
  %27998 = phi i64 [ %.pre251, %block_40d3f1 ], [ %27924, %block_40d3c8 ]
  %27999 = phi i64 [ %.pre250, %block_40d3f1 ], [ %27884, %block_40d3c8 ]
  %28000 = add i64 %27999, -8
  %28001 = add i64 %27998, 4
  store i64 %28001, i64* %3, align 8
  %28002 = inttoptr i64 %28000 to i64*
  %28003 = load i64, i64* %28002, align 8
  %28004 = add i64 %28003, 37708
  store i64 %28004, i64* %RAX.i11582.pre-phi, align 8
  %28005 = icmp ugt i64 %28003, -37709
  %28006 = zext i1 %28005 to i8
  store i8 %28006, i8* %14, align 1
  %28007 = trunc i64 %28004 to i32
  %28008 = and i32 %28007, 255
  %28009 = tail call i32 @llvm.ctpop.i32(i32 %28008)
  %28010 = trunc i32 %28009 to i8
  %28011 = and i8 %28010, 1
  %28012 = xor i8 %28011, 1
  store i8 %28012, i8* %21, align 1
  %28013 = xor i64 %28004, %28003
  %28014 = lshr i64 %28013, 4
  %28015 = trunc i64 %28014 to i8
  %28016 = and i8 %28015, 1
  store i8 %28016, i8* %27, align 1
  %28017 = icmp eq i64 %28004, 0
  %28018 = zext i1 %28017 to i8
  store i8 %28018, i8* %30, align 1
  %28019 = lshr i64 %28004, 63
  %28020 = trunc i64 %28019 to i8
  store i8 %28020, i8* %33, align 1
  %28021 = lshr i64 %28003, 63
  %28022 = xor i64 %28019, %28021
  %28023 = add nuw nsw i64 %28022, %28019
  %28024 = icmp eq i64 %28023, 2
  %28025 = zext i1 %28024 to i8
  store i8 %28025, i8* %39, align 1
  %28026 = add i64 %27999, -16
  %28027 = add i64 %27998, 14
  store i64 %28027, i64* %3, align 8
  %28028 = inttoptr i64 %28026 to i32*
  %28029 = load i32, i32* %28028, align 4
  %28030 = sext i32 %28029 to i64
  %28031 = mul nsw i64 %28030, 258
  store i64 %28031, i64* %RCX.i11580, align 8
  %28032 = lshr i64 %28031, 63
  %28033 = add i64 %28031, %28004
  store i64 %28033, i64* %RAX.i11582.pre-phi, align 8
  %28034 = icmp ult i64 %28033, %28004
  %28035 = icmp ult i64 %28033, %28031
  %28036 = or i1 %28034, %28035
  %28037 = zext i1 %28036 to i8
  store i8 %28037, i8* %14, align 1
  %28038 = trunc i64 %28033 to i32
  %28039 = and i32 %28038, 255
  %28040 = tail call i32 @llvm.ctpop.i32(i32 %28039)
  %28041 = trunc i32 %28040 to i8
  %28042 = and i8 %28041, 1
  %28043 = xor i8 %28042, 1
  store i8 %28043, i8* %21, align 1
  %28044 = xor i64 %28031, %28004
  %28045 = xor i64 %28044, %28033
  %28046 = lshr i64 %28045, 4
  %28047 = trunc i64 %28046 to i8
  %28048 = and i8 %28047, 1
  store i8 %28048, i8* %27, align 1
  %28049 = icmp eq i64 %28033, 0
  %28050 = zext i1 %28049 to i8
  store i8 %28050, i8* %30, align 1
  %28051 = lshr i64 %28033, 63
  %28052 = trunc i64 %28051 to i8
  store i8 %28052, i8* %33, align 1
  %28053 = xor i64 %28051, %28019
  %28054 = xor i64 %28051, %28032
  %28055 = add nuw nsw i64 %28053, %28054
  %28056 = icmp eq i64 %28055, 2
  %28057 = zext i1 %28056 to i8
  store i8 %28057, i8* %39, align 1
  %28058 = load i64, i64* %RBP.i, align 8
  %28059 = add i64 %28058, -20
  %28060 = add i64 %27998, 28
  store i64 %28060, i64* %3, align 8
  %28061 = inttoptr i64 %28059 to i32*
  %28062 = load i32, i32* %28061, align 4
  %28063 = sext i32 %28062 to i64
  store i64 %28063, i64* %RCX.i11580, align 8
  %28064 = add i64 %28033, %28063
  %28065 = add i64 %27998, 32
  store i64 %28065, i64* %3, align 8
  %28066 = inttoptr i64 %28064 to i8*
  %28067 = load i8, i8* %28066, align 1
  %28068 = zext i8 %28067 to i64
  store i64 %28068, i64* %576, align 8
  %28069 = zext i8 %28067 to i32
  %28070 = add i64 %28058, -60
  %28071 = add i64 %27998, 35
  store i64 %28071, i64* %3, align 8
  %28072 = inttoptr i64 %28070 to i32*
  %28073 = load i32, i32* %28072, align 4
  %28074 = sub i32 %28069, %28073
  %28075 = icmp ult i32 %28069, %28073
  %28076 = zext i1 %28075 to i8
  store i8 %28076, i8* %14, align 1
  %28077 = and i32 %28074, 255
  %28078 = tail call i32 @llvm.ctpop.i32(i32 %28077)
  %28079 = trunc i32 %28078 to i8
  %28080 = and i8 %28079, 1
  %28081 = xor i8 %28080, 1
  store i8 %28081, i8* %21, align 1
  %28082 = xor i32 %28073, %28069
  %28083 = xor i32 %28082, %28074
  %28084 = lshr i32 %28083, 4
  %28085 = trunc i32 %28084 to i8
  %28086 = and i8 %28085, 1
  store i8 %28086, i8* %27, align 1
  %28087 = icmp eq i32 %28074, 0
  %28088 = zext i1 %28087 to i8
  store i8 %28088, i8* %30, align 1
  %28089 = lshr i32 %28074, 31
  %28090 = trunc i32 %28089 to i8
  store i8 %28090, i8* %33, align 1
  %28091 = lshr i32 %28073, 31
  %28092 = add nuw nsw i32 %28089, %28091
  %28093 = icmp eq i32 %28092, 2
  %28094 = zext i1 %28093 to i8
  store i8 %28094, i8* %39, align 1
  %28095 = icmp ne i8 %28090, 0
  %28096 = xor i1 %28095, %28093
  %.v342 = select i1 %28096, i64 41, i64 76
  %28097 = add i64 %27998, %.v342
  store i64 %28097, i64* %3, align 8
  br i1 %28096, label %block_40d43d, label %block_.L_40d460

block_40d43d:                                     ; preds = %block_.L_40d414
  %28098 = add i64 %28058, -8
  %28099 = add i64 %28097, 4
  store i64 %28099, i64* %3, align 8
  %28100 = inttoptr i64 %28098 to i64*
  %28101 = load i64, i64* %28100, align 8
  %28102 = add i64 %28101, 37708
  store i64 %28102, i64* %RAX.i11582.pre-phi, align 8
  %28103 = icmp ugt i64 %28101, -37709
  %28104 = zext i1 %28103 to i8
  store i8 %28104, i8* %14, align 1
  %28105 = trunc i64 %28102 to i32
  %28106 = and i32 %28105, 255
  %28107 = tail call i32 @llvm.ctpop.i32(i32 %28106)
  %28108 = trunc i32 %28107 to i8
  %28109 = and i8 %28108, 1
  %28110 = xor i8 %28109, 1
  store i8 %28110, i8* %21, align 1
  %28111 = xor i64 %28102, %28101
  %28112 = lshr i64 %28111, 4
  %28113 = trunc i64 %28112 to i8
  %28114 = and i8 %28113, 1
  store i8 %28114, i8* %27, align 1
  %28115 = icmp eq i64 %28102, 0
  %28116 = zext i1 %28115 to i8
  store i8 %28116, i8* %30, align 1
  %28117 = lshr i64 %28102, 63
  %28118 = trunc i64 %28117 to i8
  store i8 %28118, i8* %33, align 1
  %28119 = lshr i64 %28101, 63
  %28120 = xor i64 %28117, %28119
  %28121 = add nuw nsw i64 %28120, %28117
  %28122 = icmp eq i64 %28121, 2
  %28123 = zext i1 %28122 to i8
  store i8 %28123, i8* %39, align 1
  %28124 = add i64 %28058, -16
  %28125 = add i64 %28097, 14
  store i64 %28125, i64* %3, align 8
  %28126 = inttoptr i64 %28124 to i32*
  %28127 = load i32, i32* %28126, align 4
  %28128 = sext i32 %28127 to i64
  %28129 = mul nsw i64 %28128, 258
  store i64 %28129, i64* %RCX.i11580, align 8
  %28130 = lshr i64 %28129, 63
  %28131 = add i64 %28129, %28102
  store i64 %28131, i64* %RAX.i11582.pre-phi, align 8
  %28132 = icmp ult i64 %28131, %28102
  %28133 = icmp ult i64 %28131, %28129
  %28134 = or i1 %28132, %28133
  %28135 = zext i1 %28134 to i8
  store i8 %28135, i8* %14, align 1
  %28136 = trunc i64 %28131 to i32
  %28137 = and i32 %28136, 255
  %28138 = tail call i32 @llvm.ctpop.i32(i32 %28137)
  %28139 = trunc i32 %28138 to i8
  %28140 = and i8 %28139, 1
  %28141 = xor i8 %28140, 1
  store i8 %28141, i8* %21, align 1
  %28142 = xor i64 %28129, %28102
  %28143 = xor i64 %28142, %28131
  %28144 = lshr i64 %28143, 4
  %28145 = trunc i64 %28144 to i8
  %28146 = and i8 %28145, 1
  store i8 %28146, i8* %27, align 1
  %28147 = icmp eq i64 %28131, 0
  %28148 = zext i1 %28147 to i8
  store i8 %28148, i8* %30, align 1
  %28149 = lshr i64 %28131, 63
  %28150 = trunc i64 %28149 to i8
  store i8 %28150, i8* %33, align 1
  %28151 = xor i64 %28149, %28117
  %28152 = xor i64 %28149, %28130
  %28153 = add nuw nsw i64 %28151, %28152
  %28154 = icmp eq i64 %28153, 2
  %28155 = zext i1 %28154 to i8
  store i8 %28155, i8* %39, align 1
  %28156 = load i64, i64* %RBP.i, align 8
  %28157 = add i64 %28156, -20
  %28158 = add i64 %28097, 28
  store i64 %28158, i64* %3, align 8
  %28159 = inttoptr i64 %28157 to i32*
  %28160 = load i32, i32* %28159, align 4
  %28161 = sext i32 %28160 to i64
  store i64 %28161, i64* %RCX.i11580, align 8
  %28162 = add i64 %28131, %28161
  %28163 = add i64 %28097, 32
  store i64 %28163, i64* %3, align 8
  %28164 = inttoptr i64 %28162 to i8*
  %28165 = load i8, i8* %28164, align 1
  %28166 = zext i8 %28165 to i64
  store i64 %28166, i64* %576, align 8
  %28167 = add i64 %28156, -60
  %28168 = zext i8 %28165 to i32
  %28169 = add i64 %28097, 35
  store i64 %28169, i64* %3, align 8
  %28170 = inttoptr i64 %28167 to i32*
  store i32 %28168, i32* %28170, align 4
  %.pre252 = load i64, i64* %3, align 8
  %.pre253 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40d460

block_.L_40d460:                                  ; preds = %block_.L_40d414, %block_40d43d
  %28171 = phi i64 [ %28058, %block_.L_40d414 ], [ %.pre253, %block_40d43d ]
  %28172 = phi i64 [ %28097, %block_.L_40d414 ], [ %.pre252, %block_40d43d ]
  %28173 = add i64 %28171, -20
  %28174 = add i64 %28172, 8
  store i64 %28174, i64* %3, align 8
  %28175 = inttoptr i64 %28173 to i32*
  %28176 = load i32, i32* %28175, align 4
  %28177 = add i32 %28176, 1
  %28178 = zext i32 %28177 to i64
  store i64 %28178, i64* %RAX.i11582.pre-phi, align 8
  %28179 = icmp eq i32 %28176, -1
  %28180 = icmp eq i32 %28177, 0
  %28181 = or i1 %28179, %28180
  %28182 = zext i1 %28181 to i8
  store i8 %28182, i8* %14, align 1
  %28183 = and i32 %28177, 255
  %28184 = tail call i32 @llvm.ctpop.i32(i32 %28183)
  %28185 = trunc i32 %28184 to i8
  %28186 = and i8 %28185, 1
  %28187 = xor i8 %28186, 1
  store i8 %28187, i8* %21, align 1
  %28188 = xor i32 %28177, %28176
  %28189 = lshr i32 %28188, 4
  %28190 = trunc i32 %28189 to i8
  %28191 = and i8 %28190, 1
  store i8 %28191, i8* %27, align 1
  %28192 = zext i1 %28180 to i8
  store i8 %28192, i8* %30, align 1
  %28193 = lshr i32 %28177, 31
  %28194 = trunc i32 %28193 to i8
  store i8 %28194, i8* %33, align 1
  %28195 = lshr i32 %28176, 31
  %28196 = xor i32 %28193, %28195
  %28197 = add nuw nsw i32 %28196, %28193
  %28198 = icmp eq i32 %28197, 2
  %28199 = zext i1 %28198 to i8
  store i8 %28199, i8* %39, align 1
  %28200 = add i64 %28172, 14
  store i64 %28200, i64* %3, align 8
  store i32 %28177, i32* %28175, align 4
  %28201 = load i64, i64* %3, align 8
  %28202 = add i64 %28201, -178
  store i64 %28202, i64* %3, align 8
  br label %block_.L_40d3bc

block_.L_40d473:                                  ; preds = %block_.L_40d3bc
  %28203 = add i64 %27789, -64
  %28204 = add i64 %27825, 4
  store i64 %28204, i64* %3, align 8
  %28205 = inttoptr i64 %28203 to i32*
  %28206 = load i32, i32* %28205, align 4
  %28207 = add i32 %28206, -17
  %28208 = icmp ult i32 %28206, 17
  %28209 = zext i1 %28208 to i8
  store i8 %28209, i8* %14, align 1
  %28210 = and i32 %28207, 255
  %28211 = tail call i32 @llvm.ctpop.i32(i32 %28210)
  %28212 = trunc i32 %28211 to i8
  %28213 = and i8 %28212, 1
  %28214 = xor i8 %28213, 1
  store i8 %28214, i8* %21, align 1
  %28215 = xor i32 %28206, 16
  %28216 = xor i32 %28215, %28207
  %28217 = lshr i32 %28216, 4
  %28218 = trunc i32 %28217 to i8
  %28219 = and i8 %28218, 1
  store i8 %28219, i8* %27, align 1
  %28220 = icmp eq i32 %28207, 0
  %28221 = zext i1 %28220 to i8
  store i8 %28221, i8* %30, align 1
  %28222 = lshr i32 %28207, 31
  %28223 = trunc i32 %28222 to i8
  store i8 %28223, i8* %33, align 1
  %28224 = lshr i32 %28206, 31
  %28225 = xor i32 %28222, %28224
  %28226 = add nuw nsw i32 %28225, %28224
  %28227 = icmp eq i32 %28226, 2
  %28228 = zext i1 %28227 to i8
  store i8 %28228, i8* %39, align 1
  %28229 = icmp ne i8 %28223, 0
  %28230 = xor i1 %28229, %28227
  %28231 = or i1 %28220, %28230
  %.v339 = select i1 %28231, i64 20, i64 10
  %28232 = add i64 %27825, %.v339
  store i64 %28232, i64* %3, align 8
  br i1 %28231, label %block_.L_40d487, label %block_40d47d

block_40d47d:                                     ; preds = %block_.L_40d473
  store i64 3004, i64* %RDI.i2910, align 8
  %28233 = add i64 %28232, -34141
  %28234 = add i64 %28232, 10
  %28235 = load i64, i64* %6, align 8
  %28236 = add i64 %28235, -8
  %28237 = inttoptr i64 %28236 to i64*
  store i64 %28234, i64* %28237, align 8
  store i64 %28236, i64* %6, align 8
  store i64 %28233, i64* %3, align 8
  %call2_40d482 = tail call %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* nonnull %0, i64 %28233, %struct.Memory* %MEMORY.43)
  %.pre246 = load i64, i64* %RBP.i, align 8
  %.pre247 = load i64, i64* %3, align 8
  br label %block_.L_40d487

block_.L_40d487:                                  ; preds = %block_40d47d, %block_.L_40d473
  %28238 = phi i64 [ %.pre247, %block_40d47d ], [ %28232, %block_.L_40d473 ]
  %28239 = phi i64 [ %.pre246, %block_40d47d ], [ %27789, %block_.L_40d473 ]
  %28240 = add i64 %28239, -60
  %28241 = add i64 %28238, 4
  store i64 %28241, i64* %3, align 8
  %28242 = inttoptr i64 %28240 to i32*
  %28243 = load i32, i32* %28242, align 4
  %28244 = add i32 %28243, -1
  %28245 = icmp eq i32 %28243, 0
  %28246 = zext i1 %28245 to i8
  store i8 %28246, i8* %14, align 1
  %28247 = and i32 %28244, 255
  %28248 = tail call i32 @llvm.ctpop.i32(i32 %28247)
  %28249 = trunc i32 %28248 to i8
  %28250 = and i8 %28249, 1
  %28251 = xor i8 %28250, 1
  store i8 %28251, i8* %21, align 1
  %28252 = xor i32 %28244, %28243
  %28253 = lshr i32 %28252, 4
  %28254 = trunc i32 %28253 to i8
  %28255 = and i8 %28254, 1
  store i8 %28255, i8* %27, align 1
  %28256 = icmp eq i32 %28244, 0
  %28257 = zext i1 %28256 to i8
  store i8 %28257, i8* %30, align 1
  %28258 = lshr i32 %28244, 31
  %28259 = trunc i32 %28258 to i8
  store i8 %28259, i8* %33, align 1
  %28260 = lshr i32 %28243, 31
  %28261 = xor i32 %28258, %28260
  %28262 = add nuw nsw i32 %28261, %28260
  %28263 = icmp eq i32 %28262, 2
  %28264 = zext i1 %28263 to i8
  store i8 %28264, i8* %39, align 1
  %28265 = icmp ne i8 %28259, 0
  %28266 = xor i1 %28265, %28263
  %.v340 = select i1 %28266, i64 10, i64 20
  %28267 = add i64 %28238, %.v340
  store i64 %28267, i64* %3, align 8
  br i1 %28266, label %block_40d491, label %block_.L_40d49b

block_40d491:                                     ; preds = %block_.L_40d487
  store i64 3005, i64* %RDI.i2910, align 8
  %28268 = add i64 %28267, -34161
  %28269 = add i64 %28267, 10
  %28270 = load i64, i64* %6, align 8
  %28271 = add i64 %28270, -8
  %28272 = inttoptr i64 %28271 to i64*
  store i64 %28269, i64* %28272, align 8
  store i64 %28271, i64* %6, align 8
  store i64 %28268, i64* %3, align 8
  %call2_40d496 = tail call %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* nonnull %0, i64 %28268, %struct.Memory* %MEMORY.43)
  %.pre248 = load i64, i64* %RBP.i, align 8
  %.pre249 = load i64, i64* %3, align 8
  br label %block_.L_40d49b

block_.L_40d49b:                                  ; preds = %block_.L_40d487, %block_40d491
  %28273 = phi i64 [ %28267, %block_.L_40d487 ], [ %.pre249, %block_40d491 ]
  %28274 = phi i64 [ %28239, %block_.L_40d487 ], [ %.pre248, %block_40d491 ]
  %MEMORY.48 = phi %struct.Memory* [ %MEMORY.43, %block_.L_40d487 ], [ %call2_40d496, %block_40d491 ]
  %28275 = add i64 %28274, -8
  %28276 = add i64 %28273, 4
  store i64 %28276, i64* %3, align 8
  %28277 = inttoptr i64 %28275 to i64*
  %28278 = load i64, i64* %28277, align 8
  %28279 = add i64 %28278, 39256
  store i64 %28279, i64* %RAX.i11582.pre-phi, align 8
  %28280 = icmp ugt i64 %28278, -39257
  %28281 = zext i1 %28280 to i8
  store i8 %28281, i8* %14, align 1
  %28282 = trunc i64 %28279 to i32
  %28283 = and i32 %28282, 255
  %28284 = tail call i32 @llvm.ctpop.i32(i32 %28283)
  %28285 = trunc i32 %28284 to i8
  %28286 = and i8 %28285, 1
  %28287 = xor i8 %28286, 1
  store i8 %28287, i8* %21, align 1
  %28288 = xor i64 %28278, 16
  %28289 = xor i64 %28288, %28279
  %28290 = lshr i64 %28289, 4
  %28291 = trunc i64 %28290 to i8
  %28292 = and i8 %28291, 1
  store i8 %28292, i8* %27, align 1
  %28293 = icmp eq i64 %28279, 0
  %28294 = zext i1 %28293 to i8
  store i8 %28294, i8* %30, align 1
  %28295 = lshr i64 %28279, 63
  %28296 = trunc i64 %28295 to i8
  store i8 %28296, i8* %33, align 1
  %28297 = lshr i64 %28278, 63
  %28298 = xor i64 %28295, %28297
  %28299 = add nuw nsw i64 %28298, %28295
  %28300 = icmp eq i64 %28299, 2
  %28301 = zext i1 %28300 to i8
  store i8 %28301, i8* %39, align 1
  %28302 = add i64 %28274, -16
  %28303 = add i64 %28273, 14
  store i64 %28303, i64* %3, align 8
  %28304 = inttoptr i64 %28302 to i32*
  %28305 = load i32, i32* %28304, align 4
  %28306 = sext i32 %28305 to i64
  %28307 = mul nsw i64 %28306, 1032
  store i64 %28307, i64* %RCX.i11580, align 8
  %28308 = lshr i64 %28307, 63
  %28309 = add i64 %28307, %28279
  store i64 %28309, i64* %RAX.i11582.pre-phi, align 8
  %28310 = icmp ult i64 %28309, %28279
  %28311 = icmp ult i64 %28309, %28307
  %28312 = or i1 %28310, %28311
  %28313 = zext i1 %28312 to i8
  store i8 %28313, i8* %14, align 1
  %28314 = trunc i64 %28309 to i32
  %28315 = and i32 %28314, 255
  %28316 = tail call i32 @llvm.ctpop.i32(i32 %28315)
  %28317 = trunc i32 %28316 to i8
  %28318 = and i8 %28317, 1
  %28319 = xor i8 %28318, 1
  store i8 %28319, i8* %21, align 1
  %28320 = xor i64 %28307, %28279
  %28321 = xor i64 %28320, %28309
  %28322 = lshr i64 %28321, 4
  %28323 = trunc i64 %28322 to i8
  %28324 = and i8 %28323, 1
  store i8 %28324, i8* %27, align 1
  %28325 = icmp eq i64 %28309, 0
  %28326 = zext i1 %28325 to i8
  store i8 %28326, i8* %30, align 1
  %28327 = lshr i64 %28309, 63
  %28328 = trunc i64 %28327 to i8
  store i8 %28328, i8* %33, align 1
  %28329 = xor i64 %28327, %28295
  %28330 = xor i64 %28327, %28308
  %28331 = add nuw nsw i64 %28329, %28330
  %28332 = icmp eq i64 %28331, 2
  %28333 = zext i1 %28332 to i8
  store i8 %28333, i8* %39, align 1
  %28334 = load i64, i64* %RBP.i, align 8
  %28335 = add i64 %28334, -8
  %28336 = add i64 %28273, 28
  store i64 %28336, i64* %3, align 8
  %28337 = inttoptr i64 %28335 to i64*
  %28338 = load i64, i64* %28337, align 8
  %28339 = add i64 %28338, 37708
  store i64 %28339, i64* %RCX.i11580, align 8
  %28340 = icmp ugt i64 %28338, -37709
  %28341 = zext i1 %28340 to i8
  store i8 %28341, i8* %14, align 1
  %28342 = trunc i64 %28339 to i32
  %28343 = and i32 %28342, 255
  %28344 = tail call i32 @llvm.ctpop.i32(i32 %28343)
  %28345 = trunc i32 %28344 to i8
  %28346 = and i8 %28345, 1
  %28347 = xor i8 %28346, 1
  store i8 %28347, i8* %21, align 1
  %28348 = xor i64 %28339, %28338
  %28349 = lshr i64 %28348, 4
  %28350 = trunc i64 %28349 to i8
  %28351 = and i8 %28350, 1
  store i8 %28351, i8* %27, align 1
  %28352 = icmp eq i64 %28339, 0
  %28353 = zext i1 %28352 to i8
  store i8 %28353, i8* %30, align 1
  %28354 = lshr i64 %28339, 63
  %28355 = trunc i64 %28354 to i8
  store i8 %28355, i8* %33, align 1
  %28356 = lshr i64 %28338, 63
  %28357 = xor i64 %28354, %28356
  %28358 = add nuw nsw i64 %28357, %28354
  %28359 = icmp eq i64 %28358, 2
  %28360 = zext i1 %28359 to i8
  store i8 %28360, i8* %39, align 1
  %28361 = add i64 %28334, -16
  %28362 = add i64 %28273, 39
  store i64 %28362, i64* %3, align 8
  %28363 = inttoptr i64 %28361 to i32*
  %28364 = load i32, i32* %28363, align 4
  %28365 = sext i32 %28364 to i64
  %28366 = mul nsw i64 %28365, 258
  store i64 %28366, i64* %573, align 8
  %28367 = lshr i64 %28366, 63
  %28368 = add i64 %28366, %28339
  store i64 %28368, i64* %RCX.i11580, align 8
  %28369 = icmp ult i64 %28368, %28339
  %28370 = icmp ult i64 %28368, %28366
  %28371 = or i1 %28369, %28370
  %28372 = zext i1 %28371 to i8
  store i8 %28372, i8* %14, align 1
  %28373 = trunc i64 %28368 to i32
  %28374 = and i32 %28373, 255
  %28375 = tail call i32 @llvm.ctpop.i32(i32 %28374)
  %28376 = trunc i32 %28375 to i8
  %28377 = and i8 %28376, 1
  %28378 = xor i8 %28377, 1
  store i8 %28378, i8* %21, align 1
  %28379 = xor i64 %28366, %28339
  %28380 = xor i64 %28379, %28368
  %28381 = lshr i64 %28380, 4
  %28382 = trunc i64 %28381 to i8
  %28383 = and i8 %28382, 1
  store i8 %28383, i8* %27, align 1
  %28384 = icmp eq i64 %28368, 0
  %28385 = zext i1 %28384 to i8
  store i8 %28385, i8* %30, align 1
  %28386 = lshr i64 %28368, 63
  %28387 = trunc i64 %28386 to i8
  store i8 %28387, i8* %33, align 1
  %28388 = xor i64 %28386, %28354
  %28389 = xor i64 %28386, %28367
  %28390 = add nuw nsw i64 %28388, %28389
  %28391 = icmp eq i64 %28390, 2
  %28392 = zext i1 %28391 to i8
  store i8 %28392, i8* %39, align 1
  %28393 = load i64, i64* %RBP.i, align 8
  %28394 = add i64 %28393, -60
  %28395 = add i64 %28273, 52
  store i64 %28395, i64* %3, align 8
  %28396 = inttoptr i64 %28394 to i32*
  %28397 = load i32, i32* %28396, align 4
  %28398 = zext i32 %28397 to i64
  store i64 %28398, i64* %573, align 8
  %28399 = add i64 %28393, -64
  %28400 = add i64 %28273, 55
  store i64 %28400, i64* %3, align 8
  %28401 = inttoptr i64 %28399 to i32*
  %28402 = load i32, i32* %28401, align 4
  %28403 = zext i32 %28402 to i64
  store i64 %28403, i64* %RSI.i11312, align 8
  %28404 = add i64 %28393, -56
  %28405 = add i64 %28273, 59
  store i64 %28405, i64* %3, align 8
  %28406 = inttoptr i64 %28404 to i32*
  %28407 = load i32, i32* %28406, align 4
  %28408 = zext i32 %28407 to i64
  store i64 %28408, i64* %582, align 8
  %28409 = load i64, i64* %RAX.i11582.pre-phi, align 8
  store i64 %28409, i64* %RDI.i2910, align 8
  %28410 = add i64 %28393, -256
  %28411 = add i64 %28273, 68
  store i64 %28411, i64* %3, align 8
  %28412 = inttoptr i64 %28410 to i32*
  store i32 %28402, i32* %28412, align 4
  %28413 = load i64, i64* %RCX.i11580, align 8
  %28414 = load i64, i64* %3, align 8
  store i64 %28413, i64* %RSI.i11312, align 8
  %28415 = load i64, i64* %RBP.i, align 8
  %28416 = add i64 %28415, -256
  %28417 = add i64 %28414, 9
  store i64 %28417, i64* %3, align 8
  %28418 = inttoptr i64 %28416 to i32*
  %28419 = load i32, i32* %28418, align 4
  %28420 = zext i32 %28419 to i64
  store i64 %28420, i64* %RCX.i11580, align 8
  %28421 = add i64 %28414, 27009
  %28422 = add i64 %28414, 14
  %28423 = load i64, i64* %6, align 8
  %28424 = add i64 %28423, -8
  %28425 = inttoptr i64 %28424 to i64*
  store i64 %28422, i64* %28425, align 8
  store i64 %28424, i64* %6, align 8
  store i64 %28421, i64* %3, align 8
  %call2_40d4e8 = tail call %struct.Memory* @sub_413e60.BZ2_hbAssignCodes(%struct.State* nonnull %0, i64 %28421, %struct.Memory* %MEMORY.48)
  %28426 = load i64, i64* %RBP.i, align 8
  %28427 = add i64 %28426, -16
  %28428 = load i64, i64* %3, align 8
  %28429 = add i64 %28428, 3
  store i64 %28429, i64* %3, align 8
  %28430 = inttoptr i64 %28427 to i32*
  %28431 = load i32, i32* %28430, align 4
  %28432 = add i32 %28431, 1
  %28433 = zext i32 %28432 to i64
  store i64 %28433, i64* %RAX.i11582.pre-phi, align 8
  %28434 = icmp eq i32 %28431, -1
  %28435 = icmp eq i32 %28432, 0
  %28436 = or i1 %28434, %28435
  %28437 = zext i1 %28436 to i8
  store i8 %28437, i8* %14, align 1
  %28438 = and i32 %28432, 255
  %28439 = tail call i32 @llvm.ctpop.i32(i32 %28438)
  %28440 = trunc i32 %28439 to i8
  %28441 = and i8 %28440, 1
  %28442 = xor i8 %28441, 1
  store i8 %28442, i8* %21, align 1
  %28443 = xor i32 %28432, %28431
  %28444 = lshr i32 %28443, 4
  %28445 = trunc i32 %28444 to i8
  %28446 = and i8 %28445, 1
  store i8 %28446, i8* %27, align 1
  %28447 = zext i1 %28435 to i8
  store i8 %28447, i8* %30, align 1
  %28448 = lshr i32 %28432, 31
  %28449 = trunc i32 %28448 to i8
  store i8 %28449, i8* %33, align 1
  %28450 = lshr i32 %28431, 31
  %28451 = xor i32 %28448, %28450
  %28452 = add nuw nsw i32 %28451, %28448
  %28453 = icmp eq i32 %28452, 2
  %28454 = zext i1 %28453 to i8
  store i8 %28454, i8* %39, align 1
  %28455 = add i64 %28428, 9
  store i64 %28455, i64* %3, align 8
  store i32 %28432, i32* %28430, align 4
  %28456 = load i64, i64* %3, align 8
  %28457 = add i64 %28456, -347
  store i64 %28457, i64* %3, align 8
  br label %block_.L_40d39b

block_.L_40d4fb:                                  ; preds = %block_.L_40d39b
  %28458 = add i64 %27738, -20
  %28459 = add i64 %27774, 7
  store i64 %28459, i64* %3, align 8
  %28460 = inttoptr i64 %28458 to i32*
  store i32 0, i32* %28460, align 4
  %.pre214 = load i64, i64* %3, align 8
  br label %block_.L_40d502

block_.L_40d502:                                  ; preds = %block_.L_40d566, %block_.L_40d4fb
  %28461 = phi i64 [ %28674, %block_.L_40d566 ], [ %.pre214, %block_.L_40d4fb ]
  %28462 = load i64, i64* %RBP.i, align 8
  %28463 = add i64 %28462, -20
  %28464 = add i64 %28461, 4
  store i64 %28464, i64* %3, align 8
  %28465 = inttoptr i64 %28463 to i32*
  %28466 = load i32, i32* %28465, align 4
  %28467 = add i32 %28466, -16
  %28468 = icmp ult i32 %28466, 16
  %28469 = zext i1 %28468 to i8
  store i8 %28469, i8* %14, align 1
  %28470 = and i32 %28467, 255
  %28471 = tail call i32 @llvm.ctpop.i32(i32 %28470)
  %28472 = trunc i32 %28471 to i8
  %28473 = and i8 %28472, 1
  %28474 = xor i8 %28473, 1
  store i8 %28474, i8* %21, align 1
  %28475 = xor i32 %28466, 16
  %28476 = xor i32 %28475, %28467
  %28477 = lshr i32 %28476, 4
  %28478 = trunc i32 %28477 to i8
  %28479 = and i8 %28478, 1
  store i8 %28479, i8* %27, align 1
  %28480 = icmp eq i32 %28467, 0
  %28481 = zext i1 %28480 to i8
  store i8 %28481, i8* %30, align 1
  %28482 = lshr i32 %28467, 31
  %28483 = trunc i32 %28482 to i8
  store i8 %28483, i8* %33, align 1
  %28484 = lshr i32 %28466, 31
  %28485 = xor i32 %28482, %28484
  %28486 = add nuw nsw i32 %28485, %28484
  %28487 = icmp eq i32 %28486, 2
  %28488 = zext i1 %28487 to i8
  store i8 %28488, i8* %39, align 1
  %28489 = icmp ne i8 %28483, 0
  %28490 = xor i1 %28489, %28487
  %.v314 = select i1 %28490, i64 10, i64 119
  %28491 = add i64 %28461, %.v314
  store i64 %28491, i64* %3, align 8
  br i1 %28490, label %block_40d50c, label %block_.L_40d579

block_40d50c:                                     ; preds = %block_.L_40d502
  %28492 = add i64 %28491, 4
  store i64 %28492, i64* %3, align 8
  %28493 = load i32, i32* %28465, align 4
  %28494 = sext i32 %28493 to i64
  store i64 %28494, i64* %RAX.i11582.pre-phi, align 8
  %28495 = add nsw i64 %28494, -192
  %28496 = add i64 %28495, %28462
  %28497 = add i64 %28491, 12
  store i64 %28497, i64* %3, align 8
  %28498 = inttoptr i64 %28496 to i8*
  store i8 0, i8* %28498, align 1
  %28499 = load i64, i64* %RBP.i, align 8
  %28500 = add i64 %28499, -24
  %28501 = load i64, i64* %3, align 8
  %28502 = add i64 %28501, 7
  store i64 %28502, i64* %3, align 8
  %28503 = inttoptr i64 %28500 to i32*
  store i32 0, i32* %28503, align 4
  %.pre243 = load i64, i64* %3, align 8
  br label %block_.L_40d51f

block_.L_40d51f:                                  ; preds = %block_.L_40d553, %block_40d50c
  %28504 = phi i64 [ %28644, %block_.L_40d553 ], [ %.pre243, %block_40d50c ]
  %28505 = load i64, i64* %RBP.i, align 8
  %28506 = add i64 %28505, -24
  %28507 = add i64 %28504, 4
  store i64 %28507, i64* %3, align 8
  %28508 = inttoptr i64 %28506 to i32*
  %28509 = load i32, i32* %28508, align 4
  %28510 = add i32 %28509, -16
  %28511 = icmp ult i32 %28509, 16
  %28512 = zext i1 %28511 to i8
  store i8 %28512, i8* %14, align 1
  %28513 = and i32 %28510, 255
  %28514 = tail call i32 @llvm.ctpop.i32(i32 %28513)
  %28515 = trunc i32 %28514 to i8
  %28516 = and i8 %28515, 1
  %28517 = xor i8 %28516, 1
  store i8 %28517, i8* %21, align 1
  %28518 = xor i32 %28509, 16
  %28519 = xor i32 %28518, %28510
  %28520 = lshr i32 %28519, 4
  %28521 = trunc i32 %28520 to i8
  %28522 = and i8 %28521, 1
  store i8 %28522, i8* %27, align 1
  %28523 = icmp eq i32 %28510, 0
  %28524 = zext i1 %28523 to i8
  store i8 %28524, i8* %30, align 1
  %28525 = lshr i32 %28510, 31
  %28526 = trunc i32 %28525 to i8
  store i8 %28526, i8* %33, align 1
  %28527 = lshr i32 %28509, 31
  %28528 = xor i32 %28525, %28527
  %28529 = add nuw nsw i32 %28528, %28527
  %28530 = icmp eq i32 %28529, 2
  %28531 = zext i1 %28530 to i8
  store i8 %28531, i8* %39, align 1
  %28532 = icmp ne i8 %28526, 0
  %28533 = xor i1 %28532, %28530
  %.v289 = select i1 %28533, i64 10, i64 71
  %28534 = add i64 %28504, %.v289
  store i64 %28534, i64* %3, align 8
  br i1 %28533, label %block_40d529, label %block_.L_40d566

block_40d529:                                     ; preds = %block_.L_40d51f
  %28535 = add i64 %28505, -8
  %28536 = add i64 %28534, 4
  store i64 %28536, i64* %3, align 8
  %28537 = inttoptr i64 %28535 to i64*
  %28538 = load i64, i64* %28537, align 8
  store i64 %28538, i64* %RAX.i11582.pre-phi, align 8
  %28539 = add i64 %28505, -20
  %28540 = add i64 %28534, 7
  store i64 %28540, i64* %3, align 8
  %28541 = inttoptr i64 %28539 to i32*
  %28542 = load i32, i32* %28541, align 4
  %28543 = shl i32 %28542, 4
  %28544 = zext i32 %28543 to i64
  store i64 %28544, i64* %RCX.i11580, align 8
  %28545 = lshr i32 %28542, 28
  %28546 = trunc i32 %28545 to i8
  %28547 = and i8 %28546, 1
  store i8 %28547, i8* %14, align 1
  %28548 = and i32 %28543, 240
  %28549 = tail call i32 @llvm.ctpop.i32(i32 %28548)
  %28550 = trunc i32 %28549 to i8
  %28551 = and i8 %28550, 1
  %28552 = xor i8 %28551, 1
  store i8 %28552, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %28553 = icmp eq i32 %28543, 0
  %28554 = zext i1 %28553 to i8
  store i8 %28554, i8* %30, align 1
  %28555 = lshr i32 %28542, 27
  %28556 = trunc i32 %28555 to i8
  %28557 = and i8 %28556, 1
  store i8 %28557, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %28558 = add i64 %28534, 13
  store i64 %28558, i64* %3, align 8
  %28559 = load i32, i32* %28508, align 4
  %28560 = add i32 %28559, %28543
  %28561 = zext i32 %28560 to i64
  store i64 %28561, i64* %RCX.i11580, align 8
  %28562 = icmp ult i32 %28560, %28543
  %28563 = icmp ult i32 %28560, %28559
  %28564 = or i1 %28562, %28563
  %28565 = zext i1 %28564 to i8
  store i8 %28565, i8* %14, align 1
  %28566 = and i32 %28560, 255
  %28567 = tail call i32 @llvm.ctpop.i32(i32 %28566)
  %28568 = trunc i32 %28567 to i8
  %28569 = and i8 %28568, 1
  %28570 = xor i8 %28569, 1
  store i8 %28570, i8* %21, align 1
  %28571 = xor i32 %28559, %28543
  %28572 = xor i32 %28571, %28560
  %28573 = lshr i32 %28572, 4
  %28574 = trunc i32 %28573 to i8
  %28575 = and i8 %28574, 1
  store i8 %28575, i8* %27, align 1
  %28576 = icmp eq i32 %28560, 0
  %28577 = zext i1 %28576 to i8
  store i8 %28577, i8* %30, align 1
  %28578 = lshr i32 %28560, 31
  %28579 = trunc i32 %28578 to i8
  store i8 %28579, i8* %33, align 1
  %28580 = lshr i32 %28542, 27
  %28581 = and i32 %28580, 1
  %28582 = lshr i32 %28559, 31
  %28583 = xor i32 %28578, %28581
  %28584 = xor i32 %28578, %28582
  %28585 = add nuw nsw i32 %28583, %28584
  %28586 = icmp eq i32 %28585, 2
  %28587 = zext i1 %28586 to i8
  store i8 %28587, i8* %39, align 1
  %28588 = sext i32 %28560 to i64
  store i64 %28588, i64* %573, align 8
  %28589 = add nsw i64 %28588, 128
  %28590 = add i64 %28589, %28538
  %28591 = add i64 %28534, 24
  store i64 %28591, i64* %3, align 8
  %28592 = inttoptr i64 %28590 to i8*
  %28593 = load i8, i8* %28592, align 1
  store i8 0, i8* %14, align 1
  %28594 = zext i8 %28593 to i32
  %28595 = tail call i32 @llvm.ctpop.i32(i32 %28594)
  %28596 = trunc i32 %28595 to i8
  %28597 = and i8 %28596, 1
  %28598 = xor i8 %28597, 1
  store i8 %28598, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %28599 = icmp eq i8 %28593, 0
  %28600 = zext i1 %28599 to i8
  store i8 %28600, i8* %30, align 1
  %28601 = lshr i8 %28593, 7
  store i8 %28601, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %.v337 = select i1 %28599, i64 42, i64 30
  %28602 = add i64 %28534, %.v337
  store i64 %28602, i64* %3, align 8
  br i1 %28599, label %block_.L_40d553, label %block_40d547

block_40d547:                                     ; preds = %block_40d529
  %28603 = load i64, i64* %RBP.i, align 8
  %28604 = add i64 %28603, -20
  %28605 = add i64 %28602, 4
  store i64 %28605, i64* %3, align 8
  %28606 = inttoptr i64 %28604 to i32*
  %28607 = load i32, i32* %28606, align 4
  %28608 = sext i32 %28607 to i64
  store i64 %28608, i64* %RAX.i11582.pre-phi, align 8
  %28609 = add nsw i64 %28608, -192
  %28610 = add i64 %28609, %28603
  %28611 = add i64 %28602, 12
  store i64 %28611, i64* %3, align 8
  %28612 = inttoptr i64 %28610 to i8*
  store i8 1, i8* %28612, align 1
  %.pre244 = load i64, i64* %3, align 8
  br label %block_.L_40d553

block_.L_40d553:                                  ; preds = %block_40d547, %block_40d529
  %28613 = phi i64 [ %.pre244, %block_40d547 ], [ %28602, %block_40d529 ]
  %28614 = load i64, i64* %RBP.i, align 8
  %28615 = add i64 %28614, -24
  %28616 = add i64 %28613, 8
  store i64 %28616, i64* %3, align 8
  %28617 = inttoptr i64 %28615 to i32*
  %28618 = load i32, i32* %28617, align 4
  %28619 = add i32 %28618, 1
  %28620 = zext i32 %28619 to i64
  store i64 %28620, i64* %RAX.i11582.pre-phi, align 8
  %28621 = icmp eq i32 %28618, -1
  %28622 = icmp eq i32 %28619, 0
  %28623 = or i1 %28621, %28622
  %28624 = zext i1 %28623 to i8
  store i8 %28624, i8* %14, align 1
  %28625 = and i32 %28619, 255
  %28626 = tail call i32 @llvm.ctpop.i32(i32 %28625)
  %28627 = trunc i32 %28626 to i8
  %28628 = and i8 %28627, 1
  %28629 = xor i8 %28628, 1
  store i8 %28629, i8* %21, align 1
  %28630 = xor i32 %28619, %28618
  %28631 = lshr i32 %28630, 4
  %28632 = trunc i32 %28631 to i8
  %28633 = and i8 %28632, 1
  store i8 %28633, i8* %27, align 1
  %28634 = zext i1 %28622 to i8
  store i8 %28634, i8* %30, align 1
  %28635 = lshr i32 %28619, 31
  %28636 = trunc i32 %28635 to i8
  store i8 %28636, i8* %33, align 1
  %28637 = lshr i32 %28618, 31
  %28638 = xor i32 %28635, %28637
  %28639 = add nuw nsw i32 %28638, %28635
  %28640 = icmp eq i32 %28639, 2
  %28641 = zext i1 %28640 to i8
  store i8 %28641, i8* %39, align 1
  %28642 = add i64 %28613, 14
  store i64 %28642, i64* %3, align 8
  store i32 %28619, i32* %28617, align 4
  %28643 = load i64, i64* %3, align 8
  %28644 = add i64 %28643, -66
  store i64 %28644, i64* %3, align 8
  br label %block_.L_40d51f

block_.L_40d566:                                  ; preds = %block_.L_40d51f
  %28645 = add i64 %28505, -20
  %28646 = add i64 %28534, 8
  store i64 %28646, i64* %3, align 8
  %28647 = inttoptr i64 %28645 to i32*
  %28648 = load i32, i32* %28647, align 4
  %28649 = add i32 %28648, 1
  %28650 = zext i32 %28649 to i64
  store i64 %28650, i64* %RAX.i11582.pre-phi, align 8
  %28651 = icmp eq i32 %28648, -1
  %28652 = icmp eq i32 %28649, 0
  %28653 = or i1 %28651, %28652
  %28654 = zext i1 %28653 to i8
  store i8 %28654, i8* %14, align 1
  %28655 = and i32 %28649, 255
  %28656 = tail call i32 @llvm.ctpop.i32(i32 %28655)
  %28657 = trunc i32 %28656 to i8
  %28658 = and i8 %28657, 1
  %28659 = xor i8 %28658, 1
  store i8 %28659, i8* %21, align 1
  %28660 = xor i32 %28649, %28648
  %28661 = lshr i32 %28660, 4
  %28662 = trunc i32 %28661 to i8
  %28663 = and i8 %28662, 1
  store i8 %28663, i8* %27, align 1
  %28664 = zext i1 %28652 to i8
  store i8 %28664, i8* %30, align 1
  %28665 = lshr i32 %28649, 31
  %28666 = trunc i32 %28665 to i8
  store i8 %28666, i8* %33, align 1
  %28667 = lshr i32 %28648, 31
  %28668 = xor i32 %28665, %28667
  %28669 = add nuw nsw i32 %28668, %28665
  %28670 = icmp eq i32 %28669, 2
  %28671 = zext i1 %28670 to i8
  store i8 %28671, i8* %39, align 1
  %28672 = add i64 %28534, 14
  store i64 %28672, i64* %3, align 8
  store i32 %28649, i32* %28647, align 4
  %28673 = load i64, i64* %3, align 8
  %28674 = add i64 %28673, -114
  store i64 %28674, i64* %3, align 8
  br label %block_.L_40d502

block_.L_40d579:                                  ; preds = %block_.L_40d502
  %28675 = add i64 %28462, -8
  %28676 = add i64 %28491, 4
  store i64 %28676, i64* %3, align 8
  %28677 = inttoptr i64 %28675 to i64*
  %28678 = load i64, i64* %28677, align 8
  store i64 %28678, i64* %RAX.i11582.pre-phi, align 8
  %28679 = add i64 %28678, 116
  %28680 = add i64 %28491, 7
  store i64 %28680, i64* %3, align 8
  %28681 = inttoptr i64 %28679 to i32*
  %28682 = load i32, i32* %28681, align 4
  %28683 = zext i32 %28682 to i64
  store i64 %28683, i64* %RCX.i11580, align 8
  %28684 = add i64 %28462, -76
  %28685 = add i64 %28491, 10
  store i64 %28685, i64* %3, align 8
  %28686 = inttoptr i64 %28684 to i32*
  store i32 %28682, i32* %28686, align 4
  %28687 = load i64, i64* %RBP.i, align 8
  %28688 = add i64 %28687, -20
  %28689 = load i64, i64* %3, align 8
  %28690 = add i64 %28689, 7
  store i64 %28690, i64* %3, align 8
  %28691 = inttoptr i64 %28688 to i32*
  store i32 0, i32* %28691, align 4
  %.pre215 = load i64, i64* %3, align 8
  br label %block_.L_40d58a

block_.L_40d58a:                                  ; preds = %block_.L_40d5cd, %block_.L_40d579
  %28692 = phi i64 [ %.pre215, %block_.L_40d579 ], [ %28792, %block_.L_40d5cd ]
  %28693 = load i64, i64* %RBP.i, align 8
  %28694 = add i64 %28693, -20
  %28695 = add i64 %28692, 4
  store i64 %28695, i64* %3, align 8
  %28696 = inttoptr i64 %28694 to i32*
  %28697 = load i32, i32* %28696, align 4
  %28698 = add i32 %28697, -16
  %28699 = icmp ult i32 %28697, 16
  %28700 = zext i1 %28699 to i8
  store i8 %28700, i8* %14, align 1
  %28701 = and i32 %28698, 255
  %28702 = tail call i32 @llvm.ctpop.i32(i32 %28701)
  %28703 = trunc i32 %28702 to i8
  %28704 = and i8 %28703, 1
  %28705 = xor i8 %28704, 1
  store i8 %28705, i8* %21, align 1
  %28706 = xor i32 %28697, 16
  %28707 = xor i32 %28706, %28698
  %28708 = lshr i32 %28707, 4
  %28709 = trunc i32 %28708 to i8
  %28710 = and i8 %28709, 1
  store i8 %28710, i8* %27, align 1
  %28711 = icmp eq i32 %28698, 0
  %28712 = zext i1 %28711 to i8
  store i8 %28712, i8* %30, align 1
  %28713 = lshr i32 %28698, 31
  %28714 = trunc i32 %28713 to i8
  store i8 %28714, i8* %33, align 1
  %28715 = lshr i32 %28697, 31
  %28716 = xor i32 %28713, %28715
  %28717 = add nuw nsw i32 %28716, %28715
  %28718 = icmp eq i32 %28717, 2
  %28719 = zext i1 %28718 to i8
  store i8 %28719, i8* %39, align 1
  %28720 = icmp ne i8 %28714, 0
  %28721 = xor i1 %28720, %28718
  %.v315 = select i1 %28721, i64 10, i64 86
  %28722 = add i64 %28692, %.v315
  store i64 %28722, i64* %3, align 8
  br i1 %28721, label %block_40d594, label %block_.L_40d5e0

block_40d594:                                     ; preds = %block_.L_40d58a
  %28723 = add i64 %28722, 4
  store i64 %28723, i64* %3, align 8
  %28724 = load i32, i32* %28696, align 4
  %28725 = sext i32 %28724 to i64
  store i64 %28725, i64* %RAX.i11582.pre-phi, align 8
  %28726 = add nsw i64 %28725, -192
  %28727 = add i64 %28726, %28693
  %28728 = add i64 %28722, 12
  store i64 %28728, i64* %3, align 8
  %28729 = inttoptr i64 %28727 to i8*
  %28730 = load i8, i8* %28729, align 1
  store i8 0, i8* %14, align 1
  %28731 = zext i8 %28730 to i32
  %28732 = tail call i32 @llvm.ctpop.i32(i32 %28731)
  %28733 = trunc i32 %28732 to i8
  %28734 = and i8 %28733, 1
  %28735 = xor i8 %28734, 1
  store i8 %28735, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %28736 = icmp eq i8 %28730, 0
  %28737 = zext i1 %28736 to i8
  store i8 %28737, i8* %30, align 1
  %28738 = lshr i8 %28730, 7
  store i8 %28738, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %.v336 = select i1 %28736, i64 41, i64 18
  %28739 = add i64 %28722, %.v336
  %28740 = add i64 %28739, 5
  store i64 %28740, i64* %3, align 8
  br i1 %28736, label %block_.L_40d5bd, label %block_40d5a6

block_40d5a6:                                     ; preds = %block_40d594
  store i64 1, i64* %RAX.i11582.pre-phi, align 8
  %28741 = add i64 %28693, -8
  %28742 = add i64 %28739, 9
  store i64 %28742, i64* %3, align 8
  %28743 = inttoptr i64 %28741 to i64*
  %28744 = load i64, i64* %28743, align 8
  store i64 %28744, i64* %RDI.i2910, align 8
  store i64 1, i64* %RSI.i11290, align 8
  store i64 1, i64* %573, align 8
  %28745 = add i64 %28739, -14598
  %28746 = add i64 %28739, 18
  %28747 = load i64, i64* %6, align 8
  %28748 = add i64 %28747, -8
  %28749 = inttoptr i64 %28748 to i64*
  store i64 %28746, i64* %28749, align 8
  store i64 %28748, i64* %6, align 8
  store i64 %28745, i64* %3, align 8
  %call2_40d5b3 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %28745, %struct.Memory* %MEMORY.43)
  %28750 = load i64, i64* %3, align 8
  %28751 = add i64 %28750, 21
  store i64 %28751, i64* %3, align 8
  br label %block_.L_40d5cd

block_.L_40d5bd:                                  ; preds = %block_40d594
  store i64 1, i64* %RSI.i11290, align 8
  store i64 0, i64* %576, align 8
  store i8 0, i8* %14, align 1
  store i8 1, i8* %21, align 1
  store i8 1, i8* %30, align 1
  store i8 0, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %28752 = add i64 %28693, -8
  %28753 = add i64 %28739, 11
  store i64 %28753, i64* %3, align 8
  %28754 = inttoptr i64 %28752 to i64*
  %28755 = load i64, i64* %28754, align 8
  store i64 %28755, i64* %RDI.i2910, align 8
  %28756 = add i64 %28739, -14621
  %28757 = add i64 %28739, 16
  %28758 = load i64, i64* %6, align 8
  %28759 = add i64 %28758, -8
  %28760 = inttoptr i64 %28759 to i64*
  store i64 %28757, i64* %28760, align 8
  store i64 %28759, i64* %6, align 8
  store i64 %28756, i64* %3, align 8
  %call2_40d5c8 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %28756, %struct.Memory* %MEMORY.43)
  %.pre242 = load i64, i64* %3, align 8
  br label %block_.L_40d5cd

block_.L_40d5cd:                                  ; preds = %block_.L_40d5bd, %block_40d5a6
  %28761 = phi i64 [ %.pre242, %block_.L_40d5bd ], [ %28751, %block_40d5a6 ]
  %28762 = load i64, i64* %RBP.i, align 8
  %28763 = add i64 %28762, -20
  %28764 = add i64 %28761, 8
  store i64 %28764, i64* %3, align 8
  %28765 = inttoptr i64 %28763 to i32*
  %28766 = load i32, i32* %28765, align 4
  %28767 = add i32 %28766, 1
  %28768 = zext i32 %28767 to i64
  store i64 %28768, i64* %RAX.i11582.pre-phi, align 8
  %28769 = icmp eq i32 %28766, -1
  %28770 = icmp eq i32 %28767, 0
  %28771 = or i1 %28769, %28770
  %28772 = zext i1 %28771 to i8
  store i8 %28772, i8* %14, align 1
  %28773 = and i32 %28767, 255
  %28774 = tail call i32 @llvm.ctpop.i32(i32 %28773)
  %28775 = trunc i32 %28774 to i8
  %28776 = and i8 %28775, 1
  %28777 = xor i8 %28776, 1
  store i8 %28777, i8* %21, align 1
  %28778 = xor i32 %28767, %28766
  %28779 = lshr i32 %28778, 4
  %28780 = trunc i32 %28779 to i8
  %28781 = and i8 %28780, 1
  store i8 %28781, i8* %27, align 1
  %28782 = zext i1 %28770 to i8
  store i8 %28782, i8* %30, align 1
  %28783 = lshr i32 %28767, 31
  %28784 = trunc i32 %28783 to i8
  store i8 %28784, i8* %33, align 1
  %28785 = lshr i32 %28766, 31
  %28786 = xor i32 %28783, %28785
  %28787 = add nuw nsw i32 %28786, %28783
  %28788 = icmp eq i32 %28787, 2
  %28789 = zext i1 %28788 to i8
  store i8 %28789, i8* %39, align 1
  %28790 = add i64 %28761, 14
  store i64 %28790, i64* %3, align 8
  store i32 %28767, i32* %28765, align 4
  %28791 = load i64, i64* %3, align 8
  %28792 = add i64 %28791, -81
  store i64 %28792, i64* %3, align 8
  br label %block_.L_40d58a

block_.L_40d5e0:                                  ; preds = %block_.L_40d58a
  %28793 = add i64 %28722, 7
  store i64 %28793, i64* %3, align 8
  store i32 0, i32* %28696, align 4
  %.pre216 = load i64, i64* %3, align 8
  br label %block_.L_40d5e7

block_.L_40d5e7:                                  ; preds = %block_.L_40d671, %block_.L_40d5e0
  %28794 = phi i64 [ %.pre216, %block_.L_40d5e0 ], [ %29043, %block_.L_40d671 ]
  %28795 = load i64, i64* %RBP.i, align 8
  %28796 = add i64 %28795, -20
  %28797 = add i64 %28794, 4
  store i64 %28797, i64* %3, align 8
  %28798 = inttoptr i64 %28796 to i32*
  %28799 = load i32, i32* %28798, align 4
  %28800 = add i32 %28799, -16
  %28801 = icmp ult i32 %28799, 16
  %28802 = zext i1 %28801 to i8
  store i8 %28802, i8* %14, align 1
  %28803 = and i32 %28800, 255
  %28804 = tail call i32 @llvm.ctpop.i32(i32 %28803)
  %28805 = trunc i32 %28804 to i8
  %28806 = and i8 %28805, 1
  %28807 = xor i8 %28806, 1
  store i8 %28807, i8* %21, align 1
  %28808 = xor i32 %28799, 16
  %28809 = xor i32 %28808, %28800
  %28810 = lshr i32 %28809, 4
  %28811 = trunc i32 %28810 to i8
  %28812 = and i8 %28811, 1
  store i8 %28812, i8* %27, align 1
  %28813 = icmp eq i32 %28800, 0
  %28814 = zext i1 %28813 to i8
  store i8 %28814, i8* %30, align 1
  %28815 = lshr i32 %28800, 31
  %28816 = trunc i32 %28815 to i8
  store i8 %28816, i8* %33, align 1
  %28817 = lshr i32 %28799, 31
  %28818 = xor i32 %28815, %28817
  %28819 = add nuw nsw i32 %28818, %28817
  %28820 = icmp eq i32 %28819, 2
  %28821 = zext i1 %28820 to i8
  store i8 %28821, i8* %39, align 1
  %28822 = icmp ne i8 %28816, 0
  %28823 = xor i1 %28822, %28820
  %.v316 = select i1 %28823, i64 10, i64 157
  %28824 = add i64 %28794, %.v316
  store i64 %28824, i64* %3, align 8
  br i1 %28823, label %block_40d5f1, label %block_.L_40d684

block_40d5f1:                                     ; preds = %block_.L_40d5e7
  %28825 = add i64 %28824, 4
  store i64 %28825, i64* %3, align 8
  %28826 = load i32, i32* %28798, align 4
  %28827 = sext i32 %28826 to i64
  store i64 %28827, i64* %RAX.i11582.pre-phi, align 8
  %28828 = add nsw i64 %28827, -192
  %28829 = add i64 %28828, %28795
  %28830 = add i64 %28824, 12
  store i64 %28830, i64* %3, align 8
  %28831 = inttoptr i64 %28829 to i8*
  %28832 = load i8, i8* %28831, align 1
  store i8 0, i8* %14, align 1
  %28833 = zext i8 %28832 to i32
  %28834 = tail call i32 @llvm.ctpop.i32(i32 %28833)
  %28835 = trunc i32 %28834 to i8
  %28836 = and i8 %28835, 1
  %28837 = xor i8 %28836, 1
  store i8 %28837, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %28838 = icmp eq i8 %28832, 0
  %28839 = zext i1 %28838 to i8
  store i8 %28839, i8* %30, align 1
  %28840 = lshr i8 %28832, 7
  store i8 %28840, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %.v333 = select i1 %28838, i64 128, i64 18
  %28841 = add i64 %28824, %.v333
  store i64 %28841, i64* %3, align 8
  br i1 %28838, label %block_.L_40d671, label %block_40d603

block_40d603:                                     ; preds = %block_40d5f1
  %28842 = add i64 %28795, -24
  %28843 = add i64 %28841, 7
  store i64 %28843, i64* %3, align 8
  %28844 = inttoptr i64 %28842 to i32*
  store i32 0, i32* %28844, align 4
  %.pre240 = load i64, i64* %3, align 8
  br label %block_.L_40d60a

block_.L_40d60a:                                  ; preds = %block_.L_40d659, %block_40d603
  %28845 = phi i64 [ %.pre240, %block_40d603 ], [ %29010, %block_.L_40d659 ]
  %28846 = load i64, i64* %RBP.i, align 8
  %28847 = add i64 %28846, -24
  %28848 = add i64 %28845, 4
  store i64 %28848, i64* %3, align 8
  %28849 = inttoptr i64 %28847 to i32*
  %28850 = load i32, i32* %28849, align 4
  %28851 = add i32 %28850, -16
  %28852 = icmp ult i32 %28850, 16
  %28853 = zext i1 %28852 to i8
  store i8 %28853, i8* %14, align 1
  %28854 = and i32 %28851, 255
  %28855 = tail call i32 @llvm.ctpop.i32(i32 %28854)
  %28856 = trunc i32 %28855 to i8
  %28857 = and i8 %28856, 1
  %28858 = xor i8 %28857, 1
  store i8 %28858, i8* %21, align 1
  %28859 = xor i32 %28850, 16
  %28860 = xor i32 %28859, %28851
  %28861 = lshr i32 %28860, 4
  %28862 = trunc i32 %28861 to i8
  %28863 = and i8 %28862, 1
  store i8 %28863, i8* %27, align 1
  %28864 = icmp eq i32 %28851, 0
  %28865 = zext i1 %28864 to i8
  store i8 %28865, i8* %30, align 1
  %28866 = lshr i32 %28851, 31
  %28867 = trunc i32 %28866 to i8
  store i8 %28867, i8* %33, align 1
  %28868 = lshr i32 %28850, 31
  %28869 = xor i32 %28866, %28868
  %28870 = add nuw nsw i32 %28869, %28868
  %28871 = icmp eq i32 %28870, 2
  %28872 = zext i1 %28871 to i8
  store i8 %28872, i8* %39, align 1
  %28873 = icmp ne i8 %28867, 0
  %28874 = xor i1 %28873, %28871
  %.v334 = select i1 %28874, i64 10, i64 98
  %28875 = add i64 %28845, %.v334
  store i64 %28875, i64* %3, align 8
  br i1 %28874, label %block_40d614, label %block_.L_40d66c

block_40d614:                                     ; preds = %block_.L_40d60a
  %28876 = add i64 %28846, -8
  %28877 = add i64 %28875, 4
  store i64 %28877, i64* %3, align 8
  %28878 = inttoptr i64 %28876 to i64*
  %28879 = load i64, i64* %28878, align 8
  store i64 %28879, i64* %RAX.i11582.pre-phi, align 8
  %28880 = add i64 %28846, -20
  %28881 = add i64 %28875, 7
  store i64 %28881, i64* %3, align 8
  %28882 = inttoptr i64 %28880 to i32*
  %28883 = load i32, i32* %28882, align 4
  %28884 = shl i32 %28883, 4
  %28885 = zext i32 %28884 to i64
  store i64 %28885, i64* %RCX.i11580, align 8
  %28886 = lshr i32 %28883, 28
  %28887 = trunc i32 %28886 to i8
  %28888 = and i8 %28887, 1
  store i8 %28888, i8* %14, align 1
  %28889 = and i32 %28884, 240
  %28890 = tail call i32 @llvm.ctpop.i32(i32 %28889)
  %28891 = trunc i32 %28890 to i8
  %28892 = and i8 %28891, 1
  %28893 = xor i8 %28892, 1
  store i8 %28893, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %28894 = icmp eq i32 %28884, 0
  %28895 = zext i1 %28894 to i8
  store i8 %28895, i8* %30, align 1
  %28896 = lshr i32 %28883, 27
  %28897 = trunc i32 %28896 to i8
  %28898 = and i8 %28897, 1
  store i8 %28898, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %28899 = add i64 %28875, 13
  store i64 %28899, i64* %3, align 8
  %28900 = load i32, i32* %28849, align 4
  %28901 = add i32 %28900, %28884
  %28902 = zext i32 %28901 to i64
  store i64 %28902, i64* %RCX.i11580, align 8
  %28903 = icmp ult i32 %28901, %28884
  %28904 = icmp ult i32 %28901, %28900
  %28905 = or i1 %28903, %28904
  %28906 = zext i1 %28905 to i8
  store i8 %28906, i8* %14, align 1
  %28907 = and i32 %28901, 255
  %28908 = tail call i32 @llvm.ctpop.i32(i32 %28907)
  %28909 = trunc i32 %28908 to i8
  %28910 = and i8 %28909, 1
  %28911 = xor i8 %28910, 1
  store i8 %28911, i8* %21, align 1
  %28912 = xor i32 %28900, %28884
  %28913 = xor i32 %28912, %28901
  %28914 = lshr i32 %28913, 4
  %28915 = trunc i32 %28914 to i8
  %28916 = and i8 %28915, 1
  store i8 %28916, i8* %27, align 1
  %28917 = icmp eq i32 %28901, 0
  %28918 = zext i1 %28917 to i8
  store i8 %28918, i8* %30, align 1
  %28919 = lshr i32 %28901, 31
  %28920 = trunc i32 %28919 to i8
  store i8 %28920, i8* %33, align 1
  %28921 = lshr i32 %28883, 27
  %28922 = and i32 %28921, 1
  %28923 = lshr i32 %28900, 31
  %28924 = xor i32 %28919, %28922
  %28925 = xor i32 %28919, %28923
  %28926 = add nuw nsw i32 %28924, %28925
  %28927 = icmp eq i32 %28926, 2
  %28928 = zext i1 %28927 to i8
  store i8 %28928, i8* %39, align 1
  %28929 = sext i32 %28901 to i64
  store i64 %28929, i64* %576, align 8
  %28930 = add nsw i64 %28929, 128
  %28931 = add i64 %28930, %28879
  %28932 = add i64 %28875, 24
  store i64 %28932, i64* %3, align 8
  %28933 = inttoptr i64 %28931 to i8*
  %28934 = load i8, i8* %28933, align 1
  store i8 0, i8* %14, align 1
  %28935 = zext i8 %28934 to i32
  %28936 = tail call i32 @llvm.ctpop.i32(i32 %28935)
  %28937 = trunc i32 %28936 to i8
  %28938 = and i8 %28937, 1
  %28939 = xor i8 %28938, 1
  store i8 %28939, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %28940 = icmp eq i8 %28934, 0
  %28941 = zext i1 %28940 to i8
  store i8 %28941, i8* %30, align 1
  %28942 = lshr i8 %28934, 7
  store i8 %28942, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %.v335 = select i1 %28940, i64 53, i64 30
  %28943 = add i64 %28875, %.v335
  %28944 = add i64 %28943, 5
  store i64 %28944, i64* %3, align 8
  br i1 %28940, label %block_.L_40d649, label %block_40d632

block_40d632:                                     ; preds = %block_40d614
  store i64 1, i64* %RAX.i11582.pre-phi, align 8
  %28945 = load i64, i64* %RBP.i, align 8
  %28946 = add i64 %28945, -8
  %28947 = add i64 %28943, 9
  store i64 %28947, i64* %3, align 8
  %28948 = inttoptr i64 %28946 to i64*
  %28949 = load i64, i64* %28948, align 8
  store i64 %28949, i64* %RDI.i2910, align 8
  store i64 1, i64* %RSI.i11290, align 8
  store i64 1, i64* %576, align 8
  %28950 = add i64 %28943, -14738
  %28951 = add i64 %28943, 18
  %28952 = load i64, i64* %6, align 8
  %28953 = add i64 %28952, -8
  %28954 = inttoptr i64 %28953 to i64*
  store i64 %28951, i64* %28954, align 8
  store i64 %28953, i64* %6, align 8
  store i64 %28950, i64* %3, align 8
  %call2_40d63f = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %28950, %struct.Memory* %MEMORY.43)
  %28955 = load i64, i64* %3, align 8
  %28956 = add i64 %28955, 21
  store i64 %28956, i64* %3, align 8
  br label %block_.L_40d659

block_.L_40d649:                                  ; preds = %block_40d614
  store i64 1, i64* %RSI.i11290, align 8
  %28957 = xor i64 %28902, %28929
  %28958 = trunc i64 %28957 to i32
  %28959 = and i64 %28957, 4294967295
  store i64 %28959, i64* %576, align 8
  store i8 0, i8* %14, align 1
  %28960 = and i32 %28958, 255
  %28961 = tail call i32 @llvm.ctpop.i32(i32 %28960)
  %28962 = trunc i32 %28961 to i8
  %28963 = and i8 %28962, 1
  %28964 = xor i8 %28963, 1
  store i8 %28964, i8* %21, align 1
  %28965 = icmp eq i32 %28958, 0
  %28966 = zext i1 %28965 to i8
  store i8 %28966, i8* %30, align 1
  %28967 = lshr i32 %28958, 31
  %28968 = trunc i32 %28967 to i8
  store i8 %28968, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %28969 = load i64, i64* %RBP.i, align 8
  %28970 = add i64 %28969, -8
  %28971 = add i64 %28943, 11
  store i64 %28971, i64* %3, align 8
  %28972 = inttoptr i64 %28970 to i64*
  %28973 = load i64, i64* %28972, align 8
  store i64 %28973, i64* %RDI.i2910, align 8
  %28974 = add i64 %28943, -14761
  %28975 = add i64 %28943, 16
  %28976 = load i64, i64* %6, align 8
  %28977 = add i64 %28976, -8
  %28978 = inttoptr i64 %28977 to i64*
  store i64 %28975, i64* %28978, align 8
  store i64 %28977, i64* %6, align 8
  store i64 %28974, i64* %3, align 8
  %call2_40d654 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %28974, %struct.Memory* %MEMORY.43)
  %.pre241 = load i64, i64* %3, align 8
  br label %block_.L_40d659

block_.L_40d659:                                  ; preds = %block_.L_40d649, %block_40d632
  %28979 = phi i64 [ %.pre241, %block_.L_40d649 ], [ %28956, %block_40d632 ]
  %28980 = load i64, i64* %RBP.i, align 8
  %28981 = add i64 %28980, -24
  %28982 = add i64 %28979, 8
  store i64 %28982, i64* %3, align 8
  %28983 = inttoptr i64 %28981 to i32*
  %28984 = load i32, i32* %28983, align 4
  %28985 = add i32 %28984, 1
  %28986 = zext i32 %28985 to i64
  store i64 %28986, i64* %RAX.i11582.pre-phi, align 8
  %28987 = icmp eq i32 %28984, -1
  %28988 = icmp eq i32 %28985, 0
  %28989 = or i1 %28987, %28988
  %28990 = zext i1 %28989 to i8
  store i8 %28990, i8* %14, align 1
  %28991 = and i32 %28985, 255
  %28992 = tail call i32 @llvm.ctpop.i32(i32 %28991)
  %28993 = trunc i32 %28992 to i8
  %28994 = and i8 %28993, 1
  %28995 = xor i8 %28994, 1
  store i8 %28995, i8* %21, align 1
  %28996 = xor i32 %28985, %28984
  %28997 = lshr i32 %28996, 4
  %28998 = trunc i32 %28997 to i8
  %28999 = and i8 %28998, 1
  store i8 %28999, i8* %27, align 1
  %29000 = zext i1 %28988 to i8
  store i8 %29000, i8* %30, align 1
  %29001 = lshr i32 %28985, 31
  %29002 = trunc i32 %29001 to i8
  store i8 %29002, i8* %33, align 1
  %29003 = lshr i32 %28984, 31
  %29004 = xor i32 %29001, %29003
  %29005 = add nuw nsw i32 %29004, %29001
  %29006 = icmp eq i32 %29005, 2
  %29007 = zext i1 %29006 to i8
  store i8 %29007, i8* %39, align 1
  %29008 = add i64 %28979, 14
  store i64 %29008, i64* %3, align 8
  store i32 %28985, i32* %28983, align 4
  %29009 = load i64, i64* %3, align 8
  %29010 = add i64 %29009, -93
  store i64 %29010, i64* %3, align 8
  br label %block_.L_40d60a

block_.L_40d66c:                                  ; preds = %block_.L_40d60a
  %29011 = add i64 %28875, 5
  store i64 %29011, i64* %3, align 8
  br label %block_.L_40d671

block_.L_40d671:                                  ; preds = %block_.L_40d66c, %block_40d5f1
  %29012 = phi i64 [ %28795, %block_40d5f1 ], [ %28846, %block_.L_40d66c ]
  %29013 = phi i64 [ %28841, %block_40d5f1 ], [ %29011, %block_.L_40d66c ]
  %29014 = add i64 %29012, -20
  %29015 = add i64 %29013, 8
  store i64 %29015, i64* %3, align 8
  %29016 = inttoptr i64 %29014 to i32*
  %29017 = load i32, i32* %29016, align 4
  %29018 = add i32 %29017, 1
  %29019 = zext i32 %29018 to i64
  store i64 %29019, i64* %RAX.i11582.pre-phi, align 8
  %29020 = icmp eq i32 %29017, -1
  %29021 = icmp eq i32 %29018, 0
  %29022 = or i1 %29020, %29021
  %29023 = zext i1 %29022 to i8
  store i8 %29023, i8* %14, align 1
  %29024 = and i32 %29018, 255
  %29025 = tail call i32 @llvm.ctpop.i32(i32 %29024)
  %29026 = trunc i32 %29025 to i8
  %29027 = and i8 %29026, 1
  %29028 = xor i8 %29027, 1
  store i8 %29028, i8* %21, align 1
  %29029 = xor i32 %29018, %29017
  %29030 = lshr i32 %29029, 4
  %29031 = trunc i32 %29030 to i8
  %29032 = and i8 %29031, 1
  store i8 %29032, i8* %27, align 1
  %29033 = zext i1 %29021 to i8
  store i8 %29033, i8* %30, align 1
  %29034 = lshr i32 %29018, 31
  %29035 = trunc i32 %29034 to i8
  store i8 %29035, i8* %33, align 1
  %29036 = lshr i32 %29017, 31
  %29037 = xor i32 %29034, %29036
  %29038 = add nuw nsw i32 %29037, %29034
  %29039 = icmp eq i32 %29038, 2
  %29040 = zext i1 %29039 to i8
  store i8 %29040, i8* %39, align 1
  %29041 = add i64 %29013, 14
  store i64 %29041, i64* %3, align 8
  store i32 %29018, i32* %29016, align 4
  %29042 = load i64, i64* %3, align 8
  %29043 = add i64 %29042, -152
  store i64 %29043, i64* %3, align 8
  br label %block_.L_40d5e7

block_.L_40d684:                                  ; preds = %block_.L_40d5e7
  %29044 = add i64 %28795, -8
  %29045 = add i64 %28824, 4
  store i64 %29045, i64* %3, align 8
  %29046 = inttoptr i64 %29044 to i64*
  %29047 = load i64, i64* %29046, align 8
  store i64 %29047, i64* %RAX.i11582.pre-phi, align 8
  %29048 = add i64 %29047, 656
  %29049 = add i64 %28824, 11
  store i64 %29049, i64* %3, align 8
  %29050 = inttoptr i64 %29048 to i32*
  %29051 = load i32, i32* %29050, align 4
  %29052 = add i32 %29051, -3
  %29053 = icmp ult i32 %29051, 3
  %29054 = zext i1 %29053 to i8
  store i8 %29054, i8* %14, align 1
  %29055 = and i32 %29052, 255
  %29056 = tail call i32 @llvm.ctpop.i32(i32 %29055)
  %29057 = trunc i32 %29056 to i8
  %29058 = and i8 %29057, 1
  %29059 = xor i8 %29058, 1
  store i8 %29059, i8* %21, align 1
  %29060 = xor i32 %29052, %29051
  %29061 = lshr i32 %29060, 4
  %29062 = trunc i32 %29061 to i8
  %29063 = and i8 %29062, 1
  store i8 %29063, i8* %27, align 1
  %29064 = icmp eq i32 %29052, 0
  %29065 = zext i1 %29064 to i8
  store i8 %29065, i8* %30, align 1
  %29066 = lshr i32 %29052, 31
  %29067 = trunc i32 %29066 to i8
  store i8 %29067, i8* %33, align 1
  %29068 = lshr i32 %29051, 31
  %29069 = xor i32 %29066, %29068
  %29070 = add nuw nsw i32 %29069, %29068
  %29071 = icmp eq i32 %29070, 2
  %29072 = zext i1 %29071 to i8
  store i8 %29072, i8* %39, align 1
  %29073 = icmp ne i8 %29067, 0
  %29074 = xor i1 %29073, %29071
  %.v317 = select i1 %29074, i64 60, i64 17
  %29075 = add i64 %28824, %.v317
  store i64 %29075, i64* %3, align 8
  br i1 %29074, label %block_.L_40d6c0, label %block_40d695

block_40d695:                                     ; preds = %block_.L_40d684
  store i64 ptrtoint (%G__0x4165c1_type* @G__0x4165c1 to i64), i64* %RSI.i11290, align 8
  %29076 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %29076, i64* %RDI.i2910, align 8
  %29077 = add i64 %29075, 22
  store i64 %29077, i64* %3, align 8
  %29078 = load i64, i64* %29046, align 8
  store i64 %29078, i64* %RAX.i11582.pre-phi, align 8
  %29079 = add i64 %29078, 116
  %29080 = add i64 %29075, 25
  store i64 %29080, i64* %3, align 8
  %29081 = inttoptr i64 %29079 to i32*
  %29082 = load i32, i32* %29081, align 4
  %29083 = zext i32 %29082 to i64
  store i64 %29083, i64* %RCX.i11580, align 8
  %29084 = add i64 %28795, -76
  %29085 = add i64 %29075, 28
  store i64 %29085, i64* %3, align 8
  %29086 = inttoptr i64 %29084 to i32*
  %29087 = load i32, i32* %29086, align 4
  %29088 = sub i32 %29082, %29087
  %29089 = zext i32 %29088 to i64
  store i64 %29089, i64* %RCX.i11580, align 8
  %29090 = icmp ult i32 %29082, %29087
  %29091 = zext i1 %29090 to i8
  store i8 %29091, i8* %14, align 1
  %29092 = and i32 %29088, 255
  %29093 = tail call i32 @llvm.ctpop.i32(i32 %29092)
  %29094 = trunc i32 %29093 to i8
  %29095 = and i8 %29094, 1
  %29096 = xor i8 %29095, 1
  store i8 %29096, i8* %21, align 1
  %29097 = xor i32 %29087, %29082
  %29098 = xor i32 %29097, %29088
  %29099 = lshr i32 %29098, 4
  %29100 = trunc i32 %29099 to i8
  %29101 = and i8 %29100, 1
  store i8 %29101, i8* %27, align 1
  %29102 = icmp eq i32 %29088, 0
  %29103 = zext i1 %29102 to i8
  store i8 %29103, i8* %30, align 1
  %29104 = lshr i32 %29088, 31
  %29105 = trunc i32 %29104 to i8
  store i8 %29105, i8* %33, align 1
  %29106 = lshr i32 %29082, 31
  %29107 = lshr i32 %29087, 31
  %29108 = xor i32 %29107, %29106
  %29109 = xor i32 %29104, %29106
  %29110 = add nuw nsw i32 %29109, %29108
  %29111 = icmp eq i32 %29110, 2
  %29112 = zext i1 %29111 to i8
  store i8 %29112, i8* %39, align 1
  store i64 %29089, i64* %573, align 8
  store i8 0, i8* %AL.i11425, align 1
  %29113 = add i64 %29075, -52581
  %29114 = add i64 %29075, 37
  %29115 = load i64, i64* %6, align 8
  %29116 = add i64 %29115, -8
  %29117 = inttoptr i64 %29116 to i64*
  store i64 %29114, i64* %29117, align 8
  store i64 %29116, i64* %6, align 8
  store i64 %29113, i64* %3, align 8
  %29118 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %MEMORY.43)
  %29119 = load i64, i64* %RBP.i, align 8
  %29120 = add i64 %29119, -260
  %29121 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %29122 = load i64, i64* %3, align 8
  %29123 = add i64 %29122, 6
  store i64 %29123, i64* %3, align 8
  %29124 = inttoptr i64 %29120 to i32*
  store i32 %29121, i32* %29124, align 4
  %.pre217 = load i64, i64* %3, align 8
  %.pre218 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40d6c0

block_.L_40d6c0:                                  ; preds = %block_40d695, %block_.L_40d684
  %29125 = phi i64 [ %28795, %block_.L_40d684 ], [ %.pre218, %block_40d695 ]
  %29126 = phi i64 [ %29075, %block_.L_40d684 ], [ %.pre217, %block_40d695 ]
  %MEMORY.58 = phi %struct.Memory* [ %MEMORY.43, %block_.L_40d684 ], [ %29118, %block_40d695 ]
  store i64 3, i64* %RSI.i11312, align 8
  %29127 = add i64 %29125, -8
  %29128 = add i64 %29126, 9
  store i64 %29128, i64* %3, align 8
  %29129 = inttoptr i64 %29127 to i64*
  %29130 = load i64, i64* %29129, align 8
  store i64 %29130, i64* %RAX.i11582.pre-phi, align 8
  %29131 = add i64 %29130, 116
  %29132 = add i64 %29126, 12
  store i64 %29132, i64* %3, align 8
  %29133 = inttoptr i64 %29131 to i32*
  %29134 = load i32, i32* %29133, align 4
  %29135 = zext i32 %29134 to i64
  store i64 %29135, i64* %RCX.i11580, align 8
  %29136 = add i64 %29125, -76
  %29137 = add i64 %29126, 15
  store i64 %29137, i64* %3, align 8
  %29138 = inttoptr i64 %29136 to i32*
  store i32 %29134, i32* %29138, align 4
  %29139 = load i64, i64* %RBP.i, align 8
  %29140 = add i64 %29139, -8
  %29141 = load i64, i64* %3, align 8
  %29142 = add i64 %29141, 4
  store i64 %29142, i64* %3, align 8
  %29143 = inttoptr i64 %29140 to i64*
  %29144 = load i64, i64* %29143, align 8
  store i64 %29144, i64* %RDI.i2910, align 8
  %29145 = add i64 %29139, -72
  %29146 = add i64 %29141, 7
  store i64 %29146, i64* %3, align 8
  %29147 = inttoptr i64 %29145 to i32*
  %29148 = load i32, i32* %29147, align 4
  %29149 = zext i32 %29148 to i64
  store i64 %29149, i64* %576, align 8
  %29150 = add i64 %29141, -14895
  %29151 = add i64 %29141, 12
  %29152 = load i64, i64* %6, align 8
  %29153 = add i64 %29152, -8
  %29154 = inttoptr i64 %29153 to i64*
  store i64 %29151, i64* %29154, align 8
  store i64 %29153, i64* %6, align 8
  store i64 %29150, i64* %3, align 8
  %call2_40d6d6 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29150, %struct.Memory* %MEMORY.58)
  %29155 = load i64, i64* %3, align 8
  store i64 15, i64* %RSI.i11312, align 8
  %29156 = load i64, i64* %RBP.i, align 8
  %29157 = add i64 %29156, -8
  %29158 = add i64 %29155, 9
  store i64 %29158, i64* %3, align 8
  %29159 = inttoptr i64 %29157 to i64*
  %29160 = load i64, i64* %29159, align 8
  store i64 %29160, i64* %RDI.i2910, align 8
  %29161 = add i64 %29156, -52
  %29162 = add i64 %29155, 12
  store i64 %29162, i64* %3, align 8
  %29163 = inttoptr i64 %29161 to i32*
  %29164 = load i32, i32* %29163, align 4
  %29165 = zext i32 %29164 to i64
  store i64 %29165, i64* %576, align 8
  %29166 = add i64 %29155, -14907
  %29167 = add i64 %29155, 17
  %29168 = load i64, i64* %6, align 8
  %29169 = add i64 %29168, -8
  %29170 = inttoptr i64 %29169 to i64*
  store i64 %29167, i64* %29170, align 8
  store i64 %29169, i64* %6, align 8
  store i64 %29166, i64* %3, align 8
  %call2_40d6e7 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29166, %struct.Memory* %call2_40d6d6)
  %29171 = load i64, i64* %RBP.i, align 8
  %29172 = add i64 %29171, -20
  %29173 = load i64, i64* %3, align 8
  %29174 = add i64 %29173, 7
  store i64 %29174, i64* %3, align 8
  %29175 = inttoptr i64 %29172 to i32*
  store i32 0, i32* %29175, align 4
  %.pre219 = load i64, i64* %3, align 8
  br label %block_.L_40d6f3

block_.L_40d6f3:                                  ; preds = %block_.L_40d741, %block_.L_40d6c0
  %29176 = phi i64 [ %29356, %block_.L_40d741 ], [ %.pre219, %block_.L_40d6c0 ]
  %29177 = load i64, i64* %RBP.i, align 8
  %29178 = add i64 %29177, -20
  %29179 = add i64 %29176, 3
  store i64 %29179, i64* %3, align 8
  %29180 = inttoptr i64 %29178 to i32*
  %29181 = load i32, i32* %29180, align 4
  %29182 = zext i32 %29181 to i64
  store i64 %29182, i64* %RAX.i11582.pre-phi, align 8
  %29183 = add i64 %29177, -52
  %29184 = add i64 %29176, 6
  store i64 %29184, i64* %3, align 8
  %29185 = inttoptr i64 %29183 to i32*
  %29186 = load i32, i32* %29185, align 4
  %29187 = sub i32 %29181, %29186
  %29188 = icmp ult i32 %29181, %29186
  %29189 = zext i1 %29188 to i8
  store i8 %29189, i8* %14, align 1
  %29190 = and i32 %29187, 255
  %29191 = tail call i32 @llvm.ctpop.i32(i32 %29190)
  %29192 = trunc i32 %29191 to i8
  %29193 = and i8 %29192, 1
  %29194 = xor i8 %29193, 1
  store i8 %29194, i8* %21, align 1
  %29195 = xor i32 %29186, %29181
  %29196 = xor i32 %29195, %29187
  %29197 = lshr i32 %29196, 4
  %29198 = trunc i32 %29197 to i8
  %29199 = and i8 %29198, 1
  store i8 %29199, i8* %27, align 1
  %29200 = icmp eq i32 %29187, 0
  %29201 = zext i1 %29200 to i8
  store i8 %29201, i8* %30, align 1
  %29202 = lshr i32 %29187, 31
  %29203 = trunc i32 %29202 to i8
  store i8 %29203, i8* %33, align 1
  %29204 = lshr i32 %29181, 31
  %29205 = lshr i32 %29186, 31
  %29206 = xor i32 %29205, %29204
  %29207 = xor i32 %29202, %29204
  %29208 = add nuw nsw i32 %29207, %29206
  %29209 = icmp eq i32 %29208, 2
  %29210 = zext i1 %29209 to i8
  store i8 %29210, i8* %39, align 1
  %29211 = icmp ne i8 %29203, 0
  %29212 = xor i1 %29211, %29209
  %.v318 = select i1 %29212, i64 12, i64 108
  %29213 = add i64 %29176, %.v318
  store i64 %29213, i64* %3, align 8
  br i1 %29212, label %block_40d6ff, label %block_.L_40d75f

block_40d6ff:                                     ; preds = %block_.L_40d6f3
  %29214 = add i64 %29177, -24
  %29215 = add i64 %29213, 7
  store i64 %29215, i64* %3, align 8
  %29216 = inttoptr i64 %29214 to i32*
  store i32 0, i32* %29216, align 4
  %.pre239 = load i64, i64* %3, align 8
  br label %block_.L_40d706

block_.L_40d706:                                  ; preds = %block_40d721, %block_40d6ff
  %29217 = phi i64 [ %29304, %block_40d721 ], [ %.pre239, %block_40d6ff ]
  %29218 = load i64, i64* %RBP.i, align 8
  %29219 = add i64 %29218, -24
  %29220 = add i64 %29217, 3
  store i64 %29220, i64* %3, align 8
  %29221 = inttoptr i64 %29219 to i32*
  %29222 = load i32, i32* %29221, align 4
  %29223 = zext i32 %29222 to i64
  store i64 %29223, i64* %RAX.i11582.pre-phi, align 8
  %29224 = add i64 %29218, -8
  %29225 = add i64 %29217, 7
  store i64 %29225, i64* %3, align 8
  %29226 = inttoptr i64 %29224 to i64*
  %29227 = load i64, i64* %29226, align 8
  store i64 %29227, i64* %RCX.i11580, align 8
  %29228 = add i64 %29218, -20
  %29229 = add i64 %29217, 11
  store i64 %29229, i64* %3, align 8
  %29230 = inttoptr i64 %29228 to i32*
  %29231 = load i32, i32* %29230, align 4
  %29232 = sext i32 %29231 to i64
  store i64 %29232, i64* %576, align 8
  %29233 = add nsw i64 %29232, 19706
  %29234 = add i64 %29233, %29227
  %29235 = add i64 %29217, 19
  store i64 %29235, i64* %3, align 8
  %29236 = inttoptr i64 %29234 to i8*
  %29237 = load i8, i8* %29236, align 1
  %29238 = zext i8 %29237 to i64
  store i64 %29238, i64* %RSI.i11312, align 8
  %29239 = zext i8 %29237 to i32
  %29240 = sub i32 %29222, %29239
  %29241 = icmp ult i32 %29222, %29239
  %29242 = zext i1 %29241 to i8
  store i8 %29242, i8* %14, align 1
  %29243 = and i32 %29240, 255
  %29244 = tail call i32 @llvm.ctpop.i32(i32 %29243)
  %29245 = trunc i32 %29244 to i8
  %29246 = and i8 %29245, 1
  %29247 = xor i8 %29246, 1
  store i8 %29247, i8* %21, align 1
  %29248 = xor i32 %29239, %29222
  %29249 = xor i32 %29248, %29240
  %29250 = lshr i32 %29249, 4
  %29251 = trunc i32 %29250 to i8
  %29252 = and i8 %29251, 1
  store i8 %29252, i8* %27, align 1
  %29253 = icmp eq i32 %29240, 0
  %29254 = zext i1 %29253 to i8
  store i8 %29254, i8* %30, align 1
  %29255 = lshr i32 %29240, 31
  %29256 = trunc i32 %29255 to i8
  store i8 %29256, i8* %33, align 1
  %29257 = lshr i32 %29222, 31
  %29258 = xor i32 %29255, %29257
  %29259 = add nuw nsw i32 %29258, %29257
  %29260 = icmp eq i32 %29259, 2
  %29261 = zext i1 %29260 to i8
  store i8 %29261, i8* %39, align 1
  %29262 = icmp ne i8 %29256, 0
  %29263 = xor i1 %29262, %29260
  %.v332 = select i1 %29263, i64 27, i64 59
  %29264 = add i64 %29217, %.v332
  %29265 = add i64 %29264, 5
  store i64 %29265, i64* %3, align 8
  br i1 %29263, label %block_40d721, label %block_.L_40d741

block_40d721:                                     ; preds = %block_.L_40d706
  store i64 1, i64* %RAX.i11582.pre-phi, align 8
  %29266 = add i64 %29264, 9
  store i64 %29266, i64* %3, align 8
  %29267 = load i64, i64* %29226, align 8
  store i64 %29267, i64* %RDI.i2910, align 8
  store i64 1, i64* %RSI.i11312, align 8
  store i64 1, i64* %576, align 8
  %29268 = add i64 %29264, -14977
  %29269 = add i64 %29264, 18
  %29270 = load i64, i64* %6, align 8
  %29271 = add i64 %29270, -8
  %29272 = inttoptr i64 %29271 to i64*
  store i64 %29269, i64* %29272, align 8
  store i64 %29271, i64* %6, align 8
  store i64 %29268, i64* %3, align 8
  %call2_40d72e = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29268, %struct.Memory* %call2_40d6e7)
  %29273 = load i64, i64* %RBP.i, align 8
  %29274 = add i64 %29273, -24
  %29275 = load i64, i64* %3, align 8
  %29276 = add i64 %29275, 3
  store i64 %29276, i64* %3, align 8
  %29277 = inttoptr i64 %29274 to i32*
  %29278 = load i32, i32* %29277, align 4
  %29279 = add i32 %29278, 1
  %29280 = zext i32 %29279 to i64
  store i64 %29280, i64* %RAX.i11582.pre-phi, align 8
  %29281 = icmp eq i32 %29278, -1
  %29282 = icmp eq i32 %29279, 0
  %29283 = or i1 %29281, %29282
  %29284 = zext i1 %29283 to i8
  store i8 %29284, i8* %14, align 1
  %29285 = and i32 %29279, 255
  %29286 = tail call i32 @llvm.ctpop.i32(i32 %29285)
  %29287 = trunc i32 %29286 to i8
  %29288 = and i8 %29287, 1
  %29289 = xor i8 %29288, 1
  store i8 %29289, i8* %21, align 1
  %29290 = xor i32 %29279, %29278
  %29291 = lshr i32 %29290, 4
  %29292 = trunc i32 %29291 to i8
  %29293 = and i8 %29292, 1
  store i8 %29293, i8* %27, align 1
  %29294 = zext i1 %29282 to i8
  store i8 %29294, i8* %30, align 1
  %29295 = lshr i32 %29279, 31
  %29296 = trunc i32 %29295 to i8
  store i8 %29296, i8* %33, align 1
  %29297 = lshr i32 %29278, 31
  %29298 = xor i32 %29295, %29297
  %29299 = add nuw nsw i32 %29298, %29295
  %29300 = icmp eq i32 %29299, 2
  %29301 = zext i1 %29300 to i8
  store i8 %29301, i8* %39, align 1
  %29302 = add i64 %29275, 9
  store i64 %29302, i64* %3, align 8
  store i32 %29279, i32* %29277, align 4
  %29303 = load i64, i64* %3, align 8
  %29304 = add i64 %29303, -54
  store i64 %29304, i64* %3, align 8
  br label %block_.L_40d706

block_.L_40d741:                                  ; preds = %block_.L_40d706
  store i64 1, i64* %RSI.i11312, align 8
  %29305 = zext i32 %29231 to i64
  %29306 = xor i64 %29305, %29232
  %29307 = trunc i64 %29306 to i32
  %29308 = and i64 %29306, 4294967295
  store i64 %29308, i64* %576, align 8
  store i8 0, i8* %14, align 1
  %29309 = and i32 %29307, 255
  %29310 = tail call i32 @llvm.ctpop.i32(i32 %29309)
  %29311 = trunc i32 %29310 to i8
  %29312 = and i8 %29311, 1
  %29313 = xor i8 %29312, 1
  store i8 %29313, i8* %21, align 1
  %29314 = icmp eq i32 %29307, 0
  %29315 = zext i1 %29314 to i8
  store i8 %29315, i8* %30, align 1
  %29316 = lshr i32 %29307, 31
  %29317 = trunc i32 %29316 to i8
  store i8 %29317, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %29318 = add i64 %29264, 11
  store i64 %29318, i64* %3, align 8
  %29319 = load i64, i64* %29226, align 8
  store i64 %29319, i64* %RDI.i2910, align 8
  %29320 = add i64 %29264, -15009
  %29321 = add i64 %29264, 16
  %29322 = load i64, i64* %6, align 8
  %29323 = add i64 %29322, -8
  %29324 = inttoptr i64 %29323 to i64*
  store i64 %29321, i64* %29324, align 8
  store i64 %29323, i64* %6, align 8
  store i64 %29320, i64* %3, align 8
  %call2_40d74c = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29320, %struct.Memory* %call2_40d6e7)
  %29325 = load i64, i64* %RBP.i, align 8
  %29326 = add i64 %29325, -20
  %29327 = load i64, i64* %3, align 8
  %29328 = add i64 %29327, 3
  store i64 %29328, i64* %3, align 8
  %29329 = inttoptr i64 %29326 to i32*
  %29330 = load i32, i32* %29329, align 4
  %29331 = add i32 %29330, 1
  %29332 = zext i32 %29331 to i64
  store i64 %29332, i64* %RAX.i11582.pre-phi, align 8
  %29333 = icmp eq i32 %29330, -1
  %29334 = icmp eq i32 %29331, 0
  %29335 = or i1 %29333, %29334
  %29336 = zext i1 %29335 to i8
  store i8 %29336, i8* %14, align 1
  %29337 = and i32 %29331, 255
  %29338 = tail call i32 @llvm.ctpop.i32(i32 %29337)
  %29339 = trunc i32 %29338 to i8
  %29340 = and i8 %29339, 1
  %29341 = xor i8 %29340, 1
  store i8 %29341, i8* %21, align 1
  %29342 = xor i32 %29331, %29330
  %29343 = lshr i32 %29342, 4
  %29344 = trunc i32 %29343 to i8
  %29345 = and i8 %29344, 1
  store i8 %29345, i8* %27, align 1
  %29346 = zext i1 %29334 to i8
  store i8 %29346, i8* %30, align 1
  %29347 = lshr i32 %29331, 31
  %29348 = trunc i32 %29347 to i8
  store i8 %29348, i8* %33, align 1
  %29349 = lshr i32 %29330, 31
  %29350 = xor i32 %29347, %29349
  %29351 = add nuw nsw i32 %29350, %29347
  %29352 = icmp eq i32 %29351, 2
  %29353 = zext i1 %29352 to i8
  store i8 %29353, i8* %39, align 1
  %29354 = add i64 %29327, 9
  store i64 %29354, i64* %3, align 8
  store i32 %29331, i32* %29329, align 4
  %29355 = load i64, i64* %3, align 8
  %29356 = add i64 %29355, -103
  store i64 %29356, i64* %3, align 8
  br label %block_.L_40d6f3

block_.L_40d75f:                                  ; preds = %block_.L_40d6f3
  %29357 = add i64 %29177, -8
  %29358 = add i64 %29213, 4
  store i64 %29358, i64* %3, align 8
  %29359 = inttoptr i64 %29357 to i64*
  %29360 = load i64, i64* %29359, align 8
  store i64 %29360, i64* %RAX.i11582.pre-phi, align 8
  %29361 = add i64 %29360, 656
  %29362 = add i64 %29213, 11
  store i64 %29362, i64* %3, align 8
  %29363 = inttoptr i64 %29361 to i32*
  %29364 = load i32, i32* %29363, align 4
  %29365 = add i32 %29364, -3
  %29366 = icmp ult i32 %29364, 3
  %29367 = zext i1 %29366 to i8
  store i8 %29367, i8* %14, align 1
  %29368 = and i32 %29365, 255
  %29369 = tail call i32 @llvm.ctpop.i32(i32 %29368)
  %29370 = trunc i32 %29369 to i8
  %29371 = and i8 %29370, 1
  %29372 = xor i8 %29371, 1
  store i8 %29372, i8* %21, align 1
  %29373 = xor i32 %29365, %29364
  %29374 = lshr i32 %29373, 4
  %29375 = trunc i32 %29374 to i8
  %29376 = and i8 %29375, 1
  store i8 %29376, i8* %27, align 1
  %29377 = icmp eq i32 %29365, 0
  %29378 = zext i1 %29377 to i8
  store i8 %29378, i8* %30, align 1
  %29379 = lshr i32 %29365, 31
  %29380 = trunc i32 %29379 to i8
  store i8 %29380, i8* %33, align 1
  %29381 = lshr i32 %29364, 31
  %29382 = xor i32 %29379, %29381
  %29383 = add nuw nsw i32 %29382, %29381
  %29384 = icmp eq i32 %29383, 2
  %29385 = zext i1 %29384 to i8
  store i8 %29385, i8* %39, align 1
  %29386 = icmp ne i8 %29380, 0
  %29387 = xor i1 %29386, %29384
  %.v319 = select i1 %29387, i64 60, i64 17
  %29388 = add i64 %29213, %.v319
  store i64 %29388, i64* %3, align 8
  br i1 %29387, label %block_.L_40d79b, label %block_40d770

block_40d770:                                     ; preds = %block_.L_40d75f
  store i64 ptrtoint (%G__0x4165db_type* @G__0x4165db to i64), i64* %RSI.i11312, align 8
  %29389 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %29389, i64* %RDI.i2910, align 8
  %29390 = add i64 %29388, 22
  store i64 %29390, i64* %3, align 8
  %29391 = load i64, i64* %29359, align 8
  store i64 %29391, i64* %RAX.i11582.pre-phi, align 8
  %29392 = add i64 %29391, 116
  %29393 = add i64 %29388, 25
  store i64 %29393, i64* %3, align 8
  %29394 = inttoptr i64 %29392 to i32*
  %29395 = load i32, i32* %29394, align 4
  %29396 = zext i32 %29395 to i64
  store i64 %29396, i64* %RCX.i11580, align 8
  %29397 = add i64 %29177, -76
  %29398 = add i64 %29388, 28
  store i64 %29398, i64* %3, align 8
  %29399 = inttoptr i64 %29397 to i32*
  %29400 = load i32, i32* %29399, align 4
  %29401 = sub i32 %29395, %29400
  %29402 = zext i32 %29401 to i64
  store i64 %29402, i64* %RCX.i11580, align 8
  %29403 = icmp ult i32 %29395, %29400
  %29404 = zext i1 %29403 to i8
  store i8 %29404, i8* %14, align 1
  %29405 = and i32 %29401, 255
  %29406 = tail call i32 @llvm.ctpop.i32(i32 %29405)
  %29407 = trunc i32 %29406 to i8
  %29408 = and i8 %29407, 1
  %29409 = xor i8 %29408, 1
  store i8 %29409, i8* %21, align 1
  %29410 = xor i32 %29400, %29395
  %29411 = xor i32 %29410, %29401
  %29412 = lshr i32 %29411, 4
  %29413 = trunc i32 %29412 to i8
  %29414 = and i8 %29413, 1
  store i8 %29414, i8* %27, align 1
  %29415 = icmp eq i32 %29401, 0
  %29416 = zext i1 %29415 to i8
  store i8 %29416, i8* %30, align 1
  %29417 = lshr i32 %29401, 31
  %29418 = trunc i32 %29417 to i8
  store i8 %29418, i8* %33, align 1
  %29419 = lshr i32 %29395, 31
  %29420 = lshr i32 %29400, 31
  %29421 = xor i32 %29420, %29419
  %29422 = xor i32 %29417, %29419
  %29423 = add nuw nsw i32 %29422, %29421
  %29424 = icmp eq i32 %29423, 2
  %29425 = zext i1 %29424 to i8
  store i8 %29425, i8* %39, align 1
  store i64 %29402, i64* %576, align 8
  store i8 0, i8* %AL.i11425, align 1
  %29426 = add i64 %29388, -52800
  %29427 = add i64 %29388, 37
  %29428 = load i64, i64* %6, align 8
  %29429 = add i64 %29428, -8
  %29430 = inttoptr i64 %29429 to i64*
  store i64 %29427, i64* %29430, align 8
  store i64 %29429, i64* %6, align 8
  store i64 %29426, i64* %3, align 8
  %29431 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %call2_40d6e7)
  %29432 = load i64, i64* %RBP.i, align 8
  %29433 = add i64 %29432, -264
  %29434 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %29435 = load i64, i64* %3, align 8
  %29436 = add i64 %29435, 6
  store i64 %29436, i64* %3, align 8
  %29437 = inttoptr i64 %29433 to i32*
  store i32 %29434, i32* %29437, align 4
  %.pre220 = load i64, i64* %RBP.i, align 8
  %.pre221 = load i64, i64* %3, align 8
  br label %block_.L_40d79b

block_.L_40d79b:                                  ; preds = %block_40d770, %block_.L_40d75f
  %29438 = phi i64 [ %29388, %block_.L_40d75f ], [ %.pre221, %block_40d770 ]
  %29439 = phi i64 [ %29177, %block_.L_40d75f ], [ %.pre220, %block_40d770 ]
  %MEMORY.61 = phi %struct.Memory* [ %call2_40d6e7, %block_.L_40d75f ], [ %29431, %block_40d770 ]
  %29440 = add i64 %29439, -8
  %29441 = add i64 %29438, 4
  store i64 %29441, i64* %3, align 8
  %29442 = inttoptr i64 %29440 to i64*
  %29443 = load i64, i64* %29442, align 8
  store i64 %29443, i64* %RAX.i11582.pre-phi, align 8
  %29444 = add i64 %29443, 116
  %29445 = add i64 %29438, 7
  store i64 %29445, i64* %3, align 8
  %29446 = inttoptr i64 %29444 to i32*
  %29447 = load i32, i32* %29446, align 4
  %29448 = zext i32 %29447 to i64
  store i64 %29448, i64* %RCX.i11580, align 8
  %29449 = add i64 %29439, -76
  %29450 = add i64 %29438, 10
  store i64 %29450, i64* %3, align 8
  %29451 = inttoptr i64 %29449 to i32*
  store i32 %29447, i32* %29451, align 4
  %29452 = load i64, i64* %RBP.i, align 8
  %29453 = add i64 %29452, -16
  %29454 = load i64, i64* %3, align 8
  %29455 = add i64 %29454, 7
  store i64 %29455, i64* %3, align 8
  %29456 = inttoptr i64 %29453 to i32*
  store i32 0, i32* %29456, align 4
  %.pre222 = load i64, i64* %3, align 8
  br label %block_.L_40d7ac

block_.L_40d7ac:                                  ; preds = %block_.L_40d8d8, %block_.L_40d79b
  %29457 = phi i64 [ %29995, %block_.L_40d8d8 ], [ %.pre222, %block_.L_40d79b ]
  %29458 = load i64, i64* %RBP.i, align 8
  %29459 = add i64 %29458, -16
  %29460 = add i64 %29457, 3
  store i64 %29460, i64* %3, align 8
  %29461 = inttoptr i64 %29459 to i32*
  %29462 = load i32, i32* %29461, align 4
  %29463 = zext i32 %29462 to i64
  store i64 %29463, i64* %RAX.i11582.pre-phi, align 8
  %29464 = add i64 %29458, -72
  %29465 = add i64 %29457, 6
  store i64 %29465, i64* %3, align 8
  %29466 = inttoptr i64 %29464 to i32*
  %29467 = load i32, i32* %29466, align 4
  %29468 = sub i32 %29462, %29467
  %29469 = icmp ult i32 %29462, %29467
  %29470 = zext i1 %29469 to i8
  store i8 %29470, i8* %14, align 1
  %29471 = and i32 %29468, 255
  %29472 = tail call i32 @llvm.ctpop.i32(i32 %29471)
  %29473 = trunc i32 %29472 to i8
  %29474 = and i8 %29473, 1
  %29475 = xor i8 %29474, 1
  store i8 %29475, i8* %21, align 1
  %29476 = xor i32 %29467, %29462
  %29477 = xor i32 %29476, %29468
  %29478 = lshr i32 %29477, 4
  %29479 = trunc i32 %29478 to i8
  %29480 = and i8 %29479, 1
  store i8 %29480, i8* %27, align 1
  %29481 = icmp eq i32 %29468, 0
  %29482 = zext i1 %29481 to i8
  store i8 %29482, i8* %30, align 1
  %29483 = lshr i32 %29468, 31
  %29484 = trunc i32 %29483 to i8
  store i8 %29484, i8* %33, align 1
  %29485 = lshr i32 %29462, 31
  %29486 = lshr i32 %29467, 31
  %29487 = xor i32 %29486, %29485
  %29488 = xor i32 %29483, %29485
  %29489 = add nuw nsw i32 %29488, %29487
  %29490 = icmp eq i32 %29489, 2
  %29491 = zext i1 %29490 to i8
  store i8 %29491, i8* %39, align 1
  %29492 = icmp ne i8 %29484, 0
  %29493 = xor i1 %29492, %29490
  %.v320 = select i1 %29493, i64 12, i64 314
  %29494 = add i64 %29457, %.v320
  store i64 %29494, i64* %3, align 8
  br i1 %29493, label %block_40d7b8, label %block_.L_40d8e6

block_40d7b8:                                     ; preds = %block_.L_40d7ac
  store i64 5, i64* %RSI.i11312, align 8
  %29495 = add i64 %29458, -8
  %29496 = add i64 %29494, 9
  store i64 %29496, i64* %3, align 8
  %29497 = inttoptr i64 %29495 to i64*
  %29498 = load i64, i64* %29497, align 8
  %29499 = add i64 %29498, 37708
  store i64 %29499, i64* %RAX.i11582.pre-phi, align 8
  %29500 = icmp ugt i64 %29498, -37709
  %29501 = zext i1 %29500 to i8
  store i8 %29501, i8* %14, align 1
  %29502 = trunc i64 %29499 to i32
  %29503 = and i32 %29502, 255
  %29504 = tail call i32 @llvm.ctpop.i32(i32 %29503)
  %29505 = trunc i32 %29504 to i8
  %29506 = and i8 %29505, 1
  %29507 = xor i8 %29506, 1
  store i8 %29507, i8* %21, align 1
  %29508 = xor i64 %29499, %29498
  %29509 = lshr i64 %29508, 4
  %29510 = trunc i64 %29509 to i8
  %29511 = and i8 %29510, 1
  store i8 %29511, i8* %27, align 1
  %29512 = icmp eq i64 %29499, 0
  %29513 = zext i1 %29512 to i8
  store i8 %29513, i8* %30, align 1
  %29514 = lshr i64 %29499, 63
  %29515 = trunc i64 %29514 to i8
  store i8 %29515, i8* %33, align 1
  %29516 = lshr i64 %29498, 63
  %29517 = xor i64 %29514, %29516
  %29518 = add nuw nsw i64 %29517, %29514
  %29519 = icmp eq i64 %29518, 2
  %29520 = zext i1 %29519 to i8
  store i8 %29520, i8* %39, align 1
  %29521 = add i64 %29494, 19
  store i64 %29521, i64* %3, align 8
  %29522 = load i32, i32* %29461, align 4
  %29523 = sext i32 %29522 to i64
  %29524 = mul nsw i64 %29523, 258
  store i64 %29524, i64* %RCX.i11580, align 8
  %29525 = lshr i64 %29524, 63
  %29526 = add i64 %29524, %29499
  store i64 %29526, i64* %RAX.i11582.pre-phi, align 8
  %29527 = icmp ult i64 %29526, %29499
  %29528 = icmp ult i64 %29526, %29524
  %29529 = or i1 %29527, %29528
  %29530 = zext i1 %29529 to i8
  store i8 %29530, i8* %14, align 1
  %29531 = trunc i64 %29526 to i32
  %29532 = and i32 %29531, 255
  %29533 = tail call i32 @llvm.ctpop.i32(i32 %29532)
  %29534 = trunc i32 %29533 to i8
  %29535 = and i8 %29534, 1
  %29536 = xor i8 %29535, 1
  store i8 %29536, i8* %21, align 1
  %29537 = xor i64 %29524, %29499
  %29538 = xor i64 %29537, %29526
  %29539 = lshr i64 %29538, 4
  %29540 = trunc i64 %29539 to i8
  %29541 = and i8 %29540, 1
  store i8 %29541, i8* %27, align 1
  %29542 = icmp eq i64 %29526, 0
  %29543 = zext i1 %29542 to i8
  store i8 %29543, i8* %30, align 1
  %29544 = lshr i64 %29526, 63
  %29545 = trunc i64 %29544 to i8
  store i8 %29545, i8* %33, align 1
  %29546 = xor i64 %29544, %29514
  %29547 = xor i64 %29544, %29525
  %29548 = add nuw nsw i64 %29546, %29547
  %29549 = icmp eq i64 %29548, 2
  %29550 = zext i1 %29549 to i8
  store i8 %29550, i8* %39, align 1
  %29551 = inttoptr i64 %29526 to i8*
  %29552 = add i64 %29494, 32
  store i64 %29552, i64* %3, align 8
  %29553 = load i8, i8* %29551, align 1
  %29554 = zext i8 %29553 to i64
  store i64 %29554, i64* %576, align 8
  %29555 = load i64, i64* %RBP.i, align 8
  %29556 = add i64 %29555, -196
  %29557 = zext i8 %29553 to i32
  %29558 = add i64 %29494, 38
  store i64 %29558, i64* %3, align 8
  %29559 = inttoptr i64 %29556 to i32*
  store i32 %29557, i32* %29559, align 4
  %29560 = load i64, i64* %RBP.i, align 8
  %29561 = add i64 %29560, -8
  %29562 = load i64, i64* %3, align 8
  %29563 = add i64 %29562, 4
  store i64 %29563, i64* %3, align 8
  %29564 = inttoptr i64 %29561 to i64*
  %29565 = load i64, i64* %29564, align 8
  store i64 %29565, i64* %RDI.i2910, align 8
  %29566 = add i64 %29560, -196
  %29567 = add i64 %29562, 10
  store i64 %29567, i64* %3, align 8
  %29568 = inttoptr i64 %29566 to i32*
  %29569 = load i32, i32* %29568, align 4
  %29570 = zext i32 %29569 to i64
  store i64 %29570, i64* %576, align 8
  %29571 = add i64 %29562, -15166
  %29572 = add i64 %29562, 15
  %29573 = load i64, i64* %6, align 8
  %29574 = add i64 %29573, -8
  %29575 = inttoptr i64 %29574 to i64*
  store i64 %29572, i64* %29575, align 8
  store i64 %29574, i64* %6, align 8
  store i64 %29571, i64* %3, align 8
  %call2_40d7e8 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29571, %struct.Memory* %MEMORY.61)
  %29576 = load i64, i64* %RBP.i, align 8
  %29577 = add i64 %29576, -20
  %29578 = load i64, i64* %3, align 8
  %29579 = add i64 %29578, 7
  store i64 %29579, i64* %3, align 8
  %29580 = inttoptr i64 %29577 to i32*
  store i32 0, i32* %29580, align 4
  %.pre236 = load i64, i64* %3, align 8
  br label %block_.L_40d7f4

block_.L_40d7f4:                                  ; preds = %block_.L_40d8b5, %block_40d7b8
  %29581 = phi i64 [ %29965, %block_.L_40d8b5 ], [ %.pre236, %block_40d7b8 ]
  %29582 = load i64, i64* %RBP.i, align 8
  %29583 = add i64 %29582, -20
  %29584 = add i64 %29581, 3
  store i64 %29584, i64* %3, align 8
  %29585 = inttoptr i64 %29583 to i32*
  %29586 = load i32, i32* %29585, align 4
  %29587 = zext i32 %29586 to i64
  store i64 %29587, i64* %RAX.i11582.pre-phi, align 8
  %29588 = add i64 %29582, -56
  %29589 = add i64 %29581, 6
  store i64 %29589, i64* %3, align 8
  %29590 = inttoptr i64 %29588 to i32*
  %29591 = load i32, i32* %29590, align 4
  %29592 = sub i32 %29586, %29591
  %29593 = icmp ult i32 %29586, %29591
  %29594 = zext i1 %29593 to i8
  store i8 %29594, i8* %14, align 1
  %29595 = and i32 %29592, 255
  %29596 = tail call i32 @llvm.ctpop.i32(i32 %29595)
  %29597 = trunc i32 %29596 to i8
  %29598 = and i8 %29597, 1
  %29599 = xor i8 %29598, 1
  store i8 %29599, i8* %21, align 1
  %29600 = xor i32 %29591, %29586
  %29601 = xor i32 %29600, %29592
  %29602 = lshr i32 %29601, 4
  %29603 = trunc i32 %29602 to i8
  %29604 = and i8 %29603, 1
  store i8 %29604, i8* %27, align 1
  %29605 = icmp eq i32 %29592, 0
  %29606 = zext i1 %29605 to i8
  store i8 %29606, i8* %30, align 1
  %29607 = lshr i32 %29592, 31
  %29608 = trunc i32 %29607 to i8
  store i8 %29608, i8* %33, align 1
  %29609 = lshr i32 %29586, 31
  %29610 = lshr i32 %29591, 31
  %29611 = xor i32 %29610, %29609
  %29612 = xor i32 %29607, %29609
  %29613 = add nuw nsw i32 %29612, %29611
  %29614 = icmp eq i32 %29613, 2
  %29615 = zext i1 %29614 to i8
  store i8 %29615, i8* %39, align 1
  %29616 = icmp ne i8 %29608, 0
  %29617 = xor i1 %29616, %29614
  %.v329 = select i1 %29617, i64 12, i64 223
  %29618 = add i64 %29581, %.v329
  %29619 = add i64 %29618, 5
  store i64 %29619, i64* %3, align 8
  br i1 %29617, label %block_.L_40d805.preheader, label %block_.L_40d8d8

block_.L_40d805.preheader:                        ; preds = %block_.L_40d7f4
  br label %block_.L_40d805

block_.L_40d805:                                  ; preds = %block_.L_40d805.preheader, %block_40d834
  %29620 = phi i64 [ %29764, %block_40d834 ], [ %29619, %block_.L_40d805.preheader ]
  %29621 = phi i64 [ %.pre237, %block_40d834 ], [ %29582, %block_.L_40d805.preheader ]
  %29622 = add i64 %29621, -196
  %29623 = add i64 %29620, 6
  store i64 %29623, i64* %3, align 8
  %29624 = inttoptr i64 %29622 to i32*
  %29625 = load i32, i32* %29624, align 4
  %29626 = zext i32 %29625 to i64
  store i64 %29626, i64* %RAX.i11582.pre-phi, align 8
  %29627 = add i64 %29621, -8
  %29628 = add i64 %29620, 10
  store i64 %29628, i64* %3, align 8
  %29629 = inttoptr i64 %29627 to i64*
  %29630 = load i64, i64* %29629, align 8
  %29631 = add i64 %29630, 37708
  store i64 %29631, i64* %RCX.i11580, align 8
  %29632 = icmp ugt i64 %29630, -37709
  %29633 = zext i1 %29632 to i8
  store i8 %29633, i8* %14, align 1
  %29634 = trunc i64 %29631 to i32
  %29635 = and i32 %29634, 255
  %29636 = tail call i32 @llvm.ctpop.i32(i32 %29635)
  %29637 = trunc i32 %29636 to i8
  %29638 = and i8 %29637, 1
  %29639 = xor i8 %29638, 1
  store i8 %29639, i8* %21, align 1
  %29640 = xor i64 %29631, %29630
  %29641 = lshr i64 %29640, 4
  %29642 = trunc i64 %29641 to i8
  %29643 = and i8 %29642, 1
  store i8 %29643, i8* %27, align 1
  %29644 = icmp eq i64 %29631, 0
  %29645 = zext i1 %29644 to i8
  store i8 %29645, i8* %30, align 1
  %29646 = lshr i64 %29631, 63
  %29647 = trunc i64 %29646 to i8
  store i8 %29647, i8* %33, align 1
  %29648 = lshr i64 %29630, 63
  %29649 = xor i64 %29646, %29648
  %29650 = add nuw nsw i64 %29649, %29646
  %29651 = icmp eq i64 %29650, 2
  %29652 = zext i1 %29651 to i8
  store i8 %29652, i8* %39, align 1
  %29653 = add i64 %29621, -16
  %29654 = add i64 %29620, 21
  store i64 %29654, i64* %3, align 8
  %29655 = inttoptr i64 %29653 to i32*
  %29656 = load i32, i32* %29655, align 4
  %29657 = sext i32 %29656 to i64
  %29658 = mul nsw i64 %29657, 258
  store i64 %29658, i64* %576, align 8
  %29659 = lshr i64 %29658, 63
  %29660 = add i64 %29658, %29631
  store i64 %29660, i64* %RCX.i11580, align 8
  %29661 = icmp ult i64 %29660, %29631
  %29662 = icmp ult i64 %29660, %29658
  %29663 = or i1 %29661, %29662
  %29664 = zext i1 %29663 to i8
  store i8 %29664, i8* %14, align 1
  %29665 = trunc i64 %29660 to i32
  %29666 = and i32 %29665, 255
  %29667 = tail call i32 @llvm.ctpop.i32(i32 %29666)
  %29668 = trunc i32 %29667 to i8
  %29669 = and i8 %29668, 1
  %29670 = xor i8 %29669, 1
  store i8 %29670, i8* %21, align 1
  %29671 = xor i64 %29658, %29631
  %29672 = xor i64 %29671, %29660
  %29673 = lshr i64 %29672, 4
  %29674 = trunc i64 %29673 to i8
  %29675 = and i8 %29674, 1
  store i8 %29675, i8* %27, align 1
  %29676 = icmp eq i64 %29660, 0
  %29677 = zext i1 %29676 to i8
  store i8 %29677, i8* %30, align 1
  %29678 = lshr i64 %29660, 63
  %29679 = trunc i64 %29678 to i8
  store i8 %29679, i8* %33, align 1
  %29680 = xor i64 %29678, %29646
  %29681 = xor i64 %29678, %29659
  %29682 = add nuw nsw i64 %29680, %29681
  %29683 = icmp eq i64 %29682, 2
  %29684 = zext i1 %29683 to i8
  store i8 %29684, i8* %39, align 1
  %29685 = load i64, i64* %RBP.i, align 8
  %29686 = add i64 %29685, -20
  %29687 = add i64 %29620, 35
  store i64 %29687, i64* %3, align 8
  %29688 = inttoptr i64 %29686 to i32*
  %29689 = load i32, i32* %29688, align 4
  %29690 = sext i32 %29689 to i64
  store i64 %29690, i64* %576, align 8
  %29691 = add i64 %29660, %29690
  %29692 = add i64 %29620, 39
  store i64 %29692, i64* %3, align 8
  %29693 = inttoptr i64 %29691 to i8*
  %29694 = load i8, i8* %29693, align 1
  %29695 = zext i8 %29694 to i64
  store i64 %29695, i64* %RSI.i11312, align 8
  %29696 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %29697 = zext i8 %29694 to i32
  %29698 = sub i32 %29696, %29697
  %29699 = icmp ult i32 %29696, %29697
  %29700 = zext i1 %29699 to i8
  store i8 %29700, i8* %14, align 1
  %29701 = and i32 %29698, 255
  %29702 = tail call i32 @llvm.ctpop.i32(i32 %29701)
  %29703 = trunc i32 %29702 to i8
  %29704 = and i8 %29703, 1
  %29705 = xor i8 %29704, 1
  store i8 %29705, i8* %21, align 1
  %29706 = xor i32 %29697, %29696
  %29707 = xor i32 %29706, %29698
  %29708 = lshr i32 %29707, 4
  %29709 = trunc i32 %29708 to i8
  %29710 = and i8 %29709, 1
  store i8 %29710, i8* %27, align 1
  %29711 = icmp eq i32 %29698, 0
  %29712 = zext i1 %29711 to i8
  store i8 %29712, i8* %30, align 1
  %29713 = lshr i32 %29698, 31
  %29714 = trunc i32 %29713 to i8
  store i8 %29714, i8* %33, align 1
  %29715 = lshr i32 %29696, 31
  %29716 = xor i32 %29713, %29715
  %29717 = add nuw nsw i32 %29716, %29715
  %29718 = icmp eq i32 %29717, 2
  %29719 = zext i1 %29718 to i8
  store i8 %29719, i8* %39, align 1
  %29720 = icmp ne i8 %29714, 0
  %29721 = xor i1 %29720, %29718
  %.v330 = select i1 %29721, i64 47, i64 85
  %29722 = add i64 %29620, %.v330
  store i64 %29722, i64* %3, align 8
  br i1 %29721, label %block_40d834, label %block_.L_40d85f.preheader

block_.L_40d85f.preheader:                        ; preds = %block_.L_40d805
  %29723 = add i64 %29722, 5
  br label %block_.L_40d85f

block_40d834:                                     ; preds = %block_.L_40d805
  store i64 2, i64* %RAX.i11582.pre-phi, align 8
  %29724 = add i64 %29685, -8
  %29725 = add i64 %29722, 9
  store i64 %29725, i64* %3, align 8
  %29726 = inttoptr i64 %29724 to i64*
  %29727 = load i64, i64* %29726, align 8
  store i64 %29727, i64* %RDI.i2910, align 8
  store i64 2, i64* %RSI.i11312, align 8
  store i64 2, i64* %576, align 8
  %29728 = add i64 %29722, -15252
  %29729 = add i64 %29722, 18
  %29730 = load i64, i64* %6, align 8
  %29731 = add i64 %29730, -8
  %29732 = inttoptr i64 %29731 to i64*
  store i64 %29729, i64* %29732, align 8
  store i64 %29731, i64* %6, align 8
  store i64 %29728, i64* %3, align 8
  %call2_40d841 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29728, %struct.Memory* %MEMORY.61)
  %29733 = load i64, i64* %RBP.i, align 8
  %29734 = add i64 %29733, -196
  %29735 = load i64, i64* %3, align 8
  %29736 = add i64 %29735, 6
  store i64 %29736, i64* %3, align 8
  %29737 = inttoptr i64 %29734 to i32*
  %29738 = load i32, i32* %29737, align 4
  %29739 = add i32 %29738, 1
  %29740 = zext i32 %29739 to i64
  store i64 %29740, i64* %RAX.i11582.pre-phi, align 8
  %29741 = icmp eq i32 %29738, -1
  %29742 = icmp eq i32 %29739, 0
  %29743 = or i1 %29741, %29742
  %29744 = zext i1 %29743 to i8
  store i8 %29744, i8* %14, align 1
  %29745 = and i32 %29739, 255
  %29746 = tail call i32 @llvm.ctpop.i32(i32 %29745)
  %29747 = trunc i32 %29746 to i8
  %29748 = and i8 %29747, 1
  %29749 = xor i8 %29748, 1
  store i8 %29749, i8* %21, align 1
  %29750 = xor i32 %29739, %29738
  %29751 = lshr i32 %29750, 4
  %29752 = trunc i32 %29751 to i8
  %29753 = and i8 %29752, 1
  store i8 %29753, i8* %27, align 1
  %29754 = zext i1 %29742 to i8
  store i8 %29754, i8* %30, align 1
  %29755 = lshr i32 %29739, 31
  %29756 = trunc i32 %29755 to i8
  store i8 %29756, i8* %33, align 1
  %29757 = lshr i32 %29738, 31
  %29758 = xor i32 %29755, %29757
  %29759 = add nuw nsw i32 %29758, %29755
  %29760 = icmp eq i32 %29759, 2
  %29761 = zext i1 %29760 to i8
  store i8 %29761, i8* %39, align 1
  %29762 = add i64 %29735, 15
  store i64 %29762, i64* %3, align 8
  store i32 %29739, i32* %29737, align 4
  %29763 = load i64, i64* %3, align 8
  %29764 = add i64 %29763, -80
  store i64 %29764, i64* %3, align 8
  %.pre237 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40d805

block_.L_40d85f:                                  ; preds = %block_.L_40d85f.preheader, %block_40d88e
  %29765 = phi i64 [ %.pre238, %block_40d88e ], [ %29685, %block_.L_40d85f.preheader ]
  %storemerge86 = phi i64 [ %29910, %block_40d88e ], [ %29723, %block_.L_40d85f.preheader ]
  %29766 = add i64 %29765, -196
  %29767 = add i64 %storemerge86, 6
  store i64 %29767, i64* %3, align 8
  %29768 = inttoptr i64 %29766 to i32*
  %29769 = load i32, i32* %29768, align 4
  %29770 = zext i32 %29769 to i64
  store i64 %29770, i64* %RAX.i11582.pre-phi, align 8
  %29771 = add i64 %29765, -8
  %29772 = add i64 %storemerge86, 10
  store i64 %29772, i64* %3, align 8
  %29773 = inttoptr i64 %29771 to i64*
  %29774 = load i64, i64* %29773, align 8
  %29775 = add i64 %29774, 37708
  store i64 %29775, i64* %RCX.i11580, align 8
  %29776 = icmp ugt i64 %29774, -37709
  %29777 = zext i1 %29776 to i8
  store i8 %29777, i8* %14, align 1
  %29778 = trunc i64 %29775 to i32
  %29779 = and i32 %29778, 255
  %29780 = tail call i32 @llvm.ctpop.i32(i32 %29779)
  %29781 = trunc i32 %29780 to i8
  %29782 = and i8 %29781, 1
  %29783 = xor i8 %29782, 1
  store i8 %29783, i8* %21, align 1
  %29784 = xor i64 %29775, %29774
  %29785 = lshr i64 %29784, 4
  %29786 = trunc i64 %29785 to i8
  %29787 = and i8 %29786, 1
  store i8 %29787, i8* %27, align 1
  %29788 = icmp eq i64 %29775, 0
  %29789 = zext i1 %29788 to i8
  store i8 %29789, i8* %30, align 1
  %29790 = lshr i64 %29775, 63
  %29791 = trunc i64 %29790 to i8
  store i8 %29791, i8* %33, align 1
  %29792 = lshr i64 %29774, 63
  %29793 = xor i64 %29790, %29792
  %29794 = add nuw nsw i64 %29793, %29790
  %29795 = icmp eq i64 %29794, 2
  %29796 = zext i1 %29795 to i8
  store i8 %29796, i8* %39, align 1
  %29797 = add i64 %29765, -16
  %29798 = add i64 %storemerge86, 21
  store i64 %29798, i64* %3, align 8
  %29799 = inttoptr i64 %29797 to i32*
  %29800 = load i32, i32* %29799, align 4
  %29801 = sext i32 %29800 to i64
  %29802 = mul nsw i64 %29801, 258
  store i64 %29802, i64* %576, align 8
  %29803 = lshr i64 %29802, 63
  %29804 = add i64 %29802, %29775
  store i64 %29804, i64* %RCX.i11580, align 8
  %29805 = icmp ult i64 %29804, %29775
  %29806 = icmp ult i64 %29804, %29802
  %29807 = or i1 %29805, %29806
  %29808 = zext i1 %29807 to i8
  store i8 %29808, i8* %14, align 1
  %29809 = trunc i64 %29804 to i32
  %29810 = and i32 %29809, 255
  %29811 = tail call i32 @llvm.ctpop.i32(i32 %29810)
  %29812 = trunc i32 %29811 to i8
  %29813 = and i8 %29812, 1
  %29814 = xor i8 %29813, 1
  store i8 %29814, i8* %21, align 1
  %29815 = xor i64 %29802, %29775
  %29816 = xor i64 %29815, %29804
  %29817 = lshr i64 %29816, 4
  %29818 = trunc i64 %29817 to i8
  %29819 = and i8 %29818, 1
  store i8 %29819, i8* %27, align 1
  %29820 = icmp eq i64 %29804, 0
  %29821 = zext i1 %29820 to i8
  store i8 %29821, i8* %30, align 1
  %29822 = lshr i64 %29804, 63
  %29823 = trunc i64 %29822 to i8
  store i8 %29823, i8* %33, align 1
  %29824 = xor i64 %29822, %29790
  %29825 = xor i64 %29822, %29803
  %29826 = add nuw nsw i64 %29824, %29825
  %29827 = icmp eq i64 %29826, 2
  %29828 = zext i1 %29827 to i8
  store i8 %29828, i8* %39, align 1
  %29829 = load i64, i64* %RBP.i, align 8
  %29830 = add i64 %29829, -20
  %29831 = add i64 %storemerge86, 35
  store i64 %29831, i64* %3, align 8
  %29832 = inttoptr i64 %29830 to i32*
  %29833 = load i32, i32* %29832, align 4
  %29834 = sext i32 %29833 to i64
  store i64 %29834, i64* %576, align 8
  %29835 = add i64 %29804, %29834
  %29836 = add i64 %storemerge86, 39
  store i64 %29836, i64* %3, align 8
  %29837 = inttoptr i64 %29835 to i8*
  %29838 = load i8, i8* %29837, align 1
  %29839 = zext i8 %29838 to i64
  store i64 %29839, i64* %RSI.i11312, align 8
  %29840 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %29841 = zext i8 %29838 to i32
  %29842 = sub i32 %29840, %29841
  %29843 = icmp ult i32 %29840, %29841
  %29844 = zext i1 %29843 to i8
  store i8 %29844, i8* %14, align 1
  %29845 = and i32 %29842, 255
  %29846 = tail call i32 @llvm.ctpop.i32(i32 %29845)
  %29847 = trunc i32 %29846 to i8
  %29848 = and i8 %29847, 1
  %29849 = xor i8 %29848, 1
  store i8 %29849, i8* %21, align 1
  %29850 = xor i32 %29841, %29840
  %29851 = xor i32 %29850, %29842
  %29852 = lshr i32 %29851, 4
  %29853 = trunc i32 %29852 to i8
  %29854 = and i8 %29853, 1
  store i8 %29854, i8* %27, align 1
  %29855 = icmp eq i32 %29842, 0
  %29856 = zext i1 %29855 to i8
  store i8 %29856, i8* %30, align 1
  %29857 = lshr i32 %29842, 31
  %29858 = trunc i32 %29857 to i8
  store i8 %29858, i8* %33, align 1
  %29859 = lshr i32 %29840, 31
  %29860 = xor i32 %29857, %29859
  %29861 = add nuw nsw i32 %29860, %29859
  %29862 = icmp eq i32 %29861, 2
  %29863 = zext i1 %29862 to i8
  store i8 %29863, i8* %39, align 1
  %29864 = icmp ne i8 %29858, 0
  %29865 = xor i1 %29864, %29862
  %29866 = or i1 %29855, %29865
  %.v331 = select i1 %29866, i64 86, i64 47
  %29867 = add i64 %storemerge86, %.v331
  %29868 = add i64 %29867, 5
  store i64 %29868, i64* %3, align 8
  br i1 %29866, label %block_.L_40d8b5, label %block_40d88e

block_40d88e:                                     ; preds = %block_.L_40d85f
  store i64 2, i64* %RSI.i11312, align 8
  store i64 3, i64* %576, align 8
  %29869 = add i64 %29829, -8
  %29870 = add i64 %29867, 14
  store i64 %29870, i64* %3, align 8
  %29871 = inttoptr i64 %29869 to i64*
  %29872 = load i64, i64* %29871, align 8
  store i64 %29872, i64* %RDI.i2910, align 8
  %29873 = add i64 %29867, -15342
  %29874 = add i64 %29867, 19
  %29875 = load i64, i64* %6, align 8
  %29876 = add i64 %29875, -8
  %29877 = inttoptr i64 %29876 to i64*
  store i64 %29874, i64* %29877, align 8
  store i64 %29876, i64* %6, align 8
  store i64 %29873, i64* %3, align 8
  %call2_40d89c = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29873, %struct.Memory* %MEMORY.61)
  %29878 = load i64, i64* %RBP.i, align 8
  %29879 = add i64 %29878, -196
  %29880 = load i64, i64* %3, align 8
  %29881 = add i64 %29880, 6
  store i64 %29881, i64* %3, align 8
  %29882 = inttoptr i64 %29879 to i32*
  %29883 = load i32, i32* %29882, align 4
  %29884 = add i32 %29883, -1
  %29885 = zext i32 %29884 to i64
  store i64 %29885, i64* %576, align 8
  %29886 = icmp ne i32 %29883, 0
  %29887 = zext i1 %29886 to i8
  store i8 %29887, i8* %14, align 1
  %29888 = and i32 %29884, 255
  %29889 = tail call i32 @llvm.ctpop.i32(i32 %29888)
  %29890 = trunc i32 %29889 to i8
  %29891 = and i8 %29890, 1
  %29892 = xor i8 %29891, 1
  store i8 %29892, i8* %21, align 1
  %29893 = xor i32 %29883, 16
  %29894 = xor i32 %29893, %29884
  %29895 = lshr i32 %29894, 4
  %29896 = trunc i32 %29895 to i8
  %29897 = and i8 %29896, 1
  store i8 %29897, i8* %27, align 1
  %29898 = icmp eq i32 %29884, 0
  %29899 = zext i1 %29898 to i8
  store i8 %29899, i8* %30, align 1
  %29900 = lshr i32 %29884, 31
  %29901 = trunc i32 %29900 to i8
  store i8 %29901, i8* %33, align 1
  %29902 = lshr i32 %29883, 31
  %29903 = xor i32 %29900, %29902
  %29904 = xor i32 %29900, 1
  %29905 = add nuw nsw i32 %29903, %29904
  %29906 = icmp eq i32 %29905, 2
  %29907 = zext i1 %29906 to i8
  store i8 %29907, i8* %39, align 1
  %29908 = add i64 %29880, 15
  store i64 %29908, i64* %3, align 8
  store i32 %29884, i32* %29882, align 4
  %29909 = load i64, i64* %3, align 8
  %29910 = add i64 %29909, -81
  %29911 = add i64 %29909, 5
  store i64 %29911, i64* %3, align 8
  %.pre238 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40d85f

block_.L_40d8b5:                                  ; preds = %block_.L_40d85f
  store i64 1, i64* %RSI.i11312, align 8
  %29912 = zext i32 %29833 to i64
  %29913 = xor i64 %29912, %29834
  %29914 = trunc i64 %29913 to i32
  %29915 = and i64 %29913, 4294967295
  store i64 %29915, i64* %576, align 8
  store i8 0, i8* %14, align 1
  %29916 = and i32 %29914, 255
  %29917 = tail call i32 @llvm.ctpop.i32(i32 %29916)
  %29918 = trunc i32 %29917 to i8
  %29919 = and i8 %29918, 1
  %29920 = xor i8 %29919, 1
  store i8 %29920, i8* %21, align 1
  %29921 = icmp eq i32 %29914, 0
  %29922 = zext i1 %29921 to i8
  store i8 %29922, i8* %30, align 1
  %29923 = lshr i32 %29914, 31
  %29924 = trunc i32 %29923 to i8
  store i8 %29924, i8* %33, align 1
  store i8 0, i8* %39, align 1
  store i8 0, i8* %27, align 1
  %29925 = add i64 %29829, -8
  %29926 = add i64 %29867, 11
  store i64 %29926, i64* %3, align 8
  %29927 = inttoptr i64 %29925 to i64*
  %29928 = load i64, i64* %29927, align 8
  store i64 %29928, i64* %RDI.i2910, align 8
  %29929 = add i64 %29867, -15381
  %29930 = add i64 %29867, 16
  %29931 = load i64, i64* %6, align 8
  %29932 = add i64 %29931, -8
  %29933 = inttoptr i64 %29932 to i64*
  store i64 %29930, i64* %29933, align 8
  store i64 %29932, i64* %6, align 8
  store i64 %29929, i64* %3, align 8
  %call2_40d8c0 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %29929, %struct.Memory* %MEMORY.61)
  %29934 = load i64, i64* %RBP.i, align 8
  %29935 = add i64 %29934, -20
  %29936 = load i64, i64* %3, align 8
  %29937 = add i64 %29936, 3
  store i64 %29937, i64* %3, align 8
  %29938 = inttoptr i64 %29935 to i32*
  %29939 = load i32, i32* %29938, align 4
  %29940 = add i32 %29939, 1
  %29941 = zext i32 %29940 to i64
  store i64 %29941, i64* %RAX.i11582.pre-phi, align 8
  %29942 = icmp eq i32 %29939, -1
  %29943 = icmp eq i32 %29940, 0
  %29944 = or i1 %29942, %29943
  %29945 = zext i1 %29944 to i8
  store i8 %29945, i8* %14, align 1
  %29946 = and i32 %29940, 255
  %29947 = tail call i32 @llvm.ctpop.i32(i32 %29946)
  %29948 = trunc i32 %29947 to i8
  %29949 = and i8 %29948, 1
  %29950 = xor i8 %29949, 1
  store i8 %29950, i8* %21, align 1
  %29951 = xor i32 %29940, %29939
  %29952 = lshr i32 %29951, 4
  %29953 = trunc i32 %29952 to i8
  %29954 = and i8 %29953, 1
  store i8 %29954, i8* %27, align 1
  %29955 = zext i1 %29943 to i8
  store i8 %29955, i8* %30, align 1
  %29956 = lshr i32 %29940, 31
  %29957 = trunc i32 %29956 to i8
  store i8 %29957, i8* %33, align 1
  %29958 = lshr i32 %29939, 31
  %29959 = xor i32 %29956, %29958
  %29960 = add nuw nsw i32 %29959, %29956
  %29961 = icmp eq i32 %29960, 2
  %29962 = zext i1 %29961 to i8
  store i8 %29962, i8* %39, align 1
  %29963 = add i64 %29936, 9
  store i64 %29963, i64* %3, align 8
  store i32 %29940, i32* %29938, align 4
  %29964 = load i64, i64* %3, align 8
  %29965 = add i64 %29964, -218
  store i64 %29965, i64* %3, align 8
  br label %block_.L_40d7f4

block_.L_40d8d8:                                  ; preds = %block_.L_40d7f4
  %29966 = add i64 %29582, -16
  %29967 = add i64 %29618, 8
  store i64 %29967, i64* %3, align 8
  %29968 = inttoptr i64 %29966 to i32*
  %29969 = load i32, i32* %29968, align 4
  %29970 = add i32 %29969, 1
  %29971 = zext i32 %29970 to i64
  store i64 %29971, i64* %RAX.i11582.pre-phi, align 8
  %29972 = icmp eq i32 %29969, -1
  %29973 = icmp eq i32 %29970, 0
  %29974 = or i1 %29972, %29973
  %29975 = zext i1 %29974 to i8
  store i8 %29975, i8* %14, align 1
  %29976 = and i32 %29970, 255
  %29977 = tail call i32 @llvm.ctpop.i32(i32 %29976)
  %29978 = trunc i32 %29977 to i8
  %29979 = and i8 %29978, 1
  %29980 = xor i8 %29979, 1
  store i8 %29980, i8* %21, align 1
  %29981 = xor i32 %29970, %29969
  %29982 = lshr i32 %29981, 4
  %29983 = trunc i32 %29982 to i8
  %29984 = and i8 %29983, 1
  store i8 %29984, i8* %27, align 1
  %29985 = zext i1 %29973 to i8
  store i8 %29985, i8* %30, align 1
  %29986 = lshr i32 %29970, 31
  %29987 = trunc i32 %29986 to i8
  store i8 %29987, i8* %33, align 1
  %29988 = lshr i32 %29969, 31
  %29989 = xor i32 %29986, %29988
  %29990 = add nuw nsw i32 %29989, %29986
  %29991 = icmp eq i32 %29990, 2
  %29992 = zext i1 %29991 to i8
  store i8 %29992, i8* %39, align 1
  %29993 = add i64 %29618, 14
  store i64 %29993, i64* %3, align 8
  store i32 %29970, i32* %29968, align 4
  %29994 = load i64, i64* %3, align 8
  %29995 = add i64 %29994, -309
  store i64 %29995, i64* %3, align 8
  br label %block_.L_40d7ac

block_.L_40d8e6:                                  ; preds = %block_.L_40d7ac
  %29996 = add i64 %29458, -8
  %29997 = add i64 %29494, 4
  store i64 %29997, i64* %3, align 8
  %29998 = inttoptr i64 %29996 to i64*
  %29999 = load i64, i64* %29998, align 8
  store i64 %29999, i64* %RAX.i11582.pre-phi, align 8
  %30000 = add i64 %29999, 656
  %30001 = add i64 %29494, 11
  store i64 %30001, i64* %3, align 8
  %30002 = inttoptr i64 %30000 to i32*
  %30003 = load i32, i32* %30002, align 4
  %30004 = add i32 %30003, -3
  %30005 = icmp ult i32 %30003, 3
  %30006 = zext i1 %30005 to i8
  store i8 %30006, i8* %14, align 1
  %30007 = and i32 %30004, 255
  %30008 = tail call i32 @llvm.ctpop.i32(i32 %30007)
  %30009 = trunc i32 %30008 to i8
  %30010 = and i8 %30009, 1
  %30011 = xor i8 %30010, 1
  store i8 %30011, i8* %21, align 1
  %30012 = xor i32 %30004, %30003
  %30013 = lshr i32 %30012, 4
  %30014 = trunc i32 %30013 to i8
  %30015 = and i8 %30014, 1
  store i8 %30015, i8* %27, align 1
  %30016 = icmp eq i32 %30004, 0
  %30017 = zext i1 %30016 to i8
  store i8 %30017, i8* %30, align 1
  %30018 = lshr i32 %30004, 31
  %30019 = trunc i32 %30018 to i8
  store i8 %30019, i8* %33, align 1
  %30020 = lshr i32 %30003, 31
  %30021 = xor i32 %30018, %30020
  %30022 = add nuw nsw i32 %30021, %30020
  %30023 = icmp eq i32 %30022, 2
  %30024 = zext i1 %30023 to i8
  store i8 %30024, i8* %39, align 1
  %30025 = icmp ne i8 %30019, 0
  %30026 = xor i1 %30025, %30023
  %.v321 = select i1 %30026, i64 60, i64 17
  %30027 = add i64 %29494, %.v321
  store i64 %30027, i64* %3, align 8
  br i1 %30026, label %block_.L_40d922, label %block_40d8f7

block_40d8f7:                                     ; preds = %block_.L_40d8e6
  store i64 ptrtoint (%G__0x4165ea_type* @G__0x4165ea to i64), i64* %RSI.i11312, align 8
  %30028 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %30028, i64* %RDI.i2910, align 8
  %30029 = add i64 %30027, 22
  store i64 %30029, i64* %3, align 8
  %30030 = load i64, i64* %29998, align 8
  store i64 %30030, i64* %RAX.i11582.pre-phi, align 8
  %30031 = add i64 %30030, 116
  %30032 = add i64 %30027, 25
  store i64 %30032, i64* %3, align 8
  %30033 = inttoptr i64 %30031 to i32*
  %30034 = load i32, i32* %30033, align 4
  %30035 = zext i32 %30034 to i64
  store i64 %30035, i64* %RCX.i11580, align 8
  %30036 = add i64 %29458, -76
  %30037 = add i64 %30027, 28
  store i64 %30037, i64* %3, align 8
  %30038 = inttoptr i64 %30036 to i32*
  %30039 = load i32, i32* %30038, align 4
  %30040 = sub i32 %30034, %30039
  %30041 = zext i32 %30040 to i64
  store i64 %30041, i64* %RCX.i11580, align 8
  %30042 = icmp ult i32 %30034, %30039
  %30043 = zext i1 %30042 to i8
  store i8 %30043, i8* %14, align 1
  %30044 = and i32 %30040, 255
  %30045 = tail call i32 @llvm.ctpop.i32(i32 %30044)
  %30046 = trunc i32 %30045 to i8
  %30047 = and i8 %30046, 1
  %30048 = xor i8 %30047, 1
  store i8 %30048, i8* %21, align 1
  %30049 = xor i32 %30039, %30034
  %30050 = xor i32 %30049, %30040
  %30051 = lshr i32 %30050, 4
  %30052 = trunc i32 %30051 to i8
  %30053 = and i8 %30052, 1
  store i8 %30053, i8* %27, align 1
  %30054 = icmp eq i32 %30040, 0
  %30055 = zext i1 %30054 to i8
  store i8 %30055, i8* %30, align 1
  %30056 = lshr i32 %30040, 31
  %30057 = trunc i32 %30056 to i8
  store i8 %30057, i8* %33, align 1
  %30058 = lshr i32 %30034, 31
  %30059 = lshr i32 %30039, 31
  %30060 = xor i32 %30059, %30058
  %30061 = xor i32 %30056, %30058
  %30062 = add nuw nsw i32 %30061, %30060
  %30063 = icmp eq i32 %30062, 2
  %30064 = zext i1 %30063 to i8
  store i8 %30064, i8* %39, align 1
  store i64 %30041, i64* %576, align 8
  store i8 0, i8* %AL.i11425, align 1
  %30065 = add i64 %30027, -53191
  %30066 = add i64 %30027, 37
  %30067 = load i64, i64* %6, align 8
  %30068 = add i64 %30067, -8
  %30069 = inttoptr i64 %30068 to i64*
  store i64 %30066, i64* %30069, align 8
  store i64 %30068, i64* %6, align 8
  store i64 %30065, i64* %3, align 8
  %30070 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %MEMORY.61)
  %30071 = load i64, i64* %RBP.i, align 8
  %30072 = add i64 %30071, -268
  %30073 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %30074 = load i64, i64* %3, align 8
  %30075 = add i64 %30074, 6
  store i64 %30075, i64* %3, align 8
  %30076 = inttoptr i64 %30072 to i32*
  store i32 %30073, i32* %30076, align 4
  %.pre223 = load i64, i64* %RBP.i, align 8
  %.pre224 = load i64, i64* %3, align 8
  br label %block_.L_40d922

block_.L_40d922:                                  ; preds = %block_40d8f7, %block_.L_40d8e6
  %30077 = phi i64 [ %30027, %block_.L_40d8e6 ], [ %.pre224, %block_40d8f7 ]
  %30078 = phi i64 [ %29458, %block_.L_40d8e6 ], [ %.pre223, %block_40d8f7 ]
  %MEMORY.66 = phi %struct.Memory* [ %MEMORY.61, %block_.L_40d8e6 ], [ %30070, %block_40d8f7 ]
  %30079 = add i64 %30078, -8
  %30080 = add i64 %30077, 4
  store i64 %30080, i64* %3, align 8
  %30081 = inttoptr i64 %30079 to i64*
  %30082 = load i64, i64* %30081, align 8
  store i64 %30082, i64* %RAX.i11582.pre-phi, align 8
  %30083 = add i64 %30082, 116
  %30084 = add i64 %30077, 7
  store i64 %30084, i64* %3, align 8
  %30085 = inttoptr i64 %30083 to i32*
  %30086 = load i32, i32* %30085, align 4
  %30087 = zext i32 %30086 to i64
  store i64 %30087, i64* %RCX.i11580, align 8
  %30088 = add i64 %30078, -76
  %30089 = add i64 %30077, 10
  store i64 %30089, i64* %3, align 8
  %30090 = inttoptr i64 %30088 to i32*
  store i32 %30086, i32* %30090, align 4
  %30091 = load i64, i64* %RBP.i, align 8
  %30092 = add i64 %30091, -68
  %30093 = load i64, i64* %3, align 8
  %30094 = add i64 %30093, 7
  store i64 %30094, i64* %3, align 8
  %30095 = inttoptr i64 %30092 to i32*
  store i32 0, i32* %30095, align 4
  %30096 = load i64, i64* %RBP.i, align 8
  %30097 = add i64 %30096, -28
  %30098 = load i64, i64* %3, align 8
  %30099 = add i64 %30098, 7
  store i64 %30099, i64* %3, align 8
  %30100 = inttoptr i64 %30097 to i32*
  store i32 0, i32* %30100, align 4
  %30101 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 19, i32 0
  %R9W.i2497 = bitcast %union.anon* %30101 to i16*
  %.pre225 = load i64, i64* %3, align 8
  br label %block_.L_40d93a

block_.L_40d93a:                                  ; preds = %block_.L_40e938, %block_.L_40d922
  %30102 = phi i64 [ %.pre225, %block_.L_40d922 ], [ %34917, %block_.L_40e938 ]
  %MEMORY.67 = phi %struct.Memory* [ %MEMORY.66, %block_.L_40d922 ], [ %MEMORY.72, %block_.L_40e938 ]
  %30103 = load i64, i64* %RBP.i, align 8
  %30104 = add i64 %30103, -28
  %30105 = add i64 %30102, 3
  store i64 %30105, i64* %3, align 8
  %30106 = inttoptr i64 %30104 to i32*
  %30107 = load i32, i32* %30106, align 4
  %30108 = zext i32 %30107 to i64
  store i64 %30108, i64* %RAX.i11582.pre-phi, align 8
  %30109 = add i64 %30103, -8
  %30110 = add i64 %30102, 7
  store i64 %30110, i64* %3, align 8
  %30111 = inttoptr i64 %30109 to i64*
  %30112 = load i64, i64* %30111, align 8
  store i64 %30112, i64* %RCX.i11580, align 8
  %30113 = add i64 %30112, 668
  %30114 = add i64 %30102, 13
  store i64 %30114, i64* %3, align 8
  %30115 = inttoptr i64 %30113 to i32*
  %30116 = load i32, i32* %30115, align 4
  %30117 = sub i32 %30107, %30116
  %30118 = icmp ult i32 %30107, %30116
  %30119 = zext i1 %30118 to i8
  store i8 %30119, i8* %14, align 1
  %30120 = and i32 %30117, 255
  %30121 = tail call i32 @llvm.ctpop.i32(i32 %30120)
  %30122 = trunc i32 %30121 to i8
  %30123 = and i8 %30122, 1
  %30124 = xor i8 %30123, 1
  store i8 %30124, i8* %21, align 1
  %30125 = xor i32 %30116, %30107
  %30126 = xor i32 %30125, %30117
  %30127 = lshr i32 %30126, 4
  %30128 = trunc i32 %30127 to i8
  %30129 = and i8 %30128, 1
  store i8 %30129, i8* %27, align 1
  %30130 = icmp eq i32 %30117, 0
  %30131 = zext i1 %30130 to i8
  store i8 %30131, i8* %30, align 1
  %30132 = lshr i32 %30117, 31
  %30133 = trunc i32 %30132 to i8
  store i8 %30133, i8* %33, align 1
  %30134 = lshr i32 %30107, 31
  %30135 = lshr i32 %30116, 31
  %30136 = xor i32 %30135, %30134
  %30137 = xor i32 %30132, %30134
  %30138 = add nuw nsw i32 %30137, %30136
  %30139 = icmp eq i32 %30138, 2
  %30140 = zext i1 %30139 to i8
  store i8 %30140, i8* %39, align 1
  %30141 = icmp ne i8 %30133, 0
  %30142 = xor i1 %30141, %30139
  %.v287 = select i1 %30142, i64 24, i64 19
  %30143 = add i64 %30102, %.v287
  store i64 %30143, i64* %3, align 8
  br i1 %30142, label %block_.L_40d952, label %block_40d94d

block_40d94d:                                     ; preds = %block_.L_40d93a
  %30144 = add i64 %30103, -68
  %30145 = add i64 %30143, 4101
  store i64 %30145, i64* %3, align 8
  %30146 = inttoptr i64 %30144 to i32*
  %30147 = load i32, i32* %30146, align 4
  %30148 = zext i32 %30147 to i64
  store i64 %30148, i64* %RAX.i11582.pre-phi, align 8
  %30149 = add i64 %30103, -52
  %30150 = add i64 %30143, 4104
  store i64 %30150, i64* %3, align 8
  %30151 = inttoptr i64 %30149 to i32*
  %30152 = load i32, i32* %30151, align 4
  %30153 = sub i32 %30147, %30152
  %30154 = icmp ult i32 %30147, %30152
  %30155 = zext i1 %30154 to i8
  store i8 %30155, i8* %14, align 1
  %30156 = and i32 %30153, 255
  %30157 = tail call i32 @llvm.ctpop.i32(i32 %30156)
  %30158 = trunc i32 %30157 to i8
  %30159 = and i8 %30158, 1
  %30160 = xor i8 %30159, 1
  store i8 %30160, i8* %21, align 1
  %30161 = xor i32 %30152, %30147
  %30162 = xor i32 %30161, %30153
  %30163 = lshr i32 %30162, 4
  %30164 = trunc i32 %30163 to i8
  %30165 = and i8 %30164, 1
  store i8 %30165, i8* %27, align 1
  %30166 = icmp eq i32 %30153, 0
  %30167 = zext i1 %30166 to i8
  store i8 %30167, i8* %30, align 1
  %30168 = lshr i32 %30153, 31
  %30169 = trunc i32 %30168 to i8
  store i8 %30169, i8* %33, align 1
  %30170 = lshr i32 %30147, 31
  %30171 = lshr i32 %30152, 31
  %30172 = xor i32 %30171, %30170
  %30173 = xor i32 %30168, %30170
  %30174 = add nuw nsw i32 %30173, %30172
  %30175 = icmp eq i32 %30174, 2
  %30176 = zext i1 %30175 to i8
  store i8 %30176, i8* %39, align 1
  %.v322 = select i1 %30166, i64 4120, i64 4110
  %30177 = add i64 %30143, %.v322
  store i64 %30177, i64* %3, align 8
  br i1 %30166, label %block_.L_40e965, label %block_40e95b

block_.L_40d952:                                  ; preds = %block_.L_40d93a
  %30178 = add i64 %30143, 3
  store i64 %30178, i64* %3, align 8
  %30179 = load i32, i32* %30106, align 4
  %30180 = add i32 %30179, 50
  %30181 = icmp eq i32 %30180, 0
  %30182 = zext i1 %30181 to i8
  %30183 = lshr i32 %30180, 31
  %30184 = add i32 %30179, 49
  %30185 = zext i32 %30184 to i64
  store i64 %30185, i64* %RAX.i11582.pre-phi, align 8
  store i8 %30182, i8* %14, align 1
  %30186 = and i32 %30184, 255
  %30187 = tail call i32 @llvm.ctpop.i32(i32 %30186)
  %30188 = trunc i32 %30187 to i8
  %30189 = and i8 %30188, 1
  %30190 = xor i8 %30189, 1
  store i8 %30190, i8* %21, align 1
  %30191 = xor i32 %30184, %30180
  %30192 = lshr i32 %30191, 4
  %30193 = trunc i32 %30192 to i8
  %30194 = and i8 %30193, 1
  store i8 %30194, i8* %27, align 1
  %30195 = icmp eq i32 %30184, 0
  %30196 = zext i1 %30195 to i8
  store i8 %30196, i8* %30, align 1
  %30197 = lshr i32 %30184, 31
  %30198 = trunc i32 %30197 to i8
  store i8 %30198, i8* %33, align 1
  %30199 = xor i32 %30197, %30183
  %30200 = add nuw nsw i32 %30199, %30183
  %30201 = icmp eq i32 %30200, 2
  %30202 = zext i1 %30201 to i8
  store i8 %30202, i8* %39, align 1
  %30203 = add i64 %30103, -32
  %30204 = add i64 %30143, 12
  store i64 %30204, i64* %3, align 8
  %30205 = inttoptr i64 %30203 to i32*
  store i32 %30184, i32* %30205, align 4
  %30206 = load i64, i64* %RBP.i, align 8
  %30207 = add i64 %30206, -32
  %30208 = load i64, i64* %3, align 8
  %30209 = add i64 %30208, 3
  store i64 %30209, i64* %3, align 8
  %30210 = inttoptr i64 %30207 to i32*
  %30211 = load i32, i32* %30210, align 4
  %30212 = zext i32 %30211 to i64
  store i64 %30212, i64* %RAX.i11582.pre-phi, align 8
  %30213 = add i64 %30206, -8
  %30214 = add i64 %30208, 7
  store i64 %30214, i64* %3, align 8
  %30215 = inttoptr i64 %30213 to i64*
  %30216 = load i64, i64* %30215, align 8
  store i64 %30216, i64* %RCX.i11580, align 8
  %30217 = add i64 %30216, 668
  %30218 = add i64 %30208, 13
  store i64 %30218, i64* %3, align 8
  %30219 = inttoptr i64 %30217 to i32*
  %30220 = load i32, i32* %30219, align 4
  %30221 = sub i32 %30211, %30220
  %30222 = icmp ult i32 %30211, %30220
  %30223 = zext i1 %30222 to i8
  store i8 %30223, i8* %14, align 1
  %30224 = and i32 %30221, 255
  %30225 = tail call i32 @llvm.ctpop.i32(i32 %30224)
  %30226 = trunc i32 %30225 to i8
  %30227 = and i8 %30226, 1
  %30228 = xor i8 %30227, 1
  store i8 %30228, i8* %21, align 1
  %30229 = xor i32 %30220, %30211
  %30230 = xor i32 %30229, %30221
  %30231 = lshr i32 %30230, 4
  %30232 = trunc i32 %30231 to i8
  %30233 = and i8 %30232, 1
  store i8 %30233, i8* %27, align 1
  %30234 = icmp eq i32 %30221, 0
  %30235 = zext i1 %30234 to i8
  store i8 %30235, i8* %30, align 1
  %30236 = lshr i32 %30221, 31
  %30237 = trunc i32 %30236 to i8
  store i8 %30237, i8* %33, align 1
  %30238 = lshr i32 %30211, 31
  %30239 = lshr i32 %30220, 31
  %30240 = xor i32 %30239, %30238
  %30241 = xor i32 %30236, %30238
  %30242 = add nuw nsw i32 %30241, %30240
  %30243 = icmp eq i32 %30242, 2
  %30244 = zext i1 %30243 to i8
  store i8 %30244, i8* %39, align 1
  %30245 = icmp ne i8 %30237, 0
  %30246 = xor i1 %30245, %30243
  %.v324 = select i1 %30246, i64 35, i64 19
  %30247 = add i64 %30208, %.v324
  store i64 %30247, i64* %3, align 8
  br i1 %30246, label %block_.L_40d981, label %block_40d971

block_40d971:                                     ; preds = %block_.L_40d952
  %30248 = add i64 %30247, 4
  store i64 %30248, i64* %3, align 8
  %30249 = load i64, i64* %30215, align 8
  store i64 %30249, i64* %RAX.i11582.pre-phi, align 8
  %30250 = add i64 %30249, 668
  %30251 = add i64 %30247, 10
  store i64 %30251, i64* %3, align 8
  %30252 = inttoptr i64 %30250 to i32*
  %30253 = load i32, i32* %30252, align 4
  %30254 = add i32 %30253, -1
  %30255 = zext i32 %30254 to i64
  store i64 %30255, i64* %RCX.i11580, align 8
  %30256 = icmp eq i32 %30253, 0
  %30257 = zext i1 %30256 to i8
  store i8 %30257, i8* %14, align 1
  %30258 = and i32 %30254, 255
  %30259 = tail call i32 @llvm.ctpop.i32(i32 %30258)
  %30260 = trunc i32 %30259 to i8
  %30261 = and i8 %30260, 1
  %30262 = xor i8 %30261, 1
  store i8 %30262, i8* %21, align 1
  %30263 = xor i32 %30254, %30253
  %30264 = lshr i32 %30263, 4
  %30265 = trunc i32 %30264 to i8
  %30266 = and i8 %30265, 1
  store i8 %30266, i8* %27, align 1
  %30267 = icmp eq i32 %30254, 0
  %30268 = zext i1 %30267 to i8
  store i8 %30268, i8* %30, align 1
  %30269 = lshr i32 %30254, 31
  %30270 = trunc i32 %30269 to i8
  store i8 %30270, i8* %33, align 1
  %30271 = lshr i32 %30253, 31
  %30272 = xor i32 %30269, %30271
  %30273 = add nuw nsw i32 %30272, %30271
  %30274 = icmp eq i32 %30273, 2
  %30275 = zext i1 %30274 to i8
  store i8 %30275, i8* %39, align 1
  %30276 = add i64 %30247, 16
  store i64 %30276, i64* %3, align 8
  store i32 %30254, i32* %30210, align 4
  %.pre229 = load i64, i64* %RBP.i, align 8
  %.pre230 = load i64, i64* %3, align 8
  br label %block_.L_40d981

block_.L_40d981:                                  ; preds = %block_40d971, %block_.L_40d952
  %30277 = phi i64 [ %.pre230, %block_40d971 ], [ %30247, %block_.L_40d952 ]
  %30278 = phi i64 [ %.pre229, %block_40d971 ], [ %30206, %block_.L_40d952 ]
  %30279 = add i64 %30278, -8
  %30280 = add i64 %30277, 4
  store i64 %30280, i64* %3, align 8
  %30281 = inttoptr i64 %30279 to i64*
  %30282 = load i64, i64* %30281, align 8
  store i64 %30282, i64* %RAX.i11582.pre-phi, align 8
  %30283 = add i64 %30278, -68
  %30284 = add i64 %30277, 8
  store i64 %30284, i64* %3, align 8
  %30285 = inttoptr i64 %30283 to i32*
  %30286 = load i32, i32* %30285, align 4
  %30287 = sext i32 %30286 to i64
  store i64 %30287, i64* %RCX.i11580, align 8
  %30288 = add nsw i64 %30287, 1704
  %30289 = add i64 %30288, %30282
  %30290 = add i64 %30277, 16
  store i64 %30290, i64* %3, align 8
  %30291 = inttoptr i64 %30289 to i8*
  %30292 = load i8, i8* %30291, align 1
  %30293 = zext i8 %30292 to i64
  store i64 %30293, i64* %576, align 8
  %30294 = zext i8 %30292 to i32
  %30295 = add i64 %30278, -72
  %30296 = add i64 %30277, 19
  store i64 %30296, i64* %3, align 8
  %30297 = inttoptr i64 %30295 to i32*
  %30298 = load i32, i32* %30297, align 4
  %30299 = sub i32 %30294, %30298
  %30300 = icmp ult i32 %30294, %30298
  %30301 = zext i1 %30300 to i8
  store i8 %30301, i8* %14, align 1
  %30302 = and i32 %30299, 255
  %30303 = tail call i32 @llvm.ctpop.i32(i32 %30302)
  %30304 = trunc i32 %30303 to i8
  %30305 = and i8 %30304, 1
  %30306 = xor i8 %30305, 1
  store i8 %30306, i8* %21, align 1
  %30307 = xor i32 %30298, %30294
  %30308 = xor i32 %30307, %30299
  %30309 = lshr i32 %30308, 4
  %30310 = trunc i32 %30309 to i8
  %30311 = and i8 %30310, 1
  store i8 %30311, i8* %27, align 1
  %30312 = icmp eq i32 %30299, 0
  %30313 = zext i1 %30312 to i8
  store i8 %30313, i8* %30, align 1
  %30314 = lshr i32 %30299, 31
  %30315 = trunc i32 %30314 to i8
  store i8 %30315, i8* %33, align 1
  %30316 = lshr i32 %30298, 31
  %30317 = add nuw nsw i32 %30314, %30316
  %30318 = icmp eq i32 %30317, 2
  %30319 = zext i1 %30318 to i8
  store i8 %30319, i8* %39, align 1
  %30320 = icmp ne i8 %30315, 0
  %30321 = xor i1 %30320, %30318
  %.v325 = select i1 %30321, i64 35, i64 25
  %30322 = add i64 %30277, %.v325
  store i64 %30322, i64* %3, align 8
  br i1 %30321, label %block_.L_40d9a4, label %block_40d99a

block_40d99a:                                     ; preds = %block_.L_40d981
  store i64 3006, i64* %RDI.i2910, align 8
  %30323 = add i64 %30322, -35450
  %30324 = add i64 %30322, 10
  %30325 = load i64, i64* %6, align 8
  %30326 = add i64 %30325, -8
  %30327 = inttoptr i64 %30326 to i64*
  store i64 %30324, i64* %30327, align 8
  store i64 %30326, i64* %6, align 8
  store i64 %30323, i64* %3, align 8
  %call2_40d99f = tail call %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* nonnull %0, i64 %30323, %struct.Memory* %MEMORY.67)
  %.pre231 = load i64, i64* %RBP.i, align 8
  %.pre232 = load i64, i64* %3, align 8
  br label %block_.L_40d9a4

block_.L_40d9a4:                                  ; preds = %block_40d99a, %block_.L_40d981
  %30328 = phi i64 [ %.pre232, %block_40d99a ], [ %30322, %block_.L_40d981 ]
  %30329 = phi i64 [ %.pre231, %block_40d99a ], [ %30278, %block_.L_40d981 ]
  %30330 = add i64 %30329, -72
  %30331 = add i64 %30328, 4
  store i64 %30331, i64* %3, align 8
  %30332 = inttoptr i64 %30330 to i32*
  %30333 = load i32, i32* %30332, align 4
  %30334 = add i32 %30333, -6
  %30335 = icmp ult i32 %30333, 6
  %30336 = zext i1 %30335 to i8
  store i8 %30336, i8* %14, align 1
  %30337 = and i32 %30334, 255
  %30338 = tail call i32 @llvm.ctpop.i32(i32 %30337)
  %30339 = trunc i32 %30338 to i8
  %30340 = and i8 %30339, 1
  %30341 = xor i8 %30340, 1
  store i8 %30341, i8* %21, align 1
  %30342 = xor i32 %30334, %30333
  %30343 = lshr i32 %30342, 4
  %30344 = trunc i32 %30343 to i8
  %30345 = and i8 %30344, 1
  store i8 %30345, i8* %27, align 1
  %30346 = icmp eq i32 %30334, 0
  %30347 = zext i1 %30346 to i8
  store i8 %30347, i8* %30, align 1
  %30348 = lshr i32 %30334, 31
  %30349 = trunc i32 %30348 to i8
  store i8 %30349, i8* %33, align 1
  %30350 = lshr i32 %30333, 31
  %30351 = xor i32 %30348, %30350
  %30352 = add nuw nsw i32 %30351, %30350
  %30353 = icmp eq i32 %30352, 2
  %30354 = zext i1 %30353 to i8
  store i8 %30354, i8* %39, align 1
  %.v326 = select i1 %30346, i64 10, i64 3827
  %30355 = add i64 %30328, %.v326
  store i64 %30355, i64* %3, align 8
  br i1 %30346, label %block_40d9ae, label %block_.L_40e897

block_40d9ae:                                     ; preds = %block_.L_40d9a4
  store i64 50, i64* %RAX.i11582.pre-phi, align 8
  %30356 = add i64 %30329, -32
  %30357 = add i64 %30355, 8
  store i64 %30357, i64* %3, align 8
  %30358 = inttoptr i64 %30356 to i32*
  %30359 = load i32, i32* %30358, align 4
  %30360 = zext i32 %30359 to i64
  store i64 %30360, i64* %RCX.i11580, align 8
  %30361 = add i64 %30329, -28
  %30362 = add i64 %30355, 11
  store i64 %30362, i64* %3, align 8
  %30363 = inttoptr i64 %30361 to i32*
  %30364 = load i32, i32* %30363, align 4
  %30365 = sub i32 %30359, %30364
  %30366 = add i32 %30365, 1
  %30367 = zext i32 %30366 to i64
  store i64 %30367, i64* %RCX.i11580, align 8
  %30368 = lshr i32 %30366, 31
  %30369 = sub i32 49, %30365
  %30370 = icmp ugt i32 %30366, 50
  %30371 = zext i1 %30370 to i8
  store i8 %30371, i8* %14, align 1
  %30372 = and i32 %30369, 255
  %30373 = tail call i32 @llvm.ctpop.i32(i32 %30372)
  %30374 = trunc i32 %30373 to i8
  %30375 = and i8 %30374, 1
  %30376 = xor i8 %30375, 1
  store i8 %30376, i8* %21, align 1
  %30377 = xor i32 %30366, 16
  %30378 = xor i32 %30377, %30369
  %30379 = lshr i32 %30378, 4
  %30380 = trunc i32 %30379 to i8
  %30381 = and i8 %30380, 1
  store i8 %30381, i8* %27, align 1
  %30382 = icmp eq i32 %30369, 0
  %30383 = zext i1 %30382 to i8
  store i8 %30383, i8* %30, align 1
  %30384 = lshr i32 %30369, 31
  %30385 = trunc i32 %30384 to i8
  store i8 %30385, i8* %33, align 1
  %30386 = add nuw nsw i32 %30384, %30368
  %30387 = icmp eq i32 %30386, 2
  %30388 = zext i1 %30387 to i8
  store i8 %30388, i8* %39, align 1
  %.v328 = select i1 %30382, i64 22, i64 3817
  %30389 = add i64 %30355, %.v328
  store i64 %30389, i64* %3, align 8
  %30390 = load i64, i64* %RBP.i, align 8
  br i1 %30382, label %block_40d9c4, label %block_.L_40e897

block_40d9c4:                                     ; preds = %block_40d9ae
  %30391 = add i64 %30390, -8
  %30392 = add i64 %30389, 4
  store i64 %30392, i64* %3, align 8
  %30393 = inttoptr i64 %30391 to i64*
  %30394 = load i64, i64* %30393, align 8
  %30395 = add i64 %30394, 37708
  store i64 %30395, i64* %RAX.i11582.pre-phi, align 8
  %30396 = icmp ugt i64 %30394, -37709
  %30397 = zext i1 %30396 to i8
  store i8 %30397, i8* %14, align 1
  %30398 = trunc i64 %30395 to i32
  %30399 = and i32 %30398, 255
  %30400 = tail call i32 @llvm.ctpop.i32(i32 %30399)
  %30401 = trunc i32 %30400 to i8
  %30402 = and i8 %30401, 1
  %30403 = xor i8 %30402, 1
  store i8 %30403, i8* %21, align 1
  %30404 = xor i64 %30395, %30394
  %30405 = lshr i64 %30404, 4
  %30406 = trunc i64 %30405 to i8
  %30407 = and i8 %30406, 1
  store i8 %30407, i8* %27, align 1
  %30408 = icmp eq i64 %30395, 0
  %30409 = zext i1 %30408 to i8
  store i8 %30409, i8* %30, align 1
  %30410 = lshr i64 %30395, 63
  %30411 = trunc i64 %30410 to i8
  store i8 %30411, i8* %33, align 1
  %30412 = lshr i64 %30394, 63
  %30413 = xor i64 %30410, %30412
  %30414 = add nuw nsw i64 %30413, %30410
  %30415 = icmp eq i64 %30414, 2
  %30416 = zext i1 %30415 to i8
  store i8 %30416, i8* %39, align 1
  %30417 = add i64 %30389, 14
  store i64 %30417, i64* %3, align 8
  %30418 = load i64, i64* %30393, align 8
  store i64 %30418, i64* %RCX.i11580, align 8
  %30419 = add i64 %30390, -68
  %30420 = add i64 %30389, 18
  store i64 %30420, i64* %3, align 8
  %30421 = inttoptr i64 %30419 to i32*
  %30422 = load i32, i32* %30421, align 4
  %30423 = sext i32 %30422 to i64
  store i64 %30423, i64* %576, align 8
  %30424 = add nsw i64 %30423, 1704
  %30425 = add i64 %30424, %30418
  %30426 = add i64 %30389, 26
  store i64 %30426, i64* %3, align 8
  %30427 = inttoptr i64 %30425 to i8*
  %30428 = load i8, i8* %30427, align 1
  %30429 = zext i8 %30428 to i64
  store i64 %30429, i64* %RSI.i11312, align 8
  %30430 = zext i8 %30428 to i64
  %30431 = mul nuw nsw i64 %30430, 258
  store i64 %30431, i64* %RCX.i11580, align 8
  %30432 = add i64 %30431, %30395
  store i64 %30432, i64* %RAX.i11582.pre-phi, align 8
  %30433 = icmp ult i64 %30432, %30395
  %30434 = icmp ult i64 %30432, %30431
  %30435 = or i1 %30433, %30434
  %30436 = zext i1 %30435 to i8
  store i8 %30436, i8* %14, align 1
  %30437 = trunc i64 %30432 to i32
  %30438 = and i32 %30437, 255
  %30439 = tail call i32 @llvm.ctpop.i32(i32 %30438)
  %30440 = trunc i32 %30439 to i8
  %30441 = and i8 %30440, 1
  %30442 = xor i8 %30441, 1
  store i8 %30442, i8* %21, align 1
  %30443 = xor i64 %30431, %30395
  %30444 = xor i64 %30443, %30432
  %30445 = lshr i64 %30444, 4
  %30446 = trunc i64 %30445 to i8
  %30447 = and i8 %30446, 1
  store i8 %30447, i8* %27, align 1
  %30448 = icmp eq i64 %30432, 0
  %30449 = zext i1 %30448 to i8
  store i8 %30449, i8* %30, align 1
  %30450 = lshr i64 %30432, 63
  %30451 = trunc i64 %30450 to i8
  store i8 %30451, i8* %33, align 1
  %30452 = xor i64 %30450, %30410
  %30453 = add nuw nsw i64 %30452, %30450
  %30454 = icmp eq i64 %30453, 2
  %30455 = zext i1 %30454 to i8
  store i8 %30455, i8* %39, align 1
  %30456 = load i64, i64* %RBP.i, align 8
  %30457 = add i64 %30456, -208
  %30458 = add i64 %30389, 45
  store i64 %30458, i64* %3, align 8
  %30459 = inttoptr i64 %30457 to i64*
  store i64 %30432, i64* %30459, align 8
  %30460 = load i64, i64* %RBP.i, align 8
  %30461 = add i64 %30460, -8
  %30462 = load i64, i64* %3, align 8
  %30463 = add i64 %30462, 4
  store i64 %30463, i64* %3, align 8
  %30464 = inttoptr i64 %30461 to i64*
  %30465 = load i64, i64* %30464, align 8
  %30466 = add i64 %30465, 39256
  store i64 %30466, i64* %RAX.i11582.pre-phi, align 8
  %30467 = icmp ugt i64 %30465, -39257
  %30468 = zext i1 %30467 to i8
  store i8 %30468, i8* %14, align 1
  %30469 = trunc i64 %30466 to i32
  %30470 = and i32 %30469, 255
  %30471 = tail call i32 @llvm.ctpop.i32(i32 %30470)
  %30472 = trunc i32 %30471 to i8
  %30473 = and i8 %30472, 1
  %30474 = xor i8 %30473, 1
  store i8 %30474, i8* %21, align 1
  %30475 = xor i64 %30465, 16
  %30476 = xor i64 %30475, %30466
  %30477 = lshr i64 %30476, 4
  %30478 = trunc i64 %30477 to i8
  %30479 = and i8 %30478, 1
  store i8 %30479, i8* %27, align 1
  %30480 = icmp eq i64 %30466, 0
  %30481 = zext i1 %30480 to i8
  store i8 %30481, i8* %30, align 1
  %30482 = lshr i64 %30466, 63
  %30483 = trunc i64 %30482 to i8
  store i8 %30483, i8* %33, align 1
  %30484 = lshr i64 %30465, 63
  %30485 = xor i64 %30482, %30484
  %30486 = add nuw nsw i64 %30485, %30482
  %30487 = icmp eq i64 %30486, 2
  %30488 = zext i1 %30487 to i8
  store i8 %30488, i8* %39, align 1
  %30489 = add i64 %30462, 14
  store i64 %30489, i64* %3, align 8
  %30490 = load i64, i64* %30464, align 8
  store i64 %30490, i64* %RCX.i11580, align 8
  %30491 = add i64 %30460, -68
  %30492 = add i64 %30462, 18
  store i64 %30492, i64* %3, align 8
  %30493 = inttoptr i64 %30491 to i32*
  %30494 = load i32, i32* %30493, align 4
  %30495 = sext i32 %30494 to i64
  store i64 %30495, i64* %576, align 8
  %30496 = add nsw i64 %30495, 1704
  %30497 = add i64 %30496, %30490
  %30498 = add i64 %30462, 26
  store i64 %30498, i64* %3, align 8
  %30499 = inttoptr i64 %30497 to i8*
  %30500 = load i8, i8* %30499, align 1
  %30501 = zext i8 %30500 to i64
  store i64 %30501, i64* %RSI.i11312, align 8
  %30502 = zext i8 %30500 to i64
  %30503 = mul nuw nsw i64 %30502, 1032
  store i64 %30503, i64* %RCX.i11580, align 8
  %30504 = add i64 %30503, %30466
  store i64 %30504, i64* %RAX.i11582.pre-phi, align 8
  %30505 = icmp ult i64 %30504, %30466
  %30506 = icmp ult i64 %30504, %30503
  %30507 = or i1 %30505, %30506
  %30508 = zext i1 %30507 to i8
  store i8 %30508, i8* %14, align 1
  %30509 = trunc i64 %30504 to i32
  %30510 = and i32 %30509, 255
  %30511 = tail call i32 @llvm.ctpop.i32(i32 %30510)
  %30512 = trunc i32 %30511 to i8
  %30513 = and i8 %30512, 1
  %30514 = xor i8 %30513, 1
  store i8 %30514, i8* %21, align 1
  %30515 = xor i64 %30503, %30466
  %30516 = xor i64 %30515, %30504
  %30517 = lshr i64 %30516, 4
  %30518 = trunc i64 %30517 to i8
  %30519 = and i8 %30518, 1
  store i8 %30519, i8* %27, align 1
  %30520 = icmp eq i64 %30504, 0
  %30521 = zext i1 %30520 to i8
  store i8 %30521, i8* %30, align 1
  %30522 = lshr i64 %30504, 63
  %30523 = trunc i64 %30522 to i8
  store i8 %30523, i8* %33, align 1
  %30524 = xor i64 %30522, %30482
  %30525 = add nuw nsw i64 %30524, %30522
  %30526 = icmp eq i64 %30525, 2
  %30527 = zext i1 %30526 to i8
  store i8 %30527, i8* %39, align 1
  %30528 = load i64, i64* %RBP.i, align 8
  %30529 = add i64 %30528, -216
  %30530 = add i64 %30462, 45
  store i64 %30530, i64* %3, align 8
  %30531 = inttoptr i64 %30529 to i64*
  store i64 %30504, i64* %30531, align 8
  %30532 = load i64, i64* %RBP.i, align 8
  %30533 = add i64 %30532, -120
  %30534 = load i64, i64* %3, align 8
  %30535 = add i64 %30534, 4
  store i64 %30535, i64* %3, align 8
  %30536 = inttoptr i64 %30533 to i64*
  %30537 = load i64, i64* %30536, align 8
  store i64 %30537, i64* %RAX.i11582.pre-phi, align 8
  %30538 = add i64 %30532, -28
  %30539 = add i64 %30534, 7
  store i64 %30539, i64* %3, align 8
  %30540 = inttoptr i64 %30538 to i32*
  %30541 = load i32, i32* %30540, align 4
  %30542 = zext i32 %30541 to i64
  store i64 %30542, i64* %RSI.i11312, align 8
  store i8 0, i8* %14, align 1
  %30543 = and i32 %30541, 255
  %30544 = tail call i32 @llvm.ctpop.i32(i32 %30543)
  %30545 = trunc i32 %30544 to i8
  %30546 = and i8 %30545, 1
  %30547 = xor i8 %30546, 1
  store i8 %30547, i8* %21, align 1
  store i8 0, i8* %27, align 1
  %30548 = icmp eq i32 %30541, 0
  %30549 = zext i1 %30548 to i8
  store i8 %30549, i8* %30, align 1
  %30550 = lshr i32 %30541, 31
  %30551 = trunc i32 %30550 to i8
  store i8 %30551, i8* %33, align 1
  store i8 0, i8* %39, align 1
  %30552 = sext i32 %30541 to i64
  store i64 %30552, i64* %RCX.i11580, align 8
  %30553 = shl nsw i64 %30552, 1
  %30554 = add i64 %30537, %30553
  %30555 = add i64 %30534, 17
  store i64 %30555, i64* %3, align 8
  %30556 = inttoptr i64 %30554 to i16*
  %30557 = load i16, i16* %30556, align 2
  store i16 %30557, i16* %DI.i6029, align 2
  %30558 = add i64 %30532, -198
  %30559 = add i64 %30534, 24
  store i64 %30559, i64* %3, align 8
  %30560 = inttoptr i64 %30558 to i16*
  store i16 %30557, i16* %30560, align 2
  %30561 = load i64, i64* %RBP.i, align 8
  %30562 = add i64 %30561, -8
  %30563 = load i64, i64* %3, align 8
  %30564 = add i64 %30563, 4
  store i64 %30564, i64* %3, align 8
  %30565 = inttoptr i64 %30562 to i64*
  %30566 = load i64, i64* %30565, align 8
  store i64 %30566, i64* %RDI.i2910, align 8
  %30567 = add i64 %30561, -208
  %30568 = add i64 %30563, 11
  store i64 %30568, i64* %3, align 8
  %30569 = inttoptr i64 %30567 to i64*
  %30570 = load i64, i64* %30569, align 8
  store i64 %30570, i64* %RAX.i11582.pre-phi, align 8
  %30571 = add i64 %30561, -198
  %30572 = add i64 %30563, 18
  store i64 %30572, i64* %3, align 8
  %30573 = inttoptr i64 %30571 to i16*
  %30574 = load i16, i16* %30573, align 2
  %30575 = zext i16 %30574 to i64
  store i64 %30575, i64* %RSI.i11312, align 8
  %30576 = zext i16 %30574 to i64
  store i64 %30576, i64* %RCX.i11580, align 8
  %30577 = add i64 %30570, %30576
  %30578 = add i64 %30563, 24
  store i64 %30578, i64* %3, align 8
  %30579 = inttoptr i64 %30577 to i8*
  %30580 = load i8, i8* %30579, align 1
  %30581 = zext i8 %30580 to i64
  store i64 %30581, i64* %RSI.i11312, align 8
  %30582 = add i64 %30561, -216
  %30583 = add i64 %30563, 31
  store i64 %30583, i64* %3, align 8
  %30584 = inttoptr i64 %30582 to i64*
  %30585 = load i64, i64* %30584, align 8
  store i64 %30585, i64* %RAX.i11582.pre-phi, align 8
  %30586 = add i64 %30563, 39
  store i64 %30586, i64* %3, align 8
  %30587 = load i16, i16* %30573, align 2
  %30588 = zext i16 %30587 to i64
  store i64 %30588, i64* %1608, align 8
  %30589 = zext i16 %30587 to i64
  store i64 %30589, i64* %RCX.i11580, align 8
  %30590 = shl nuw nsw i64 %30589, 2
  %30591 = add i64 %30585, %30590
  %30592 = add i64 %30563, 45
  store i64 %30592, i64* %3, align 8
  %30593 = inttoptr i64 %30591 to i32*
  %30594 = load i32, i32* %30593, align 4
  %30595 = zext i32 %30594 to i64
  store i64 %30595, i64* %576, align 8
  %30596 = add i64 %30563, -15766
  %30597 = add i64 %30563, 50
  %30598 = load i64, i64* %6, align 8
  %30599 = add i64 %30598, -8
  %30600 = inttoptr i64 %30599 to i64*
  store i64 %30597, i64* %30600, align 8
  store i64 %30599, i64* %6, align 8
  store i64 %30596, i64* %3, align 8
  %call2_40da63 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %30596, %struct.Memory* %MEMORY.67)
  %30601 = load i64, i64* %RBP.i, align 8
  %30602 = add i64 %30601, -120
  %30603 = load i64, i64* %3, align 8
  %30604 = add i64 %30603, 4
  store i64 %30604, i64* %3, align 8
  %30605 = inttoptr i64 %30602 to i64*
  %30606 = load i64, i64* %30605, align 8
  store i64 %30606, i64* %RAX.i11582.pre-phi, align 8
  %30607 = add i64 %30601, -28
  %30608 = add i64 %30603, 7
  store i64 %30608, i64* %3, align 8
  %30609 = inttoptr i64 %30607 to i32*
  %30610 = load i32, i32* %30609, align 4
  %30611 = add i32 %30610, 1
  %30612 = zext i32 %30611 to i64
  store i64 %30612, i64* %576, align 8
  %30613 = icmp eq i32 %30610, -1
  %30614 = icmp eq i32 %30611, 0
  %30615 = or i1 %30613, %30614
  %30616 = zext i1 %30615 to i8
  store i8 %30616, i8* %14, align 1
  %30617 = and i32 %30611, 255
  %30618 = tail call i32 @llvm.ctpop.i32(i32 %30617)
  %30619 = trunc i32 %30618 to i8
  %30620 = and i8 %30619, 1
  %30621 = xor i8 %30620, 1
  store i8 %30621, i8* %21, align 1
  %30622 = xor i32 %30611, %30610
  %30623 = lshr i32 %30622, 4
  %30624 = trunc i32 %30623 to i8
  %30625 = and i8 %30624, 1
  store i8 %30625, i8* %27, align 1
  %30626 = zext i1 %30614 to i8
  store i8 %30626, i8* %30, align 1
  %30627 = lshr i32 %30611, 31
  %30628 = trunc i32 %30627 to i8
  store i8 %30628, i8* %33, align 1
  %30629 = lshr i32 %30610, 31
  %30630 = xor i32 %30627, %30629
  %30631 = add nuw nsw i32 %30630, %30627
  %30632 = icmp eq i32 %30631, 2
  %30633 = zext i1 %30632 to i8
  store i8 %30633, i8* %39, align 1
  %30634 = sext i32 %30611 to i64
  store i64 %30634, i64* %RCX.i11580, align 8
  %30635 = shl nsw i64 %30634, 1
  %30636 = add i64 %30606, %30635
  %30637 = add i64 %30603, 18
  store i64 %30637, i64* %3, align 8
  %30638 = inttoptr i64 %30636 to i16*
  %30639 = load i16, i16* %30638, align 2
  store i16 %30639, i16* %R9W.i2497, align 2
  %30640 = add i64 %30601, -198
  %30641 = add i64 %30603, 26
  store i64 %30641, i64* %3, align 8
  %30642 = inttoptr i64 %30640 to i16*
  store i16 %30639, i16* %30642, align 2
  %30643 = load i64, i64* %RBP.i, align 8
  %30644 = add i64 %30643, -8
  %30645 = load i64, i64* %3, align 8
  %30646 = add i64 %30645, 4
  store i64 %30646, i64* %3, align 8
  %30647 = inttoptr i64 %30644 to i64*
  %30648 = load i64, i64* %30647, align 8
  store i64 %30648, i64* %RDI.i2910, align 8
  %30649 = add i64 %30643, -208
  %30650 = add i64 %30645, 11
  store i64 %30650, i64* %3, align 8
  %30651 = inttoptr i64 %30649 to i64*
  %30652 = load i64, i64* %30651, align 8
  store i64 %30652, i64* %RAX.i11582.pre-phi, align 8
  %30653 = add i64 %30643, -198
  %30654 = add i64 %30645, 18
  store i64 %30654, i64* %3, align 8
  %30655 = inttoptr i64 %30653 to i16*
  %30656 = load i16, i16* %30655, align 2
  %30657 = zext i16 %30656 to i64
  store i64 %30657, i64* %576, align 8
  %30658 = zext i16 %30656 to i64
  store i64 %30658, i64* %RCX.i11580, align 8
  %30659 = add i64 %30652, %30658
  %30660 = add i64 %30645, 24
  store i64 %30660, i64* %3, align 8
  %30661 = inttoptr i64 %30659 to i8*
  %30662 = load i8, i8* %30661, align 1
  %30663 = zext i8 %30662 to i64
  store i64 %30663, i64* %RSI.i11312, align 8
  %30664 = add i64 %30643, -216
  %30665 = add i64 %30645, 31
  store i64 %30665, i64* %3, align 8
  %30666 = inttoptr i64 %30664 to i64*
  %30667 = load i64, i64* %30666, align 8
  store i64 %30667, i64* %RAX.i11582.pre-phi, align 8
  %30668 = add i64 %30645, 38
  store i64 %30668, i64* %3, align 8
  %30669 = load i16, i16* %30655, align 2
  %30670 = zext i16 %30669 to i64
  store i64 %30670, i64* %576, align 8
  %30671 = zext i16 %30669 to i64
  store i64 %30671, i64* %RCX.i11580, align 8
  %30672 = shl nuw nsw i64 %30671, 2
  %30673 = add i64 %30667, %30672
  %30674 = add i64 %30645, 43
  store i64 %30674, i64* %3, align 8
  %30675 = inttoptr i64 %30673 to i32*
  %30676 = load i32, i32* %30675, align 4
  %30677 = zext i32 %30676 to i64
  store i64 %30677, i64* %576, align 8
  %30678 = add i64 %30645, -15842
  %30679 = add i64 %30645, 48
  %30680 = load i64, i64* %6, align 8
  %30681 = add i64 %30680, -8
  %30682 = inttoptr i64 %30681 to i64*
  store i64 %30679, i64* %30682, align 8
  store i64 %30681, i64* %6, align 8
  store i64 %30678, i64* %3, align 8
  %call2_40daad = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %30678, %struct.Memory* %call2_40da63)
  %30683 = load i64, i64* %RBP.i, align 8
  %30684 = add i64 %30683, -120
  %30685 = load i64, i64* %3, align 8
  %30686 = add i64 %30685, 4
  store i64 %30686, i64* %3, align 8
  %30687 = inttoptr i64 %30684 to i64*
  %30688 = load i64, i64* %30687, align 8
  store i64 %30688, i64* %RAX.i11582.pre-phi, align 8
  %30689 = add i64 %30683, -28
  %30690 = add i64 %30685, 7
  store i64 %30690, i64* %3, align 8
  %30691 = inttoptr i64 %30689 to i32*
  %30692 = load i32, i32* %30691, align 4
  %30693 = add i32 %30692, 2
  %30694 = zext i32 %30693 to i64
  store i64 %30694, i64* %576, align 8
  %30695 = icmp ugt i32 %30692, -3
  %30696 = zext i1 %30695 to i8
  store i8 %30696, i8* %14, align 1
  %30697 = and i32 %30693, 255
  %30698 = tail call i32 @llvm.ctpop.i32(i32 %30697)
  %30699 = trunc i32 %30698 to i8
  %30700 = and i8 %30699, 1
  %30701 = xor i8 %30700, 1
  store i8 %30701, i8* %21, align 1
  %30702 = xor i32 %30693, %30692
  %30703 = lshr i32 %30702, 4
  %30704 = trunc i32 %30703 to i8
  %30705 = and i8 %30704, 1
  store i8 %30705, i8* %27, align 1
  %30706 = icmp eq i32 %30693, 0
  %30707 = zext i1 %30706 to i8
  store i8 %30707, i8* %30, align 1
  %30708 = lshr i32 %30693, 31
  %30709 = trunc i32 %30708 to i8
  store i8 %30709, i8* %33, align 1
  %30710 = lshr i32 %30692, 31
  %30711 = xor i32 %30708, %30710
  %30712 = add nuw nsw i32 %30711, %30708
  %30713 = icmp eq i32 %30712, 2
  %30714 = zext i1 %30713 to i8
  store i8 %30714, i8* %39, align 1
  %30715 = sext i32 %30693 to i64
  store i64 %30715, i64* %RCX.i11580, align 8
  %30716 = shl nsw i64 %30715, 1
  %30717 = add i64 %30688, %30716
  %30718 = add i64 %30685, 18
  store i64 %30718, i64* %3, align 8
  %30719 = inttoptr i64 %30717 to i16*
  %30720 = load i16, i16* %30719, align 2
  store i16 %30720, i16* %R9W.i2497, align 2
  %30721 = add i64 %30683, -198
  %30722 = add i64 %30685, 26
  store i64 %30722, i64* %3, align 8
  %30723 = inttoptr i64 %30721 to i16*
  store i16 %30720, i16* %30723, align 2
  %30724 = load i64, i64* %RBP.i, align 8
  %30725 = add i64 %30724, -8
  %30726 = load i64, i64* %3, align 8
  %30727 = add i64 %30726, 4
  store i64 %30727, i64* %3, align 8
  %30728 = inttoptr i64 %30725 to i64*
  %30729 = load i64, i64* %30728, align 8
  store i64 %30729, i64* %RDI.i2910, align 8
  %30730 = add i64 %30724, -208
  %30731 = add i64 %30726, 11
  store i64 %30731, i64* %3, align 8
  %30732 = inttoptr i64 %30730 to i64*
  %30733 = load i64, i64* %30732, align 8
  store i64 %30733, i64* %RAX.i11582.pre-phi, align 8
  %30734 = add i64 %30724, -198
  %30735 = add i64 %30726, 18
  store i64 %30735, i64* %3, align 8
  %30736 = inttoptr i64 %30734 to i16*
  %30737 = load i16, i16* %30736, align 2
  %30738 = zext i16 %30737 to i64
  store i64 %30738, i64* %576, align 8
  %30739 = zext i16 %30737 to i64
  store i64 %30739, i64* %RCX.i11580, align 8
  %30740 = add i64 %30733, %30739
  %30741 = add i64 %30726, 24
  store i64 %30741, i64* %3, align 8
  %30742 = inttoptr i64 %30740 to i8*
  %30743 = load i8, i8* %30742, align 1
  %30744 = zext i8 %30743 to i64
  store i64 %30744, i64* %RSI.i11312, align 8
  %30745 = add i64 %30724, -216
  %30746 = add i64 %30726, 31
  store i64 %30746, i64* %3, align 8
  %30747 = inttoptr i64 %30745 to i64*
  %30748 = load i64, i64* %30747, align 8
  store i64 %30748, i64* %RAX.i11582.pre-phi, align 8
  %30749 = add i64 %30726, 38
  store i64 %30749, i64* %3, align 8
  %30750 = load i16, i16* %30736, align 2
  %30751 = zext i16 %30750 to i64
  store i64 %30751, i64* %576, align 8
  %30752 = zext i16 %30750 to i64
  store i64 %30752, i64* %RCX.i11580, align 8
  %30753 = shl nuw nsw i64 %30752, 2
  %30754 = add i64 %30748, %30753
  %30755 = add i64 %30726, 43
  store i64 %30755, i64* %3, align 8
  %30756 = inttoptr i64 %30754 to i32*
  %30757 = load i32, i32* %30756, align 4
  %30758 = zext i32 %30757 to i64
  store i64 %30758, i64* %576, align 8
  %30759 = add i64 %30726, -15916
  %30760 = add i64 %30726, 48
  %30761 = load i64, i64* %6, align 8
  %30762 = add i64 %30761, -8
  %30763 = inttoptr i64 %30762 to i64*
  store i64 %30760, i64* %30763, align 8
  store i64 %30762, i64* %6, align 8
  store i64 %30759, i64* %3, align 8
  %call2_40daf7 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %30759, %struct.Memory* %call2_40daad)
  %30764 = load i64, i64* %RBP.i, align 8
  %30765 = add i64 %30764, -120
  %30766 = load i64, i64* %3, align 8
  %30767 = add i64 %30766, 4
  store i64 %30767, i64* %3, align 8
  %30768 = inttoptr i64 %30765 to i64*
  %30769 = load i64, i64* %30768, align 8
  store i64 %30769, i64* %RAX.i11582.pre-phi, align 8
  %30770 = add i64 %30764, -28
  %30771 = add i64 %30766, 7
  store i64 %30771, i64* %3, align 8
  %30772 = inttoptr i64 %30770 to i32*
  %30773 = load i32, i32* %30772, align 4
  %30774 = add i32 %30773, 3
  %30775 = zext i32 %30774 to i64
  store i64 %30775, i64* %576, align 8
  %30776 = icmp ugt i32 %30773, -4
  %30777 = zext i1 %30776 to i8
  store i8 %30777, i8* %14, align 1
  %30778 = and i32 %30774, 255
  %30779 = tail call i32 @llvm.ctpop.i32(i32 %30778)
  %30780 = trunc i32 %30779 to i8
  %30781 = and i8 %30780, 1
  %30782 = xor i8 %30781, 1
  store i8 %30782, i8* %21, align 1
  %30783 = xor i32 %30774, %30773
  %30784 = lshr i32 %30783, 4
  %30785 = trunc i32 %30784 to i8
  %30786 = and i8 %30785, 1
  store i8 %30786, i8* %27, align 1
  %30787 = icmp eq i32 %30774, 0
  %30788 = zext i1 %30787 to i8
  store i8 %30788, i8* %30, align 1
  %30789 = lshr i32 %30774, 31
  %30790 = trunc i32 %30789 to i8
  store i8 %30790, i8* %33, align 1
  %30791 = lshr i32 %30773, 31
  %30792 = xor i32 %30789, %30791
  %30793 = add nuw nsw i32 %30792, %30789
  %30794 = icmp eq i32 %30793, 2
  %30795 = zext i1 %30794 to i8
  store i8 %30795, i8* %39, align 1
  %30796 = sext i32 %30774 to i64
  store i64 %30796, i64* %RCX.i11580, align 8
  %30797 = shl nsw i64 %30796, 1
  %30798 = add i64 %30769, %30797
  %30799 = add i64 %30766, 18
  store i64 %30799, i64* %3, align 8
  %30800 = inttoptr i64 %30798 to i16*
  %30801 = load i16, i16* %30800, align 2
  store i16 %30801, i16* %R9W.i2497, align 2
  %30802 = add i64 %30764, -198
  %30803 = add i64 %30766, 26
  store i64 %30803, i64* %3, align 8
  %30804 = inttoptr i64 %30802 to i16*
  store i16 %30801, i16* %30804, align 2
  %30805 = load i64, i64* %RBP.i, align 8
  %30806 = add i64 %30805, -8
  %30807 = load i64, i64* %3, align 8
  %30808 = add i64 %30807, 4
  store i64 %30808, i64* %3, align 8
  %30809 = inttoptr i64 %30806 to i64*
  %30810 = load i64, i64* %30809, align 8
  store i64 %30810, i64* %RDI.i2910, align 8
  %30811 = add i64 %30805, -208
  %30812 = add i64 %30807, 11
  store i64 %30812, i64* %3, align 8
  %30813 = inttoptr i64 %30811 to i64*
  %30814 = load i64, i64* %30813, align 8
  store i64 %30814, i64* %RAX.i11582.pre-phi, align 8
  %30815 = add i64 %30805, -198
  %30816 = add i64 %30807, 18
  store i64 %30816, i64* %3, align 8
  %30817 = inttoptr i64 %30815 to i16*
  %30818 = load i16, i16* %30817, align 2
  %30819 = zext i16 %30818 to i64
  store i64 %30819, i64* %576, align 8
  %30820 = zext i16 %30818 to i64
  store i64 %30820, i64* %RCX.i11580, align 8
  %30821 = add i64 %30814, %30820
  %30822 = add i64 %30807, 24
  store i64 %30822, i64* %3, align 8
  %30823 = inttoptr i64 %30821 to i8*
  %30824 = load i8, i8* %30823, align 1
  %30825 = zext i8 %30824 to i64
  store i64 %30825, i64* %RSI.i11312, align 8
  %30826 = add i64 %30805, -216
  %30827 = add i64 %30807, 31
  store i64 %30827, i64* %3, align 8
  %30828 = inttoptr i64 %30826 to i64*
  %30829 = load i64, i64* %30828, align 8
  store i64 %30829, i64* %RAX.i11582.pre-phi, align 8
  %30830 = add i64 %30807, 38
  store i64 %30830, i64* %3, align 8
  %30831 = load i16, i16* %30817, align 2
  %30832 = zext i16 %30831 to i64
  store i64 %30832, i64* %576, align 8
  %30833 = zext i16 %30831 to i64
  store i64 %30833, i64* %RCX.i11580, align 8
  %30834 = shl nuw nsw i64 %30833, 2
  %30835 = add i64 %30829, %30834
  %30836 = add i64 %30807, 43
  store i64 %30836, i64* %3, align 8
  %30837 = inttoptr i64 %30835 to i32*
  %30838 = load i32, i32* %30837, align 4
  %30839 = zext i32 %30838 to i64
  store i64 %30839, i64* %576, align 8
  %30840 = add i64 %30807, -15990
  %30841 = add i64 %30807, 48
  %30842 = load i64, i64* %6, align 8
  %30843 = add i64 %30842, -8
  %30844 = inttoptr i64 %30843 to i64*
  store i64 %30841, i64* %30844, align 8
  store i64 %30843, i64* %6, align 8
  store i64 %30840, i64* %3, align 8
  %call2_40db41 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %30840, %struct.Memory* %call2_40daf7)
  %30845 = load i64, i64* %RBP.i, align 8
  %30846 = add i64 %30845, -120
  %30847 = load i64, i64* %3, align 8
  %30848 = add i64 %30847, 4
  store i64 %30848, i64* %3, align 8
  %30849 = inttoptr i64 %30846 to i64*
  %30850 = load i64, i64* %30849, align 8
  store i64 %30850, i64* %RAX.i11582.pre-phi, align 8
  %30851 = add i64 %30845, -28
  %30852 = add i64 %30847, 7
  store i64 %30852, i64* %3, align 8
  %30853 = inttoptr i64 %30851 to i32*
  %30854 = load i32, i32* %30853, align 4
  %30855 = add i32 %30854, 4
  %30856 = zext i32 %30855 to i64
  store i64 %30856, i64* %576, align 8
  %30857 = icmp ugt i32 %30854, -5
  %30858 = zext i1 %30857 to i8
  store i8 %30858, i8* %14, align 1
  %30859 = and i32 %30855, 255
  %30860 = tail call i32 @llvm.ctpop.i32(i32 %30859)
  %30861 = trunc i32 %30860 to i8
  %30862 = and i8 %30861, 1
  %30863 = xor i8 %30862, 1
  store i8 %30863, i8* %21, align 1
  %30864 = xor i32 %30855, %30854
  %30865 = lshr i32 %30864, 4
  %30866 = trunc i32 %30865 to i8
  %30867 = and i8 %30866, 1
  store i8 %30867, i8* %27, align 1
  %30868 = icmp eq i32 %30855, 0
  %30869 = zext i1 %30868 to i8
  store i8 %30869, i8* %30, align 1
  %30870 = lshr i32 %30855, 31
  %30871 = trunc i32 %30870 to i8
  store i8 %30871, i8* %33, align 1
  %30872 = lshr i32 %30854, 31
  %30873 = xor i32 %30870, %30872
  %30874 = add nuw nsw i32 %30873, %30870
  %30875 = icmp eq i32 %30874, 2
  %30876 = zext i1 %30875 to i8
  store i8 %30876, i8* %39, align 1
  %30877 = sext i32 %30855 to i64
  store i64 %30877, i64* %RCX.i11580, align 8
  %30878 = shl nsw i64 %30877, 1
  %30879 = add i64 %30850, %30878
  %30880 = add i64 %30847, 18
  store i64 %30880, i64* %3, align 8
  %30881 = inttoptr i64 %30879 to i16*
  %30882 = load i16, i16* %30881, align 2
  store i16 %30882, i16* %R9W.i2497, align 2
  %30883 = add i64 %30845, -198
  %30884 = add i64 %30847, 26
  store i64 %30884, i64* %3, align 8
  %30885 = inttoptr i64 %30883 to i16*
  store i16 %30882, i16* %30885, align 2
  %30886 = load i64, i64* %RBP.i, align 8
  %30887 = add i64 %30886, -8
  %30888 = load i64, i64* %3, align 8
  %30889 = add i64 %30888, 4
  store i64 %30889, i64* %3, align 8
  %30890 = inttoptr i64 %30887 to i64*
  %30891 = load i64, i64* %30890, align 8
  store i64 %30891, i64* %RDI.i2910, align 8
  %30892 = add i64 %30886, -208
  %30893 = add i64 %30888, 11
  store i64 %30893, i64* %3, align 8
  %30894 = inttoptr i64 %30892 to i64*
  %30895 = load i64, i64* %30894, align 8
  store i64 %30895, i64* %RAX.i11582.pre-phi, align 8
  %30896 = add i64 %30886, -198
  %30897 = add i64 %30888, 18
  store i64 %30897, i64* %3, align 8
  %30898 = inttoptr i64 %30896 to i16*
  %30899 = load i16, i16* %30898, align 2
  %30900 = zext i16 %30899 to i64
  store i64 %30900, i64* %576, align 8
  %30901 = zext i16 %30899 to i64
  store i64 %30901, i64* %RCX.i11580, align 8
  %30902 = add i64 %30895, %30901
  %30903 = add i64 %30888, 24
  store i64 %30903, i64* %3, align 8
  %30904 = inttoptr i64 %30902 to i8*
  %30905 = load i8, i8* %30904, align 1
  %30906 = zext i8 %30905 to i64
  store i64 %30906, i64* %RSI.i11312, align 8
  %30907 = add i64 %30886, -216
  %30908 = add i64 %30888, 31
  store i64 %30908, i64* %3, align 8
  %30909 = inttoptr i64 %30907 to i64*
  %30910 = load i64, i64* %30909, align 8
  store i64 %30910, i64* %RAX.i11582.pre-phi, align 8
  %30911 = add i64 %30888, 38
  store i64 %30911, i64* %3, align 8
  %30912 = load i16, i16* %30898, align 2
  %30913 = zext i16 %30912 to i64
  store i64 %30913, i64* %576, align 8
  %30914 = zext i16 %30912 to i64
  store i64 %30914, i64* %RCX.i11580, align 8
  %30915 = shl nuw nsw i64 %30914, 2
  %30916 = add i64 %30910, %30915
  %30917 = add i64 %30888, 43
  store i64 %30917, i64* %3, align 8
  %30918 = inttoptr i64 %30916 to i32*
  %30919 = load i32, i32* %30918, align 4
  %30920 = zext i32 %30919 to i64
  store i64 %30920, i64* %576, align 8
  %30921 = add i64 %30888, -16064
  %30922 = add i64 %30888, 48
  %30923 = load i64, i64* %6, align 8
  %30924 = add i64 %30923, -8
  %30925 = inttoptr i64 %30924 to i64*
  store i64 %30922, i64* %30925, align 8
  store i64 %30924, i64* %6, align 8
  store i64 %30921, i64* %3, align 8
  %call2_40db8b = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %30921, %struct.Memory* %call2_40db41)
  %30926 = load i64, i64* %RBP.i, align 8
  %30927 = add i64 %30926, -120
  %30928 = load i64, i64* %3, align 8
  %30929 = add i64 %30928, 4
  store i64 %30929, i64* %3, align 8
  %30930 = inttoptr i64 %30927 to i64*
  %30931 = load i64, i64* %30930, align 8
  store i64 %30931, i64* %RAX.i11582.pre-phi, align 8
  %30932 = add i64 %30926, -28
  %30933 = add i64 %30928, 7
  store i64 %30933, i64* %3, align 8
  %30934 = inttoptr i64 %30932 to i32*
  %30935 = load i32, i32* %30934, align 4
  %30936 = add i32 %30935, 5
  %30937 = zext i32 %30936 to i64
  store i64 %30937, i64* %576, align 8
  %30938 = icmp ugt i32 %30935, -6
  %30939 = zext i1 %30938 to i8
  store i8 %30939, i8* %14, align 1
  %30940 = and i32 %30936, 255
  %30941 = tail call i32 @llvm.ctpop.i32(i32 %30940)
  %30942 = trunc i32 %30941 to i8
  %30943 = and i8 %30942, 1
  %30944 = xor i8 %30943, 1
  store i8 %30944, i8* %21, align 1
  %30945 = xor i32 %30936, %30935
  %30946 = lshr i32 %30945, 4
  %30947 = trunc i32 %30946 to i8
  %30948 = and i8 %30947, 1
  store i8 %30948, i8* %27, align 1
  %30949 = icmp eq i32 %30936, 0
  %30950 = zext i1 %30949 to i8
  store i8 %30950, i8* %30, align 1
  %30951 = lshr i32 %30936, 31
  %30952 = trunc i32 %30951 to i8
  store i8 %30952, i8* %33, align 1
  %30953 = lshr i32 %30935, 31
  %30954 = xor i32 %30951, %30953
  %30955 = add nuw nsw i32 %30954, %30951
  %30956 = icmp eq i32 %30955, 2
  %30957 = zext i1 %30956 to i8
  store i8 %30957, i8* %39, align 1
  %30958 = sext i32 %30936 to i64
  store i64 %30958, i64* %RCX.i11580, align 8
  %30959 = shl nsw i64 %30958, 1
  %30960 = add i64 %30931, %30959
  %30961 = add i64 %30928, 18
  store i64 %30961, i64* %3, align 8
  %30962 = inttoptr i64 %30960 to i16*
  %30963 = load i16, i16* %30962, align 2
  store i16 %30963, i16* %R9W.i2497, align 2
  %30964 = add i64 %30926, -198
  %30965 = add i64 %30928, 26
  store i64 %30965, i64* %3, align 8
  %30966 = inttoptr i64 %30964 to i16*
  store i16 %30963, i16* %30966, align 2
  %30967 = load i64, i64* %RBP.i, align 8
  %30968 = add i64 %30967, -8
  %30969 = load i64, i64* %3, align 8
  %30970 = add i64 %30969, 4
  store i64 %30970, i64* %3, align 8
  %30971 = inttoptr i64 %30968 to i64*
  %30972 = load i64, i64* %30971, align 8
  store i64 %30972, i64* %RDI.i2910, align 8
  %30973 = add i64 %30967, -208
  %30974 = add i64 %30969, 11
  store i64 %30974, i64* %3, align 8
  %30975 = inttoptr i64 %30973 to i64*
  %30976 = load i64, i64* %30975, align 8
  store i64 %30976, i64* %RAX.i11582.pre-phi, align 8
  %30977 = add i64 %30967, -198
  %30978 = add i64 %30969, 18
  store i64 %30978, i64* %3, align 8
  %30979 = inttoptr i64 %30977 to i16*
  %30980 = load i16, i16* %30979, align 2
  %30981 = zext i16 %30980 to i64
  store i64 %30981, i64* %576, align 8
  %30982 = zext i16 %30980 to i64
  store i64 %30982, i64* %RCX.i11580, align 8
  %30983 = add i64 %30976, %30982
  %30984 = add i64 %30969, 24
  store i64 %30984, i64* %3, align 8
  %30985 = inttoptr i64 %30983 to i8*
  %30986 = load i8, i8* %30985, align 1
  %30987 = zext i8 %30986 to i64
  store i64 %30987, i64* %RSI.i11312, align 8
  %30988 = add i64 %30967, -216
  %30989 = add i64 %30969, 31
  store i64 %30989, i64* %3, align 8
  %30990 = inttoptr i64 %30988 to i64*
  %30991 = load i64, i64* %30990, align 8
  store i64 %30991, i64* %RAX.i11582.pre-phi, align 8
  %30992 = add i64 %30969, 38
  store i64 %30992, i64* %3, align 8
  %30993 = load i16, i16* %30979, align 2
  %30994 = zext i16 %30993 to i64
  store i64 %30994, i64* %576, align 8
  %30995 = zext i16 %30993 to i64
  store i64 %30995, i64* %RCX.i11580, align 8
  %30996 = shl nuw nsw i64 %30995, 2
  %30997 = add i64 %30991, %30996
  %30998 = add i64 %30969, 43
  store i64 %30998, i64* %3, align 8
  %30999 = inttoptr i64 %30997 to i32*
  %31000 = load i32, i32* %30999, align 4
  %31001 = zext i32 %31000 to i64
  store i64 %31001, i64* %576, align 8
  %31002 = add i64 %30969, -16138
  %31003 = add i64 %30969, 48
  %31004 = load i64, i64* %6, align 8
  %31005 = add i64 %31004, -8
  %31006 = inttoptr i64 %31005 to i64*
  store i64 %31003, i64* %31006, align 8
  store i64 %31005, i64* %6, align 8
  store i64 %31002, i64* %3, align 8
  %call2_40dbd5 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31002, %struct.Memory* %call2_40db8b)
  %31007 = load i64, i64* %RBP.i, align 8
  %31008 = add i64 %31007, -120
  %31009 = load i64, i64* %3, align 8
  %31010 = add i64 %31009, 4
  store i64 %31010, i64* %3, align 8
  %31011 = inttoptr i64 %31008 to i64*
  %31012 = load i64, i64* %31011, align 8
  store i64 %31012, i64* %RAX.i11582.pre-phi, align 8
  %31013 = add i64 %31007, -28
  %31014 = add i64 %31009, 7
  store i64 %31014, i64* %3, align 8
  %31015 = inttoptr i64 %31013 to i32*
  %31016 = load i32, i32* %31015, align 4
  %31017 = add i32 %31016, 6
  %31018 = zext i32 %31017 to i64
  store i64 %31018, i64* %576, align 8
  %31019 = icmp ugt i32 %31016, -7
  %31020 = zext i1 %31019 to i8
  store i8 %31020, i8* %14, align 1
  %31021 = and i32 %31017, 255
  %31022 = tail call i32 @llvm.ctpop.i32(i32 %31021)
  %31023 = trunc i32 %31022 to i8
  %31024 = and i8 %31023, 1
  %31025 = xor i8 %31024, 1
  store i8 %31025, i8* %21, align 1
  %31026 = xor i32 %31017, %31016
  %31027 = lshr i32 %31026, 4
  %31028 = trunc i32 %31027 to i8
  %31029 = and i8 %31028, 1
  store i8 %31029, i8* %27, align 1
  %31030 = icmp eq i32 %31017, 0
  %31031 = zext i1 %31030 to i8
  store i8 %31031, i8* %30, align 1
  %31032 = lshr i32 %31017, 31
  %31033 = trunc i32 %31032 to i8
  store i8 %31033, i8* %33, align 1
  %31034 = lshr i32 %31016, 31
  %31035 = xor i32 %31032, %31034
  %31036 = add nuw nsw i32 %31035, %31032
  %31037 = icmp eq i32 %31036, 2
  %31038 = zext i1 %31037 to i8
  store i8 %31038, i8* %39, align 1
  %31039 = sext i32 %31017 to i64
  store i64 %31039, i64* %RCX.i11580, align 8
  %31040 = shl nsw i64 %31039, 1
  %31041 = add i64 %31012, %31040
  %31042 = add i64 %31009, 18
  store i64 %31042, i64* %3, align 8
  %31043 = inttoptr i64 %31041 to i16*
  %31044 = load i16, i16* %31043, align 2
  store i16 %31044, i16* %R9W.i2497, align 2
  %31045 = add i64 %31007, -198
  %31046 = add i64 %31009, 26
  store i64 %31046, i64* %3, align 8
  %31047 = inttoptr i64 %31045 to i16*
  store i16 %31044, i16* %31047, align 2
  %31048 = load i64, i64* %RBP.i, align 8
  %31049 = add i64 %31048, -8
  %31050 = load i64, i64* %3, align 8
  %31051 = add i64 %31050, 4
  store i64 %31051, i64* %3, align 8
  %31052 = inttoptr i64 %31049 to i64*
  %31053 = load i64, i64* %31052, align 8
  store i64 %31053, i64* %RDI.i2910, align 8
  %31054 = add i64 %31048, -208
  %31055 = add i64 %31050, 11
  store i64 %31055, i64* %3, align 8
  %31056 = inttoptr i64 %31054 to i64*
  %31057 = load i64, i64* %31056, align 8
  store i64 %31057, i64* %RAX.i11582.pre-phi, align 8
  %31058 = add i64 %31048, -198
  %31059 = add i64 %31050, 18
  store i64 %31059, i64* %3, align 8
  %31060 = inttoptr i64 %31058 to i16*
  %31061 = load i16, i16* %31060, align 2
  %31062 = zext i16 %31061 to i64
  store i64 %31062, i64* %576, align 8
  %31063 = zext i16 %31061 to i64
  store i64 %31063, i64* %RCX.i11580, align 8
  %31064 = add i64 %31057, %31063
  %31065 = add i64 %31050, 24
  store i64 %31065, i64* %3, align 8
  %31066 = inttoptr i64 %31064 to i8*
  %31067 = load i8, i8* %31066, align 1
  %31068 = zext i8 %31067 to i64
  store i64 %31068, i64* %RSI.i11312, align 8
  %31069 = add i64 %31048, -216
  %31070 = add i64 %31050, 31
  store i64 %31070, i64* %3, align 8
  %31071 = inttoptr i64 %31069 to i64*
  %31072 = load i64, i64* %31071, align 8
  store i64 %31072, i64* %RAX.i11582.pre-phi, align 8
  %31073 = add i64 %31050, 38
  store i64 %31073, i64* %3, align 8
  %31074 = load i16, i16* %31060, align 2
  %31075 = zext i16 %31074 to i64
  store i64 %31075, i64* %576, align 8
  %31076 = zext i16 %31074 to i64
  store i64 %31076, i64* %RCX.i11580, align 8
  %31077 = shl nuw nsw i64 %31076, 2
  %31078 = add i64 %31072, %31077
  %31079 = add i64 %31050, 43
  store i64 %31079, i64* %3, align 8
  %31080 = inttoptr i64 %31078 to i32*
  %31081 = load i32, i32* %31080, align 4
  %31082 = zext i32 %31081 to i64
  store i64 %31082, i64* %576, align 8
  %31083 = add i64 %31050, -16212
  %31084 = add i64 %31050, 48
  %31085 = load i64, i64* %6, align 8
  %31086 = add i64 %31085, -8
  %31087 = inttoptr i64 %31086 to i64*
  store i64 %31084, i64* %31087, align 8
  store i64 %31086, i64* %6, align 8
  store i64 %31083, i64* %3, align 8
  %call2_40dc1f = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31083, %struct.Memory* %call2_40dbd5)
  %31088 = load i64, i64* %RBP.i, align 8
  %31089 = add i64 %31088, -120
  %31090 = load i64, i64* %3, align 8
  %31091 = add i64 %31090, 4
  store i64 %31091, i64* %3, align 8
  %31092 = inttoptr i64 %31089 to i64*
  %31093 = load i64, i64* %31092, align 8
  store i64 %31093, i64* %RAX.i11582.pre-phi, align 8
  %31094 = add i64 %31088, -28
  %31095 = add i64 %31090, 7
  store i64 %31095, i64* %3, align 8
  %31096 = inttoptr i64 %31094 to i32*
  %31097 = load i32, i32* %31096, align 4
  %31098 = add i32 %31097, 7
  %31099 = zext i32 %31098 to i64
  store i64 %31099, i64* %576, align 8
  %31100 = icmp ugt i32 %31097, -8
  %31101 = zext i1 %31100 to i8
  store i8 %31101, i8* %14, align 1
  %31102 = and i32 %31098, 255
  %31103 = tail call i32 @llvm.ctpop.i32(i32 %31102)
  %31104 = trunc i32 %31103 to i8
  %31105 = and i8 %31104, 1
  %31106 = xor i8 %31105, 1
  store i8 %31106, i8* %21, align 1
  %31107 = xor i32 %31098, %31097
  %31108 = lshr i32 %31107, 4
  %31109 = trunc i32 %31108 to i8
  %31110 = and i8 %31109, 1
  store i8 %31110, i8* %27, align 1
  %31111 = icmp eq i32 %31098, 0
  %31112 = zext i1 %31111 to i8
  store i8 %31112, i8* %30, align 1
  %31113 = lshr i32 %31098, 31
  %31114 = trunc i32 %31113 to i8
  store i8 %31114, i8* %33, align 1
  %31115 = lshr i32 %31097, 31
  %31116 = xor i32 %31113, %31115
  %31117 = add nuw nsw i32 %31116, %31113
  %31118 = icmp eq i32 %31117, 2
  %31119 = zext i1 %31118 to i8
  store i8 %31119, i8* %39, align 1
  %31120 = sext i32 %31098 to i64
  store i64 %31120, i64* %RCX.i11580, align 8
  %31121 = shl nsw i64 %31120, 1
  %31122 = add i64 %31093, %31121
  %31123 = add i64 %31090, 18
  store i64 %31123, i64* %3, align 8
  %31124 = inttoptr i64 %31122 to i16*
  %31125 = load i16, i16* %31124, align 2
  store i16 %31125, i16* %R9W.i2497, align 2
  %31126 = add i64 %31088, -198
  %31127 = add i64 %31090, 26
  store i64 %31127, i64* %3, align 8
  %31128 = inttoptr i64 %31126 to i16*
  store i16 %31125, i16* %31128, align 2
  %31129 = load i64, i64* %RBP.i, align 8
  %31130 = add i64 %31129, -8
  %31131 = load i64, i64* %3, align 8
  %31132 = add i64 %31131, 4
  store i64 %31132, i64* %3, align 8
  %31133 = inttoptr i64 %31130 to i64*
  %31134 = load i64, i64* %31133, align 8
  store i64 %31134, i64* %RDI.i2910, align 8
  %31135 = add i64 %31129, -208
  %31136 = add i64 %31131, 11
  store i64 %31136, i64* %3, align 8
  %31137 = inttoptr i64 %31135 to i64*
  %31138 = load i64, i64* %31137, align 8
  store i64 %31138, i64* %RAX.i11582.pre-phi, align 8
  %31139 = add i64 %31129, -198
  %31140 = add i64 %31131, 18
  store i64 %31140, i64* %3, align 8
  %31141 = inttoptr i64 %31139 to i16*
  %31142 = load i16, i16* %31141, align 2
  %31143 = zext i16 %31142 to i64
  store i64 %31143, i64* %576, align 8
  %31144 = zext i16 %31142 to i64
  store i64 %31144, i64* %RCX.i11580, align 8
  %31145 = add i64 %31138, %31144
  %31146 = add i64 %31131, 24
  store i64 %31146, i64* %3, align 8
  %31147 = inttoptr i64 %31145 to i8*
  %31148 = load i8, i8* %31147, align 1
  %31149 = zext i8 %31148 to i64
  store i64 %31149, i64* %RSI.i11312, align 8
  %31150 = add i64 %31129, -216
  %31151 = add i64 %31131, 31
  store i64 %31151, i64* %3, align 8
  %31152 = inttoptr i64 %31150 to i64*
  %31153 = load i64, i64* %31152, align 8
  store i64 %31153, i64* %RAX.i11582.pre-phi, align 8
  %31154 = add i64 %31131, 38
  store i64 %31154, i64* %3, align 8
  %31155 = load i16, i16* %31141, align 2
  %31156 = zext i16 %31155 to i64
  store i64 %31156, i64* %576, align 8
  %31157 = zext i16 %31155 to i64
  store i64 %31157, i64* %RCX.i11580, align 8
  %31158 = shl nuw nsw i64 %31157, 2
  %31159 = add i64 %31153, %31158
  %31160 = add i64 %31131, 43
  store i64 %31160, i64* %3, align 8
  %31161 = inttoptr i64 %31159 to i32*
  %31162 = load i32, i32* %31161, align 4
  %31163 = zext i32 %31162 to i64
  store i64 %31163, i64* %576, align 8
  %31164 = add i64 %31131, -16286
  %31165 = add i64 %31131, 48
  %31166 = load i64, i64* %6, align 8
  %31167 = add i64 %31166, -8
  %31168 = inttoptr i64 %31167 to i64*
  store i64 %31165, i64* %31168, align 8
  store i64 %31167, i64* %6, align 8
  store i64 %31164, i64* %3, align 8
  %call2_40dc69 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31164, %struct.Memory* %call2_40dc1f)
  %31169 = load i64, i64* %RBP.i, align 8
  %31170 = add i64 %31169, -120
  %31171 = load i64, i64* %3, align 8
  %31172 = add i64 %31171, 4
  store i64 %31172, i64* %3, align 8
  %31173 = inttoptr i64 %31170 to i64*
  %31174 = load i64, i64* %31173, align 8
  store i64 %31174, i64* %RAX.i11582.pre-phi, align 8
  %31175 = add i64 %31169, -28
  %31176 = add i64 %31171, 7
  store i64 %31176, i64* %3, align 8
  %31177 = inttoptr i64 %31175 to i32*
  %31178 = load i32, i32* %31177, align 4
  %31179 = add i32 %31178, 8
  %31180 = zext i32 %31179 to i64
  store i64 %31180, i64* %576, align 8
  %31181 = icmp ugt i32 %31178, -9
  %31182 = zext i1 %31181 to i8
  store i8 %31182, i8* %14, align 1
  %31183 = and i32 %31179, 255
  %31184 = tail call i32 @llvm.ctpop.i32(i32 %31183)
  %31185 = trunc i32 %31184 to i8
  %31186 = and i8 %31185, 1
  %31187 = xor i8 %31186, 1
  store i8 %31187, i8* %21, align 1
  %31188 = xor i32 %31179, %31178
  %31189 = lshr i32 %31188, 4
  %31190 = trunc i32 %31189 to i8
  %31191 = and i8 %31190, 1
  store i8 %31191, i8* %27, align 1
  %31192 = icmp eq i32 %31179, 0
  %31193 = zext i1 %31192 to i8
  store i8 %31193, i8* %30, align 1
  %31194 = lshr i32 %31179, 31
  %31195 = trunc i32 %31194 to i8
  store i8 %31195, i8* %33, align 1
  %31196 = lshr i32 %31178, 31
  %31197 = xor i32 %31194, %31196
  %31198 = add nuw nsw i32 %31197, %31194
  %31199 = icmp eq i32 %31198, 2
  %31200 = zext i1 %31199 to i8
  store i8 %31200, i8* %39, align 1
  %31201 = sext i32 %31179 to i64
  store i64 %31201, i64* %RCX.i11580, align 8
  %31202 = shl nsw i64 %31201, 1
  %31203 = add i64 %31174, %31202
  %31204 = add i64 %31171, 18
  store i64 %31204, i64* %3, align 8
  %31205 = inttoptr i64 %31203 to i16*
  %31206 = load i16, i16* %31205, align 2
  store i16 %31206, i16* %R9W.i2497, align 2
  %31207 = add i64 %31169, -198
  %31208 = add i64 %31171, 26
  store i64 %31208, i64* %3, align 8
  %31209 = inttoptr i64 %31207 to i16*
  store i16 %31206, i16* %31209, align 2
  %31210 = load i64, i64* %RBP.i, align 8
  %31211 = add i64 %31210, -8
  %31212 = load i64, i64* %3, align 8
  %31213 = add i64 %31212, 4
  store i64 %31213, i64* %3, align 8
  %31214 = inttoptr i64 %31211 to i64*
  %31215 = load i64, i64* %31214, align 8
  store i64 %31215, i64* %RDI.i2910, align 8
  %31216 = add i64 %31210, -208
  %31217 = add i64 %31212, 11
  store i64 %31217, i64* %3, align 8
  %31218 = inttoptr i64 %31216 to i64*
  %31219 = load i64, i64* %31218, align 8
  store i64 %31219, i64* %RAX.i11582.pre-phi, align 8
  %31220 = add i64 %31210, -198
  %31221 = add i64 %31212, 18
  store i64 %31221, i64* %3, align 8
  %31222 = inttoptr i64 %31220 to i16*
  %31223 = load i16, i16* %31222, align 2
  %31224 = zext i16 %31223 to i64
  store i64 %31224, i64* %576, align 8
  %31225 = zext i16 %31223 to i64
  store i64 %31225, i64* %RCX.i11580, align 8
  %31226 = add i64 %31219, %31225
  %31227 = add i64 %31212, 24
  store i64 %31227, i64* %3, align 8
  %31228 = inttoptr i64 %31226 to i8*
  %31229 = load i8, i8* %31228, align 1
  %31230 = zext i8 %31229 to i64
  store i64 %31230, i64* %RSI.i11312, align 8
  %31231 = add i64 %31210, -216
  %31232 = add i64 %31212, 31
  store i64 %31232, i64* %3, align 8
  %31233 = inttoptr i64 %31231 to i64*
  %31234 = load i64, i64* %31233, align 8
  store i64 %31234, i64* %RAX.i11582.pre-phi, align 8
  %31235 = add i64 %31212, 38
  store i64 %31235, i64* %3, align 8
  %31236 = load i16, i16* %31222, align 2
  %31237 = zext i16 %31236 to i64
  store i64 %31237, i64* %576, align 8
  %31238 = zext i16 %31236 to i64
  store i64 %31238, i64* %RCX.i11580, align 8
  %31239 = shl nuw nsw i64 %31238, 2
  %31240 = add i64 %31234, %31239
  %31241 = add i64 %31212, 43
  store i64 %31241, i64* %3, align 8
  %31242 = inttoptr i64 %31240 to i32*
  %31243 = load i32, i32* %31242, align 4
  %31244 = zext i32 %31243 to i64
  store i64 %31244, i64* %576, align 8
  %31245 = add i64 %31212, -16360
  %31246 = add i64 %31212, 48
  %31247 = load i64, i64* %6, align 8
  %31248 = add i64 %31247, -8
  %31249 = inttoptr i64 %31248 to i64*
  store i64 %31246, i64* %31249, align 8
  store i64 %31248, i64* %6, align 8
  store i64 %31245, i64* %3, align 8
  %call2_40dcb3 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31245, %struct.Memory* %call2_40dc69)
  %31250 = load i64, i64* %RBP.i, align 8
  %31251 = add i64 %31250, -120
  %31252 = load i64, i64* %3, align 8
  %31253 = add i64 %31252, 4
  store i64 %31253, i64* %3, align 8
  %31254 = inttoptr i64 %31251 to i64*
  %31255 = load i64, i64* %31254, align 8
  store i64 %31255, i64* %RAX.i11582.pre-phi, align 8
  %31256 = add i64 %31250, -28
  %31257 = add i64 %31252, 7
  store i64 %31257, i64* %3, align 8
  %31258 = inttoptr i64 %31256 to i32*
  %31259 = load i32, i32* %31258, align 4
  %31260 = add i32 %31259, 9
  %31261 = zext i32 %31260 to i64
  store i64 %31261, i64* %576, align 8
  %31262 = icmp ugt i32 %31259, -10
  %31263 = zext i1 %31262 to i8
  store i8 %31263, i8* %14, align 1
  %31264 = and i32 %31260, 255
  %31265 = tail call i32 @llvm.ctpop.i32(i32 %31264)
  %31266 = trunc i32 %31265 to i8
  %31267 = and i8 %31266, 1
  %31268 = xor i8 %31267, 1
  store i8 %31268, i8* %21, align 1
  %31269 = xor i32 %31260, %31259
  %31270 = lshr i32 %31269, 4
  %31271 = trunc i32 %31270 to i8
  %31272 = and i8 %31271, 1
  store i8 %31272, i8* %27, align 1
  %31273 = icmp eq i32 %31260, 0
  %31274 = zext i1 %31273 to i8
  store i8 %31274, i8* %30, align 1
  %31275 = lshr i32 %31260, 31
  %31276 = trunc i32 %31275 to i8
  store i8 %31276, i8* %33, align 1
  %31277 = lshr i32 %31259, 31
  %31278 = xor i32 %31275, %31277
  %31279 = add nuw nsw i32 %31278, %31275
  %31280 = icmp eq i32 %31279, 2
  %31281 = zext i1 %31280 to i8
  store i8 %31281, i8* %39, align 1
  %31282 = sext i32 %31260 to i64
  store i64 %31282, i64* %RCX.i11580, align 8
  %31283 = shl nsw i64 %31282, 1
  %31284 = add i64 %31255, %31283
  %31285 = add i64 %31252, 18
  store i64 %31285, i64* %3, align 8
  %31286 = inttoptr i64 %31284 to i16*
  %31287 = load i16, i16* %31286, align 2
  store i16 %31287, i16* %R9W.i2497, align 2
  %31288 = add i64 %31250, -198
  %31289 = add i64 %31252, 26
  store i64 %31289, i64* %3, align 8
  %31290 = inttoptr i64 %31288 to i16*
  store i16 %31287, i16* %31290, align 2
  %31291 = load i64, i64* %RBP.i, align 8
  %31292 = add i64 %31291, -8
  %31293 = load i64, i64* %3, align 8
  %31294 = add i64 %31293, 4
  store i64 %31294, i64* %3, align 8
  %31295 = inttoptr i64 %31292 to i64*
  %31296 = load i64, i64* %31295, align 8
  store i64 %31296, i64* %RDI.i2910, align 8
  %31297 = add i64 %31291, -208
  %31298 = add i64 %31293, 11
  store i64 %31298, i64* %3, align 8
  %31299 = inttoptr i64 %31297 to i64*
  %31300 = load i64, i64* %31299, align 8
  store i64 %31300, i64* %RAX.i11582.pre-phi, align 8
  %31301 = add i64 %31291, -198
  %31302 = add i64 %31293, 18
  store i64 %31302, i64* %3, align 8
  %31303 = inttoptr i64 %31301 to i16*
  %31304 = load i16, i16* %31303, align 2
  %31305 = zext i16 %31304 to i64
  store i64 %31305, i64* %576, align 8
  %31306 = zext i16 %31304 to i64
  store i64 %31306, i64* %RCX.i11580, align 8
  %31307 = add i64 %31300, %31306
  %31308 = add i64 %31293, 24
  store i64 %31308, i64* %3, align 8
  %31309 = inttoptr i64 %31307 to i8*
  %31310 = load i8, i8* %31309, align 1
  %31311 = zext i8 %31310 to i64
  store i64 %31311, i64* %RSI.i11312, align 8
  %31312 = add i64 %31291, -216
  %31313 = add i64 %31293, 31
  store i64 %31313, i64* %3, align 8
  %31314 = inttoptr i64 %31312 to i64*
  %31315 = load i64, i64* %31314, align 8
  store i64 %31315, i64* %RAX.i11582.pre-phi, align 8
  %31316 = add i64 %31293, 38
  store i64 %31316, i64* %3, align 8
  %31317 = load i16, i16* %31303, align 2
  %31318 = zext i16 %31317 to i64
  store i64 %31318, i64* %576, align 8
  %31319 = zext i16 %31317 to i64
  store i64 %31319, i64* %RCX.i11580, align 8
  %31320 = shl nuw nsw i64 %31319, 2
  %31321 = add i64 %31315, %31320
  %31322 = add i64 %31293, 43
  store i64 %31322, i64* %3, align 8
  %31323 = inttoptr i64 %31321 to i32*
  %31324 = load i32, i32* %31323, align 4
  %31325 = zext i32 %31324 to i64
  store i64 %31325, i64* %576, align 8
  %31326 = add i64 %31293, -16434
  %31327 = add i64 %31293, 48
  %31328 = load i64, i64* %6, align 8
  %31329 = add i64 %31328, -8
  %31330 = inttoptr i64 %31329 to i64*
  store i64 %31327, i64* %31330, align 8
  store i64 %31329, i64* %6, align 8
  store i64 %31326, i64* %3, align 8
  %call2_40dcfd = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31326, %struct.Memory* %call2_40dcb3)
  %31331 = load i64, i64* %RBP.i, align 8
  %31332 = add i64 %31331, -120
  %31333 = load i64, i64* %3, align 8
  %31334 = add i64 %31333, 4
  store i64 %31334, i64* %3, align 8
  %31335 = inttoptr i64 %31332 to i64*
  %31336 = load i64, i64* %31335, align 8
  store i64 %31336, i64* %RAX.i11582.pre-phi, align 8
  %31337 = add i64 %31331, -28
  %31338 = add i64 %31333, 7
  store i64 %31338, i64* %3, align 8
  %31339 = inttoptr i64 %31337 to i32*
  %31340 = load i32, i32* %31339, align 4
  %31341 = add i32 %31340, 10
  %31342 = zext i32 %31341 to i64
  store i64 %31342, i64* %576, align 8
  %31343 = icmp ugt i32 %31340, -11
  %31344 = zext i1 %31343 to i8
  store i8 %31344, i8* %14, align 1
  %31345 = and i32 %31341, 255
  %31346 = tail call i32 @llvm.ctpop.i32(i32 %31345)
  %31347 = trunc i32 %31346 to i8
  %31348 = and i8 %31347, 1
  %31349 = xor i8 %31348, 1
  store i8 %31349, i8* %21, align 1
  %31350 = xor i32 %31341, %31340
  %31351 = lshr i32 %31350, 4
  %31352 = trunc i32 %31351 to i8
  %31353 = and i8 %31352, 1
  store i8 %31353, i8* %27, align 1
  %31354 = icmp eq i32 %31341, 0
  %31355 = zext i1 %31354 to i8
  store i8 %31355, i8* %30, align 1
  %31356 = lshr i32 %31341, 31
  %31357 = trunc i32 %31356 to i8
  store i8 %31357, i8* %33, align 1
  %31358 = lshr i32 %31340, 31
  %31359 = xor i32 %31356, %31358
  %31360 = add nuw nsw i32 %31359, %31356
  %31361 = icmp eq i32 %31360, 2
  %31362 = zext i1 %31361 to i8
  store i8 %31362, i8* %39, align 1
  %31363 = sext i32 %31341 to i64
  store i64 %31363, i64* %RCX.i11580, align 8
  %31364 = shl nsw i64 %31363, 1
  %31365 = add i64 %31336, %31364
  %31366 = add i64 %31333, 18
  store i64 %31366, i64* %3, align 8
  %31367 = inttoptr i64 %31365 to i16*
  %31368 = load i16, i16* %31367, align 2
  store i16 %31368, i16* %R9W.i2497, align 2
  %31369 = add i64 %31331, -198
  %31370 = add i64 %31333, 26
  store i64 %31370, i64* %3, align 8
  %31371 = inttoptr i64 %31369 to i16*
  store i16 %31368, i16* %31371, align 2
  %31372 = load i64, i64* %RBP.i, align 8
  %31373 = add i64 %31372, -8
  %31374 = load i64, i64* %3, align 8
  %31375 = add i64 %31374, 4
  store i64 %31375, i64* %3, align 8
  %31376 = inttoptr i64 %31373 to i64*
  %31377 = load i64, i64* %31376, align 8
  store i64 %31377, i64* %RDI.i2910, align 8
  %31378 = add i64 %31372, -208
  %31379 = add i64 %31374, 11
  store i64 %31379, i64* %3, align 8
  %31380 = inttoptr i64 %31378 to i64*
  %31381 = load i64, i64* %31380, align 8
  store i64 %31381, i64* %RAX.i11582.pre-phi, align 8
  %31382 = add i64 %31372, -198
  %31383 = add i64 %31374, 18
  store i64 %31383, i64* %3, align 8
  %31384 = inttoptr i64 %31382 to i16*
  %31385 = load i16, i16* %31384, align 2
  %31386 = zext i16 %31385 to i64
  store i64 %31386, i64* %576, align 8
  %31387 = zext i16 %31385 to i64
  store i64 %31387, i64* %RCX.i11580, align 8
  %31388 = add i64 %31381, %31387
  %31389 = add i64 %31374, 24
  store i64 %31389, i64* %3, align 8
  %31390 = inttoptr i64 %31388 to i8*
  %31391 = load i8, i8* %31390, align 1
  %31392 = zext i8 %31391 to i64
  store i64 %31392, i64* %RSI.i11312, align 8
  %31393 = add i64 %31372, -216
  %31394 = add i64 %31374, 31
  store i64 %31394, i64* %3, align 8
  %31395 = inttoptr i64 %31393 to i64*
  %31396 = load i64, i64* %31395, align 8
  store i64 %31396, i64* %RAX.i11582.pre-phi, align 8
  %31397 = add i64 %31374, 38
  store i64 %31397, i64* %3, align 8
  %31398 = load i16, i16* %31384, align 2
  %31399 = zext i16 %31398 to i64
  store i64 %31399, i64* %576, align 8
  %31400 = zext i16 %31398 to i64
  store i64 %31400, i64* %RCX.i11580, align 8
  %31401 = shl nuw nsw i64 %31400, 2
  %31402 = add i64 %31396, %31401
  %31403 = add i64 %31374, 43
  store i64 %31403, i64* %3, align 8
  %31404 = inttoptr i64 %31402 to i32*
  %31405 = load i32, i32* %31404, align 4
  %31406 = zext i32 %31405 to i64
  store i64 %31406, i64* %576, align 8
  %31407 = add i64 %31374, -16508
  %31408 = add i64 %31374, 48
  %31409 = load i64, i64* %6, align 8
  %31410 = add i64 %31409, -8
  %31411 = inttoptr i64 %31410 to i64*
  store i64 %31408, i64* %31411, align 8
  store i64 %31410, i64* %6, align 8
  store i64 %31407, i64* %3, align 8
  %call2_40dd47 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31407, %struct.Memory* %call2_40dcfd)
  %31412 = load i64, i64* %RBP.i, align 8
  %31413 = add i64 %31412, -120
  %31414 = load i64, i64* %3, align 8
  %31415 = add i64 %31414, 4
  store i64 %31415, i64* %3, align 8
  %31416 = inttoptr i64 %31413 to i64*
  %31417 = load i64, i64* %31416, align 8
  store i64 %31417, i64* %RAX.i11582.pre-phi, align 8
  %31418 = add i64 %31412, -28
  %31419 = add i64 %31414, 7
  store i64 %31419, i64* %3, align 8
  %31420 = inttoptr i64 %31418 to i32*
  %31421 = load i32, i32* %31420, align 4
  %31422 = add i32 %31421, 11
  %31423 = zext i32 %31422 to i64
  store i64 %31423, i64* %576, align 8
  %31424 = icmp ugt i32 %31421, -12
  %31425 = zext i1 %31424 to i8
  store i8 %31425, i8* %14, align 1
  %31426 = and i32 %31422, 255
  %31427 = tail call i32 @llvm.ctpop.i32(i32 %31426)
  %31428 = trunc i32 %31427 to i8
  %31429 = and i8 %31428, 1
  %31430 = xor i8 %31429, 1
  store i8 %31430, i8* %21, align 1
  %31431 = xor i32 %31422, %31421
  %31432 = lshr i32 %31431, 4
  %31433 = trunc i32 %31432 to i8
  %31434 = and i8 %31433, 1
  store i8 %31434, i8* %27, align 1
  %31435 = icmp eq i32 %31422, 0
  %31436 = zext i1 %31435 to i8
  store i8 %31436, i8* %30, align 1
  %31437 = lshr i32 %31422, 31
  %31438 = trunc i32 %31437 to i8
  store i8 %31438, i8* %33, align 1
  %31439 = lshr i32 %31421, 31
  %31440 = xor i32 %31437, %31439
  %31441 = add nuw nsw i32 %31440, %31437
  %31442 = icmp eq i32 %31441, 2
  %31443 = zext i1 %31442 to i8
  store i8 %31443, i8* %39, align 1
  %31444 = sext i32 %31422 to i64
  store i64 %31444, i64* %RCX.i11580, align 8
  %31445 = shl nsw i64 %31444, 1
  %31446 = add i64 %31417, %31445
  %31447 = add i64 %31414, 18
  store i64 %31447, i64* %3, align 8
  %31448 = inttoptr i64 %31446 to i16*
  %31449 = load i16, i16* %31448, align 2
  store i16 %31449, i16* %R9W.i2497, align 2
  %31450 = add i64 %31412, -198
  %31451 = add i64 %31414, 26
  store i64 %31451, i64* %3, align 8
  %31452 = inttoptr i64 %31450 to i16*
  store i16 %31449, i16* %31452, align 2
  %31453 = load i64, i64* %RBP.i, align 8
  %31454 = add i64 %31453, -8
  %31455 = load i64, i64* %3, align 8
  %31456 = add i64 %31455, 4
  store i64 %31456, i64* %3, align 8
  %31457 = inttoptr i64 %31454 to i64*
  %31458 = load i64, i64* %31457, align 8
  store i64 %31458, i64* %RDI.i2910, align 8
  %31459 = add i64 %31453, -208
  %31460 = add i64 %31455, 11
  store i64 %31460, i64* %3, align 8
  %31461 = inttoptr i64 %31459 to i64*
  %31462 = load i64, i64* %31461, align 8
  store i64 %31462, i64* %RAX.i11582.pre-phi, align 8
  %31463 = add i64 %31453, -198
  %31464 = add i64 %31455, 18
  store i64 %31464, i64* %3, align 8
  %31465 = inttoptr i64 %31463 to i16*
  %31466 = load i16, i16* %31465, align 2
  %31467 = zext i16 %31466 to i64
  store i64 %31467, i64* %576, align 8
  %31468 = zext i16 %31466 to i64
  store i64 %31468, i64* %RCX.i11580, align 8
  %31469 = add i64 %31462, %31468
  %31470 = add i64 %31455, 24
  store i64 %31470, i64* %3, align 8
  %31471 = inttoptr i64 %31469 to i8*
  %31472 = load i8, i8* %31471, align 1
  %31473 = zext i8 %31472 to i64
  store i64 %31473, i64* %RSI.i11312, align 8
  %31474 = add i64 %31453, -216
  %31475 = add i64 %31455, 31
  store i64 %31475, i64* %3, align 8
  %31476 = inttoptr i64 %31474 to i64*
  %31477 = load i64, i64* %31476, align 8
  store i64 %31477, i64* %RAX.i11582.pre-phi, align 8
  %31478 = add i64 %31455, 38
  store i64 %31478, i64* %3, align 8
  %31479 = load i16, i16* %31465, align 2
  %31480 = zext i16 %31479 to i64
  store i64 %31480, i64* %576, align 8
  %31481 = zext i16 %31479 to i64
  store i64 %31481, i64* %RCX.i11580, align 8
  %31482 = shl nuw nsw i64 %31481, 2
  %31483 = add i64 %31477, %31482
  %31484 = add i64 %31455, 43
  store i64 %31484, i64* %3, align 8
  %31485 = inttoptr i64 %31483 to i32*
  %31486 = load i32, i32* %31485, align 4
  %31487 = zext i32 %31486 to i64
  store i64 %31487, i64* %576, align 8
  %31488 = add i64 %31455, -16582
  %31489 = add i64 %31455, 48
  %31490 = load i64, i64* %6, align 8
  %31491 = add i64 %31490, -8
  %31492 = inttoptr i64 %31491 to i64*
  store i64 %31489, i64* %31492, align 8
  store i64 %31491, i64* %6, align 8
  store i64 %31488, i64* %3, align 8
  %call2_40dd91 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31488, %struct.Memory* %call2_40dd47)
  %31493 = load i64, i64* %RBP.i, align 8
  %31494 = add i64 %31493, -120
  %31495 = load i64, i64* %3, align 8
  %31496 = add i64 %31495, 4
  store i64 %31496, i64* %3, align 8
  %31497 = inttoptr i64 %31494 to i64*
  %31498 = load i64, i64* %31497, align 8
  store i64 %31498, i64* %RAX.i11582.pre-phi, align 8
  %31499 = add i64 %31493, -28
  %31500 = add i64 %31495, 7
  store i64 %31500, i64* %3, align 8
  %31501 = inttoptr i64 %31499 to i32*
  %31502 = load i32, i32* %31501, align 4
  %31503 = add i32 %31502, 12
  %31504 = zext i32 %31503 to i64
  store i64 %31504, i64* %576, align 8
  %31505 = icmp ugt i32 %31502, -13
  %31506 = zext i1 %31505 to i8
  store i8 %31506, i8* %14, align 1
  %31507 = and i32 %31503, 255
  %31508 = tail call i32 @llvm.ctpop.i32(i32 %31507)
  %31509 = trunc i32 %31508 to i8
  %31510 = and i8 %31509, 1
  %31511 = xor i8 %31510, 1
  store i8 %31511, i8* %21, align 1
  %31512 = xor i32 %31503, %31502
  %31513 = lshr i32 %31512, 4
  %31514 = trunc i32 %31513 to i8
  %31515 = and i8 %31514, 1
  store i8 %31515, i8* %27, align 1
  %31516 = icmp eq i32 %31503, 0
  %31517 = zext i1 %31516 to i8
  store i8 %31517, i8* %30, align 1
  %31518 = lshr i32 %31503, 31
  %31519 = trunc i32 %31518 to i8
  store i8 %31519, i8* %33, align 1
  %31520 = lshr i32 %31502, 31
  %31521 = xor i32 %31518, %31520
  %31522 = add nuw nsw i32 %31521, %31518
  %31523 = icmp eq i32 %31522, 2
  %31524 = zext i1 %31523 to i8
  store i8 %31524, i8* %39, align 1
  %31525 = sext i32 %31503 to i64
  store i64 %31525, i64* %RCX.i11580, align 8
  %31526 = shl nsw i64 %31525, 1
  %31527 = add i64 %31498, %31526
  %31528 = add i64 %31495, 18
  store i64 %31528, i64* %3, align 8
  %31529 = inttoptr i64 %31527 to i16*
  %31530 = load i16, i16* %31529, align 2
  store i16 %31530, i16* %R9W.i2497, align 2
  %31531 = add i64 %31493, -198
  %31532 = add i64 %31495, 26
  store i64 %31532, i64* %3, align 8
  %31533 = inttoptr i64 %31531 to i16*
  store i16 %31530, i16* %31533, align 2
  %31534 = load i64, i64* %RBP.i, align 8
  %31535 = add i64 %31534, -8
  %31536 = load i64, i64* %3, align 8
  %31537 = add i64 %31536, 4
  store i64 %31537, i64* %3, align 8
  %31538 = inttoptr i64 %31535 to i64*
  %31539 = load i64, i64* %31538, align 8
  store i64 %31539, i64* %RDI.i2910, align 8
  %31540 = add i64 %31534, -208
  %31541 = add i64 %31536, 11
  store i64 %31541, i64* %3, align 8
  %31542 = inttoptr i64 %31540 to i64*
  %31543 = load i64, i64* %31542, align 8
  store i64 %31543, i64* %RAX.i11582.pre-phi, align 8
  %31544 = add i64 %31534, -198
  %31545 = add i64 %31536, 18
  store i64 %31545, i64* %3, align 8
  %31546 = inttoptr i64 %31544 to i16*
  %31547 = load i16, i16* %31546, align 2
  %31548 = zext i16 %31547 to i64
  store i64 %31548, i64* %576, align 8
  %31549 = zext i16 %31547 to i64
  store i64 %31549, i64* %RCX.i11580, align 8
  %31550 = add i64 %31543, %31549
  %31551 = add i64 %31536, 24
  store i64 %31551, i64* %3, align 8
  %31552 = inttoptr i64 %31550 to i8*
  %31553 = load i8, i8* %31552, align 1
  %31554 = zext i8 %31553 to i64
  store i64 %31554, i64* %RSI.i11312, align 8
  %31555 = add i64 %31534, -216
  %31556 = add i64 %31536, 31
  store i64 %31556, i64* %3, align 8
  %31557 = inttoptr i64 %31555 to i64*
  %31558 = load i64, i64* %31557, align 8
  store i64 %31558, i64* %RAX.i11582.pre-phi, align 8
  %31559 = add i64 %31536, 38
  store i64 %31559, i64* %3, align 8
  %31560 = load i16, i16* %31546, align 2
  %31561 = zext i16 %31560 to i64
  store i64 %31561, i64* %576, align 8
  %31562 = zext i16 %31560 to i64
  store i64 %31562, i64* %RCX.i11580, align 8
  %31563 = shl nuw nsw i64 %31562, 2
  %31564 = add i64 %31558, %31563
  %31565 = add i64 %31536, 43
  store i64 %31565, i64* %3, align 8
  %31566 = inttoptr i64 %31564 to i32*
  %31567 = load i32, i32* %31566, align 4
  %31568 = zext i32 %31567 to i64
  store i64 %31568, i64* %576, align 8
  %31569 = add i64 %31536, -16656
  %31570 = add i64 %31536, 48
  %31571 = load i64, i64* %6, align 8
  %31572 = add i64 %31571, -8
  %31573 = inttoptr i64 %31572 to i64*
  store i64 %31570, i64* %31573, align 8
  store i64 %31572, i64* %6, align 8
  store i64 %31569, i64* %3, align 8
  %call2_40dddb = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31569, %struct.Memory* %call2_40dd91)
  %31574 = load i64, i64* %RBP.i, align 8
  %31575 = add i64 %31574, -120
  %31576 = load i64, i64* %3, align 8
  %31577 = add i64 %31576, 4
  store i64 %31577, i64* %3, align 8
  %31578 = inttoptr i64 %31575 to i64*
  %31579 = load i64, i64* %31578, align 8
  store i64 %31579, i64* %RAX.i11582.pre-phi, align 8
  %31580 = add i64 %31574, -28
  %31581 = add i64 %31576, 7
  store i64 %31581, i64* %3, align 8
  %31582 = inttoptr i64 %31580 to i32*
  %31583 = load i32, i32* %31582, align 4
  %31584 = add i32 %31583, 13
  %31585 = zext i32 %31584 to i64
  store i64 %31585, i64* %576, align 8
  %31586 = icmp ugt i32 %31583, -14
  %31587 = zext i1 %31586 to i8
  store i8 %31587, i8* %14, align 1
  %31588 = and i32 %31584, 255
  %31589 = tail call i32 @llvm.ctpop.i32(i32 %31588)
  %31590 = trunc i32 %31589 to i8
  %31591 = and i8 %31590, 1
  %31592 = xor i8 %31591, 1
  store i8 %31592, i8* %21, align 1
  %31593 = xor i32 %31584, %31583
  %31594 = lshr i32 %31593, 4
  %31595 = trunc i32 %31594 to i8
  %31596 = and i8 %31595, 1
  store i8 %31596, i8* %27, align 1
  %31597 = icmp eq i32 %31584, 0
  %31598 = zext i1 %31597 to i8
  store i8 %31598, i8* %30, align 1
  %31599 = lshr i32 %31584, 31
  %31600 = trunc i32 %31599 to i8
  store i8 %31600, i8* %33, align 1
  %31601 = lshr i32 %31583, 31
  %31602 = xor i32 %31599, %31601
  %31603 = add nuw nsw i32 %31602, %31599
  %31604 = icmp eq i32 %31603, 2
  %31605 = zext i1 %31604 to i8
  store i8 %31605, i8* %39, align 1
  %31606 = sext i32 %31584 to i64
  store i64 %31606, i64* %RCX.i11580, align 8
  %31607 = shl nsw i64 %31606, 1
  %31608 = add i64 %31579, %31607
  %31609 = add i64 %31576, 18
  store i64 %31609, i64* %3, align 8
  %31610 = inttoptr i64 %31608 to i16*
  %31611 = load i16, i16* %31610, align 2
  store i16 %31611, i16* %R9W.i2497, align 2
  %31612 = add i64 %31574, -198
  %31613 = add i64 %31576, 26
  store i64 %31613, i64* %3, align 8
  %31614 = inttoptr i64 %31612 to i16*
  store i16 %31611, i16* %31614, align 2
  %31615 = load i64, i64* %RBP.i, align 8
  %31616 = add i64 %31615, -8
  %31617 = load i64, i64* %3, align 8
  %31618 = add i64 %31617, 4
  store i64 %31618, i64* %3, align 8
  %31619 = inttoptr i64 %31616 to i64*
  %31620 = load i64, i64* %31619, align 8
  store i64 %31620, i64* %RDI.i2910, align 8
  %31621 = add i64 %31615, -208
  %31622 = add i64 %31617, 11
  store i64 %31622, i64* %3, align 8
  %31623 = inttoptr i64 %31621 to i64*
  %31624 = load i64, i64* %31623, align 8
  store i64 %31624, i64* %RAX.i11582.pre-phi, align 8
  %31625 = add i64 %31615, -198
  %31626 = add i64 %31617, 18
  store i64 %31626, i64* %3, align 8
  %31627 = inttoptr i64 %31625 to i16*
  %31628 = load i16, i16* %31627, align 2
  %31629 = zext i16 %31628 to i64
  store i64 %31629, i64* %576, align 8
  %31630 = zext i16 %31628 to i64
  store i64 %31630, i64* %RCX.i11580, align 8
  %31631 = add i64 %31624, %31630
  %31632 = add i64 %31617, 24
  store i64 %31632, i64* %3, align 8
  %31633 = inttoptr i64 %31631 to i8*
  %31634 = load i8, i8* %31633, align 1
  %31635 = zext i8 %31634 to i64
  store i64 %31635, i64* %RSI.i11312, align 8
  %31636 = add i64 %31615, -216
  %31637 = add i64 %31617, 31
  store i64 %31637, i64* %3, align 8
  %31638 = inttoptr i64 %31636 to i64*
  %31639 = load i64, i64* %31638, align 8
  store i64 %31639, i64* %RAX.i11582.pre-phi, align 8
  %31640 = add i64 %31617, 38
  store i64 %31640, i64* %3, align 8
  %31641 = load i16, i16* %31627, align 2
  %31642 = zext i16 %31641 to i64
  store i64 %31642, i64* %576, align 8
  %31643 = zext i16 %31641 to i64
  store i64 %31643, i64* %RCX.i11580, align 8
  %31644 = shl nuw nsw i64 %31643, 2
  %31645 = add i64 %31639, %31644
  %31646 = add i64 %31617, 43
  store i64 %31646, i64* %3, align 8
  %31647 = inttoptr i64 %31645 to i32*
  %31648 = load i32, i32* %31647, align 4
  %31649 = zext i32 %31648 to i64
  store i64 %31649, i64* %576, align 8
  %31650 = add i64 %31617, -16730
  %31651 = add i64 %31617, 48
  %31652 = load i64, i64* %6, align 8
  %31653 = add i64 %31652, -8
  %31654 = inttoptr i64 %31653 to i64*
  store i64 %31651, i64* %31654, align 8
  store i64 %31653, i64* %6, align 8
  store i64 %31650, i64* %3, align 8
  %call2_40de25 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31650, %struct.Memory* %call2_40dddb)
  %31655 = load i64, i64* %RBP.i, align 8
  %31656 = add i64 %31655, -120
  %31657 = load i64, i64* %3, align 8
  %31658 = add i64 %31657, 4
  store i64 %31658, i64* %3, align 8
  %31659 = inttoptr i64 %31656 to i64*
  %31660 = load i64, i64* %31659, align 8
  store i64 %31660, i64* %RAX.i11582.pre-phi, align 8
  %31661 = add i64 %31655, -28
  %31662 = add i64 %31657, 7
  store i64 %31662, i64* %3, align 8
  %31663 = inttoptr i64 %31661 to i32*
  %31664 = load i32, i32* %31663, align 4
  %31665 = add i32 %31664, 14
  %31666 = zext i32 %31665 to i64
  store i64 %31666, i64* %576, align 8
  %31667 = icmp ugt i32 %31664, -15
  %31668 = zext i1 %31667 to i8
  store i8 %31668, i8* %14, align 1
  %31669 = and i32 %31665, 255
  %31670 = tail call i32 @llvm.ctpop.i32(i32 %31669)
  %31671 = trunc i32 %31670 to i8
  %31672 = and i8 %31671, 1
  %31673 = xor i8 %31672, 1
  store i8 %31673, i8* %21, align 1
  %31674 = xor i32 %31665, %31664
  %31675 = lshr i32 %31674, 4
  %31676 = trunc i32 %31675 to i8
  %31677 = and i8 %31676, 1
  store i8 %31677, i8* %27, align 1
  %31678 = icmp eq i32 %31665, 0
  %31679 = zext i1 %31678 to i8
  store i8 %31679, i8* %30, align 1
  %31680 = lshr i32 %31665, 31
  %31681 = trunc i32 %31680 to i8
  store i8 %31681, i8* %33, align 1
  %31682 = lshr i32 %31664, 31
  %31683 = xor i32 %31680, %31682
  %31684 = add nuw nsw i32 %31683, %31680
  %31685 = icmp eq i32 %31684, 2
  %31686 = zext i1 %31685 to i8
  store i8 %31686, i8* %39, align 1
  %31687 = sext i32 %31665 to i64
  store i64 %31687, i64* %RCX.i11580, align 8
  %31688 = shl nsw i64 %31687, 1
  %31689 = add i64 %31660, %31688
  %31690 = add i64 %31657, 18
  store i64 %31690, i64* %3, align 8
  %31691 = inttoptr i64 %31689 to i16*
  %31692 = load i16, i16* %31691, align 2
  store i16 %31692, i16* %R9W.i2497, align 2
  %31693 = add i64 %31655, -198
  %31694 = add i64 %31657, 26
  store i64 %31694, i64* %3, align 8
  %31695 = inttoptr i64 %31693 to i16*
  store i16 %31692, i16* %31695, align 2
  %31696 = load i64, i64* %RBP.i, align 8
  %31697 = add i64 %31696, -8
  %31698 = load i64, i64* %3, align 8
  %31699 = add i64 %31698, 4
  store i64 %31699, i64* %3, align 8
  %31700 = inttoptr i64 %31697 to i64*
  %31701 = load i64, i64* %31700, align 8
  store i64 %31701, i64* %RDI.i2910, align 8
  %31702 = add i64 %31696, -208
  %31703 = add i64 %31698, 11
  store i64 %31703, i64* %3, align 8
  %31704 = inttoptr i64 %31702 to i64*
  %31705 = load i64, i64* %31704, align 8
  store i64 %31705, i64* %RAX.i11582.pre-phi, align 8
  %31706 = add i64 %31696, -198
  %31707 = add i64 %31698, 18
  store i64 %31707, i64* %3, align 8
  %31708 = inttoptr i64 %31706 to i16*
  %31709 = load i16, i16* %31708, align 2
  %31710 = zext i16 %31709 to i64
  store i64 %31710, i64* %576, align 8
  %31711 = zext i16 %31709 to i64
  store i64 %31711, i64* %RCX.i11580, align 8
  %31712 = add i64 %31705, %31711
  %31713 = add i64 %31698, 24
  store i64 %31713, i64* %3, align 8
  %31714 = inttoptr i64 %31712 to i8*
  %31715 = load i8, i8* %31714, align 1
  %31716 = zext i8 %31715 to i64
  store i64 %31716, i64* %RSI.i11312, align 8
  %31717 = add i64 %31696, -216
  %31718 = add i64 %31698, 31
  store i64 %31718, i64* %3, align 8
  %31719 = inttoptr i64 %31717 to i64*
  %31720 = load i64, i64* %31719, align 8
  store i64 %31720, i64* %RAX.i11582.pre-phi, align 8
  %31721 = add i64 %31698, 38
  store i64 %31721, i64* %3, align 8
  %31722 = load i16, i16* %31708, align 2
  %31723 = zext i16 %31722 to i64
  store i64 %31723, i64* %576, align 8
  %31724 = zext i16 %31722 to i64
  store i64 %31724, i64* %RCX.i11580, align 8
  %31725 = shl nuw nsw i64 %31724, 2
  %31726 = add i64 %31720, %31725
  %31727 = add i64 %31698, 43
  store i64 %31727, i64* %3, align 8
  %31728 = inttoptr i64 %31726 to i32*
  %31729 = load i32, i32* %31728, align 4
  %31730 = zext i32 %31729 to i64
  store i64 %31730, i64* %576, align 8
  %31731 = add i64 %31698, -16804
  %31732 = add i64 %31698, 48
  %31733 = load i64, i64* %6, align 8
  %31734 = add i64 %31733, -8
  %31735 = inttoptr i64 %31734 to i64*
  store i64 %31732, i64* %31735, align 8
  store i64 %31734, i64* %6, align 8
  store i64 %31731, i64* %3, align 8
  %call2_40de6f = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31731, %struct.Memory* %call2_40de25)
  %31736 = load i64, i64* %RBP.i, align 8
  %31737 = add i64 %31736, -120
  %31738 = load i64, i64* %3, align 8
  %31739 = add i64 %31738, 4
  store i64 %31739, i64* %3, align 8
  %31740 = inttoptr i64 %31737 to i64*
  %31741 = load i64, i64* %31740, align 8
  store i64 %31741, i64* %RAX.i11582.pre-phi, align 8
  %31742 = add i64 %31736, -28
  %31743 = add i64 %31738, 7
  store i64 %31743, i64* %3, align 8
  %31744 = inttoptr i64 %31742 to i32*
  %31745 = load i32, i32* %31744, align 4
  %31746 = add i32 %31745, 15
  %31747 = zext i32 %31746 to i64
  store i64 %31747, i64* %576, align 8
  %31748 = icmp ugt i32 %31745, -16
  %31749 = zext i1 %31748 to i8
  store i8 %31749, i8* %14, align 1
  %31750 = and i32 %31746, 255
  %31751 = tail call i32 @llvm.ctpop.i32(i32 %31750)
  %31752 = trunc i32 %31751 to i8
  %31753 = and i8 %31752, 1
  %31754 = xor i8 %31753, 1
  store i8 %31754, i8* %21, align 1
  %31755 = xor i32 %31746, %31745
  %31756 = lshr i32 %31755, 4
  %31757 = trunc i32 %31756 to i8
  %31758 = and i8 %31757, 1
  store i8 %31758, i8* %27, align 1
  %31759 = icmp eq i32 %31746, 0
  %31760 = zext i1 %31759 to i8
  store i8 %31760, i8* %30, align 1
  %31761 = lshr i32 %31746, 31
  %31762 = trunc i32 %31761 to i8
  store i8 %31762, i8* %33, align 1
  %31763 = lshr i32 %31745, 31
  %31764 = xor i32 %31761, %31763
  %31765 = add nuw nsw i32 %31764, %31761
  %31766 = icmp eq i32 %31765, 2
  %31767 = zext i1 %31766 to i8
  store i8 %31767, i8* %39, align 1
  %31768 = sext i32 %31746 to i64
  store i64 %31768, i64* %RCX.i11580, align 8
  %31769 = shl nsw i64 %31768, 1
  %31770 = add i64 %31741, %31769
  %31771 = add i64 %31738, 18
  store i64 %31771, i64* %3, align 8
  %31772 = inttoptr i64 %31770 to i16*
  %31773 = load i16, i16* %31772, align 2
  store i16 %31773, i16* %R9W.i2497, align 2
  %31774 = add i64 %31736, -198
  %31775 = add i64 %31738, 26
  store i64 %31775, i64* %3, align 8
  %31776 = inttoptr i64 %31774 to i16*
  store i16 %31773, i16* %31776, align 2
  %31777 = load i64, i64* %RBP.i, align 8
  %31778 = add i64 %31777, -8
  %31779 = load i64, i64* %3, align 8
  %31780 = add i64 %31779, 4
  store i64 %31780, i64* %3, align 8
  %31781 = inttoptr i64 %31778 to i64*
  %31782 = load i64, i64* %31781, align 8
  store i64 %31782, i64* %RDI.i2910, align 8
  %31783 = add i64 %31777, -208
  %31784 = add i64 %31779, 11
  store i64 %31784, i64* %3, align 8
  %31785 = inttoptr i64 %31783 to i64*
  %31786 = load i64, i64* %31785, align 8
  store i64 %31786, i64* %RAX.i11582.pre-phi, align 8
  %31787 = add i64 %31777, -198
  %31788 = add i64 %31779, 18
  store i64 %31788, i64* %3, align 8
  %31789 = inttoptr i64 %31787 to i16*
  %31790 = load i16, i16* %31789, align 2
  %31791 = zext i16 %31790 to i64
  store i64 %31791, i64* %576, align 8
  %31792 = zext i16 %31790 to i64
  store i64 %31792, i64* %RCX.i11580, align 8
  %31793 = add i64 %31786, %31792
  %31794 = add i64 %31779, 24
  store i64 %31794, i64* %3, align 8
  %31795 = inttoptr i64 %31793 to i8*
  %31796 = load i8, i8* %31795, align 1
  %31797 = zext i8 %31796 to i64
  store i64 %31797, i64* %RSI.i11312, align 8
  %31798 = add i64 %31777, -216
  %31799 = add i64 %31779, 31
  store i64 %31799, i64* %3, align 8
  %31800 = inttoptr i64 %31798 to i64*
  %31801 = load i64, i64* %31800, align 8
  store i64 %31801, i64* %RAX.i11582.pre-phi, align 8
  %31802 = add i64 %31779, 38
  store i64 %31802, i64* %3, align 8
  %31803 = load i16, i16* %31789, align 2
  %31804 = zext i16 %31803 to i64
  store i64 %31804, i64* %576, align 8
  %31805 = zext i16 %31803 to i64
  store i64 %31805, i64* %RCX.i11580, align 8
  %31806 = shl nuw nsw i64 %31805, 2
  %31807 = add i64 %31801, %31806
  %31808 = add i64 %31779, 43
  store i64 %31808, i64* %3, align 8
  %31809 = inttoptr i64 %31807 to i32*
  %31810 = load i32, i32* %31809, align 4
  %31811 = zext i32 %31810 to i64
  store i64 %31811, i64* %576, align 8
  %31812 = add i64 %31779, -16878
  %31813 = add i64 %31779, 48
  %31814 = load i64, i64* %6, align 8
  %31815 = add i64 %31814, -8
  %31816 = inttoptr i64 %31815 to i64*
  store i64 %31813, i64* %31816, align 8
  store i64 %31815, i64* %6, align 8
  store i64 %31812, i64* %3, align 8
  %call2_40deb9 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31812, %struct.Memory* %call2_40de6f)
  %31817 = load i64, i64* %RBP.i, align 8
  %31818 = add i64 %31817, -120
  %31819 = load i64, i64* %3, align 8
  %31820 = add i64 %31819, 4
  store i64 %31820, i64* %3, align 8
  %31821 = inttoptr i64 %31818 to i64*
  %31822 = load i64, i64* %31821, align 8
  store i64 %31822, i64* %RAX.i11582.pre-phi, align 8
  %31823 = add i64 %31817, -28
  %31824 = add i64 %31819, 7
  store i64 %31824, i64* %3, align 8
  %31825 = inttoptr i64 %31823 to i32*
  %31826 = load i32, i32* %31825, align 4
  %31827 = add i32 %31826, 16
  %31828 = zext i32 %31827 to i64
  store i64 %31828, i64* %576, align 8
  %31829 = icmp ugt i32 %31826, -17
  %31830 = zext i1 %31829 to i8
  store i8 %31830, i8* %14, align 1
  %31831 = and i32 %31827, 255
  %31832 = tail call i32 @llvm.ctpop.i32(i32 %31831)
  %31833 = trunc i32 %31832 to i8
  %31834 = and i8 %31833, 1
  %31835 = xor i8 %31834, 1
  store i8 %31835, i8* %21, align 1
  %31836 = xor i32 %31826, 16
  %31837 = xor i32 %31836, %31827
  %31838 = lshr i32 %31837, 4
  %31839 = trunc i32 %31838 to i8
  %31840 = and i8 %31839, 1
  store i8 %31840, i8* %27, align 1
  %31841 = icmp eq i32 %31827, 0
  %31842 = zext i1 %31841 to i8
  store i8 %31842, i8* %30, align 1
  %31843 = lshr i32 %31827, 31
  %31844 = trunc i32 %31843 to i8
  store i8 %31844, i8* %33, align 1
  %31845 = lshr i32 %31826, 31
  %31846 = xor i32 %31843, %31845
  %31847 = add nuw nsw i32 %31846, %31843
  %31848 = icmp eq i32 %31847, 2
  %31849 = zext i1 %31848 to i8
  store i8 %31849, i8* %39, align 1
  %31850 = sext i32 %31827 to i64
  store i64 %31850, i64* %RCX.i11580, align 8
  %31851 = shl nsw i64 %31850, 1
  %31852 = add i64 %31822, %31851
  %31853 = add i64 %31819, 18
  store i64 %31853, i64* %3, align 8
  %31854 = inttoptr i64 %31852 to i16*
  %31855 = load i16, i16* %31854, align 2
  store i16 %31855, i16* %R9W.i2497, align 2
  %31856 = add i64 %31817, -198
  %31857 = add i64 %31819, 26
  store i64 %31857, i64* %3, align 8
  %31858 = inttoptr i64 %31856 to i16*
  store i16 %31855, i16* %31858, align 2
  %31859 = load i64, i64* %RBP.i, align 8
  %31860 = add i64 %31859, -8
  %31861 = load i64, i64* %3, align 8
  %31862 = add i64 %31861, 4
  store i64 %31862, i64* %3, align 8
  %31863 = inttoptr i64 %31860 to i64*
  %31864 = load i64, i64* %31863, align 8
  store i64 %31864, i64* %RDI.i2910, align 8
  %31865 = add i64 %31859, -208
  %31866 = add i64 %31861, 11
  store i64 %31866, i64* %3, align 8
  %31867 = inttoptr i64 %31865 to i64*
  %31868 = load i64, i64* %31867, align 8
  store i64 %31868, i64* %RAX.i11582.pre-phi, align 8
  %31869 = add i64 %31859, -198
  %31870 = add i64 %31861, 18
  store i64 %31870, i64* %3, align 8
  %31871 = inttoptr i64 %31869 to i16*
  %31872 = load i16, i16* %31871, align 2
  %31873 = zext i16 %31872 to i64
  store i64 %31873, i64* %576, align 8
  %31874 = zext i16 %31872 to i64
  store i64 %31874, i64* %RCX.i11580, align 8
  %31875 = add i64 %31868, %31874
  %31876 = add i64 %31861, 24
  store i64 %31876, i64* %3, align 8
  %31877 = inttoptr i64 %31875 to i8*
  %31878 = load i8, i8* %31877, align 1
  %31879 = zext i8 %31878 to i64
  store i64 %31879, i64* %RSI.i11312, align 8
  %31880 = add i64 %31859, -216
  %31881 = add i64 %31861, 31
  store i64 %31881, i64* %3, align 8
  %31882 = inttoptr i64 %31880 to i64*
  %31883 = load i64, i64* %31882, align 8
  store i64 %31883, i64* %RAX.i11582.pre-phi, align 8
  %31884 = add i64 %31861, 38
  store i64 %31884, i64* %3, align 8
  %31885 = load i16, i16* %31871, align 2
  %31886 = zext i16 %31885 to i64
  store i64 %31886, i64* %576, align 8
  %31887 = zext i16 %31885 to i64
  store i64 %31887, i64* %RCX.i11580, align 8
  %31888 = shl nuw nsw i64 %31887, 2
  %31889 = add i64 %31883, %31888
  %31890 = add i64 %31861, 43
  store i64 %31890, i64* %3, align 8
  %31891 = inttoptr i64 %31889 to i32*
  %31892 = load i32, i32* %31891, align 4
  %31893 = zext i32 %31892 to i64
  store i64 %31893, i64* %576, align 8
  %31894 = add i64 %31861, -16952
  %31895 = add i64 %31861, 48
  %31896 = load i64, i64* %6, align 8
  %31897 = add i64 %31896, -8
  %31898 = inttoptr i64 %31897 to i64*
  store i64 %31895, i64* %31898, align 8
  store i64 %31897, i64* %6, align 8
  store i64 %31894, i64* %3, align 8
  %call2_40df03 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31894, %struct.Memory* %call2_40deb9)
  %31899 = load i64, i64* %RBP.i, align 8
  %31900 = add i64 %31899, -120
  %31901 = load i64, i64* %3, align 8
  %31902 = add i64 %31901, 4
  store i64 %31902, i64* %3, align 8
  %31903 = inttoptr i64 %31900 to i64*
  %31904 = load i64, i64* %31903, align 8
  store i64 %31904, i64* %RAX.i11582.pre-phi, align 8
  %31905 = add i64 %31899, -28
  %31906 = add i64 %31901, 7
  store i64 %31906, i64* %3, align 8
  %31907 = inttoptr i64 %31905 to i32*
  %31908 = load i32, i32* %31907, align 4
  %31909 = add i32 %31908, 17
  %31910 = zext i32 %31909 to i64
  store i64 %31910, i64* %576, align 8
  %31911 = icmp ugt i32 %31908, -18
  %31912 = zext i1 %31911 to i8
  store i8 %31912, i8* %14, align 1
  %31913 = and i32 %31909, 255
  %31914 = tail call i32 @llvm.ctpop.i32(i32 %31913)
  %31915 = trunc i32 %31914 to i8
  %31916 = and i8 %31915, 1
  %31917 = xor i8 %31916, 1
  store i8 %31917, i8* %21, align 1
  %31918 = xor i32 %31908, 16
  %31919 = xor i32 %31918, %31909
  %31920 = lshr i32 %31919, 4
  %31921 = trunc i32 %31920 to i8
  %31922 = and i8 %31921, 1
  store i8 %31922, i8* %27, align 1
  %31923 = icmp eq i32 %31909, 0
  %31924 = zext i1 %31923 to i8
  store i8 %31924, i8* %30, align 1
  %31925 = lshr i32 %31909, 31
  %31926 = trunc i32 %31925 to i8
  store i8 %31926, i8* %33, align 1
  %31927 = lshr i32 %31908, 31
  %31928 = xor i32 %31925, %31927
  %31929 = add nuw nsw i32 %31928, %31925
  %31930 = icmp eq i32 %31929, 2
  %31931 = zext i1 %31930 to i8
  store i8 %31931, i8* %39, align 1
  %31932 = sext i32 %31909 to i64
  store i64 %31932, i64* %RCX.i11580, align 8
  %31933 = shl nsw i64 %31932, 1
  %31934 = add i64 %31904, %31933
  %31935 = add i64 %31901, 18
  store i64 %31935, i64* %3, align 8
  %31936 = inttoptr i64 %31934 to i16*
  %31937 = load i16, i16* %31936, align 2
  store i16 %31937, i16* %R9W.i2497, align 2
  %31938 = add i64 %31899, -198
  %31939 = add i64 %31901, 26
  store i64 %31939, i64* %3, align 8
  %31940 = inttoptr i64 %31938 to i16*
  store i16 %31937, i16* %31940, align 2
  %31941 = load i64, i64* %RBP.i, align 8
  %31942 = add i64 %31941, -8
  %31943 = load i64, i64* %3, align 8
  %31944 = add i64 %31943, 4
  store i64 %31944, i64* %3, align 8
  %31945 = inttoptr i64 %31942 to i64*
  %31946 = load i64, i64* %31945, align 8
  store i64 %31946, i64* %RDI.i2910, align 8
  %31947 = add i64 %31941, -208
  %31948 = add i64 %31943, 11
  store i64 %31948, i64* %3, align 8
  %31949 = inttoptr i64 %31947 to i64*
  %31950 = load i64, i64* %31949, align 8
  store i64 %31950, i64* %RAX.i11582.pre-phi, align 8
  %31951 = add i64 %31941, -198
  %31952 = add i64 %31943, 18
  store i64 %31952, i64* %3, align 8
  %31953 = inttoptr i64 %31951 to i16*
  %31954 = load i16, i16* %31953, align 2
  %31955 = zext i16 %31954 to i64
  store i64 %31955, i64* %576, align 8
  %31956 = zext i16 %31954 to i64
  store i64 %31956, i64* %RCX.i11580, align 8
  %31957 = add i64 %31950, %31956
  %31958 = add i64 %31943, 24
  store i64 %31958, i64* %3, align 8
  %31959 = inttoptr i64 %31957 to i8*
  %31960 = load i8, i8* %31959, align 1
  %31961 = zext i8 %31960 to i64
  store i64 %31961, i64* %RSI.i11312, align 8
  %31962 = add i64 %31941, -216
  %31963 = add i64 %31943, 31
  store i64 %31963, i64* %3, align 8
  %31964 = inttoptr i64 %31962 to i64*
  %31965 = load i64, i64* %31964, align 8
  store i64 %31965, i64* %RAX.i11582.pre-phi, align 8
  %31966 = add i64 %31943, 38
  store i64 %31966, i64* %3, align 8
  %31967 = load i16, i16* %31953, align 2
  %31968 = zext i16 %31967 to i64
  store i64 %31968, i64* %576, align 8
  %31969 = zext i16 %31967 to i64
  store i64 %31969, i64* %RCX.i11580, align 8
  %31970 = shl nuw nsw i64 %31969, 2
  %31971 = add i64 %31965, %31970
  %31972 = add i64 %31943, 43
  store i64 %31972, i64* %3, align 8
  %31973 = inttoptr i64 %31971 to i32*
  %31974 = load i32, i32* %31973, align 4
  %31975 = zext i32 %31974 to i64
  store i64 %31975, i64* %576, align 8
  %31976 = add i64 %31943, -17026
  %31977 = add i64 %31943, 48
  %31978 = load i64, i64* %6, align 8
  %31979 = add i64 %31978, -8
  %31980 = inttoptr i64 %31979 to i64*
  store i64 %31977, i64* %31980, align 8
  store i64 %31979, i64* %6, align 8
  store i64 %31976, i64* %3, align 8
  %call2_40df4d = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %31976, %struct.Memory* %call2_40df03)
  %31981 = load i64, i64* %RBP.i, align 8
  %31982 = add i64 %31981, -120
  %31983 = load i64, i64* %3, align 8
  %31984 = add i64 %31983, 4
  store i64 %31984, i64* %3, align 8
  %31985 = inttoptr i64 %31982 to i64*
  %31986 = load i64, i64* %31985, align 8
  store i64 %31986, i64* %RAX.i11582.pre-phi, align 8
  %31987 = add i64 %31981, -28
  %31988 = add i64 %31983, 7
  store i64 %31988, i64* %3, align 8
  %31989 = inttoptr i64 %31987 to i32*
  %31990 = load i32, i32* %31989, align 4
  %31991 = add i32 %31990, 18
  %31992 = zext i32 %31991 to i64
  store i64 %31992, i64* %576, align 8
  %31993 = icmp ugt i32 %31990, -19
  %31994 = zext i1 %31993 to i8
  store i8 %31994, i8* %14, align 1
  %31995 = and i32 %31991, 255
  %31996 = tail call i32 @llvm.ctpop.i32(i32 %31995)
  %31997 = trunc i32 %31996 to i8
  %31998 = and i8 %31997, 1
  %31999 = xor i8 %31998, 1
  store i8 %31999, i8* %21, align 1
  %32000 = xor i32 %31990, 16
  %32001 = xor i32 %32000, %31991
  %32002 = lshr i32 %32001, 4
  %32003 = trunc i32 %32002 to i8
  %32004 = and i8 %32003, 1
  store i8 %32004, i8* %27, align 1
  %32005 = icmp eq i32 %31991, 0
  %32006 = zext i1 %32005 to i8
  store i8 %32006, i8* %30, align 1
  %32007 = lshr i32 %31991, 31
  %32008 = trunc i32 %32007 to i8
  store i8 %32008, i8* %33, align 1
  %32009 = lshr i32 %31990, 31
  %32010 = xor i32 %32007, %32009
  %32011 = add nuw nsw i32 %32010, %32007
  %32012 = icmp eq i32 %32011, 2
  %32013 = zext i1 %32012 to i8
  store i8 %32013, i8* %39, align 1
  %32014 = sext i32 %31991 to i64
  store i64 %32014, i64* %RCX.i11580, align 8
  %32015 = shl nsw i64 %32014, 1
  %32016 = add i64 %31986, %32015
  %32017 = add i64 %31983, 18
  store i64 %32017, i64* %3, align 8
  %32018 = inttoptr i64 %32016 to i16*
  %32019 = load i16, i16* %32018, align 2
  store i16 %32019, i16* %R9W.i2497, align 2
  %32020 = add i64 %31981, -198
  %32021 = add i64 %31983, 26
  store i64 %32021, i64* %3, align 8
  %32022 = inttoptr i64 %32020 to i16*
  store i16 %32019, i16* %32022, align 2
  %32023 = load i64, i64* %RBP.i, align 8
  %32024 = add i64 %32023, -8
  %32025 = load i64, i64* %3, align 8
  %32026 = add i64 %32025, 4
  store i64 %32026, i64* %3, align 8
  %32027 = inttoptr i64 %32024 to i64*
  %32028 = load i64, i64* %32027, align 8
  store i64 %32028, i64* %RDI.i2910, align 8
  %32029 = add i64 %32023, -208
  %32030 = add i64 %32025, 11
  store i64 %32030, i64* %3, align 8
  %32031 = inttoptr i64 %32029 to i64*
  %32032 = load i64, i64* %32031, align 8
  store i64 %32032, i64* %RAX.i11582.pre-phi, align 8
  %32033 = add i64 %32023, -198
  %32034 = add i64 %32025, 18
  store i64 %32034, i64* %3, align 8
  %32035 = inttoptr i64 %32033 to i16*
  %32036 = load i16, i16* %32035, align 2
  %32037 = zext i16 %32036 to i64
  store i64 %32037, i64* %576, align 8
  %32038 = zext i16 %32036 to i64
  store i64 %32038, i64* %RCX.i11580, align 8
  %32039 = add i64 %32032, %32038
  %32040 = add i64 %32025, 24
  store i64 %32040, i64* %3, align 8
  %32041 = inttoptr i64 %32039 to i8*
  %32042 = load i8, i8* %32041, align 1
  %32043 = zext i8 %32042 to i64
  store i64 %32043, i64* %RSI.i11312, align 8
  %32044 = add i64 %32023, -216
  %32045 = add i64 %32025, 31
  store i64 %32045, i64* %3, align 8
  %32046 = inttoptr i64 %32044 to i64*
  %32047 = load i64, i64* %32046, align 8
  store i64 %32047, i64* %RAX.i11582.pre-phi, align 8
  %32048 = add i64 %32025, 38
  store i64 %32048, i64* %3, align 8
  %32049 = load i16, i16* %32035, align 2
  %32050 = zext i16 %32049 to i64
  store i64 %32050, i64* %576, align 8
  %32051 = zext i16 %32049 to i64
  store i64 %32051, i64* %RCX.i11580, align 8
  %32052 = shl nuw nsw i64 %32051, 2
  %32053 = add i64 %32047, %32052
  %32054 = add i64 %32025, 43
  store i64 %32054, i64* %3, align 8
  %32055 = inttoptr i64 %32053 to i32*
  %32056 = load i32, i32* %32055, align 4
  %32057 = zext i32 %32056 to i64
  store i64 %32057, i64* %576, align 8
  %32058 = add i64 %32025, -17100
  %32059 = add i64 %32025, 48
  %32060 = load i64, i64* %6, align 8
  %32061 = add i64 %32060, -8
  %32062 = inttoptr i64 %32061 to i64*
  store i64 %32059, i64* %32062, align 8
  store i64 %32061, i64* %6, align 8
  store i64 %32058, i64* %3, align 8
  %call2_40df97 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32058, %struct.Memory* %call2_40df4d)
  %32063 = load i64, i64* %RBP.i, align 8
  %32064 = add i64 %32063, -120
  %32065 = load i64, i64* %3, align 8
  %32066 = add i64 %32065, 4
  store i64 %32066, i64* %3, align 8
  %32067 = inttoptr i64 %32064 to i64*
  %32068 = load i64, i64* %32067, align 8
  store i64 %32068, i64* %RAX.i11582.pre-phi, align 8
  %32069 = add i64 %32063, -28
  %32070 = add i64 %32065, 7
  store i64 %32070, i64* %3, align 8
  %32071 = inttoptr i64 %32069 to i32*
  %32072 = load i32, i32* %32071, align 4
  %32073 = add i32 %32072, 19
  %32074 = zext i32 %32073 to i64
  store i64 %32074, i64* %576, align 8
  %32075 = icmp ugt i32 %32072, -20
  %32076 = zext i1 %32075 to i8
  store i8 %32076, i8* %14, align 1
  %32077 = and i32 %32073, 255
  %32078 = tail call i32 @llvm.ctpop.i32(i32 %32077)
  %32079 = trunc i32 %32078 to i8
  %32080 = and i8 %32079, 1
  %32081 = xor i8 %32080, 1
  store i8 %32081, i8* %21, align 1
  %32082 = xor i32 %32072, 16
  %32083 = xor i32 %32082, %32073
  %32084 = lshr i32 %32083, 4
  %32085 = trunc i32 %32084 to i8
  %32086 = and i8 %32085, 1
  store i8 %32086, i8* %27, align 1
  %32087 = icmp eq i32 %32073, 0
  %32088 = zext i1 %32087 to i8
  store i8 %32088, i8* %30, align 1
  %32089 = lshr i32 %32073, 31
  %32090 = trunc i32 %32089 to i8
  store i8 %32090, i8* %33, align 1
  %32091 = lshr i32 %32072, 31
  %32092 = xor i32 %32089, %32091
  %32093 = add nuw nsw i32 %32092, %32089
  %32094 = icmp eq i32 %32093, 2
  %32095 = zext i1 %32094 to i8
  store i8 %32095, i8* %39, align 1
  %32096 = sext i32 %32073 to i64
  store i64 %32096, i64* %RCX.i11580, align 8
  %32097 = shl nsw i64 %32096, 1
  %32098 = add i64 %32068, %32097
  %32099 = add i64 %32065, 18
  store i64 %32099, i64* %3, align 8
  %32100 = inttoptr i64 %32098 to i16*
  %32101 = load i16, i16* %32100, align 2
  store i16 %32101, i16* %R9W.i2497, align 2
  %32102 = add i64 %32063, -198
  %32103 = add i64 %32065, 26
  store i64 %32103, i64* %3, align 8
  %32104 = inttoptr i64 %32102 to i16*
  store i16 %32101, i16* %32104, align 2
  %32105 = load i64, i64* %RBP.i, align 8
  %32106 = add i64 %32105, -8
  %32107 = load i64, i64* %3, align 8
  %32108 = add i64 %32107, 4
  store i64 %32108, i64* %3, align 8
  %32109 = inttoptr i64 %32106 to i64*
  %32110 = load i64, i64* %32109, align 8
  store i64 %32110, i64* %RDI.i2910, align 8
  %32111 = add i64 %32105, -208
  %32112 = add i64 %32107, 11
  store i64 %32112, i64* %3, align 8
  %32113 = inttoptr i64 %32111 to i64*
  %32114 = load i64, i64* %32113, align 8
  store i64 %32114, i64* %RAX.i11582.pre-phi, align 8
  %32115 = add i64 %32105, -198
  %32116 = add i64 %32107, 18
  store i64 %32116, i64* %3, align 8
  %32117 = inttoptr i64 %32115 to i16*
  %32118 = load i16, i16* %32117, align 2
  %32119 = zext i16 %32118 to i64
  store i64 %32119, i64* %576, align 8
  %32120 = zext i16 %32118 to i64
  store i64 %32120, i64* %RCX.i11580, align 8
  %32121 = add i64 %32114, %32120
  %32122 = add i64 %32107, 24
  store i64 %32122, i64* %3, align 8
  %32123 = inttoptr i64 %32121 to i8*
  %32124 = load i8, i8* %32123, align 1
  %32125 = zext i8 %32124 to i64
  store i64 %32125, i64* %RSI.i11312, align 8
  %32126 = add i64 %32105, -216
  %32127 = add i64 %32107, 31
  store i64 %32127, i64* %3, align 8
  %32128 = inttoptr i64 %32126 to i64*
  %32129 = load i64, i64* %32128, align 8
  store i64 %32129, i64* %RAX.i11582.pre-phi, align 8
  %32130 = add i64 %32107, 38
  store i64 %32130, i64* %3, align 8
  %32131 = load i16, i16* %32117, align 2
  %32132 = zext i16 %32131 to i64
  store i64 %32132, i64* %576, align 8
  %32133 = zext i16 %32131 to i64
  store i64 %32133, i64* %RCX.i11580, align 8
  %32134 = shl nuw nsw i64 %32133, 2
  %32135 = add i64 %32129, %32134
  %32136 = add i64 %32107, 43
  store i64 %32136, i64* %3, align 8
  %32137 = inttoptr i64 %32135 to i32*
  %32138 = load i32, i32* %32137, align 4
  %32139 = zext i32 %32138 to i64
  store i64 %32139, i64* %576, align 8
  %32140 = add i64 %32107, -17174
  %32141 = add i64 %32107, 48
  %32142 = load i64, i64* %6, align 8
  %32143 = add i64 %32142, -8
  %32144 = inttoptr i64 %32143 to i64*
  store i64 %32141, i64* %32144, align 8
  store i64 %32143, i64* %6, align 8
  store i64 %32140, i64* %3, align 8
  %call2_40dfe1 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32140, %struct.Memory* %call2_40df97)
  %32145 = load i64, i64* %RBP.i, align 8
  %32146 = add i64 %32145, -120
  %32147 = load i64, i64* %3, align 8
  %32148 = add i64 %32147, 4
  store i64 %32148, i64* %3, align 8
  %32149 = inttoptr i64 %32146 to i64*
  %32150 = load i64, i64* %32149, align 8
  store i64 %32150, i64* %RAX.i11582.pre-phi, align 8
  %32151 = add i64 %32145, -28
  %32152 = add i64 %32147, 7
  store i64 %32152, i64* %3, align 8
  %32153 = inttoptr i64 %32151 to i32*
  %32154 = load i32, i32* %32153, align 4
  %32155 = add i32 %32154, 20
  %32156 = zext i32 %32155 to i64
  store i64 %32156, i64* %576, align 8
  %32157 = icmp ugt i32 %32154, -21
  %32158 = zext i1 %32157 to i8
  store i8 %32158, i8* %14, align 1
  %32159 = and i32 %32155, 255
  %32160 = tail call i32 @llvm.ctpop.i32(i32 %32159)
  %32161 = trunc i32 %32160 to i8
  %32162 = and i8 %32161, 1
  %32163 = xor i8 %32162, 1
  store i8 %32163, i8* %21, align 1
  %32164 = xor i32 %32154, 16
  %32165 = xor i32 %32164, %32155
  %32166 = lshr i32 %32165, 4
  %32167 = trunc i32 %32166 to i8
  %32168 = and i8 %32167, 1
  store i8 %32168, i8* %27, align 1
  %32169 = icmp eq i32 %32155, 0
  %32170 = zext i1 %32169 to i8
  store i8 %32170, i8* %30, align 1
  %32171 = lshr i32 %32155, 31
  %32172 = trunc i32 %32171 to i8
  store i8 %32172, i8* %33, align 1
  %32173 = lshr i32 %32154, 31
  %32174 = xor i32 %32171, %32173
  %32175 = add nuw nsw i32 %32174, %32171
  %32176 = icmp eq i32 %32175, 2
  %32177 = zext i1 %32176 to i8
  store i8 %32177, i8* %39, align 1
  %32178 = sext i32 %32155 to i64
  store i64 %32178, i64* %RCX.i11580, align 8
  %32179 = shl nsw i64 %32178, 1
  %32180 = add i64 %32150, %32179
  %32181 = add i64 %32147, 18
  store i64 %32181, i64* %3, align 8
  %32182 = inttoptr i64 %32180 to i16*
  %32183 = load i16, i16* %32182, align 2
  store i16 %32183, i16* %R9W.i2497, align 2
  %32184 = add i64 %32145, -198
  %32185 = add i64 %32147, 26
  store i64 %32185, i64* %3, align 8
  %32186 = inttoptr i64 %32184 to i16*
  store i16 %32183, i16* %32186, align 2
  %32187 = load i64, i64* %RBP.i, align 8
  %32188 = add i64 %32187, -8
  %32189 = load i64, i64* %3, align 8
  %32190 = add i64 %32189, 4
  store i64 %32190, i64* %3, align 8
  %32191 = inttoptr i64 %32188 to i64*
  %32192 = load i64, i64* %32191, align 8
  store i64 %32192, i64* %RDI.i2910, align 8
  %32193 = add i64 %32187, -208
  %32194 = add i64 %32189, 11
  store i64 %32194, i64* %3, align 8
  %32195 = inttoptr i64 %32193 to i64*
  %32196 = load i64, i64* %32195, align 8
  store i64 %32196, i64* %RAX.i11582.pre-phi, align 8
  %32197 = add i64 %32187, -198
  %32198 = add i64 %32189, 18
  store i64 %32198, i64* %3, align 8
  %32199 = inttoptr i64 %32197 to i16*
  %32200 = load i16, i16* %32199, align 2
  %32201 = zext i16 %32200 to i64
  store i64 %32201, i64* %576, align 8
  %32202 = zext i16 %32200 to i64
  store i64 %32202, i64* %RCX.i11580, align 8
  %32203 = add i64 %32196, %32202
  %32204 = add i64 %32189, 24
  store i64 %32204, i64* %3, align 8
  %32205 = inttoptr i64 %32203 to i8*
  %32206 = load i8, i8* %32205, align 1
  %32207 = zext i8 %32206 to i64
  store i64 %32207, i64* %RSI.i11312, align 8
  %32208 = add i64 %32187, -216
  %32209 = add i64 %32189, 31
  store i64 %32209, i64* %3, align 8
  %32210 = inttoptr i64 %32208 to i64*
  %32211 = load i64, i64* %32210, align 8
  store i64 %32211, i64* %RAX.i11582.pre-phi, align 8
  %32212 = add i64 %32189, 38
  store i64 %32212, i64* %3, align 8
  %32213 = load i16, i16* %32199, align 2
  %32214 = zext i16 %32213 to i64
  store i64 %32214, i64* %576, align 8
  %32215 = zext i16 %32213 to i64
  store i64 %32215, i64* %RCX.i11580, align 8
  %32216 = shl nuw nsw i64 %32215, 2
  %32217 = add i64 %32211, %32216
  %32218 = add i64 %32189, 43
  store i64 %32218, i64* %3, align 8
  %32219 = inttoptr i64 %32217 to i32*
  %32220 = load i32, i32* %32219, align 4
  %32221 = zext i32 %32220 to i64
  store i64 %32221, i64* %576, align 8
  %32222 = add i64 %32189, -17248
  %32223 = add i64 %32189, 48
  %32224 = load i64, i64* %6, align 8
  %32225 = add i64 %32224, -8
  %32226 = inttoptr i64 %32225 to i64*
  store i64 %32223, i64* %32226, align 8
  store i64 %32225, i64* %6, align 8
  store i64 %32222, i64* %3, align 8
  %call2_40e02b = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32222, %struct.Memory* %call2_40dfe1)
  %32227 = load i64, i64* %RBP.i, align 8
  %32228 = add i64 %32227, -120
  %32229 = load i64, i64* %3, align 8
  %32230 = add i64 %32229, 4
  store i64 %32230, i64* %3, align 8
  %32231 = inttoptr i64 %32228 to i64*
  %32232 = load i64, i64* %32231, align 8
  store i64 %32232, i64* %RAX.i11582.pre-phi, align 8
  %32233 = add i64 %32227, -28
  %32234 = add i64 %32229, 7
  store i64 %32234, i64* %3, align 8
  %32235 = inttoptr i64 %32233 to i32*
  %32236 = load i32, i32* %32235, align 4
  %32237 = add i32 %32236, 21
  %32238 = zext i32 %32237 to i64
  store i64 %32238, i64* %576, align 8
  %32239 = icmp ugt i32 %32236, -22
  %32240 = zext i1 %32239 to i8
  store i8 %32240, i8* %14, align 1
  %32241 = and i32 %32237, 255
  %32242 = tail call i32 @llvm.ctpop.i32(i32 %32241)
  %32243 = trunc i32 %32242 to i8
  %32244 = and i8 %32243, 1
  %32245 = xor i8 %32244, 1
  store i8 %32245, i8* %21, align 1
  %32246 = xor i32 %32236, 16
  %32247 = xor i32 %32246, %32237
  %32248 = lshr i32 %32247, 4
  %32249 = trunc i32 %32248 to i8
  %32250 = and i8 %32249, 1
  store i8 %32250, i8* %27, align 1
  %32251 = icmp eq i32 %32237, 0
  %32252 = zext i1 %32251 to i8
  store i8 %32252, i8* %30, align 1
  %32253 = lshr i32 %32237, 31
  %32254 = trunc i32 %32253 to i8
  store i8 %32254, i8* %33, align 1
  %32255 = lshr i32 %32236, 31
  %32256 = xor i32 %32253, %32255
  %32257 = add nuw nsw i32 %32256, %32253
  %32258 = icmp eq i32 %32257, 2
  %32259 = zext i1 %32258 to i8
  store i8 %32259, i8* %39, align 1
  %32260 = sext i32 %32237 to i64
  store i64 %32260, i64* %RCX.i11580, align 8
  %32261 = shl nsw i64 %32260, 1
  %32262 = add i64 %32232, %32261
  %32263 = add i64 %32229, 18
  store i64 %32263, i64* %3, align 8
  %32264 = inttoptr i64 %32262 to i16*
  %32265 = load i16, i16* %32264, align 2
  store i16 %32265, i16* %R9W.i2497, align 2
  %32266 = add i64 %32227, -198
  %32267 = add i64 %32229, 26
  store i64 %32267, i64* %3, align 8
  %32268 = inttoptr i64 %32266 to i16*
  store i16 %32265, i16* %32268, align 2
  %32269 = load i64, i64* %RBP.i, align 8
  %32270 = add i64 %32269, -8
  %32271 = load i64, i64* %3, align 8
  %32272 = add i64 %32271, 4
  store i64 %32272, i64* %3, align 8
  %32273 = inttoptr i64 %32270 to i64*
  %32274 = load i64, i64* %32273, align 8
  store i64 %32274, i64* %RDI.i2910, align 8
  %32275 = add i64 %32269, -208
  %32276 = add i64 %32271, 11
  store i64 %32276, i64* %3, align 8
  %32277 = inttoptr i64 %32275 to i64*
  %32278 = load i64, i64* %32277, align 8
  store i64 %32278, i64* %RAX.i11582.pre-phi, align 8
  %32279 = add i64 %32269, -198
  %32280 = add i64 %32271, 18
  store i64 %32280, i64* %3, align 8
  %32281 = inttoptr i64 %32279 to i16*
  %32282 = load i16, i16* %32281, align 2
  %32283 = zext i16 %32282 to i64
  store i64 %32283, i64* %576, align 8
  %32284 = zext i16 %32282 to i64
  store i64 %32284, i64* %RCX.i11580, align 8
  %32285 = add i64 %32278, %32284
  %32286 = add i64 %32271, 24
  store i64 %32286, i64* %3, align 8
  %32287 = inttoptr i64 %32285 to i8*
  %32288 = load i8, i8* %32287, align 1
  %32289 = zext i8 %32288 to i64
  store i64 %32289, i64* %RSI.i11312, align 8
  %32290 = add i64 %32269, -216
  %32291 = add i64 %32271, 31
  store i64 %32291, i64* %3, align 8
  %32292 = inttoptr i64 %32290 to i64*
  %32293 = load i64, i64* %32292, align 8
  store i64 %32293, i64* %RAX.i11582.pre-phi, align 8
  %32294 = add i64 %32271, 38
  store i64 %32294, i64* %3, align 8
  %32295 = load i16, i16* %32281, align 2
  %32296 = zext i16 %32295 to i64
  store i64 %32296, i64* %576, align 8
  %32297 = zext i16 %32295 to i64
  store i64 %32297, i64* %RCX.i11580, align 8
  %32298 = shl nuw nsw i64 %32297, 2
  %32299 = add i64 %32293, %32298
  %32300 = add i64 %32271, 43
  store i64 %32300, i64* %3, align 8
  %32301 = inttoptr i64 %32299 to i32*
  %32302 = load i32, i32* %32301, align 4
  %32303 = zext i32 %32302 to i64
  store i64 %32303, i64* %576, align 8
  %32304 = add i64 %32271, -17322
  %32305 = add i64 %32271, 48
  %32306 = load i64, i64* %6, align 8
  %32307 = add i64 %32306, -8
  %32308 = inttoptr i64 %32307 to i64*
  store i64 %32305, i64* %32308, align 8
  store i64 %32307, i64* %6, align 8
  store i64 %32304, i64* %3, align 8
  %call2_40e075 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32304, %struct.Memory* %call2_40e02b)
  %32309 = load i64, i64* %RBP.i, align 8
  %32310 = add i64 %32309, -120
  %32311 = load i64, i64* %3, align 8
  %32312 = add i64 %32311, 4
  store i64 %32312, i64* %3, align 8
  %32313 = inttoptr i64 %32310 to i64*
  %32314 = load i64, i64* %32313, align 8
  store i64 %32314, i64* %RAX.i11582.pre-phi, align 8
  %32315 = add i64 %32309, -28
  %32316 = add i64 %32311, 7
  store i64 %32316, i64* %3, align 8
  %32317 = inttoptr i64 %32315 to i32*
  %32318 = load i32, i32* %32317, align 4
  %32319 = add i32 %32318, 22
  %32320 = zext i32 %32319 to i64
  store i64 %32320, i64* %576, align 8
  %32321 = icmp ugt i32 %32318, -23
  %32322 = zext i1 %32321 to i8
  store i8 %32322, i8* %14, align 1
  %32323 = and i32 %32319, 255
  %32324 = tail call i32 @llvm.ctpop.i32(i32 %32323)
  %32325 = trunc i32 %32324 to i8
  %32326 = and i8 %32325, 1
  %32327 = xor i8 %32326, 1
  store i8 %32327, i8* %21, align 1
  %32328 = xor i32 %32318, 16
  %32329 = xor i32 %32328, %32319
  %32330 = lshr i32 %32329, 4
  %32331 = trunc i32 %32330 to i8
  %32332 = and i8 %32331, 1
  store i8 %32332, i8* %27, align 1
  %32333 = icmp eq i32 %32319, 0
  %32334 = zext i1 %32333 to i8
  store i8 %32334, i8* %30, align 1
  %32335 = lshr i32 %32319, 31
  %32336 = trunc i32 %32335 to i8
  store i8 %32336, i8* %33, align 1
  %32337 = lshr i32 %32318, 31
  %32338 = xor i32 %32335, %32337
  %32339 = add nuw nsw i32 %32338, %32335
  %32340 = icmp eq i32 %32339, 2
  %32341 = zext i1 %32340 to i8
  store i8 %32341, i8* %39, align 1
  %32342 = sext i32 %32319 to i64
  store i64 %32342, i64* %RCX.i11580, align 8
  %32343 = shl nsw i64 %32342, 1
  %32344 = add i64 %32314, %32343
  %32345 = add i64 %32311, 18
  store i64 %32345, i64* %3, align 8
  %32346 = inttoptr i64 %32344 to i16*
  %32347 = load i16, i16* %32346, align 2
  store i16 %32347, i16* %R9W.i2497, align 2
  %32348 = add i64 %32309, -198
  %32349 = add i64 %32311, 26
  store i64 %32349, i64* %3, align 8
  %32350 = inttoptr i64 %32348 to i16*
  store i16 %32347, i16* %32350, align 2
  %32351 = load i64, i64* %RBP.i, align 8
  %32352 = add i64 %32351, -8
  %32353 = load i64, i64* %3, align 8
  %32354 = add i64 %32353, 4
  store i64 %32354, i64* %3, align 8
  %32355 = inttoptr i64 %32352 to i64*
  %32356 = load i64, i64* %32355, align 8
  store i64 %32356, i64* %RDI.i2910, align 8
  %32357 = add i64 %32351, -208
  %32358 = add i64 %32353, 11
  store i64 %32358, i64* %3, align 8
  %32359 = inttoptr i64 %32357 to i64*
  %32360 = load i64, i64* %32359, align 8
  store i64 %32360, i64* %RAX.i11582.pre-phi, align 8
  %32361 = add i64 %32351, -198
  %32362 = add i64 %32353, 18
  store i64 %32362, i64* %3, align 8
  %32363 = inttoptr i64 %32361 to i16*
  %32364 = load i16, i16* %32363, align 2
  %32365 = zext i16 %32364 to i64
  store i64 %32365, i64* %576, align 8
  %32366 = zext i16 %32364 to i64
  store i64 %32366, i64* %RCX.i11580, align 8
  %32367 = add i64 %32360, %32366
  %32368 = add i64 %32353, 24
  store i64 %32368, i64* %3, align 8
  %32369 = inttoptr i64 %32367 to i8*
  %32370 = load i8, i8* %32369, align 1
  %32371 = zext i8 %32370 to i64
  store i64 %32371, i64* %RSI.i11312, align 8
  %32372 = add i64 %32351, -216
  %32373 = add i64 %32353, 31
  store i64 %32373, i64* %3, align 8
  %32374 = inttoptr i64 %32372 to i64*
  %32375 = load i64, i64* %32374, align 8
  store i64 %32375, i64* %RAX.i11582.pre-phi, align 8
  %32376 = add i64 %32353, 38
  store i64 %32376, i64* %3, align 8
  %32377 = load i16, i16* %32363, align 2
  %32378 = zext i16 %32377 to i64
  store i64 %32378, i64* %576, align 8
  %32379 = zext i16 %32377 to i64
  store i64 %32379, i64* %RCX.i11580, align 8
  %32380 = shl nuw nsw i64 %32379, 2
  %32381 = add i64 %32375, %32380
  %32382 = add i64 %32353, 43
  store i64 %32382, i64* %3, align 8
  %32383 = inttoptr i64 %32381 to i32*
  %32384 = load i32, i32* %32383, align 4
  %32385 = zext i32 %32384 to i64
  store i64 %32385, i64* %576, align 8
  %32386 = add i64 %32353, -17396
  %32387 = add i64 %32353, 48
  %32388 = load i64, i64* %6, align 8
  %32389 = add i64 %32388, -8
  %32390 = inttoptr i64 %32389 to i64*
  store i64 %32387, i64* %32390, align 8
  store i64 %32389, i64* %6, align 8
  store i64 %32386, i64* %3, align 8
  %call2_40e0bf = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32386, %struct.Memory* %call2_40e075)
  %32391 = load i64, i64* %RBP.i, align 8
  %32392 = add i64 %32391, -120
  %32393 = load i64, i64* %3, align 8
  %32394 = add i64 %32393, 4
  store i64 %32394, i64* %3, align 8
  %32395 = inttoptr i64 %32392 to i64*
  %32396 = load i64, i64* %32395, align 8
  store i64 %32396, i64* %RAX.i11582.pre-phi, align 8
  %32397 = add i64 %32391, -28
  %32398 = add i64 %32393, 7
  store i64 %32398, i64* %3, align 8
  %32399 = inttoptr i64 %32397 to i32*
  %32400 = load i32, i32* %32399, align 4
  %32401 = add i32 %32400, 23
  %32402 = zext i32 %32401 to i64
  store i64 %32402, i64* %576, align 8
  %32403 = icmp ugt i32 %32400, -24
  %32404 = zext i1 %32403 to i8
  store i8 %32404, i8* %14, align 1
  %32405 = and i32 %32401, 255
  %32406 = tail call i32 @llvm.ctpop.i32(i32 %32405)
  %32407 = trunc i32 %32406 to i8
  %32408 = and i8 %32407, 1
  %32409 = xor i8 %32408, 1
  store i8 %32409, i8* %21, align 1
  %32410 = xor i32 %32400, 16
  %32411 = xor i32 %32410, %32401
  %32412 = lshr i32 %32411, 4
  %32413 = trunc i32 %32412 to i8
  %32414 = and i8 %32413, 1
  store i8 %32414, i8* %27, align 1
  %32415 = icmp eq i32 %32401, 0
  %32416 = zext i1 %32415 to i8
  store i8 %32416, i8* %30, align 1
  %32417 = lshr i32 %32401, 31
  %32418 = trunc i32 %32417 to i8
  store i8 %32418, i8* %33, align 1
  %32419 = lshr i32 %32400, 31
  %32420 = xor i32 %32417, %32419
  %32421 = add nuw nsw i32 %32420, %32417
  %32422 = icmp eq i32 %32421, 2
  %32423 = zext i1 %32422 to i8
  store i8 %32423, i8* %39, align 1
  %32424 = sext i32 %32401 to i64
  store i64 %32424, i64* %RCX.i11580, align 8
  %32425 = shl nsw i64 %32424, 1
  %32426 = add i64 %32396, %32425
  %32427 = add i64 %32393, 18
  store i64 %32427, i64* %3, align 8
  %32428 = inttoptr i64 %32426 to i16*
  %32429 = load i16, i16* %32428, align 2
  store i16 %32429, i16* %R9W.i2497, align 2
  %32430 = add i64 %32391, -198
  %32431 = add i64 %32393, 26
  store i64 %32431, i64* %3, align 8
  %32432 = inttoptr i64 %32430 to i16*
  store i16 %32429, i16* %32432, align 2
  %32433 = load i64, i64* %RBP.i, align 8
  %32434 = add i64 %32433, -8
  %32435 = load i64, i64* %3, align 8
  %32436 = add i64 %32435, 4
  store i64 %32436, i64* %3, align 8
  %32437 = inttoptr i64 %32434 to i64*
  %32438 = load i64, i64* %32437, align 8
  store i64 %32438, i64* %RDI.i2910, align 8
  %32439 = add i64 %32433, -208
  %32440 = add i64 %32435, 11
  store i64 %32440, i64* %3, align 8
  %32441 = inttoptr i64 %32439 to i64*
  %32442 = load i64, i64* %32441, align 8
  store i64 %32442, i64* %RAX.i11582.pre-phi, align 8
  %32443 = add i64 %32433, -198
  %32444 = add i64 %32435, 18
  store i64 %32444, i64* %3, align 8
  %32445 = inttoptr i64 %32443 to i16*
  %32446 = load i16, i16* %32445, align 2
  %32447 = zext i16 %32446 to i64
  store i64 %32447, i64* %576, align 8
  %32448 = zext i16 %32446 to i64
  store i64 %32448, i64* %RCX.i11580, align 8
  %32449 = add i64 %32442, %32448
  %32450 = add i64 %32435, 24
  store i64 %32450, i64* %3, align 8
  %32451 = inttoptr i64 %32449 to i8*
  %32452 = load i8, i8* %32451, align 1
  %32453 = zext i8 %32452 to i64
  store i64 %32453, i64* %RSI.i11312, align 8
  %32454 = add i64 %32433, -216
  %32455 = add i64 %32435, 31
  store i64 %32455, i64* %3, align 8
  %32456 = inttoptr i64 %32454 to i64*
  %32457 = load i64, i64* %32456, align 8
  store i64 %32457, i64* %RAX.i11582.pre-phi, align 8
  %32458 = add i64 %32435, 38
  store i64 %32458, i64* %3, align 8
  %32459 = load i16, i16* %32445, align 2
  %32460 = zext i16 %32459 to i64
  store i64 %32460, i64* %576, align 8
  %32461 = zext i16 %32459 to i64
  store i64 %32461, i64* %RCX.i11580, align 8
  %32462 = shl nuw nsw i64 %32461, 2
  %32463 = add i64 %32457, %32462
  %32464 = add i64 %32435, 43
  store i64 %32464, i64* %3, align 8
  %32465 = inttoptr i64 %32463 to i32*
  %32466 = load i32, i32* %32465, align 4
  %32467 = zext i32 %32466 to i64
  store i64 %32467, i64* %576, align 8
  %32468 = add i64 %32435, -17470
  %32469 = add i64 %32435, 48
  %32470 = load i64, i64* %6, align 8
  %32471 = add i64 %32470, -8
  %32472 = inttoptr i64 %32471 to i64*
  store i64 %32469, i64* %32472, align 8
  store i64 %32471, i64* %6, align 8
  store i64 %32468, i64* %3, align 8
  %call2_40e109 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32468, %struct.Memory* %call2_40e0bf)
  %32473 = load i64, i64* %RBP.i, align 8
  %32474 = add i64 %32473, -120
  %32475 = load i64, i64* %3, align 8
  %32476 = add i64 %32475, 4
  store i64 %32476, i64* %3, align 8
  %32477 = inttoptr i64 %32474 to i64*
  %32478 = load i64, i64* %32477, align 8
  store i64 %32478, i64* %RAX.i11582.pre-phi, align 8
  %32479 = add i64 %32473, -28
  %32480 = add i64 %32475, 7
  store i64 %32480, i64* %3, align 8
  %32481 = inttoptr i64 %32479 to i32*
  %32482 = load i32, i32* %32481, align 4
  %32483 = add i32 %32482, 24
  %32484 = zext i32 %32483 to i64
  store i64 %32484, i64* %576, align 8
  %32485 = icmp ugt i32 %32482, -25
  %32486 = zext i1 %32485 to i8
  store i8 %32486, i8* %14, align 1
  %32487 = and i32 %32483, 255
  %32488 = tail call i32 @llvm.ctpop.i32(i32 %32487)
  %32489 = trunc i32 %32488 to i8
  %32490 = and i8 %32489, 1
  %32491 = xor i8 %32490, 1
  store i8 %32491, i8* %21, align 1
  %32492 = xor i32 %32482, 16
  %32493 = xor i32 %32492, %32483
  %32494 = lshr i32 %32493, 4
  %32495 = trunc i32 %32494 to i8
  %32496 = and i8 %32495, 1
  store i8 %32496, i8* %27, align 1
  %32497 = icmp eq i32 %32483, 0
  %32498 = zext i1 %32497 to i8
  store i8 %32498, i8* %30, align 1
  %32499 = lshr i32 %32483, 31
  %32500 = trunc i32 %32499 to i8
  store i8 %32500, i8* %33, align 1
  %32501 = lshr i32 %32482, 31
  %32502 = xor i32 %32499, %32501
  %32503 = add nuw nsw i32 %32502, %32499
  %32504 = icmp eq i32 %32503, 2
  %32505 = zext i1 %32504 to i8
  store i8 %32505, i8* %39, align 1
  %32506 = sext i32 %32483 to i64
  store i64 %32506, i64* %RCX.i11580, align 8
  %32507 = shl nsw i64 %32506, 1
  %32508 = add i64 %32478, %32507
  %32509 = add i64 %32475, 18
  store i64 %32509, i64* %3, align 8
  %32510 = inttoptr i64 %32508 to i16*
  %32511 = load i16, i16* %32510, align 2
  store i16 %32511, i16* %R9W.i2497, align 2
  %32512 = add i64 %32473, -198
  %32513 = add i64 %32475, 26
  store i64 %32513, i64* %3, align 8
  %32514 = inttoptr i64 %32512 to i16*
  store i16 %32511, i16* %32514, align 2
  %32515 = load i64, i64* %RBP.i, align 8
  %32516 = add i64 %32515, -8
  %32517 = load i64, i64* %3, align 8
  %32518 = add i64 %32517, 4
  store i64 %32518, i64* %3, align 8
  %32519 = inttoptr i64 %32516 to i64*
  %32520 = load i64, i64* %32519, align 8
  store i64 %32520, i64* %RDI.i2910, align 8
  %32521 = add i64 %32515, -208
  %32522 = add i64 %32517, 11
  store i64 %32522, i64* %3, align 8
  %32523 = inttoptr i64 %32521 to i64*
  %32524 = load i64, i64* %32523, align 8
  store i64 %32524, i64* %RAX.i11582.pre-phi, align 8
  %32525 = add i64 %32515, -198
  %32526 = add i64 %32517, 18
  store i64 %32526, i64* %3, align 8
  %32527 = inttoptr i64 %32525 to i16*
  %32528 = load i16, i16* %32527, align 2
  %32529 = zext i16 %32528 to i64
  store i64 %32529, i64* %576, align 8
  %32530 = zext i16 %32528 to i64
  store i64 %32530, i64* %RCX.i11580, align 8
  %32531 = add i64 %32524, %32530
  %32532 = add i64 %32517, 24
  store i64 %32532, i64* %3, align 8
  %32533 = inttoptr i64 %32531 to i8*
  %32534 = load i8, i8* %32533, align 1
  %32535 = zext i8 %32534 to i64
  store i64 %32535, i64* %RSI.i11312, align 8
  %32536 = add i64 %32515, -216
  %32537 = add i64 %32517, 31
  store i64 %32537, i64* %3, align 8
  %32538 = inttoptr i64 %32536 to i64*
  %32539 = load i64, i64* %32538, align 8
  store i64 %32539, i64* %RAX.i11582.pre-phi, align 8
  %32540 = add i64 %32517, 38
  store i64 %32540, i64* %3, align 8
  %32541 = load i16, i16* %32527, align 2
  %32542 = zext i16 %32541 to i64
  store i64 %32542, i64* %576, align 8
  %32543 = zext i16 %32541 to i64
  store i64 %32543, i64* %RCX.i11580, align 8
  %32544 = shl nuw nsw i64 %32543, 2
  %32545 = add i64 %32539, %32544
  %32546 = add i64 %32517, 43
  store i64 %32546, i64* %3, align 8
  %32547 = inttoptr i64 %32545 to i32*
  %32548 = load i32, i32* %32547, align 4
  %32549 = zext i32 %32548 to i64
  store i64 %32549, i64* %576, align 8
  %32550 = add i64 %32517, -17544
  %32551 = add i64 %32517, 48
  %32552 = load i64, i64* %6, align 8
  %32553 = add i64 %32552, -8
  %32554 = inttoptr i64 %32553 to i64*
  store i64 %32551, i64* %32554, align 8
  store i64 %32553, i64* %6, align 8
  store i64 %32550, i64* %3, align 8
  %call2_40e153 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32550, %struct.Memory* %call2_40e109)
  %32555 = load i64, i64* %RBP.i, align 8
  %32556 = add i64 %32555, -120
  %32557 = load i64, i64* %3, align 8
  %32558 = add i64 %32557, 4
  store i64 %32558, i64* %3, align 8
  %32559 = inttoptr i64 %32556 to i64*
  %32560 = load i64, i64* %32559, align 8
  store i64 %32560, i64* %RAX.i11582.pre-phi, align 8
  %32561 = add i64 %32555, -28
  %32562 = add i64 %32557, 7
  store i64 %32562, i64* %3, align 8
  %32563 = inttoptr i64 %32561 to i32*
  %32564 = load i32, i32* %32563, align 4
  %32565 = add i32 %32564, 25
  %32566 = zext i32 %32565 to i64
  store i64 %32566, i64* %576, align 8
  %32567 = icmp ugt i32 %32564, -26
  %32568 = zext i1 %32567 to i8
  store i8 %32568, i8* %14, align 1
  %32569 = and i32 %32565, 255
  %32570 = tail call i32 @llvm.ctpop.i32(i32 %32569)
  %32571 = trunc i32 %32570 to i8
  %32572 = and i8 %32571, 1
  %32573 = xor i8 %32572, 1
  store i8 %32573, i8* %21, align 1
  %32574 = xor i32 %32564, 16
  %32575 = xor i32 %32574, %32565
  %32576 = lshr i32 %32575, 4
  %32577 = trunc i32 %32576 to i8
  %32578 = and i8 %32577, 1
  store i8 %32578, i8* %27, align 1
  %32579 = icmp eq i32 %32565, 0
  %32580 = zext i1 %32579 to i8
  store i8 %32580, i8* %30, align 1
  %32581 = lshr i32 %32565, 31
  %32582 = trunc i32 %32581 to i8
  store i8 %32582, i8* %33, align 1
  %32583 = lshr i32 %32564, 31
  %32584 = xor i32 %32581, %32583
  %32585 = add nuw nsw i32 %32584, %32581
  %32586 = icmp eq i32 %32585, 2
  %32587 = zext i1 %32586 to i8
  store i8 %32587, i8* %39, align 1
  %32588 = sext i32 %32565 to i64
  store i64 %32588, i64* %RCX.i11580, align 8
  %32589 = shl nsw i64 %32588, 1
  %32590 = add i64 %32560, %32589
  %32591 = add i64 %32557, 18
  store i64 %32591, i64* %3, align 8
  %32592 = inttoptr i64 %32590 to i16*
  %32593 = load i16, i16* %32592, align 2
  store i16 %32593, i16* %R9W.i2497, align 2
  %32594 = add i64 %32555, -198
  %32595 = add i64 %32557, 26
  store i64 %32595, i64* %3, align 8
  %32596 = inttoptr i64 %32594 to i16*
  store i16 %32593, i16* %32596, align 2
  %32597 = load i64, i64* %RBP.i, align 8
  %32598 = add i64 %32597, -8
  %32599 = load i64, i64* %3, align 8
  %32600 = add i64 %32599, 4
  store i64 %32600, i64* %3, align 8
  %32601 = inttoptr i64 %32598 to i64*
  %32602 = load i64, i64* %32601, align 8
  store i64 %32602, i64* %RDI.i2910, align 8
  %32603 = add i64 %32597, -208
  %32604 = add i64 %32599, 11
  store i64 %32604, i64* %3, align 8
  %32605 = inttoptr i64 %32603 to i64*
  %32606 = load i64, i64* %32605, align 8
  store i64 %32606, i64* %RAX.i11582.pre-phi, align 8
  %32607 = add i64 %32597, -198
  %32608 = add i64 %32599, 18
  store i64 %32608, i64* %3, align 8
  %32609 = inttoptr i64 %32607 to i16*
  %32610 = load i16, i16* %32609, align 2
  %32611 = zext i16 %32610 to i64
  store i64 %32611, i64* %576, align 8
  %32612 = zext i16 %32610 to i64
  store i64 %32612, i64* %RCX.i11580, align 8
  %32613 = add i64 %32606, %32612
  %32614 = add i64 %32599, 24
  store i64 %32614, i64* %3, align 8
  %32615 = inttoptr i64 %32613 to i8*
  %32616 = load i8, i8* %32615, align 1
  %32617 = zext i8 %32616 to i64
  store i64 %32617, i64* %RSI.i11312, align 8
  %32618 = add i64 %32597, -216
  %32619 = add i64 %32599, 31
  store i64 %32619, i64* %3, align 8
  %32620 = inttoptr i64 %32618 to i64*
  %32621 = load i64, i64* %32620, align 8
  store i64 %32621, i64* %RAX.i11582.pre-phi, align 8
  %32622 = add i64 %32599, 38
  store i64 %32622, i64* %3, align 8
  %32623 = load i16, i16* %32609, align 2
  %32624 = zext i16 %32623 to i64
  store i64 %32624, i64* %576, align 8
  %32625 = zext i16 %32623 to i64
  store i64 %32625, i64* %RCX.i11580, align 8
  %32626 = shl nuw nsw i64 %32625, 2
  %32627 = add i64 %32621, %32626
  %32628 = add i64 %32599, 43
  store i64 %32628, i64* %3, align 8
  %32629 = inttoptr i64 %32627 to i32*
  %32630 = load i32, i32* %32629, align 4
  %32631 = zext i32 %32630 to i64
  store i64 %32631, i64* %576, align 8
  %32632 = add i64 %32599, -17618
  %32633 = add i64 %32599, 48
  %32634 = load i64, i64* %6, align 8
  %32635 = add i64 %32634, -8
  %32636 = inttoptr i64 %32635 to i64*
  store i64 %32633, i64* %32636, align 8
  store i64 %32635, i64* %6, align 8
  store i64 %32632, i64* %3, align 8
  %call2_40e19d = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32632, %struct.Memory* %call2_40e153)
  %32637 = load i64, i64* %RBP.i, align 8
  %32638 = add i64 %32637, -120
  %32639 = load i64, i64* %3, align 8
  %32640 = add i64 %32639, 4
  store i64 %32640, i64* %3, align 8
  %32641 = inttoptr i64 %32638 to i64*
  %32642 = load i64, i64* %32641, align 8
  store i64 %32642, i64* %RAX.i11582.pre-phi, align 8
  %32643 = add i64 %32637, -28
  %32644 = add i64 %32639, 7
  store i64 %32644, i64* %3, align 8
  %32645 = inttoptr i64 %32643 to i32*
  %32646 = load i32, i32* %32645, align 4
  %32647 = add i32 %32646, 26
  %32648 = zext i32 %32647 to i64
  store i64 %32648, i64* %576, align 8
  %32649 = icmp ugt i32 %32646, -27
  %32650 = zext i1 %32649 to i8
  store i8 %32650, i8* %14, align 1
  %32651 = and i32 %32647, 255
  %32652 = tail call i32 @llvm.ctpop.i32(i32 %32651)
  %32653 = trunc i32 %32652 to i8
  %32654 = and i8 %32653, 1
  %32655 = xor i8 %32654, 1
  store i8 %32655, i8* %21, align 1
  %32656 = xor i32 %32646, 16
  %32657 = xor i32 %32656, %32647
  %32658 = lshr i32 %32657, 4
  %32659 = trunc i32 %32658 to i8
  %32660 = and i8 %32659, 1
  store i8 %32660, i8* %27, align 1
  %32661 = icmp eq i32 %32647, 0
  %32662 = zext i1 %32661 to i8
  store i8 %32662, i8* %30, align 1
  %32663 = lshr i32 %32647, 31
  %32664 = trunc i32 %32663 to i8
  store i8 %32664, i8* %33, align 1
  %32665 = lshr i32 %32646, 31
  %32666 = xor i32 %32663, %32665
  %32667 = add nuw nsw i32 %32666, %32663
  %32668 = icmp eq i32 %32667, 2
  %32669 = zext i1 %32668 to i8
  store i8 %32669, i8* %39, align 1
  %32670 = sext i32 %32647 to i64
  store i64 %32670, i64* %RCX.i11580, align 8
  %32671 = shl nsw i64 %32670, 1
  %32672 = add i64 %32642, %32671
  %32673 = add i64 %32639, 18
  store i64 %32673, i64* %3, align 8
  %32674 = inttoptr i64 %32672 to i16*
  %32675 = load i16, i16* %32674, align 2
  store i16 %32675, i16* %R9W.i2497, align 2
  %32676 = add i64 %32637, -198
  %32677 = add i64 %32639, 26
  store i64 %32677, i64* %3, align 8
  %32678 = inttoptr i64 %32676 to i16*
  store i16 %32675, i16* %32678, align 2
  %32679 = load i64, i64* %RBP.i, align 8
  %32680 = add i64 %32679, -8
  %32681 = load i64, i64* %3, align 8
  %32682 = add i64 %32681, 4
  store i64 %32682, i64* %3, align 8
  %32683 = inttoptr i64 %32680 to i64*
  %32684 = load i64, i64* %32683, align 8
  store i64 %32684, i64* %RDI.i2910, align 8
  %32685 = add i64 %32679, -208
  %32686 = add i64 %32681, 11
  store i64 %32686, i64* %3, align 8
  %32687 = inttoptr i64 %32685 to i64*
  %32688 = load i64, i64* %32687, align 8
  store i64 %32688, i64* %RAX.i11582.pre-phi, align 8
  %32689 = add i64 %32679, -198
  %32690 = add i64 %32681, 18
  store i64 %32690, i64* %3, align 8
  %32691 = inttoptr i64 %32689 to i16*
  %32692 = load i16, i16* %32691, align 2
  %32693 = zext i16 %32692 to i64
  store i64 %32693, i64* %576, align 8
  %32694 = zext i16 %32692 to i64
  store i64 %32694, i64* %RCX.i11580, align 8
  %32695 = add i64 %32688, %32694
  %32696 = add i64 %32681, 24
  store i64 %32696, i64* %3, align 8
  %32697 = inttoptr i64 %32695 to i8*
  %32698 = load i8, i8* %32697, align 1
  %32699 = zext i8 %32698 to i64
  store i64 %32699, i64* %RSI.i11312, align 8
  %32700 = add i64 %32679, -216
  %32701 = add i64 %32681, 31
  store i64 %32701, i64* %3, align 8
  %32702 = inttoptr i64 %32700 to i64*
  %32703 = load i64, i64* %32702, align 8
  store i64 %32703, i64* %RAX.i11582.pre-phi, align 8
  %32704 = add i64 %32681, 38
  store i64 %32704, i64* %3, align 8
  %32705 = load i16, i16* %32691, align 2
  %32706 = zext i16 %32705 to i64
  store i64 %32706, i64* %576, align 8
  %32707 = zext i16 %32705 to i64
  store i64 %32707, i64* %RCX.i11580, align 8
  %32708 = shl nuw nsw i64 %32707, 2
  %32709 = add i64 %32703, %32708
  %32710 = add i64 %32681, 43
  store i64 %32710, i64* %3, align 8
  %32711 = inttoptr i64 %32709 to i32*
  %32712 = load i32, i32* %32711, align 4
  %32713 = zext i32 %32712 to i64
  store i64 %32713, i64* %576, align 8
  %32714 = add i64 %32681, -17692
  %32715 = add i64 %32681, 48
  %32716 = load i64, i64* %6, align 8
  %32717 = add i64 %32716, -8
  %32718 = inttoptr i64 %32717 to i64*
  store i64 %32715, i64* %32718, align 8
  store i64 %32717, i64* %6, align 8
  store i64 %32714, i64* %3, align 8
  %call2_40e1e7 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32714, %struct.Memory* %call2_40e19d)
  %32719 = load i64, i64* %RBP.i, align 8
  %32720 = add i64 %32719, -120
  %32721 = load i64, i64* %3, align 8
  %32722 = add i64 %32721, 4
  store i64 %32722, i64* %3, align 8
  %32723 = inttoptr i64 %32720 to i64*
  %32724 = load i64, i64* %32723, align 8
  store i64 %32724, i64* %RAX.i11582.pre-phi, align 8
  %32725 = add i64 %32719, -28
  %32726 = add i64 %32721, 7
  store i64 %32726, i64* %3, align 8
  %32727 = inttoptr i64 %32725 to i32*
  %32728 = load i32, i32* %32727, align 4
  %32729 = add i32 %32728, 27
  %32730 = zext i32 %32729 to i64
  store i64 %32730, i64* %576, align 8
  %32731 = icmp ugt i32 %32728, -28
  %32732 = zext i1 %32731 to i8
  store i8 %32732, i8* %14, align 1
  %32733 = and i32 %32729, 255
  %32734 = tail call i32 @llvm.ctpop.i32(i32 %32733)
  %32735 = trunc i32 %32734 to i8
  %32736 = and i8 %32735, 1
  %32737 = xor i8 %32736, 1
  store i8 %32737, i8* %21, align 1
  %32738 = xor i32 %32728, 16
  %32739 = xor i32 %32738, %32729
  %32740 = lshr i32 %32739, 4
  %32741 = trunc i32 %32740 to i8
  %32742 = and i8 %32741, 1
  store i8 %32742, i8* %27, align 1
  %32743 = icmp eq i32 %32729, 0
  %32744 = zext i1 %32743 to i8
  store i8 %32744, i8* %30, align 1
  %32745 = lshr i32 %32729, 31
  %32746 = trunc i32 %32745 to i8
  store i8 %32746, i8* %33, align 1
  %32747 = lshr i32 %32728, 31
  %32748 = xor i32 %32745, %32747
  %32749 = add nuw nsw i32 %32748, %32745
  %32750 = icmp eq i32 %32749, 2
  %32751 = zext i1 %32750 to i8
  store i8 %32751, i8* %39, align 1
  %32752 = sext i32 %32729 to i64
  store i64 %32752, i64* %RCX.i11580, align 8
  %32753 = shl nsw i64 %32752, 1
  %32754 = add i64 %32724, %32753
  %32755 = add i64 %32721, 18
  store i64 %32755, i64* %3, align 8
  %32756 = inttoptr i64 %32754 to i16*
  %32757 = load i16, i16* %32756, align 2
  store i16 %32757, i16* %R9W.i2497, align 2
  %32758 = add i64 %32719, -198
  %32759 = add i64 %32721, 26
  store i64 %32759, i64* %3, align 8
  %32760 = inttoptr i64 %32758 to i16*
  store i16 %32757, i16* %32760, align 2
  %32761 = load i64, i64* %RBP.i, align 8
  %32762 = add i64 %32761, -8
  %32763 = load i64, i64* %3, align 8
  %32764 = add i64 %32763, 4
  store i64 %32764, i64* %3, align 8
  %32765 = inttoptr i64 %32762 to i64*
  %32766 = load i64, i64* %32765, align 8
  store i64 %32766, i64* %RDI.i2910, align 8
  %32767 = add i64 %32761, -208
  %32768 = add i64 %32763, 11
  store i64 %32768, i64* %3, align 8
  %32769 = inttoptr i64 %32767 to i64*
  %32770 = load i64, i64* %32769, align 8
  store i64 %32770, i64* %RAX.i11582.pre-phi, align 8
  %32771 = add i64 %32761, -198
  %32772 = add i64 %32763, 18
  store i64 %32772, i64* %3, align 8
  %32773 = inttoptr i64 %32771 to i16*
  %32774 = load i16, i16* %32773, align 2
  %32775 = zext i16 %32774 to i64
  store i64 %32775, i64* %576, align 8
  %32776 = zext i16 %32774 to i64
  store i64 %32776, i64* %RCX.i11580, align 8
  %32777 = add i64 %32770, %32776
  %32778 = add i64 %32763, 24
  store i64 %32778, i64* %3, align 8
  %32779 = inttoptr i64 %32777 to i8*
  %32780 = load i8, i8* %32779, align 1
  %32781 = zext i8 %32780 to i64
  store i64 %32781, i64* %RSI.i11312, align 8
  %32782 = add i64 %32761, -216
  %32783 = add i64 %32763, 31
  store i64 %32783, i64* %3, align 8
  %32784 = inttoptr i64 %32782 to i64*
  %32785 = load i64, i64* %32784, align 8
  store i64 %32785, i64* %RAX.i11582.pre-phi, align 8
  %32786 = add i64 %32763, 38
  store i64 %32786, i64* %3, align 8
  %32787 = load i16, i16* %32773, align 2
  %32788 = zext i16 %32787 to i64
  store i64 %32788, i64* %576, align 8
  %32789 = zext i16 %32787 to i64
  store i64 %32789, i64* %RCX.i11580, align 8
  %32790 = shl nuw nsw i64 %32789, 2
  %32791 = add i64 %32785, %32790
  %32792 = add i64 %32763, 43
  store i64 %32792, i64* %3, align 8
  %32793 = inttoptr i64 %32791 to i32*
  %32794 = load i32, i32* %32793, align 4
  %32795 = zext i32 %32794 to i64
  store i64 %32795, i64* %576, align 8
  %32796 = add i64 %32763, -17766
  %32797 = add i64 %32763, 48
  %32798 = load i64, i64* %6, align 8
  %32799 = add i64 %32798, -8
  %32800 = inttoptr i64 %32799 to i64*
  store i64 %32797, i64* %32800, align 8
  store i64 %32799, i64* %6, align 8
  store i64 %32796, i64* %3, align 8
  %call2_40e231 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32796, %struct.Memory* %call2_40e1e7)
  %32801 = load i64, i64* %RBP.i, align 8
  %32802 = add i64 %32801, -120
  %32803 = load i64, i64* %3, align 8
  %32804 = add i64 %32803, 4
  store i64 %32804, i64* %3, align 8
  %32805 = inttoptr i64 %32802 to i64*
  %32806 = load i64, i64* %32805, align 8
  store i64 %32806, i64* %RAX.i11582.pre-phi, align 8
  %32807 = add i64 %32801, -28
  %32808 = add i64 %32803, 7
  store i64 %32808, i64* %3, align 8
  %32809 = inttoptr i64 %32807 to i32*
  %32810 = load i32, i32* %32809, align 4
  %32811 = add i32 %32810, 28
  %32812 = zext i32 %32811 to i64
  store i64 %32812, i64* %576, align 8
  %32813 = icmp ugt i32 %32810, -29
  %32814 = zext i1 %32813 to i8
  store i8 %32814, i8* %14, align 1
  %32815 = and i32 %32811, 255
  %32816 = tail call i32 @llvm.ctpop.i32(i32 %32815)
  %32817 = trunc i32 %32816 to i8
  %32818 = and i8 %32817, 1
  %32819 = xor i8 %32818, 1
  store i8 %32819, i8* %21, align 1
  %32820 = xor i32 %32810, 16
  %32821 = xor i32 %32820, %32811
  %32822 = lshr i32 %32821, 4
  %32823 = trunc i32 %32822 to i8
  %32824 = and i8 %32823, 1
  store i8 %32824, i8* %27, align 1
  %32825 = icmp eq i32 %32811, 0
  %32826 = zext i1 %32825 to i8
  store i8 %32826, i8* %30, align 1
  %32827 = lshr i32 %32811, 31
  %32828 = trunc i32 %32827 to i8
  store i8 %32828, i8* %33, align 1
  %32829 = lshr i32 %32810, 31
  %32830 = xor i32 %32827, %32829
  %32831 = add nuw nsw i32 %32830, %32827
  %32832 = icmp eq i32 %32831, 2
  %32833 = zext i1 %32832 to i8
  store i8 %32833, i8* %39, align 1
  %32834 = sext i32 %32811 to i64
  store i64 %32834, i64* %RCX.i11580, align 8
  %32835 = shl nsw i64 %32834, 1
  %32836 = add i64 %32806, %32835
  %32837 = add i64 %32803, 18
  store i64 %32837, i64* %3, align 8
  %32838 = inttoptr i64 %32836 to i16*
  %32839 = load i16, i16* %32838, align 2
  store i16 %32839, i16* %R9W.i2497, align 2
  %32840 = add i64 %32801, -198
  %32841 = add i64 %32803, 26
  store i64 %32841, i64* %3, align 8
  %32842 = inttoptr i64 %32840 to i16*
  store i16 %32839, i16* %32842, align 2
  %32843 = load i64, i64* %RBP.i, align 8
  %32844 = add i64 %32843, -8
  %32845 = load i64, i64* %3, align 8
  %32846 = add i64 %32845, 4
  store i64 %32846, i64* %3, align 8
  %32847 = inttoptr i64 %32844 to i64*
  %32848 = load i64, i64* %32847, align 8
  store i64 %32848, i64* %RDI.i2910, align 8
  %32849 = add i64 %32843, -208
  %32850 = add i64 %32845, 11
  store i64 %32850, i64* %3, align 8
  %32851 = inttoptr i64 %32849 to i64*
  %32852 = load i64, i64* %32851, align 8
  store i64 %32852, i64* %RAX.i11582.pre-phi, align 8
  %32853 = add i64 %32843, -198
  %32854 = add i64 %32845, 18
  store i64 %32854, i64* %3, align 8
  %32855 = inttoptr i64 %32853 to i16*
  %32856 = load i16, i16* %32855, align 2
  %32857 = zext i16 %32856 to i64
  store i64 %32857, i64* %576, align 8
  %32858 = zext i16 %32856 to i64
  store i64 %32858, i64* %RCX.i11580, align 8
  %32859 = add i64 %32852, %32858
  %32860 = add i64 %32845, 24
  store i64 %32860, i64* %3, align 8
  %32861 = inttoptr i64 %32859 to i8*
  %32862 = load i8, i8* %32861, align 1
  %32863 = zext i8 %32862 to i64
  store i64 %32863, i64* %RSI.i11312, align 8
  %32864 = add i64 %32843, -216
  %32865 = add i64 %32845, 31
  store i64 %32865, i64* %3, align 8
  %32866 = inttoptr i64 %32864 to i64*
  %32867 = load i64, i64* %32866, align 8
  store i64 %32867, i64* %RAX.i11582.pre-phi, align 8
  %32868 = add i64 %32845, 38
  store i64 %32868, i64* %3, align 8
  %32869 = load i16, i16* %32855, align 2
  %32870 = zext i16 %32869 to i64
  store i64 %32870, i64* %576, align 8
  %32871 = zext i16 %32869 to i64
  store i64 %32871, i64* %RCX.i11580, align 8
  %32872 = shl nuw nsw i64 %32871, 2
  %32873 = add i64 %32867, %32872
  %32874 = add i64 %32845, 43
  store i64 %32874, i64* %3, align 8
  %32875 = inttoptr i64 %32873 to i32*
  %32876 = load i32, i32* %32875, align 4
  %32877 = zext i32 %32876 to i64
  store i64 %32877, i64* %576, align 8
  %32878 = add i64 %32845, -17840
  %32879 = add i64 %32845, 48
  %32880 = load i64, i64* %6, align 8
  %32881 = add i64 %32880, -8
  %32882 = inttoptr i64 %32881 to i64*
  store i64 %32879, i64* %32882, align 8
  store i64 %32881, i64* %6, align 8
  store i64 %32878, i64* %3, align 8
  %call2_40e27b = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32878, %struct.Memory* %call2_40e231)
  %32883 = load i64, i64* %RBP.i, align 8
  %32884 = add i64 %32883, -120
  %32885 = load i64, i64* %3, align 8
  %32886 = add i64 %32885, 4
  store i64 %32886, i64* %3, align 8
  %32887 = inttoptr i64 %32884 to i64*
  %32888 = load i64, i64* %32887, align 8
  store i64 %32888, i64* %RAX.i11582.pre-phi, align 8
  %32889 = add i64 %32883, -28
  %32890 = add i64 %32885, 7
  store i64 %32890, i64* %3, align 8
  %32891 = inttoptr i64 %32889 to i32*
  %32892 = load i32, i32* %32891, align 4
  %32893 = add i32 %32892, 29
  %32894 = zext i32 %32893 to i64
  store i64 %32894, i64* %576, align 8
  %32895 = icmp ugt i32 %32892, -30
  %32896 = zext i1 %32895 to i8
  store i8 %32896, i8* %14, align 1
  %32897 = and i32 %32893, 255
  %32898 = tail call i32 @llvm.ctpop.i32(i32 %32897)
  %32899 = trunc i32 %32898 to i8
  %32900 = and i8 %32899, 1
  %32901 = xor i8 %32900, 1
  store i8 %32901, i8* %21, align 1
  %32902 = xor i32 %32892, 16
  %32903 = xor i32 %32902, %32893
  %32904 = lshr i32 %32903, 4
  %32905 = trunc i32 %32904 to i8
  %32906 = and i8 %32905, 1
  store i8 %32906, i8* %27, align 1
  %32907 = icmp eq i32 %32893, 0
  %32908 = zext i1 %32907 to i8
  store i8 %32908, i8* %30, align 1
  %32909 = lshr i32 %32893, 31
  %32910 = trunc i32 %32909 to i8
  store i8 %32910, i8* %33, align 1
  %32911 = lshr i32 %32892, 31
  %32912 = xor i32 %32909, %32911
  %32913 = add nuw nsw i32 %32912, %32909
  %32914 = icmp eq i32 %32913, 2
  %32915 = zext i1 %32914 to i8
  store i8 %32915, i8* %39, align 1
  %32916 = sext i32 %32893 to i64
  store i64 %32916, i64* %RCX.i11580, align 8
  %32917 = shl nsw i64 %32916, 1
  %32918 = add i64 %32888, %32917
  %32919 = add i64 %32885, 18
  store i64 %32919, i64* %3, align 8
  %32920 = inttoptr i64 %32918 to i16*
  %32921 = load i16, i16* %32920, align 2
  store i16 %32921, i16* %R9W.i2497, align 2
  %32922 = add i64 %32883, -198
  %32923 = add i64 %32885, 26
  store i64 %32923, i64* %3, align 8
  %32924 = inttoptr i64 %32922 to i16*
  store i16 %32921, i16* %32924, align 2
  %32925 = load i64, i64* %RBP.i, align 8
  %32926 = add i64 %32925, -8
  %32927 = load i64, i64* %3, align 8
  %32928 = add i64 %32927, 4
  store i64 %32928, i64* %3, align 8
  %32929 = inttoptr i64 %32926 to i64*
  %32930 = load i64, i64* %32929, align 8
  store i64 %32930, i64* %RDI.i2910, align 8
  %32931 = add i64 %32925, -208
  %32932 = add i64 %32927, 11
  store i64 %32932, i64* %3, align 8
  %32933 = inttoptr i64 %32931 to i64*
  %32934 = load i64, i64* %32933, align 8
  store i64 %32934, i64* %RAX.i11582.pre-phi, align 8
  %32935 = add i64 %32925, -198
  %32936 = add i64 %32927, 18
  store i64 %32936, i64* %3, align 8
  %32937 = inttoptr i64 %32935 to i16*
  %32938 = load i16, i16* %32937, align 2
  %32939 = zext i16 %32938 to i64
  store i64 %32939, i64* %576, align 8
  %32940 = zext i16 %32938 to i64
  store i64 %32940, i64* %RCX.i11580, align 8
  %32941 = add i64 %32934, %32940
  %32942 = add i64 %32927, 24
  store i64 %32942, i64* %3, align 8
  %32943 = inttoptr i64 %32941 to i8*
  %32944 = load i8, i8* %32943, align 1
  %32945 = zext i8 %32944 to i64
  store i64 %32945, i64* %RSI.i11312, align 8
  %32946 = add i64 %32925, -216
  %32947 = add i64 %32927, 31
  store i64 %32947, i64* %3, align 8
  %32948 = inttoptr i64 %32946 to i64*
  %32949 = load i64, i64* %32948, align 8
  store i64 %32949, i64* %RAX.i11582.pre-phi, align 8
  %32950 = add i64 %32927, 38
  store i64 %32950, i64* %3, align 8
  %32951 = load i16, i16* %32937, align 2
  %32952 = zext i16 %32951 to i64
  store i64 %32952, i64* %576, align 8
  %32953 = zext i16 %32951 to i64
  store i64 %32953, i64* %RCX.i11580, align 8
  %32954 = shl nuw nsw i64 %32953, 2
  %32955 = add i64 %32949, %32954
  %32956 = add i64 %32927, 43
  store i64 %32956, i64* %3, align 8
  %32957 = inttoptr i64 %32955 to i32*
  %32958 = load i32, i32* %32957, align 4
  %32959 = zext i32 %32958 to i64
  store i64 %32959, i64* %576, align 8
  %32960 = add i64 %32927, -17914
  %32961 = add i64 %32927, 48
  %32962 = load i64, i64* %6, align 8
  %32963 = add i64 %32962, -8
  %32964 = inttoptr i64 %32963 to i64*
  store i64 %32961, i64* %32964, align 8
  store i64 %32963, i64* %6, align 8
  store i64 %32960, i64* %3, align 8
  %call2_40e2c5 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %32960, %struct.Memory* %call2_40e27b)
  %32965 = load i64, i64* %RBP.i, align 8
  %32966 = add i64 %32965, -120
  %32967 = load i64, i64* %3, align 8
  %32968 = add i64 %32967, 4
  store i64 %32968, i64* %3, align 8
  %32969 = inttoptr i64 %32966 to i64*
  %32970 = load i64, i64* %32969, align 8
  store i64 %32970, i64* %RAX.i11582.pre-phi, align 8
  %32971 = add i64 %32965, -28
  %32972 = add i64 %32967, 7
  store i64 %32972, i64* %3, align 8
  %32973 = inttoptr i64 %32971 to i32*
  %32974 = load i32, i32* %32973, align 4
  %32975 = add i32 %32974, 30
  %32976 = zext i32 %32975 to i64
  store i64 %32976, i64* %576, align 8
  %32977 = icmp ugt i32 %32974, -31
  %32978 = zext i1 %32977 to i8
  store i8 %32978, i8* %14, align 1
  %32979 = and i32 %32975, 255
  %32980 = tail call i32 @llvm.ctpop.i32(i32 %32979)
  %32981 = trunc i32 %32980 to i8
  %32982 = and i8 %32981, 1
  %32983 = xor i8 %32982, 1
  store i8 %32983, i8* %21, align 1
  %32984 = xor i32 %32974, 16
  %32985 = xor i32 %32984, %32975
  %32986 = lshr i32 %32985, 4
  %32987 = trunc i32 %32986 to i8
  %32988 = and i8 %32987, 1
  store i8 %32988, i8* %27, align 1
  %32989 = icmp eq i32 %32975, 0
  %32990 = zext i1 %32989 to i8
  store i8 %32990, i8* %30, align 1
  %32991 = lshr i32 %32975, 31
  %32992 = trunc i32 %32991 to i8
  store i8 %32992, i8* %33, align 1
  %32993 = lshr i32 %32974, 31
  %32994 = xor i32 %32991, %32993
  %32995 = add nuw nsw i32 %32994, %32991
  %32996 = icmp eq i32 %32995, 2
  %32997 = zext i1 %32996 to i8
  store i8 %32997, i8* %39, align 1
  %32998 = sext i32 %32975 to i64
  store i64 %32998, i64* %RCX.i11580, align 8
  %32999 = shl nsw i64 %32998, 1
  %33000 = add i64 %32970, %32999
  %33001 = add i64 %32967, 18
  store i64 %33001, i64* %3, align 8
  %33002 = inttoptr i64 %33000 to i16*
  %33003 = load i16, i16* %33002, align 2
  store i16 %33003, i16* %R9W.i2497, align 2
  %33004 = add i64 %32965, -198
  %33005 = add i64 %32967, 26
  store i64 %33005, i64* %3, align 8
  %33006 = inttoptr i64 %33004 to i16*
  store i16 %33003, i16* %33006, align 2
  %33007 = load i64, i64* %RBP.i, align 8
  %33008 = add i64 %33007, -8
  %33009 = load i64, i64* %3, align 8
  %33010 = add i64 %33009, 4
  store i64 %33010, i64* %3, align 8
  %33011 = inttoptr i64 %33008 to i64*
  %33012 = load i64, i64* %33011, align 8
  store i64 %33012, i64* %RDI.i2910, align 8
  %33013 = add i64 %33007, -208
  %33014 = add i64 %33009, 11
  store i64 %33014, i64* %3, align 8
  %33015 = inttoptr i64 %33013 to i64*
  %33016 = load i64, i64* %33015, align 8
  store i64 %33016, i64* %RAX.i11582.pre-phi, align 8
  %33017 = add i64 %33007, -198
  %33018 = add i64 %33009, 18
  store i64 %33018, i64* %3, align 8
  %33019 = inttoptr i64 %33017 to i16*
  %33020 = load i16, i16* %33019, align 2
  %33021 = zext i16 %33020 to i64
  store i64 %33021, i64* %576, align 8
  %33022 = zext i16 %33020 to i64
  store i64 %33022, i64* %RCX.i11580, align 8
  %33023 = add i64 %33016, %33022
  %33024 = add i64 %33009, 24
  store i64 %33024, i64* %3, align 8
  %33025 = inttoptr i64 %33023 to i8*
  %33026 = load i8, i8* %33025, align 1
  %33027 = zext i8 %33026 to i64
  store i64 %33027, i64* %RSI.i11312, align 8
  %33028 = add i64 %33007, -216
  %33029 = add i64 %33009, 31
  store i64 %33029, i64* %3, align 8
  %33030 = inttoptr i64 %33028 to i64*
  %33031 = load i64, i64* %33030, align 8
  store i64 %33031, i64* %RAX.i11582.pre-phi, align 8
  %33032 = add i64 %33009, 38
  store i64 %33032, i64* %3, align 8
  %33033 = load i16, i16* %33019, align 2
  %33034 = zext i16 %33033 to i64
  store i64 %33034, i64* %576, align 8
  %33035 = zext i16 %33033 to i64
  store i64 %33035, i64* %RCX.i11580, align 8
  %33036 = shl nuw nsw i64 %33035, 2
  %33037 = add i64 %33031, %33036
  %33038 = add i64 %33009, 43
  store i64 %33038, i64* %3, align 8
  %33039 = inttoptr i64 %33037 to i32*
  %33040 = load i32, i32* %33039, align 4
  %33041 = zext i32 %33040 to i64
  store i64 %33041, i64* %576, align 8
  %33042 = add i64 %33009, -17988
  %33043 = add i64 %33009, 48
  %33044 = load i64, i64* %6, align 8
  %33045 = add i64 %33044, -8
  %33046 = inttoptr i64 %33045 to i64*
  store i64 %33043, i64* %33046, align 8
  store i64 %33045, i64* %6, align 8
  store i64 %33042, i64* %3, align 8
  %call2_40e30f = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33042, %struct.Memory* %call2_40e2c5)
  %33047 = load i64, i64* %RBP.i, align 8
  %33048 = add i64 %33047, -120
  %33049 = load i64, i64* %3, align 8
  %33050 = add i64 %33049, 4
  store i64 %33050, i64* %3, align 8
  %33051 = inttoptr i64 %33048 to i64*
  %33052 = load i64, i64* %33051, align 8
  store i64 %33052, i64* %RAX.i11582.pre-phi, align 8
  %33053 = add i64 %33047, -28
  %33054 = add i64 %33049, 7
  store i64 %33054, i64* %3, align 8
  %33055 = inttoptr i64 %33053 to i32*
  %33056 = load i32, i32* %33055, align 4
  %33057 = add i32 %33056, 31
  %33058 = zext i32 %33057 to i64
  store i64 %33058, i64* %576, align 8
  %33059 = icmp ugt i32 %33056, -32
  %33060 = zext i1 %33059 to i8
  store i8 %33060, i8* %14, align 1
  %33061 = and i32 %33057, 255
  %33062 = tail call i32 @llvm.ctpop.i32(i32 %33061)
  %33063 = trunc i32 %33062 to i8
  %33064 = and i8 %33063, 1
  %33065 = xor i8 %33064, 1
  store i8 %33065, i8* %21, align 1
  %33066 = xor i32 %33056, 16
  %33067 = xor i32 %33066, %33057
  %33068 = lshr i32 %33067, 4
  %33069 = trunc i32 %33068 to i8
  %33070 = and i8 %33069, 1
  store i8 %33070, i8* %27, align 1
  %33071 = icmp eq i32 %33057, 0
  %33072 = zext i1 %33071 to i8
  store i8 %33072, i8* %30, align 1
  %33073 = lshr i32 %33057, 31
  %33074 = trunc i32 %33073 to i8
  store i8 %33074, i8* %33, align 1
  %33075 = lshr i32 %33056, 31
  %33076 = xor i32 %33073, %33075
  %33077 = add nuw nsw i32 %33076, %33073
  %33078 = icmp eq i32 %33077, 2
  %33079 = zext i1 %33078 to i8
  store i8 %33079, i8* %39, align 1
  %33080 = sext i32 %33057 to i64
  store i64 %33080, i64* %RCX.i11580, align 8
  %33081 = shl nsw i64 %33080, 1
  %33082 = add i64 %33052, %33081
  %33083 = add i64 %33049, 18
  store i64 %33083, i64* %3, align 8
  %33084 = inttoptr i64 %33082 to i16*
  %33085 = load i16, i16* %33084, align 2
  store i16 %33085, i16* %R9W.i2497, align 2
  %33086 = add i64 %33047, -198
  %33087 = add i64 %33049, 26
  store i64 %33087, i64* %3, align 8
  %33088 = inttoptr i64 %33086 to i16*
  store i16 %33085, i16* %33088, align 2
  %33089 = load i64, i64* %RBP.i, align 8
  %33090 = add i64 %33089, -8
  %33091 = load i64, i64* %3, align 8
  %33092 = add i64 %33091, 4
  store i64 %33092, i64* %3, align 8
  %33093 = inttoptr i64 %33090 to i64*
  %33094 = load i64, i64* %33093, align 8
  store i64 %33094, i64* %RDI.i2910, align 8
  %33095 = add i64 %33089, -208
  %33096 = add i64 %33091, 11
  store i64 %33096, i64* %3, align 8
  %33097 = inttoptr i64 %33095 to i64*
  %33098 = load i64, i64* %33097, align 8
  store i64 %33098, i64* %RAX.i11582.pre-phi, align 8
  %33099 = add i64 %33089, -198
  %33100 = add i64 %33091, 18
  store i64 %33100, i64* %3, align 8
  %33101 = inttoptr i64 %33099 to i16*
  %33102 = load i16, i16* %33101, align 2
  %33103 = zext i16 %33102 to i64
  store i64 %33103, i64* %576, align 8
  %33104 = zext i16 %33102 to i64
  store i64 %33104, i64* %RCX.i11580, align 8
  %33105 = add i64 %33098, %33104
  %33106 = add i64 %33091, 24
  store i64 %33106, i64* %3, align 8
  %33107 = inttoptr i64 %33105 to i8*
  %33108 = load i8, i8* %33107, align 1
  %33109 = zext i8 %33108 to i64
  store i64 %33109, i64* %RSI.i11312, align 8
  %33110 = add i64 %33089, -216
  %33111 = add i64 %33091, 31
  store i64 %33111, i64* %3, align 8
  %33112 = inttoptr i64 %33110 to i64*
  %33113 = load i64, i64* %33112, align 8
  store i64 %33113, i64* %RAX.i11582.pre-phi, align 8
  %33114 = add i64 %33091, 38
  store i64 %33114, i64* %3, align 8
  %33115 = load i16, i16* %33101, align 2
  %33116 = zext i16 %33115 to i64
  store i64 %33116, i64* %576, align 8
  %33117 = zext i16 %33115 to i64
  store i64 %33117, i64* %RCX.i11580, align 8
  %33118 = shl nuw nsw i64 %33117, 2
  %33119 = add i64 %33113, %33118
  %33120 = add i64 %33091, 43
  store i64 %33120, i64* %3, align 8
  %33121 = inttoptr i64 %33119 to i32*
  %33122 = load i32, i32* %33121, align 4
  %33123 = zext i32 %33122 to i64
  store i64 %33123, i64* %576, align 8
  %33124 = add i64 %33091, -18062
  %33125 = add i64 %33091, 48
  %33126 = load i64, i64* %6, align 8
  %33127 = add i64 %33126, -8
  %33128 = inttoptr i64 %33127 to i64*
  store i64 %33125, i64* %33128, align 8
  store i64 %33127, i64* %6, align 8
  store i64 %33124, i64* %3, align 8
  %call2_40e359 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33124, %struct.Memory* %call2_40e30f)
  %33129 = load i64, i64* %RBP.i, align 8
  %33130 = add i64 %33129, -120
  %33131 = load i64, i64* %3, align 8
  %33132 = add i64 %33131, 4
  store i64 %33132, i64* %3, align 8
  %33133 = inttoptr i64 %33130 to i64*
  %33134 = load i64, i64* %33133, align 8
  store i64 %33134, i64* %RAX.i11582.pre-phi, align 8
  %33135 = add i64 %33129, -28
  %33136 = add i64 %33131, 7
  store i64 %33136, i64* %3, align 8
  %33137 = inttoptr i64 %33135 to i32*
  %33138 = load i32, i32* %33137, align 4
  %33139 = add i32 %33138, 32
  %33140 = zext i32 %33139 to i64
  store i64 %33140, i64* %576, align 8
  %33141 = icmp ugt i32 %33138, -33
  %33142 = zext i1 %33141 to i8
  store i8 %33142, i8* %14, align 1
  %33143 = and i32 %33139, 255
  %33144 = tail call i32 @llvm.ctpop.i32(i32 %33143)
  %33145 = trunc i32 %33144 to i8
  %33146 = and i8 %33145, 1
  %33147 = xor i8 %33146, 1
  store i8 %33147, i8* %21, align 1
  %33148 = xor i32 %33139, %33138
  %33149 = lshr i32 %33148, 4
  %33150 = trunc i32 %33149 to i8
  %33151 = and i8 %33150, 1
  store i8 %33151, i8* %27, align 1
  %33152 = icmp eq i32 %33139, 0
  %33153 = zext i1 %33152 to i8
  store i8 %33153, i8* %30, align 1
  %33154 = lshr i32 %33139, 31
  %33155 = trunc i32 %33154 to i8
  store i8 %33155, i8* %33, align 1
  %33156 = lshr i32 %33138, 31
  %33157 = xor i32 %33154, %33156
  %33158 = add nuw nsw i32 %33157, %33154
  %33159 = icmp eq i32 %33158, 2
  %33160 = zext i1 %33159 to i8
  store i8 %33160, i8* %39, align 1
  %33161 = sext i32 %33139 to i64
  store i64 %33161, i64* %RCX.i11580, align 8
  %33162 = shl nsw i64 %33161, 1
  %33163 = add i64 %33134, %33162
  %33164 = add i64 %33131, 18
  store i64 %33164, i64* %3, align 8
  %33165 = inttoptr i64 %33163 to i16*
  %33166 = load i16, i16* %33165, align 2
  store i16 %33166, i16* %R9W.i2497, align 2
  %33167 = add i64 %33129, -198
  %33168 = add i64 %33131, 26
  store i64 %33168, i64* %3, align 8
  %33169 = inttoptr i64 %33167 to i16*
  store i16 %33166, i16* %33169, align 2
  %33170 = load i64, i64* %RBP.i, align 8
  %33171 = add i64 %33170, -8
  %33172 = load i64, i64* %3, align 8
  %33173 = add i64 %33172, 4
  store i64 %33173, i64* %3, align 8
  %33174 = inttoptr i64 %33171 to i64*
  %33175 = load i64, i64* %33174, align 8
  store i64 %33175, i64* %RDI.i2910, align 8
  %33176 = add i64 %33170, -208
  %33177 = add i64 %33172, 11
  store i64 %33177, i64* %3, align 8
  %33178 = inttoptr i64 %33176 to i64*
  %33179 = load i64, i64* %33178, align 8
  store i64 %33179, i64* %RAX.i11582.pre-phi, align 8
  %33180 = add i64 %33170, -198
  %33181 = add i64 %33172, 18
  store i64 %33181, i64* %3, align 8
  %33182 = inttoptr i64 %33180 to i16*
  %33183 = load i16, i16* %33182, align 2
  %33184 = zext i16 %33183 to i64
  store i64 %33184, i64* %576, align 8
  %33185 = zext i16 %33183 to i64
  store i64 %33185, i64* %RCX.i11580, align 8
  %33186 = add i64 %33179, %33185
  %33187 = add i64 %33172, 24
  store i64 %33187, i64* %3, align 8
  %33188 = inttoptr i64 %33186 to i8*
  %33189 = load i8, i8* %33188, align 1
  %33190 = zext i8 %33189 to i64
  store i64 %33190, i64* %RSI.i11312, align 8
  %33191 = add i64 %33170, -216
  %33192 = add i64 %33172, 31
  store i64 %33192, i64* %3, align 8
  %33193 = inttoptr i64 %33191 to i64*
  %33194 = load i64, i64* %33193, align 8
  store i64 %33194, i64* %RAX.i11582.pre-phi, align 8
  %33195 = add i64 %33172, 38
  store i64 %33195, i64* %3, align 8
  %33196 = load i16, i16* %33182, align 2
  %33197 = zext i16 %33196 to i64
  store i64 %33197, i64* %576, align 8
  %33198 = zext i16 %33196 to i64
  store i64 %33198, i64* %RCX.i11580, align 8
  %33199 = shl nuw nsw i64 %33198, 2
  %33200 = add i64 %33194, %33199
  %33201 = add i64 %33172, 43
  store i64 %33201, i64* %3, align 8
  %33202 = inttoptr i64 %33200 to i32*
  %33203 = load i32, i32* %33202, align 4
  %33204 = zext i32 %33203 to i64
  store i64 %33204, i64* %576, align 8
  %33205 = add i64 %33172, -18136
  %33206 = add i64 %33172, 48
  %33207 = load i64, i64* %6, align 8
  %33208 = add i64 %33207, -8
  %33209 = inttoptr i64 %33208 to i64*
  store i64 %33206, i64* %33209, align 8
  store i64 %33208, i64* %6, align 8
  store i64 %33205, i64* %3, align 8
  %call2_40e3a3 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33205, %struct.Memory* %call2_40e359)
  %33210 = load i64, i64* %RBP.i, align 8
  %33211 = add i64 %33210, -120
  %33212 = load i64, i64* %3, align 8
  %33213 = add i64 %33212, 4
  store i64 %33213, i64* %3, align 8
  %33214 = inttoptr i64 %33211 to i64*
  %33215 = load i64, i64* %33214, align 8
  store i64 %33215, i64* %RAX.i11582.pre-phi, align 8
  %33216 = add i64 %33210, -28
  %33217 = add i64 %33212, 7
  store i64 %33217, i64* %3, align 8
  %33218 = inttoptr i64 %33216 to i32*
  %33219 = load i32, i32* %33218, align 4
  %33220 = add i32 %33219, 33
  %33221 = zext i32 %33220 to i64
  store i64 %33221, i64* %576, align 8
  %33222 = icmp ugt i32 %33219, -34
  %33223 = zext i1 %33222 to i8
  store i8 %33223, i8* %14, align 1
  %33224 = and i32 %33220, 255
  %33225 = tail call i32 @llvm.ctpop.i32(i32 %33224)
  %33226 = trunc i32 %33225 to i8
  %33227 = and i8 %33226, 1
  %33228 = xor i8 %33227, 1
  store i8 %33228, i8* %21, align 1
  %33229 = xor i32 %33220, %33219
  %33230 = lshr i32 %33229, 4
  %33231 = trunc i32 %33230 to i8
  %33232 = and i8 %33231, 1
  store i8 %33232, i8* %27, align 1
  %33233 = icmp eq i32 %33220, 0
  %33234 = zext i1 %33233 to i8
  store i8 %33234, i8* %30, align 1
  %33235 = lshr i32 %33220, 31
  %33236 = trunc i32 %33235 to i8
  store i8 %33236, i8* %33, align 1
  %33237 = lshr i32 %33219, 31
  %33238 = xor i32 %33235, %33237
  %33239 = add nuw nsw i32 %33238, %33235
  %33240 = icmp eq i32 %33239, 2
  %33241 = zext i1 %33240 to i8
  store i8 %33241, i8* %39, align 1
  %33242 = sext i32 %33220 to i64
  store i64 %33242, i64* %RCX.i11580, align 8
  %33243 = shl nsw i64 %33242, 1
  %33244 = add i64 %33215, %33243
  %33245 = add i64 %33212, 18
  store i64 %33245, i64* %3, align 8
  %33246 = inttoptr i64 %33244 to i16*
  %33247 = load i16, i16* %33246, align 2
  store i16 %33247, i16* %R9W.i2497, align 2
  %33248 = add i64 %33210, -198
  %33249 = add i64 %33212, 26
  store i64 %33249, i64* %3, align 8
  %33250 = inttoptr i64 %33248 to i16*
  store i16 %33247, i16* %33250, align 2
  %33251 = load i64, i64* %RBP.i, align 8
  %33252 = add i64 %33251, -8
  %33253 = load i64, i64* %3, align 8
  %33254 = add i64 %33253, 4
  store i64 %33254, i64* %3, align 8
  %33255 = inttoptr i64 %33252 to i64*
  %33256 = load i64, i64* %33255, align 8
  store i64 %33256, i64* %RDI.i2910, align 8
  %33257 = add i64 %33251, -208
  %33258 = add i64 %33253, 11
  store i64 %33258, i64* %3, align 8
  %33259 = inttoptr i64 %33257 to i64*
  %33260 = load i64, i64* %33259, align 8
  store i64 %33260, i64* %RAX.i11582.pre-phi, align 8
  %33261 = add i64 %33251, -198
  %33262 = add i64 %33253, 18
  store i64 %33262, i64* %3, align 8
  %33263 = inttoptr i64 %33261 to i16*
  %33264 = load i16, i16* %33263, align 2
  %33265 = zext i16 %33264 to i64
  store i64 %33265, i64* %576, align 8
  %33266 = zext i16 %33264 to i64
  store i64 %33266, i64* %RCX.i11580, align 8
  %33267 = add i64 %33260, %33266
  %33268 = add i64 %33253, 24
  store i64 %33268, i64* %3, align 8
  %33269 = inttoptr i64 %33267 to i8*
  %33270 = load i8, i8* %33269, align 1
  %33271 = zext i8 %33270 to i64
  store i64 %33271, i64* %RSI.i11312, align 8
  %33272 = add i64 %33251, -216
  %33273 = add i64 %33253, 31
  store i64 %33273, i64* %3, align 8
  %33274 = inttoptr i64 %33272 to i64*
  %33275 = load i64, i64* %33274, align 8
  store i64 %33275, i64* %RAX.i11582.pre-phi, align 8
  %33276 = add i64 %33253, 38
  store i64 %33276, i64* %3, align 8
  %33277 = load i16, i16* %33263, align 2
  %33278 = zext i16 %33277 to i64
  store i64 %33278, i64* %576, align 8
  %33279 = zext i16 %33277 to i64
  store i64 %33279, i64* %RCX.i11580, align 8
  %33280 = shl nuw nsw i64 %33279, 2
  %33281 = add i64 %33275, %33280
  %33282 = add i64 %33253, 43
  store i64 %33282, i64* %3, align 8
  %33283 = inttoptr i64 %33281 to i32*
  %33284 = load i32, i32* %33283, align 4
  %33285 = zext i32 %33284 to i64
  store i64 %33285, i64* %576, align 8
  %33286 = add i64 %33253, -18210
  %33287 = add i64 %33253, 48
  %33288 = load i64, i64* %6, align 8
  %33289 = add i64 %33288, -8
  %33290 = inttoptr i64 %33289 to i64*
  store i64 %33287, i64* %33290, align 8
  store i64 %33289, i64* %6, align 8
  store i64 %33286, i64* %3, align 8
  %call2_40e3ed = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33286, %struct.Memory* %call2_40e3a3)
  %33291 = load i64, i64* %RBP.i, align 8
  %33292 = add i64 %33291, -120
  %33293 = load i64, i64* %3, align 8
  %33294 = add i64 %33293, 4
  store i64 %33294, i64* %3, align 8
  %33295 = inttoptr i64 %33292 to i64*
  %33296 = load i64, i64* %33295, align 8
  store i64 %33296, i64* %RAX.i11582.pre-phi, align 8
  %33297 = add i64 %33291, -28
  %33298 = add i64 %33293, 7
  store i64 %33298, i64* %3, align 8
  %33299 = inttoptr i64 %33297 to i32*
  %33300 = load i32, i32* %33299, align 4
  %33301 = add i32 %33300, 34
  %33302 = zext i32 %33301 to i64
  store i64 %33302, i64* %576, align 8
  %33303 = icmp ugt i32 %33300, -35
  %33304 = zext i1 %33303 to i8
  store i8 %33304, i8* %14, align 1
  %33305 = and i32 %33301, 255
  %33306 = tail call i32 @llvm.ctpop.i32(i32 %33305)
  %33307 = trunc i32 %33306 to i8
  %33308 = and i8 %33307, 1
  %33309 = xor i8 %33308, 1
  store i8 %33309, i8* %21, align 1
  %33310 = xor i32 %33301, %33300
  %33311 = lshr i32 %33310, 4
  %33312 = trunc i32 %33311 to i8
  %33313 = and i8 %33312, 1
  store i8 %33313, i8* %27, align 1
  %33314 = icmp eq i32 %33301, 0
  %33315 = zext i1 %33314 to i8
  store i8 %33315, i8* %30, align 1
  %33316 = lshr i32 %33301, 31
  %33317 = trunc i32 %33316 to i8
  store i8 %33317, i8* %33, align 1
  %33318 = lshr i32 %33300, 31
  %33319 = xor i32 %33316, %33318
  %33320 = add nuw nsw i32 %33319, %33316
  %33321 = icmp eq i32 %33320, 2
  %33322 = zext i1 %33321 to i8
  store i8 %33322, i8* %39, align 1
  %33323 = sext i32 %33301 to i64
  store i64 %33323, i64* %RCX.i11580, align 8
  %33324 = shl nsw i64 %33323, 1
  %33325 = add i64 %33296, %33324
  %33326 = add i64 %33293, 18
  store i64 %33326, i64* %3, align 8
  %33327 = inttoptr i64 %33325 to i16*
  %33328 = load i16, i16* %33327, align 2
  store i16 %33328, i16* %R9W.i2497, align 2
  %33329 = add i64 %33291, -198
  %33330 = add i64 %33293, 26
  store i64 %33330, i64* %3, align 8
  %33331 = inttoptr i64 %33329 to i16*
  store i16 %33328, i16* %33331, align 2
  %33332 = load i64, i64* %RBP.i, align 8
  %33333 = add i64 %33332, -8
  %33334 = load i64, i64* %3, align 8
  %33335 = add i64 %33334, 4
  store i64 %33335, i64* %3, align 8
  %33336 = inttoptr i64 %33333 to i64*
  %33337 = load i64, i64* %33336, align 8
  store i64 %33337, i64* %RDI.i2910, align 8
  %33338 = add i64 %33332, -208
  %33339 = add i64 %33334, 11
  store i64 %33339, i64* %3, align 8
  %33340 = inttoptr i64 %33338 to i64*
  %33341 = load i64, i64* %33340, align 8
  store i64 %33341, i64* %RAX.i11582.pre-phi, align 8
  %33342 = add i64 %33332, -198
  %33343 = add i64 %33334, 18
  store i64 %33343, i64* %3, align 8
  %33344 = inttoptr i64 %33342 to i16*
  %33345 = load i16, i16* %33344, align 2
  %33346 = zext i16 %33345 to i64
  store i64 %33346, i64* %576, align 8
  %33347 = zext i16 %33345 to i64
  store i64 %33347, i64* %RCX.i11580, align 8
  %33348 = add i64 %33341, %33347
  %33349 = add i64 %33334, 24
  store i64 %33349, i64* %3, align 8
  %33350 = inttoptr i64 %33348 to i8*
  %33351 = load i8, i8* %33350, align 1
  %33352 = zext i8 %33351 to i64
  store i64 %33352, i64* %RSI.i11312, align 8
  %33353 = add i64 %33332, -216
  %33354 = add i64 %33334, 31
  store i64 %33354, i64* %3, align 8
  %33355 = inttoptr i64 %33353 to i64*
  %33356 = load i64, i64* %33355, align 8
  store i64 %33356, i64* %RAX.i11582.pre-phi, align 8
  %33357 = add i64 %33334, 38
  store i64 %33357, i64* %3, align 8
  %33358 = load i16, i16* %33344, align 2
  %33359 = zext i16 %33358 to i64
  store i64 %33359, i64* %576, align 8
  %33360 = zext i16 %33358 to i64
  store i64 %33360, i64* %RCX.i11580, align 8
  %33361 = shl nuw nsw i64 %33360, 2
  %33362 = add i64 %33356, %33361
  %33363 = add i64 %33334, 43
  store i64 %33363, i64* %3, align 8
  %33364 = inttoptr i64 %33362 to i32*
  %33365 = load i32, i32* %33364, align 4
  %33366 = zext i32 %33365 to i64
  store i64 %33366, i64* %576, align 8
  %33367 = add i64 %33334, -18284
  %33368 = add i64 %33334, 48
  %33369 = load i64, i64* %6, align 8
  %33370 = add i64 %33369, -8
  %33371 = inttoptr i64 %33370 to i64*
  store i64 %33368, i64* %33371, align 8
  store i64 %33370, i64* %6, align 8
  store i64 %33367, i64* %3, align 8
  %call2_40e437 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33367, %struct.Memory* %call2_40e3ed)
  %33372 = load i64, i64* %RBP.i, align 8
  %33373 = add i64 %33372, -120
  %33374 = load i64, i64* %3, align 8
  %33375 = add i64 %33374, 4
  store i64 %33375, i64* %3, align 8
  %33376 = inttoptr i64 %33373 to i64*
  %33377 = load i64, i64* %33376, align 8
  store i64 %33377, i64* %RAX.i11582.pre-phi, align 8
  %33378 = add i64 %33372, -28
  %33379 = add i64 %33374, 7
  store i64 %33379, i64* %3, align 8
  %33380 = inttoptr i64 %33378 to i32*
  %33381 = load i32, i32* %33380, align 4
  %33382 = add i32 %33381, 35
  %33383 = zext i32 %33382 to i64
  store i64 %33383, i64* %576, align 8
  %33384 = icmp ugt i32 %33381, -36
  %33385 = zext i1 %33384 to i8
  store i8 %33385, i8* %14, align 1
  %33386 = and i32 %33382, 255
  %33387 = tail call i32 @llvm.ctpop.i32(i32 %33386)
  %33388 = trunc i32 %33387 to i8
  %33389 = and i8 %33388, 1
  %33390 = xor i8 %33389, 1
  store i8 %33390, i8* %21, align 1
  %33391 = xor i32 %33382, %33381
  %33392 = lshr i32 %33391, 4
  %33393 = trunc i32 %33392 to i8
  %33394 = and i8 %33393, 1
  store i8 %33394, i8* %27, align 1
  %33395 = icmp eq i32 %33382, 0
  %33396 = zext i1 %33395 to i8
  store i8 %33396, i8* %30, align 1
  %33397 = lshr i32 %33382, 31
  %33398 = trunc i32 %33397 to i8
  store i8 %33398, i8* %33, align 1
  %33399 = lshr i32 %33381, 31
  %33400 = xor i32 %33397, %33399
  %33401 = add nuw nsw i32 %33400, %33397
  %33402 = icmp eq i32 %33401, 2
  %33403 = zext i1 %33402 to i8
  store i8 %33403, i8* %39, align 1
  %33404 = sext i32 %33382 to i64
  store i64 %33404, i64* %RCX.i11580, align 8
  %33405 = shl nsw i64 %33404, 1
  %33406 = add i64 %33377, %33405
  %33407 = add i64 %33374, 18
  store i64 %33407, i64* %3, align 8
  %33408 = inttoptr i64 %33406 to i16*
  %33409 = load i16, i16* %33408, align 2
  store i16 %33409, i16* %R9W.i2497, align 2
  %33410 = add i64 %33372, -198
  %33411 = add i64 %33374, 26
  store i64 %33411, i64* %3, align 8
  %33412 = inttoptr i64 %33410 to i16*
  store i16 %33409, i16* %33412, align 2
  %33413 = load i64, i64* %RBP.i, align 8
  %33414 = add i64 %33413, -8
  %33415 = load i64, i64* %3, align 8
  %33416 = add i64 %33415, 4
  store i64 %33416, i64* %3, align 8
  %33417 = inttoptr i64 %33414 to i64*
  %33418 = load i64, i64* %33417, align 8
  store i64 %33418, i64* %RDI.i2910, align 8
  %33419 = add i64 %33413, -208
  %33420 = add i64 %33415, 11
  store i64 %33420, i64* %3, align 8
  %33421 = inttoptr i64 %33419 to i64*
  %33422 = load i64, i64* %33421, align 8
  store i64 %33422, i64* %RAX.i11582.pre-phi, align 8
  %33423 = add i64 %33413, -198
  %33424 = add i64 %33415, 18
  store i64 %33424, i64* %3, align 8
  %33425 = inttoptr i64 %33423 to i16*
  %33426 = load i16, i16* %33425, align 2
  %33427 = zext i16 %33426 to i64
  store i64 %33427, i64* %576, align 8
  %33428 = zext i16 %33426 to i64
  store i64 %33428, i64* %RCX.i11580, align 8
  %33429 = add i64 %33422, %33428
  %33430 = add i64 %33415, 24
  store i64 %33430, i64* %3, align 8
  %33431 = inttoptr i64 %33429 to i8*
  %33432 = load i8, i8* %33431, align 1
  %33433 = zext i8 %33432 to i64
  store i64 %33433, i64* %RSI.i11312, align 8
  %33434 = add i64 %33413, -216
  %33435 = add i64 %33415, 31
  store i64 %33435, i64* %3, align 8
  %33436 = inttoptr i64 %33434 to i64*
  %33437 = load i64, i64* %33436, align 8
  store i64 %33437, i64* %RAX.i11582.pre-phi, align 8
  %33438 = add i64 %33415, 38
  store i64 %33438, i64* %3, align 8
  %33439 = load i16, i16* %33425, align 2
  %33440 = zext i16 %33439 to i64
  store i64 %33440, i64* %576, align 8
  %33441 = zext i16 %33439 to i64
  store i64 %33441, i64* %RCX.i11580, align 8
  %33442 = shl nuw nsw i64 %33441, 2
  %33443 = add i64 %33437, %33442
  %33444 = add i64 %33415, 43
  store i64 %33444, i64* %3, align 8
  %33445 = inttoptr i64 %33443 to i32*
  %33446 = load i32, i32* %33445, align 4
  %33447 = zext i32 %33446 to i64
  store i64 %33447, i64* %576, align 8
  %33448 = add i64 %33415, -18358
  %33449 = add i64 %33415, 48
  %33450 = load i64, i64* %6, align 8
  %33451 = add i64 %33450, -8
  %33452 = inttoptr i64 %33451 to i64*
  store i64 %33449, i64* %33452, align 8
  store i64 %33451, i64* %6, align 8
  store i64 %33448, i64* %3, align 8
  %call2_40e481 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33448, %struct.Memory* %call2_40e437)
  %33453 = load i64, i64* %RBP.i, align 8
  %33454 = add i64 %33453, -120
  %33455 = load i64, i64* %3, align 8
  %33456 = add i64 %33455, 4
  store i64 %33456, i64* %3, align 8
  %33457 = inttoptr i64 %33454 to i64*
  %33458 = load i64, i64* %33457, align 8
  store i64 %33458, i64* %RAX.i11582.pre-phi, align 8
  %33459 = add i64 %33453, -28
  %33460 = add i64 %33455, 7
  store i64 %33460, i64* %3, align 8
  %33461 = inttoptr i64 %33459 to i32*
  %33462 = load i32, i32* %33461, align 4
  %33463 = add i32 %33462, 36
  %33464 = zext i32 %33463 to i64
  store i64 %33464, i64* %576, align 8
  %33465 = icmp ugt i32 %33462, -37
  %33466 = zext i1 %33465 to i8
  store i8 %33466, i8* %14, align 1
  %33467 = and i32 %33463, 255
  %33468 = tail call i32 @llvm.ctpop.i32(i32 %33467)
  %33469 = trunc i32 %33468 to i8
  %33470 = and i8 %33469, 1
  %33471 = xor i8 %33470, 1
  store i8 %33471, i8* %21, align 1
  %33472 = xor i32 %33463, %33462
  %33473 = lshr i32 %33472, 4
  %33474 = trunc i32 %33473 to i8
  %33475 = and i8 %33474, 1
  store i8 %33475, i8* %27, align 1
  %33476 = icmp eq i32 %33463, 0
  %33477 = zext i1 %33476 to i8
  store i8 %33477, i8* %30, align 1
  %33478 = lshr i32 %33463, 31
  %33479 = trunc i32 %33478 to i8
  store i8 %33479, i8* %33, align 1
  %33480 = lshr i32 %33462, 31
  %33481 = xor i32 %33478, %33480
  %33482 = add nuw nsw i32 %33481, %33478
  %33483 = icmp eq i32 %33482, 2
  %33484 = zext i1 %33483 to i8
  store i8 %33484, i8* %39, align 1
  %33485 = sext i32 %33463 to i64
  store i64 %33485, i64* %RCX.i11580, align 8
  %33486 = shl nsw i64 %33485, 1
  %33487 = add i64 %33458, %33486
  %33488 = add i64 %33455, 18
  store i64 %33488, i64* %3, align 8
  %33489 = inttoptr i64 %33487 to i16*
  %33490 = load i16, i16* %33489, align 2
  store i16 %33490, i16* %R9W.i2497, align 2
  %33491 = add i64 %33453, -198
  %33492 = add i64 %33455, 26
  store i64 %33492, i64* %3, align 8
  %33493 = inttoptr i64 %33491 to i16*
  store i16 %33490, i16* %33493, align 2
  %33494 = load i64, i64* %RBP.i, align 8
  %33495 = add i64 %33494, -8
  %33496 = load i64, i64* %3, align 8
  %33497 = add i64 %33496, 4
  store i64 %33497, i64* %3, align 8
  %33498 = inttoptr i64 %33495 to i64*
  %33499 = load i64, i64* %33498, align 8
  store i64 %33499, i64* %RDI.i2910, align 8
  %33500 = add i64 %33494, -208
  %33501 = add i64 %33496, 11
  store i64 %33501, i64* %3, align 8
  %33502 = inttoptr i64 %33500 to i64*
  %33503 = load i64, i64* %33502, align 8
  store i64 %33503, i64* %RAX.i11582.pre-phi, align 8
  %33504 = add i64 %33494, -198
  %33505 = add i64 %33496, 18
  store i64 %33505, i64* %3, align 8
  %33506 = inttoptr i64 %33504 to i16*
  %33507 = load i16, i16* %33506, align 2
  %33508 = zext i16 %33507 to i64
  store i64 %33508, i64* %576, align 8
  %33509 = zext i16 %33507 to i64
  store i64 %33509, i64* %RCX.i11580, align 8
  %33510 = add i64 %33503, %33509
  %33511 = add i64 %33496, 24
  store i64 %33511, i64* %3, align 8
  %33512 = inttoptr i64 %33510 to i8*
  %33513 = load i8, i8* %33512, align 1
  %33514 = zext i8 %33513 to i64
  store i64 %33514, i64* %RSI.i11312, align 8
  %33515 = add i64 %33494, -216
  %33516 = add i64 %33496, 31
  store i64 %33516, i64* %3, align 8
  %33517 = inttoptr i64 %33515 to i64*
  %33518 = load i64, i64* %33517, align 8
  store i64 %33518, i64* %RAX.i11582.pre-phi, align 8
  %33519 = add i64 %33496, 38
  store i64 %33519, i64* %3, align 8
  %33520 = load i16, i16* %33506, align 2
  %33521 = zext i16 %33520 to i64
  store i64 %33521, i64* %576, align 8
  %33522 = zext i16 %33520 to i64
  store i64 %33522, i64* %RCX.i11580, align 8
  %33523 = shl nuw nsw i64 %33522, 2
  %33524 = add i64 %33518, %33523
  %33525 = add i64 %33496, 43
  store i64 %33525, i64* %3, align 8
  %33526 = inttoptr i64 %33524 to i32*
  %33527 = load i32, i32* %33526, align 4
  %33528 = zext i32 %33527 to i64
  store i64 %33528, i64* %576, align 8
  %33529 = add i64 %33496, -18432
  %33530 = add i64 %33496, 48
  %33531 = load i64, i64* %6, align 8
  %33532 = add i64 %33531, -8
  %33533 = inttoptr i64 %33532 to i64*
  store i64 %33530, i64* %33533, align 8
  store i64 %33532, i64* %6, align 8
  store i64 %33529, i64* %3, align 8
  %call2_40e4cb = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33529, %struct.Memory* %call2_40e481)
  %33534 = load i64, i64* %RBP.i, align 8
  %33535 = add i64 %33534, -120
  %33536 = load i64, i64* %3, align 8
  %33537 = add i64 %33536, 4
  store i64 %33537, i64* %3, align 8
  %33538 = inttoptr i64 %33535 to i64*
  %33539 = load i64, i64* %33538, align 8
  store i64 %33539, i64* %RAX.i11582.pre-phi, align 8
  %33540 = add i64 %33534, -28
  %33541 = add i64 %33536, 7
  store i64 %33541, i64* %3, align 8
  %33542 = inttoptr i64 %33540 to i32*
  %33543 = load i32, i32* %33542, align 4
  %33544 = add i32 %33543, 37
  %33545 = zext i32 %33544 to i64
  store i64 %33545, i64* %576, align 8
  %33546 = icmp ugt i32 %33543, -38
  %33547 = zext i1 %33546 to i8
  store i8 %33547, i8* %14, align 1
  %33548 = and i32 %33544, 255
  %33549 = tail call i32 @llvm.ctpop.i32(i32 %33548)
  %33550 = trunc i32 %33549 to i8
  %33551 = and i8 %33550, 1
  %33552 = xor i8 %33551, 1
  store i8 %33552, i8* %21, align 1
  %33553 = xor i32 %33544, %33543
  %33554 = lshr i32 %33553, 4
  %33555 = trunc i32 %33554 to i8
  %33556 = and i8 %33555, 1
  store i8 %33556, i8* %27, align 1
  %33557 = icmp eq i32 %33544, 0
  %33558 = zext i1 %33557 to i8
  store i8 %33558, i8* %30, align 1
  %33559 = lshr i32 %33544, 31
  %33560 = trunc i32 %33559 to i8
  store i8 %33560, i8* %33, align 1
  %33561 = lshr i32 %33543, 31
  %33562 = xor i32 %33559, %33561
  %33563 = add nuw nsw i32 %33562, %33559
  %33564 = icmp eq i32 %33563, 2
  %33565 = zext i1 %33564 to i8
  store i8 %33565, i8* %39, align 1
  %33566 = sext i32 %33544 to i64
  store i64 %33566, i64* %RCX.i11580, align 8
  %33567 = shl nsw i64 %33566, 1
  %33568 = add i64 %33539, %33567
  %33569 = add i64 %33536, 18
  store i64 %33569, i64* %3, align 8
  %33570 = inttoptr i64 %33568 to i16*
  %33571 = load i16, i16* %33570, align 2
  store i16 %33571, i16* %R9W.i2497, align 2
  %33572 = add i64 %33534, -198
  %33573 = add i64 %33536, 26
  store i64 %33573, i64* %3, align 8
  %33574 = inttoptr i64 %33572 to i16*
  store i16 %33571, i16* %33574, align 2
  %33575 = load i64, i64* %RBP.i, align 8
  %33576 = add i64 %33575, -8
  %33577 = load i64, i64* %3, align 8
  %33578 = add i64 %33577, 4
  store i64 %33578, i64* %3, align 8
  %33579 = inttoptr i64 %33576 to i64*
  %33580 = load i64, i64* %33579, align 8
  store i64 %33580, i64* %RDI.i2910, align 8
  %33581 = add i64 %33575, -208
  %33582 = add i64 %33577, 11
  store i64 %33582, i64* %3, align 8
  %33583 = inttoptr i64 %33581 to i64*
  %33584 = load i64, i64* %33583, align 8
  store i64 %33584, i64* %RAX.i11582.pre-phi, align 8
  %33585 = add i64 %33575, -198
  %33586 = add i64 %33577, 18
  store i64 %33586, i64* %3, align 8
  %33587 = inttoptr i64 %33585 to i16*
  %33588 = load i16, i16* %33587, align 2
  %33589 = zext i16 %33588 to i64
  store i64 %33589, i64* %576, align 8
  %33590 = zext i16 %33588 to i64
  store i64 %33590, i64* %RCX.i11580, align 8
  %33591 = add i64 %33584, %33590
  %33592 = add i64 %33577, 24
  store i64 %33592, i64* %3, align 8
  %33593 = inttoptr i64 %33591 to i8*
  %33594 = load i8, i8* %33593, align 1
  %33595 = zext i8 %33594 to i64
  store i64 %33595, i64* %RSI.i11312, align 8
  %33596 = add i64 %33575, -216
  %33597 = add i64 %33577, 31
  store i64 %33597, i64* %3, align 8
  %33598 = inttoptr i64 %33596 to i64*
  %33599 = load i64, i64* %33598, align 8
  store i64 %33599, i64* %RAX.i11582.pre-phi, align 8
  %33600 = add i64 %33577, 38
  store i64 %33600, i64* %3, align 8
  %33601 = load i16, i16* %33587, align 2
  %33602 = zext i16 %33601 to i64
  store i64 %33602, i64* %576, align 8
  %33603 = zext i16 %33601 to i64
  store i64 %33603, i64* %RCX.i11580, align 8
  %33604 = shl nuw nsw i64 %33603, 2
  %33605 = add i64 %33599, %33604
  %33606 = add i64 %33577, 43
  store i64 %33606, i64* %3, align 8
  %33607 = inttoptr i64 %33605 to i32*
  %33608 = load i32, i32* %33607, align 4
  %33609 = zext i32 %33608 to i64
  store i64 %33609, i64* %576, align 8
  %33610 = add i64 %33577, -18506
  %33611 = add i64 %33577, 48
  %33612 = load i64, i64* %6, align 8
  %33613 = add i64 %33612, -8
  %33614 = inttoptr i64 %33613 to i64*
  store i64 %33611, i64* %33614, align 8
  store i64 %33613, i64* %6, align 8
  store i64 %33610, i64* %3, align 8
  %call2_40e515 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33610, %struct.Memory* %call2_40e4cb)
  %33615 = load i64, i64* %RBP.i, align 8
  %33616 = add i64 %33615, -120
  %33617 = load i64, i64* %3, align 8
  %33618 = add i64 %33617, 4
  store i64 %33618, i64* %3, align 8
  %33619 = inttoptr i64 %33616 to i64*
  %33620 = load i64, i64* %33619, align 8
  store i64 %33620, i64* %RAX.i11582.pre-phi, align 8
  %33621 = add i64 %33615, -28
  %33622 = add i64 %33617, 7
  store i64 %33622, i64* %3, align 8
  %33623 = inttoptr i64 %33621 to i32*
  %33624 = load i32, i32* %33623, align 4
  %33625 = add i32 %33624, 38
  %33626 = zext i32 %33625 to i64
  store i64 %33626, i64* %576, align 8
  %33627 = icmp ugt i32 %33624, -39
  %33628 = zext i1 %33627 to i8
  store i8 %33628, i8* %14, align 1
  %33629 = and i32 %33625, 255
  %33630 = tail call i32 @llvm.ctpop.i32(i32 %33629)
  %33631 = trunc i32 %33630 to i8
  %33632 = and i8 %33631, 1
  %33633 = xor i8 %33632, 1
  store i8 %33633, i8* %21, align 1
  %33634 = xor i32 %33625, %33624
  %33635 = lshr i32 %33634, 4
  %33636 = trunc i32 %33635 to i8
  %33637 = and i8 %33636, 1
  store i8 %33637, i8* %27, align 1
  %33638 = icmp eq i32 %33625, 0
  %33639 = zext i1 %33638 to i8
  store i8 %33639, i8* %30, align 1
  %33640 = lshr i32 %33625, 31
  %33641 = trunc i32 %33640 to i8
  store i8 %33641, i8* %33, align 1
  %33642 = lshr i32 %33624, 31
  %33643 = xor i32 %33640, %33642
  %33644 = add nuw nsw i32 %33643, %33640
  %33645 = icmp eq i32 %33644, 2
  %33646 = zext i1 %33645 to i8
  store i8 %33646, i8* %39, align 1
  %33647 = sext i32 %33625 to i64
  store i64 %33647, i64* %RCX.i11580, align 8
  %33648 = shl nsw i64 %33647, 1
  %33649 = add i64 %33620, %33648
  %33650 = add i64 %33617, 18
  store i64 %33650, i64* %3, align 8
  %33651 = inttoptr i64 %33649 to i16*
  %33652 = load i16, i16* %33651, align 2
  store i16 %33652, i16* %R9W.i2497, align 2
  %33653 = add i64 %33615, -198
  %33654 = add i64 %33617, 26
  store i64 %33654, i64* %3, align 8
  %33655 = inttoptr i64 %33653 to i16*
  store i16 %33652, i16* %33655, align 2
  %33656 = load i64, i64* %RBP.i, align 8
  %33657 = add i64 %33656, -8
  %33658 = load i64, i64* %3, align 8
  %33659 = add i64 %33658, 4
  store i64 %33659, i64* %3, align 8
  %33660 = inttoptr i64 %33657 to i64*
  %33661 = load i64, i64* %33660, align 8
  store i64 %33661, i64* %RDI.i2910, align 8
  %33662 = add i64 %33656, -208
  %33663 = add i64 %33658, 11
  store i64 %33663, i64* %3, align 8
  %33664 = inttoptr i64 %33662 to i64*
  %33665 = load i64, i64* %33664, align 8
  store i64 %33665, i64* %RAX.i11582.pre-phi, align 8
  %33666 = add i64 %33656, -198
  %33667 = add i64 %33658, 18
  store i64 %33667, i64* %3, align 8
  %33668 = inttoptr i64 %33666 to i16*
  %33669 = load i16, i16* %33668, align 2
  %33670 = zext i16 %33669 to i64
  store i64 %33670, i64* %576, align 8
  %33671 = zext i16 %33669 to i64
  store i64 %33671, i64* %RCX.i11580, align 8
  %33672 = add i64 %33665, %33671
  %33673 = add i64 %33658, 24
  store i64 %33673, i64* %3, align 8
  %33674 = inttoptr i64 %33672 to i8*
  %33675 = load i8, i8* %33674, align 1
  %33676 = zext i8 %33675 to i64
  store i64 %33676, i64* %RSI.i11312, align 8
  %33677 = add i64 %33656, -216
  %33678 = add i64 %33658, 31
  store i64 %33678, i64* %3, align 8
  %33679 = inttoptr i64 %33677 to i64*
  %33680 = load i64, i64* %33679, align 8
  store i64 %33680, i64* %RAX.i11582.pre-phi, align 8
  %33681 = add i64 %33658, 38
  store i64 %33681, i64* %3, align 8
  %33682 = load i16, i16* %33668, align 2
  %33683 = zext i16 %33682 to i64
  store i64 %33683, i64* %576, align 8
  %33684 = zext i16 %33682 to i64
  store i64 %33684, i64* %RCX.i11580, align 8
  %33685 = shl nuw nsw i64 %33684, 2
  %33686 = add i64 %33680, %33685
  %33687 = add i64 %33658, 43
  store i64 %33687, i64* %3, align 8
  %33688 = inttoptr i64 %33686 to i32*
  %33689 = load i32, i32* %33688, align 4
  %33690 = zext i32 %33689 to i64
  store i64 %33690, i64* %576, align 8
  %33691 = add i64 %33658, -18580
  %33692 = add i64 %33658, 48
  %33693 = load i64, i64* %6, align 8
  %33694 = add i64 %33693, -8
  %33695 = inttoptr i64 %33694 to i64*
  store i64 %33692, i64* %33695, align 8
  store i64 %33694, i64* %6, align 8
  store i64 %33691, i64* %3, align 8
  %call2_40e55f = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33691, %struct.Memory* %call2_40e515)
  %33696 = load i64, i64* %RBP.i, align 8
  %33697 = add i64 %33696, -120
  %33698 = load i64, i64* %3, align 8
  %33699 = add i64 %33698, 4
  store i64 %33699, i64* %3, align 8
  %33700 = inttoptr i64 %33697 to i64*
  %33701 = load i64, i64* %33700, align 8
  store i64 %33701, i64* %RAX.i11582.pre-phi, align 8
  %33702 = add i64 %33696, -28
  %33703 = add i64 %33698, 7
  store i64 %33703, i64* %3, align 8
  %33704 = inttoptr i64 %33702 to i32*
  %33705 = load i32, i32* %33704, align 4
  %33706 = add i32 %33705, 39
  %33707 = zext i32 %33706 to i64
  store i64 %33707, i64* %576, align 8
  %33708 = icmp ugt i32 %33705, -40
  %33709 = zext i1 %33708 to i8
  store i8 %33709, i8* %14, align 1
  %33710 = and i32 %33706, 255
  %33711 = tail call i32 @llvm.ctpop.i32(i32 %33710)
  %33712 = trunc i32 %33711 to i8
  %33713 = and i8 %33712, 1
  %33714 = xor i8 %33713, 1
  store i8 %33714, i8* %21, align 1
  %33715 = xor i32 %33706, %33705
  %33716 = lshr i32 %33715, 4
  %33717 = trunc i32 %33716 to i8
  %33718 = and i8 %33717, 1
  store i8 %33718, i8* %27, align 1
  %33719 = icmp eq i32 %33706, 0
  %33720 = zext i1 %33719 to i8
  store i8 %33720, i8* %30, align 1
  %33721 = lshr i32 %33706, 31
  %33722 = trunc i32 %33721 to i8
  store i8 %33722, i8* %33, align 1
  %33723 = lshr i32 %33705, 31
  %33724 = xor i32 %33721, %33723
  %33725 = add nuw nsw i32 %33724, %33721
  %33726 = icmp eq i32 %33725, 2
  %33727 = zext i1 %33726 to i8
  store i8 %33727, i8* %39, align 1
  %33728 = sext i32 %33706 to i64
  store i64 %33728, i64* %RCX.i11580, align 8
  %33729 = shl nsw i64 %33728, 1
  %33730 = add i64 %33701, %33729
  %33731 = add i64 %33698, 18
  store i64 %33731, i64* %3, align 8
  %33732 = inttoptr i64 %33730 to i16*
  %33733 = load i16, i16* %33732, align 2
  store i16 %33733, i16* %R9W.i2497, align 2
  %33734 = add i64 %33696, -198
  %33735 = add i64 %33698, 26
  store i64 %33735, i64* %3, align 8
  %33736 = inttoptr i64 %33734 to i16*
  store i16 %33733, i16* %33736, align 2
  %33737 = load i64, i64* %RBP.i, align 8
  %33738 = add i64 %33737, -8
  %33739 = load i64, i64* %3, align 8
  %33740 = add i64 %33739, 4
  store i64 %33740, i64* %3, align 8
  %33741 = inttoptr i64 %33738 to i64*
  %33742 = load i64, i64* %33741, align 8
  store i64 %33742, i64* %RDI.i2910, align 8
  %33743 = add i64 %33737, -208
  %33744 = add i64 %33739, 11
  store i64 %33744, i64* %3, align 8
  %33745 = inttoptr i64 %33743 to i64*
  %33746 = load i64, i64* %33745, align 8
  store i64 %33746, i64* %RAX.i11582.pre-phi, align 8
  %33747 = add i64 %33737, -198
  %33748 = add i64 %33739, 18
  store i64 %33748, i64* %3, align 8
  %33749 = inttoptr i64 %33747 to i16*
  %33750 = load i16, i16* %33749, align 2
  %33751 = zext i16 %33750 to i64
  store i64 %33751, i64* %576, align 8
  %33752 = zext i16 %33750 to i64
  store i64 %33752, i64* %RCX.i11580, align 8
  %33753 = add i64 %33746, %33752
  %33754 = add i64 %33739, 24
  store i64 %33754, i64* %3, align 8
  %33755 = inttoptr i64 %33753 to i8*
  %33756 = load i8, i8* %33755, align 1
  %33757 = zext i8 %33756 to i64
  store i64 %33757, i64* %RSI.i11312, align 8
  %33758 = add i64 %33737, -216
  %33759 = add i64 %33739, 31
  store i64 %33759, i64* %3, align 8
  %33760 = inttoptr i64 %33758 to i64*
  %33761 = load i64, i64* %33760, align 8
  store i64 %33761, i64* %RAX.i11582.pre-phi, align 8
  %33762 = add i64 %33739, 38
  store i64 %33762, i64* %3, align 8
  %33763 = load i16, i16* %33749, align 2
  %33764 = zext i16 %33763 to i64
  store i64 %33764, i64* %576, align 8
  %33765 = zext i16 %33763 to i64
  store i64 %33765, i64* %RCX.i11580, align 8
  %33766 = shl nuw nsw i64 %33765, 2
  %33767 = add i64 %33761, %33766
  %33768 = add i64 %33739, 43
  store i64 %33768, i64* %3, align 8
  %33769 = inttoptr i64 %33767 to i32*
  %33770 = load i32, i32* %33769, align 4
  %33771 = zext i32 %33770 to i64
  store i64 %33771, i64* %576, align 8
  %33772 = add i64 %33739, -18654
  %33773 = add i64 %33739, 48
  %33774 = load i64, i64* %6, align 8
  %33775 = add i64 %33774, -8
  %33776 = inttoptr i64 %33775 to i64*
  store i64 %33773, i64* %33776, align 8
  store i64 %33775, i64* %6, align 8
  store i64 %33772, i64* %3, align 8
  %call2_40e5a9 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33772, %struct.Memory* %call2_40e55f)
  %33777 = load i64, i64* %RBP.i, align 8
  %33778 = add i64 %33777, -120
  %33779 = load i64, i64* %3, align 8
  %33780 = add i64 %33779, 4
  store i64 %33780, i64* %3, align 8
  %33781 = inttoptr i64 %33778 to i64*
  %33782 = load i64, i64* %33781, align 8
  store i64 %33782, i64* %RAX.i11582.pre-phi, align 8
  %33783 = add i64 %33777, -28
  %33784 = add i64 %33779, 7
  store i64 %33784, i64* %3, align 8
  %33785 = inttoptr i64 %33783 to i32*
  %33786 = load i32, i32* %33785, align 4
  %33787 = add i32 %33786, 40
  %33788 = zext i32 %33787 to i64
  store i64 %33788, i64* %576, align 8
  %33789 = icmp ugt i32 %33786, -41
  %33790 = zext i1 %33789 to i8
  store i8 %33790, i8* %14, align 1
  %33791 = and i32 %33787, 255
  %33792 = tail call i32 @llvm.ctpop.i32(i32 %33791)
  %33793 = trunc i32 %33792 to i8
  %33794 = and i8 %33793, 1
  %33795 = xor i8 %33794, 1
  store i8 %33795, i8* %21, align 1
  %33796 = xor i32 %33787, %33786
  %33797 = lshr i32 %33796, 4
  %33798 = trunc i32 %33797 to i8
  %33799 = and i8 %33798, 1
  store i8 %33799, i8* %27, align 1
  %33800 = icmp eq i32 %33787, 0
  %33801 = zext i1 %33800 to i8
  store i8 %33801, i8* %30, align 1
  %33802 = lshr i32 %33787, 31
  %33803 = trunc i32 %33802 to i8
  store i8 %33803, i8* %33, align 1
  %33804 = lshr i32 %33786, 31
  %33805 = xor i32 %33802, %33804
  %33806 = add nuw nsw i32 %33805, %33802
  %33807 = icmp eq i32 %33806, 2
  %33808 = zext i1 %33807 to i8
  store i8 %33808, i8* %39, align 1
  %33809 = sext i32 %33787 to i64
  store i64 %33809, i64* %RCX.i11580, align 8
  %33810 = shl nsw i64 %33809, 1
  %33811 = add i64 %33782, %33810
  %33812 = add i64 %33779, 18
  store i64 %33812, i64* %3, align 8
  %33813 = inttoptr i64 %33811 to i16*
  %33814 = load i16, i16* %33813, align 2
  store i16 %33814, i16* %R9W.i2497, align 2
  %33815 = add i64 %33777, -198
  %33816 = add i64 %33779, 26
  store i64 %33816, i64* %3, align 8
  %33817 = inttoptr i64 %33815 to i16*
  store i16 %33814, i16* %33817, align 2
  %33818 = load i64, i64* %RBP.i, align 8
  %33819 = add i64 %33818, -8
  %33820 = load i64, i64* %3, align 8
  %33821 = add i64 %33820, 4
  store i64 %33821, i64* %3, align 8
  %33822 = inttoptr i64 %33819 to i64*
  %33823 = load i64, i64* %33822, align 8
  store i64 %33823, i64* %RDI.i2910, align 8
  %33824 = add i64 %33818, -208
  %33825 = add i64 %33820, 11
  store i64 %33825, i64* %3, align 8
  %33826 = inttoptr i64 %33824 to i64*
  %33827 = load i64, i64* %33826, align 8
  store i64 %33827, i64* %RAX.i11582.pre-phi, align 8
  %33828 = add i64 %33818, -198
  %33829 = add i64 %33820, 18
  store i64 %33829, i64* %3, align 8
  %33830 = inttoptr i64 %33828 to i16*
  %33831 = load i16, i16* %33830, align 2
  %33832 = zext i16 %33831 to i64
  store i64 %33832, i64* %576, align 8
  %33833 = zext i16 %33831 to i64
  store i64 %33833, i64* %RCX.i11580, align 8
  %33834 = add i64 %33827, %33833
  %33835 = add i64 %33820, 24
  store i64 %33835, i64* %3, align 8
  %33836 = inttoptr i64 %33834 to i8*
  %33837 = load i8, i8* %33836, align 1
  %33838 = zext i8 %33837 to i64
  store i64 %33838, i64* %RSI.i11312, align 8
  %33839 = add i64 %33818, -216
  %33840 = add i64 %33820, 31
  store i64 %33840, i64* %3, align 8
  %33841 = inttoptr i64 %33839 to i64*
  %33842 = load i64, i64* %33841, align 8
  store i64 %33842, i64* %RAX.i11582.pre-phi, align 8
  %33843 = add i64 %33820, 38
  store i64 %33843, i64* %3, align 8
  %33844 = load i16, i16* %33830, align 2
  %33845 = zext i16 %33844 to i64
  store i64 %33845, i64* %576, align 8
  %33846 = zext i16 %33844 to i64
  store i64 %33846, i64* %RCX.i11580, align 8
  %33847 = shl nuw nsw i64 %33846, 2
  %33848 = add i64 %33842, %33847
  %33849 = add i64 %33820, 43
  store i64 %33849, i64* %3, align 8
  %33850 = inttoptr i64 %33848 to i32*
  %33851 = load i32, i32* %33850, align 4
  %33852 = zext i32 %33851 to i64
  store i64 %33852, i64* %576, align 8
  %33853 = add i64 %33820, -18728
  %33854 = add i64 %33820, 48
  %33855 = load i64, i64* %6, align 8
  %33856 = add i64 %33855, -8
  %33857 = inttoptr i64 %33856 to i64*
  store i64 %33854, i64* %33857, align 8
  store i64 %33856, i64* %6, align 8
  store i64 %33853, i64* %3, align 8
  %call2_40e5f3 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33853, %struct.Memory* %call2_40e5a9)
  %33858 = load i64, i64* %RBP.i, align 8
  %33859 = add i64 %33858, -120
  %33860 = load i64, i64* %3, align 8
  %33861 = add i64 %33860, 4
  store i64 %33861, i64* %3, align 8
  %33862 = inttoptr i64 %33859 to i64*
  %33863 = load i64, i64* %33862, align 8
  store i64 %33863, i64* %RAX.i11582.pre-phi, align 8
  %33864 = add i64 %33858, -28
  %33865 = add i64 %33860, 7
  store i64 %33865, i64* %3, align 8
  %33866 = inttoptr i64 %33864 to i32*
  %33867 = load i32, i32* %33866, align 4
  %33868 = add i32 %33867, 41
  %33869 = zext i32 %33868 to i64
  store i64 %33869, i64* %576, align 8
  %33870 = icmp ugt i32 %33867, -42
  %33871 = zext i1 %33870 to i8
  store i8 %33871, i8* %14, align 1
  %33872 = and i32 %33868, 255
  %33873 = tail call i32 @llvm.ctpop.i32(i32 %33872)
  %33874 = trunc i32 %33873 to i8
  %33875 = and i8 %33874, 1
  %33876 = xor i8 %33875, 1
  store i8 %33876, i8* %21, align 1
  %33877 = xor i32 %33868, %33867
  %33878 = lshr i32 %33877, 4
  %33879 = trunc i32 %33878 to i8
  %33880 = and i8 %33879, 1
  store i8 %33880, i8* %27, align 1
  %33881 = icmp eq i32 %33868, 0
  %33882 = zext i1 %33881 to i8
  store i8 %33882, i8* %30, align 1
  %33883 = lshr i32 %33868, 31
  %33884 = trunc i32 %33883 to i8
  store i8 %33884, i8* %33, align 1
  %33885 = lshr i32 %33867, 31
  %33886 = xor i32 %33883, %33885
  %33887 = add nuw nsw i32 %33886, %33883
  %33888 = icmp eq i32 %33887, 2
  %33889 = zext i1 %33888 to i8
  store i8 %33889, i8* %39, align 1
  %33890 = sext i32 %33868 to i64
  store i64 %33890, i64* %RCX.i11580, align 8
  %33891 = shl nsw i64 %33890, 1
  %33892 = add i64 %33863, %33891
  %33893 = add i64 %33860, 18
  store i64 %33893, i64* %3, align 8
  %33894 = inttoptr i64 %33892 to i16*
  %33895 = load i16, i16* %33894, align 2
  store i16 %33895, i16* %R9W.i2497, align 2
  %33896 = add i64 %33858, -198
  %33897 = add i64 %33860, 26
  store i64 %33897, i64* %3, align 8
  %33898 = inttoptr i64 %33896 to i16*
  store i16 %33895, i16* %33898, align 2
  %33899 = load i64, i64* %RBP.i, align 8
  %33900 = add i64 %33899, -8
  %33901 = load i64, i64* %3, align 8
  %33902 = add i64 %33901, 4
  store i64 %33902, i64* %3, align 8
  %33903 = inttoptr i64 %33900 to i64*
  %33904 = load i64, i64* %33903, align 8
  store i64 %33904, i64* %RDI.i2910, align 8
  %33905 = add i64 %33899, -208
  %33906 = add i64 %33901, 11
  store i64 %33906, i64* %3, align 8
  %33907 = inttoptr i64 %33905 to i64*
  %33908 = load i64, i64* %33907, align 8
  store i64 %33908, i64* %RAX.i11582.pre-phi, align 8
  %33909 = add i64 %33899, -198
  %33910 = add i64 %33901, 18
  store i64 %33910, i64* %3, align 8
  %33911 = inttoptr i64 %33909 to i16*
  %33912 = load i16, i16* %33911, align 2
  %33913 = zext i16 %33912 to i64
  store i64 %33913, i64* %576, align 8
  %33914 = zext i16 %33912 to i64
  store i64 %33914, i64* %RCX.i11580, align 8
  %33915 = add i64 %33908, %33914
  %33916 = add i64 %33901, 24
  store i64 %33916, i64* %3, align 8
  %33917 = inttoptr i64 %33915 to i8*
  %33918 = load i8, i8* %33917, align 1
  %33919 = zext i8 %33918 to i64
  store i64 %33919, i64* %RSI.i11312, align 8
  %33920 = add i64 %33899, -216
  %33921 = add i64 %33901, 31
  store i64 %33921, i64* %3, align 8
  %33922 = inttoptr i64 %33920 to i64*
  %33923 = load i64, i64* %33922, align 8
  store i64 %33923, i64* %RAX.i11582.pre-phi, align 8
  %33924 = add i64 %33901, 38
  store i64 %33924, i64* %3, align 8
  %33925 = load i16, i16* %33911, align 2
  %33926 = zext i16 %33925 to i64
  store i64 %33926, i64* %576, align 8
  %33927 = zext i16 %33925 to i64
  store i64 %33927, i64* %RCX.i11580, align 8
  %33928 = shl nuw nsw i64 %33927, 2
  %33929 = add i64 %33923, %33928
  %33930 = add i64 %33901, 43
  store i64 %33930, i64* %3, align 8
  %33931 = inttoptr i64 %33929 to i32*
  %33932 = load i32, i32* %33931, align 4
  %33933 = zext i32 %33932 to i64
  store i64 %33933, i64* %576, align 8
  %33934 = add i64 %33901, -18802
  %33935 = add i64 %33901, 48
  %33936 = load i64, i64* %6, align 8
  %33937 = add i64 %33936, -8
  %33938 = inttoptr i64 %33937 to i64*
  store i64 %33935, i64* %33938, align 8
  store i64 %33937, i64* %6, align 8
  store i64 %33934, i64* %3, align 8
  %call2_40e63d = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %33934, %struct.Memory* %call2_40e5f3)
  %33939 = load i64, i64* %RBP.i, align 8
  %33940 = add i64 %33939, -120
  %33941 = load i64, i64* %3, align 8
  %33942 = add i64 %33941, 4
  store i64 %33942, i64* %3, align 8
  %33943 = inttoptr i64 %33940 to i64*
  %33944 = load i64, i64* %33943, align 8
  store i64 %33944, i64* %RAX.i11582.pre-phi, align 8
  %33945 = add i64 %33939, -28
  %33946 = add i64 %33941, 7
  store i64 %33946, i64* %3, align 8
  %33947 = inttoptr i64 %33945 to i32*
  %33948 = load i32, i32* %33947, align 4
  %33949 = add i32 %33948, 42
  %33950 = zext i32 %33949 to i64
  store i64 %33950, i64* %576, align 8
  %33951 = icmp ugt i32 %33948, -43
  %33952 = zext i1 %33951 to i8
  store i8 %33952, i8* %14, align 1
  %33953 = and i32 %33949, 255
  %33954 = tail call i32 @llvm.ctpop.i32(i32 %33953)
  %33955 = trunc i32 %33954 to i8
  %33956 = and i8 %33955, 1
  %33957 = xor i8 %33956, 1
  store i8 %33957, i8* %21, align 1
  %33958 = xor i32 %33949, %33948
  %33959 = lshr i32 %33958, 4
  %33960 = trunc i32 %33959 to i8
  %33961 = and i8 %33960, 1
  store i8 %33961, i8* %27, align 1
  %33962 = icmp eq i32 %33949, 0
  %33963 = zext i1 %33962 to i8
  store i8 %33963, i8* %30, align 1
  %33964 = lshr i32 %33949, 31
  %33965 = trunc i32 %33964 to i8
  store i8 %33965, i8* %33, align 1
  %33966 = lshr i32 %33948, 31
  %33967 = xor i32 %33964, %33966
  %33968 = add nuw nsw i32 %33967, %33964
  %33969 = icmp eq i32 %33968, 2
  %33970 = zext i1 %33969 to i8
  store i8 %33970, i8* %39, align 1
  %33971 = sext i32 %33949 to i64
  store i64 %33971, i64* %RCX.i11580, align 8
  %33972 = shl nsw i64 %33971, 1
  %33973 = add i64 %33944, %33972
  %33974 = add i64 %33941, 18
  store i64 %33974, i64* %3, align 8
  %33975 = inttoptr i64 %33973 to i16*
  %33976 = load i16, i16* %33975, align 2
  store i16 %33976, i16* %R9W.i2497, align 2
  %33977 = add i64 %33939, -198
  %33978 = add i64 %33941, 26
  store i64 %33978, i64* %3, align 8
  %33979 = inttoptr i64 %33977 to i16*
  store i16 %33976, i16* %33979, align 2
  %33980 = load i64, i64* %RBP.i, align 8
  %33981 = add i64 %33980, -8
  %33982 = load i64, i64* %3, align 8
  %33983 = add i64 %33982, 4
  store i64 %33983, i64* %3, align 8
  %33984 = inttoptr i64 %33981 to i64*
  %33985 = load i64, i64* %33984, align 8
  store i64 %33985, i64* %RDI.i2910, align 8
  %33986 = add i64 %33980, -208
  %33987 = add i64 %33982, 11
  store i64 %33987, i64* %3, align 8
  %33988 = inttoptr i64 %33986 to i64*
  %33989 = load i64, i64* %33988, align 8
  store i64 %33989, i64* %RAX.i11582.pre-phi, align 8
  %33990 = add i64 %33980, -198
  %33991 = add i64 %33982, 18
  store i64 %33991, i64* %3, align 8
  %33992 = inttoptr i64 %33990 to i16*
  %33993 = load i16, i16* %33992, align 2
  %33994 = zext i16 %33993 to i64
  store i64 %33994, i64* %576, align 8
  %33995 = zext i16 %33993 to i64
  store i64 %33995, i64* %RCX.i11580, align 8
  %33996 = add i64 %33989, %33995
  %33997 = add i64 %33982, 24
  store i64 %33997, i64* %3, align 8
  %33998 = inttoptr i64 %33996 to i8*
  %33999 = load i8, i8* %33998, align 1
  %34000 = zext i8 %33999 to i64
  store i64 %34000, i64* %RSI.i11312, align 8
  %34001 = add i64 %33980, -216
  %34002 = add i64 %33982, 31
  store i64 %34002, i64* %3, align 8
  %34003 = inttoptr i64 %34001 to i64*
  %34004 = load i64, i64* %34003, align 8
  store i64 %34004, i64* %RAX.i11582.pre-phi, align 8
  %34005 = add i64 %33982, 38
  store i64 %34005, i64* %3, align 8
  %34006 = load i16, i16* %33992, align 2
  %34007 = zext i16 %34006 to i64
  store i64 %34007, i64* %576, align 8
  %34008 = zext i16 %34006 to i64
  store i64 %34008, i64* %RCX.i11580, align 8
  %34009 = shl nuw nsw i64 %34008, 2
  %34010 = add i64 %34004, %34009
  %34011 = add i64 %33982, 43
  store i64 %34011, i64* %3, align 8
  %34012 = inttoptr i64 %34010 to i32*
  %34013 = load i32, i32* %34012, align 4
  %34014 = zext i32 %34013 to i64
  store i64 %34014, i64* %576, align 8
  %34015 = add i64 %33982, -18876
  %34016 = add i64 %33982, 48
  %34017 = load i64, i64* %6, align 8
  %34018 = add i64 %34017, -8
  %34019 = inttoptr i64 %34018 to i64*
  store i64 %34016, i64* %34019, align 8
  store i64 %34018, i64* %6, align 8
  store i64 %34015, i64* %3, align 8
  %call2_40e687 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34015, %struct.Memory* %call2_40e63d)
  %34020 = load i64, i64* %RBP.i, align 8
  %34021 = add i64 %34020, -120
  %34022 = load i64, i64* %3, align 8
  %34023 = add i64 %34022, 4
  store i64 %34023, i64* %3, align 8
  %34024 = inttoptr i64 %34021 to i64*
  %34025 = load i64, i64* %34024, align 8
  store i64 %34025, i64* %RAX.i11582.pre-phi, align 8
  %34026 = add i64 %34020, -28
  %34027 = add i64 %34022, 7
  store i64 %34027, i64* %3, align 8
  %34028 = inttoptr i64 %34026 to i32*
  %34029 = load i32, i32* %34028, align 4
  %34030 = add i32 %34029, 43
  %34031 = zext i32 %34030 to i64
  store i64 %34031, i64* %576, align 8
  %34032 = icmp ugt i32 %34029, -44
  %34033 = zext i1 %34032 to i8
  store i8 %34033, i8* %14, align 1
  %34034 = and i32 %34030, 255
  %34035 = tail call i32 @llvm.ctpop.i32(i32 %34034)
  %34036 = trunc i32 %34035 to i8
  %34037 = and i8 %34036, 1
  %34038 = xor i8 %34037, 1
  store i8 %34038, i8* %21, align 1
  %34039 = xor i32 %34030, %34029
  %34040 = lshr i32 %34039, 4
  %34041 = trunc i32 %34040 to i8
  %34042 = and i8 %34041, 1
  store i8 %34042, i8* %27, align 1
  %34043 = icmp eq i32 %34030, 0
  %34044 = zext i1 %34043 to i8
  store i8 %34044, i8* %30, align 1
  %34045 = lshr i32 %34030, 31
  %34046 = trunc i32 %34045 to i8
  store i8 %34046, i8* %33, align 1
  %34047 = lshr i32 %34029, 31
  %34048 = xor i32 %34045, %34047
  %34049 = add nuw nsw i32 %34048, %34045
  %34050 = icmp eq i32 %34049, 2
  %34051 = zext i1 %34050 to i8
  store i8 %34051, i8* %39, align 1
  %34052 = sext i32 %34030 to i64
  store i64 %34052, i64* %RCX.i11580, align 8
  %34053 = shl nsw i64 %34052, 1
  %34054 = add i64 %34025, %34053
  %34055 = add i64 %34022, 18
  store i64 %34055, i64* %3, align 8
  %34056 = inttoptr i64 %34054 to i16*
  %34057 = load i16, i16* %34056, align 2
  store i16 %34057, i16* %R9W.i2497, align 2
  %34058 = add i64 %34020, -198
  %34059 = add i64 %34022, 26
  store i64 %34059, i64* %3, align 8
  %34060 = inttoptr i64 %34058 to i16*
  store i16 %34057, i16* %34060, align 2
  %34061 = load i64, i64* %RBP.i, align 8
  %34062 = add i64 %34061, -8
  %34063 = load i64, i64* %3, align 8
  %34064 = add i64 %34063, 4
  store i64 %34064, i64* %3, align 8
  %34065 = inttoptr i64 %34062 to i64*
  %34066 = load i64, i64* %34065, align 8
  store i64 %34066, i64* %RDI.i2910, align 8
  %34067 = add i64 %34061, -208
  %34068 = add i64 %34063, 11
  store i64 %34068, i64* %3, align 8
  %34069 = inttoptr i64 %34067 to i64*
  %34070 = load i64, i64* %34069, align 8
  store i64 %34070, i64* %RAX.i11582.pre-phi, align 8
  %34071 = add i64 %34061, -198
  %34072 = add i64 %34063, 18
  store i64 %34072, i64* %3, align 8
  %34073 = inttoptr i64 %34071 to i16*
  %34074 = load i16, i16* %34073, align 2
  %34075 = zext i16 %34074 to i64
  store i64 %34075, i64* %576, align 8
  %34076 = zext i16 %34074 to i64
  store i64 %34076, i64* %RCX.i11580, align 8
  %34077 = add i64 %34070, %34076
  %34078 = add i64 %34063, 24
  store i64 %34078, i64* %3, align 8
  %34079 = inttoptr i64 %34077 to i8*
  %34080 = load i8, i8* %34079, align 1
  %34081 = zext i8 %34080 to i64
  store i64 %34081, i64* %RSI.i11312, align 8
  %34082 = add i64 %34061, -216
  %34083 = add i64 %34063, 31
  store i64 %34083, i64* %3, align 8
  %34084 = inttoptr i64 %34082 to i64*
  %34085 = load i64, i64* %34084, align 8
  store i64 %34085, i64* %RAX.i11582.pre-phi, align 8
  %34086 = add i64 %34063, 38
  store i64 %34086, i64* %3, align 8
  %34087 = load i16, i16* %34073, align 2
  %34088 = zext i16 %34087 to i64
  store i64 %34088, i64* %576, align 8
  %34089 = zext i16 %34087 to i64
  store i64 %34089, i64* %RCX.i11580, align 8
  %34090 = shl nuw nsw i64 %34089, 2
  %34091 = add i64 %34085, %34090
  %34092 = add i64 %34063, 43
  store i64 %34092, i64* %3, align 8
  %34093 = inttoptr i64 %34091 to i32*
  %34094 = load i32, i32* %34093, align 4
  %34095 = zext i32 %34094 to i64
  store i64 %34095, i64* %576, align 8
  %34096 = add i64 %34063, -18950
  %34097 = add i64 %34063, 48
  %34098 = load i64, i64* %6, align 8
  %34099 = add i64 %34098, -8
  %34100 = inttoptr i64 %34099 to i64*
  store i64 %34097, i64* %34100, align 8
  store i64 %34099, i64* %6, align 8
  store i64 %34096, i64* %3, align 8
  %call2_40e6d1 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34096, %struct.Memory* %call2_40e687)
  %34101 = load i64, i64* %RBP.i, align 8
  %34102 = add i64 %34101, -120
  %34103 = load i64, i64* %3, align 8
  %34104 = add i64 %34103, 4
  store i64 %34104, i64* %3, align 8
  %34105 = inttoptr i64 %34102 to i64*
  %34106 = load i64, i64* %34105, align 8
  store i64 %34106, i64* %RAX.i11582.pre-phi, align 8
  %34107 = add i64 %34101, -28
  %34108 = add i64 %34103, 7
  store i64 %34108, i64* %3, align 8
  %34109 = inttoptr i64 %34107 to i32*
  %34110 = load i32, i32* %34109, align 4
  %34111 = add i32 %34110, 44
  %34112 = zext i32 %34111 to i64
  store i64 %34112, i64* %576, align 8
  %34113 = icmp ugt i32 %34110, -45
  %34114 = zext i1 %34113 to i8
  store i8 %34114, i8* %14, align 1
  %34115 = and i32 %34111, 255
  %34116 = tail call i32 @llvm.ctpop.i32(i32 %34115)
  %34117 = trunc i32 %34116 to i8
  %34118 = and i8 %34117, 1
  %34119 = xor i8 %34118, 1
  store i8 %34119, i8* %21, align 1
  %34120 = xor i32 %34111, %34110
  %34121 = lshr i32 %34120, 4
  %34122 = trunc i32 %34121 to i8
  %34123 = and i8 %34122, 1
  store i8 %34123, i8* %27, align 1
  %34124 = icmp eq i32 %34111, 0
  %34125 = zext i1 %34124 to i8
  store i8 %34125, i8* %30, align 1
  %34126 = lshr i32 %34111, 31
  %34127 = trunc i32 %34126 to i8
  store i8 %34127, i8* %33, align 1
  %34128 = lshr i32 %34110, 31
  %34129 = xor i32 %34126, %34128
  %34130 = add nuw nsw i32 %34129, %34126
  %34131 = icmp eq i32 %34130, 2
  %34132 = zext i1 %34131 to i8
  store i8 %34132, i8* %39, align 1
  %34133 = sext i32 %34111 to i64
  store i64 %34133, i64* %RCX.i11580, align 8
  %34134 = shl nsw i64 %34133, 1
  %34135 = add i64 %34106, %34134
  %34136 = add i64 %34103, 18
  store i64 %34136, i64* %3, align 8
  %34137 = inttoptr i64 %34135 to i16*
  %34138 = load i16, i16* %34137, align 2
  store i16 %34138, i16* %R9W.i2497, align 2
  %34139 = add i64 %34101, -198
  %34140 = add i64 %34103, 26
  store i64 %34140, i64* %3, align 8
  %34141 = inttoptr i64 %34139 to i16*
  store i16 %34138, i16* %34141, align 2
  %34142 = load i64, i64* %RBP.i, align 8
  %34143 = add i64 %34142, -8
  %34144 = load i64, i64* %3, align 8
  %34145 = add i64 %34144, 4
  store i64 %34145, i64* %3, align 8
  %34146 = inttoptr i64 %34143 to i64*
  %34147 = load i64, i64* %34146, align 8
  store i64 %34147, i64* %RDI.i2910, align 8
  %34148 = add i64 %34142, -208
  %34149 = add i64 %34144, 11
  store i64 %34149, i64* %3, align 8
  %34150 = inttoptr i64 %34148 to i64*
  %34151 = load i64, i64* %34150, align 8
  store i64 %34151, i64* %RAX.i11582.pre-phi, align 8
  %34152 = add i64 %34142, -198
  %34153 = add i64 %34144, 18
  store i64 %34153, i64* %3, align 8
  %34154 = inttoptr i64 %34152 to i16*
  %34155 = load i16, i16* %34154, align 2
  %34156 = zext i16 %34155 to i64
  store i64 %34156, i64* %576, align 8
  %34157 = zext i16 %34155 to i64
  store i64 %34157, i64* %RCX.i11580, align 8
  %34158 = add i64 %34151, %34157
  %34159 = add i64 %34144, 24
  store i64 %34159, i64* %3, align 8
  %34160 = inttoptr i64 %34158 to i8*
  %34161 = load i8, i8* %34160, align 1
  %34162 = zext i8 %34161 to i64
  store i64 %34162, i64* %RSI.i11312, align 8
  %34163 = add i64 %34142, -216
  %34164 = add i64 %34144, 31
  store i64 %34164, i64* %3, align 8
  %34165 = inttoptr i64 %34163 to i64*
  %34166 = load i64, i64* %34165, align 8
  store i64 %34166, i64* %RAX.i11582.pre-phi, align 8
  %34167 = add i64 %34144, 38
  store i64 %34167, i64* %3, align 8
  %34168 = load i16, i16* %34154, align 2
  %34169 = zext i16 %34168 to i64
  store i64 %34169, i64* %576, align 8
  %34170 = zext i16 %34168 to i64
  store i64 %34170, i64* %RCX.i11580, align 8
  %34171 = shl nuw nsw i64 %34170, 2
  %34172 = add i64 %34166, %34171
  %34173 = add i64 %34144, 43
  store i64 %34173, i64* %3, align 8
  %34174 = inttoptr i64 %34172 to i32*
  %34175 = load i32, i32* %34174, align 4
  %34176 = zext i32 %34175 to i64
  store i64 %34176, i64* %576, align 8
  %34177 = add i64 %34144, -19024
  %34178 = add i64 %34144, 48
  %34179 = load i64, i64* %6, align 8
  %34180 = add i64 %34179, -8
  %34181 = inttoptr i64 %34180 to i64*
  store i64 %34178, i64* %34181, align 8
  store i64 %34180, i64* %6, align 8
  store i64 %34177, i64* %3, align 8
  %call2_40e71b = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34177, %struct.Memory* %call2_40e6d1)
  %34182 = load i64, i64* %RBP.i, align 8
  %34183 = add i64 %34182, -120
  %34184 = load i64, i64* %3, align 8
  %34185 = add i64 %34184, 4
  store i64 %34185, i64* %3, align 8
  %34186 = inttoptr i64 %34183 to i64*
  %34187 = load i64, i64* %34186, align 8
  store i64 %34187, i64* %RAX.i11582.pre-phi, align 8
  %34188 = add i64 %34182, -28
  %34189 = add i64 %34184, 7
  store i64 %34189, i64* %3, align 8
  %34190 = inttoptr i64 %34188 to i32*
  %34191 = load i32, i32* %34190, align 4
  %34192 = add i32 %34191, 45
  %34193 = zext i32 %34192 to i64
  store i64 %34193, i64* %576, align 8
  %34194 = icmp ugt i32 %34191, -46
  %34195 = zext i1 %34194 to i8
  store i8 %34195, i8* %14, align 1
  %34196 = and i32 %34192, 255
  %34197 = tail call i32 @llvm.ctpop.i32(i32 %34196)
  %34198 = trunc i32 %34197 to i8
  %34199 = and i8 %34198, 1
  %34200 = xor i8 %34199, 1
  store i8 %34200, i8* %21, align 1
  %34201 = xor i32 %34192, %34191
  %34202 = lshr i32 %34201, 4
  %34203 = trunc i32 %34202 to i8
  %34204 = and i8 %34203, 1
  store i8 %34204, i8* %27, align 1
  %34205 = icmp eq i32 %34192, 0
  %34206 = zext i1 %34205 to i8
  store i8 %34206, i8* %30, align 1
  %34207 = lshr i32 %34192, 31
  %34208 = trunc i32 %34207 to i8
  store i8 %34208, i8* %33, align 1
  %34209 = lshr i32 %34191, 31
  %34210 = xor i32 %34207, %34209
  %34211 = add nuw nsw i32 %34210, %34207
  %34212 = icmp eq i32 %34211, 2
  %34213 = zext i1 %34212 to i8
  store i8 %34213, i8* %39, align 1
  %34214 = sext i32 %34192 to i64
  store i64 %34214, i64* %RCX.i11580, align 8
  %34215 = shl nsw i64 %34214, 1
  %34216 = add i64 %34187, %34215
  %34217 = add i64 %34184, 18
  store i64 %34217, i64* %3, align 8
  %34218 = inttoptr i64 %34216 to i16*
  %34219 = load i16, i16* %34218, align 2
  store i16 %34219, i16* %R9W.i2497, align 2
  %34220 = add i64 %34182, -198
  %34221 = add i64 %34184, 26
  store i64 %34221, i64* %3, align 8
  %34222 = inttoptr i64 %34220 to i16*
  store i16 %34219, i16* %34222, align 2
  %34223 = load i64, i64* %RBP.i, align 8
  %34224 = add i64 %34223, -8
  %34225 = load i64, i64* %3, align 8
  %34226 = add i64 %34225, 4
  store i64 %34226, i64* %3, align 8
  %34227 = inttoptr i64 %34224 to i64*
  %34228 = load i64, i64* %34227, align 8
  store i64 %34228, i64* %RDI.i2910, align 8
  %34229 = add i64 %34223, -208
  %34230 = add i64 %34225, 11
  store i64 %34230, i64* %3, align 8
  %34231 = inttoptr i64 %34229 to i64*
  %34232 = load i64, i64* %34231, align 8
  store i64 %34232, i64* %RAX.i11582.pre-phi, align 8
  %34233 = add i64 %34223, -198
  %34234 = add i64 %34225, 18
  store i64 %34234, i64* %3, align 8
  %34235 = inttoptr i64 %34233 to i16*
  %34236 = load i16, i16* %34235, align 2
  %34237 = zext i16 %34236 to i64
  store i64 %34237, i64* %576, align 8
  %34238 = zext i16 %34236 to i64
  store i64 %34238, i64* %RCX.i11580, align 8
  %34239 = add i64 %34232, %34238
  %34240 = add i64 %34225, 24
  store i64 %34240, i64* %3, align 8
  %34241 = inttoptr i64 %34239 to i8*
  %34242 = load i8, i8* %34241, align 1
  %34243 = zext i8 %34242 to i64
  store i64 %34243, i64* %RSI.i11312, align 8
  %34244 = add i64 %34223, -216
  %34245 = add i64 %34225, 31
  store i64 %34245, i64* %3, align 8
  %34246 = inttoptr i64 %34244 to i64*
  %34247 = load i64, i64* %34246, align 8
  store i64 %34247, i64* %RAX.i11582.pre-phi, align 8
  %34248 = add i64 %34225, 38
  store i64 %34248, i64* %3, align 8
  %34249 = load i16, i16* %34235, align 2
  %34250 = zext i16 %34249 to i64
  store i64 %34250, i64* %576, align 8
  %34251 = zext i16 %34249 to i64
  store i64 %34251, i64* %RCX.i11580, align 8
  %34252 = shl nuw nsw i64 %34251, 2
  %34253 = add i64 %34247, %34252
  %34254 = add i64 %34225, 43
  store i64 %34254, i64* %3, align 8
  %34255 = inttoptr i64 %34253 to i32*
  %34256 = load i32, i32* %34255, align 4
  %34257 = zext i32 %34256 to i64
  store i64 %34257, i64* %576, align 8
  %34258 = add i64 %34225, -19098
  %34259 = add i64 %34225, 48
  %34260 = load i64, i64* %6, align 8
  %34261 = add i64 %34260, -8
  %34262 = inttoptr i64 %34261 to i64*
  store i64 %34259, i64* %34262, align 8
  store i64 %34261, i64* %6, align 8
  store i64 %34258, i64* %3, align 8
  %call2_40e765 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34258, %struct.Memory* %call2_40e71b)
  %34263 = load i64, i64* %RBP.i, align 8
  %34264 = add i64 %34263, -120
  %34265 = load i64, i64* %3, align 8
  %34266 = add i64 %34265, 4
  store i64 %34266, i64* %3, align 8
  %34267 = inttoptr i64 %34264 to i64*
  %34268 = load i64, i64* %34267, align 8
  store i64 %34268, i64* %RAX.i11582.pre-phi, align 8
  %34269 = add i64 %34263, -28
  %34270 = add i64 %34265, 7
  store i64 %34270, i64* %3, align 8
  %34271 = inttoptr i64 %34269 to i32*
  %34272 = load i32, i32* %34271, align 4
  %34273 = add i32 %34272, 46
  %34274 = zext i32 %34273 to i64
  store i64 %34274, i64* %576, align 8
  %34275 = icmp ugt i32 %34272, -47
  %34276 = zext i1 %34275 to i8
  store i8 %34276, i8* %14, align 1
  %34277 = and i32 %34273, 255
  %34278 = tail call i32 @llvm.ctpop.i32(i32 %34277)
  %34279 = trunc i32 %34278 to i8
  %34280 = and i8 %34279, 1
  %34281 = xor i8 %34280, 1
  store i8 %34281, i8* %21, align 1
  %34282 = xor i32 %34273, %34272
  %34283 = lshr i32 %34282, 4
  %34284 = trunc i32 %34283 to i8
  %34285 = and i8 %34284, 1
  store i8 %34285, i8* %27, align 1
  %34286 = icmp eq i32 %34273, 0
  %34287 = zext i1 %34286 to i8
  store i8 %34287, i8* %30, align 1
  %34288 = lshr i32 %34273, 31
  %34289 = trunc i32 %34288 to i8
  store i8 %34289, i8* %33, align 1
  %34290 = lshr i32 %34272, 31
  %34291 = xor i32 %34288, %34290
  %34292 = add nuw nsw i32 %34291, %34288
  %34293 = icmp eq i32 %34292, 2
  %34294 = zext i1 %34293 to i8
  store i8 %34294, i8* %39, align 1
  %34295 = sext i32 %34273 to i64
  store i64 %34295, i64* %RCX.i11580, align 8
  %34296 = shl nsw i64 %34295, 1
  %34297 = add i64 %34268, %34296
  %34298 = add i64 %34265, 18
  store i64 %34298, i64* %3, align 8
  %34299 = inttoptr i64 %34297 to i16*
  %34300 = load i16, i16* %34299, align 2
  store i16 %34300, i16* %R9W.i2497, align 2
  %34301 = add i64 %34263, -198
  %34302 = add i64 %34265, 26
  store i64 %34302, i64* %3, align 8
  %34303 = inttoptr i64 %34301 to i16*
  store i16 %34300, i16* %34303, align 2
  %34304 = load i64, i64* %RBP.i, align 8
  %34305 = add i64 %34304, -8
  %34306 = load i64, i64* %3, align 8
  %34307 = add i64 %34306, 4
  store i64 %34307, i64* %3, align 8
  %34308 = inttoptr i64 %34305 to i64*
  %34309 = load i64, i64* %34308, align 8
  store i64 %34309, i64* %RDI.i2910, align 8
  %34310 = add i64 %34304, -208
  %34311 = add i64 %34306, 11
  store i64 %34311, i64* %3, align 8
  %34312 = inttoptr i64 %34310 to i64*
  %34313 = load i64, i64* %34312, align 8
  store i64 %34313, i64* %RAX.i11582.pre-phi, align 8
  %34314 = add i64 %34304, -198
  %34315 = add i64 %34306, 18
  store i64 %34315, i64* %3, align 8
  %34316 = inttoptr i64 %34314 to i16*
  %34317 = load i16, i16* %34316, align 2
  %34318 = zext i16 %34317 to i64
  store i64 %34318, i64* %576, align 8
  %34319 = zext i16 %34317 to i64
  store i64 %34319, i64* %RCX.i11580, align 8
  %34320 = add i64 %34313, %34319
  %34321 = add i64 %34306, 24
  store i64 %34321, i64* %3, align 8
  %34322 = inttoptr i64 %34320 to i8*
  %34323 = load i8, i8* %34322, align 1
  %34324 = zext i8 %34323 to i64
  store i64 %34324, i64* %RSI.i11312, align 8
  %34325 = add i64 %34304, -216
  %34326 = add i64 %34306, 31
  store i64 %34326, i64* %3, align 8
  %34327 = inttoptr i64 %34325 to i64*
  %34328 = load i64, i64* %34327, align 8
  store i64 %34328, i64* %RAX.i11582.pre-phi, align 8
  %34329 = add i64 %34306, 38
  store i64 %34329, i64* %3, align 8
  %34330 = load i16, i16* %34316, align 2
  %34331 = zext i16 %34330 to i64
  store i64 %34331, i64* %576, align 8
  %34332 = zext i16 %34330 to i64
  store i64 %34332, i64* %RCX.i11580, align 8
  %34333 = shl nuw nsw i64 %34332, 2
  %34334 = add i64 %34328, %34333
  %34335 = add i64 %34306, 43
  store i64 %34335, i64* %3, align 8
  %34336 = inttoptr i64 %34334 to i32*
  %34337 = load i32, i32* %34336, align 4
  %34338 = zext i32 %34337 to i64
  store i64 %34338, i64* %576, align 8
  %34339 = add i64 %34306, -19172
  %34340 = add i64 %34306, 48
  %34341 = load i64, i64* %6, align 8
  %34342 = add i64 %34341, -8
  %34343 = inttoptr i64 %34342 to i64*
  store i64 %34340, i64* %34343, align 8
  store i64 %34342, i64* %6, align 8
  store i64 %34339, i64* %3, align 8
  %call2_40e7af = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34339, %struct.Memory* %call2_40e765)
  %34344 = load i64, i64* %RBP.i, align 8
  %34345 = add i64 %34344, -120
  %34346 = load i64, i64* %3, align 8
  %34347 = add i64 %34346, 4
  store i64 %34347, i64* %3, align 8
  %34348 = inttoptr i64 %34345 to i64*
  %34349 = load i64, i64* %34348, align 8
  store i64 %34349, i64* %RAX.i11582.pre-phi, align 8
  %34350 = add i64 %34344, -28
  %34351 = add i64 %34346, 7
  store i64 %34351, i64* %3, align 8
  %34352 = inttoptr i64 %34350 to i32*
  %34353 = load i32, i32* %34352, align 4
  %34354 = add i32 %34353, 47
  %34355 = zext i32 %34354 to i64
  store i64 %34355, i64* %576, align 8
  %34356 = icmp ugt i32 %34353, -48
  %34357 = zext i1 %34356 to i8
  store i8 %34357, i8* %14, align 1
  %34358 = and i32 %34354, 255
  %34359 = tail call i32 @llvm.ctpop.i32(i32 %34358)
  %34360 = trunc i32 %34359 to i8
  %34361 = and i8 %34360, 1
  %34362 = xor i8 %34361, 1
  store i8 %34362, i8* %21, align 1
  %34363 = xor i32 %34354, %34353
  %34364 = lshr i32 %34363, 4
  %34365 = trunc i32 %34364 to i8
  %34366 = and i8 %34365, 1
  store i8 %34366, i8* %27, align 1
  %34367 = icmp eq i32 %34354, 0
  %34368 = zext i1 %34367 to i8
  store i8 %34368, i8* %30, align 1
  %34369 = lshr i32 %34354, 31
  %34370 = trunc i32 %34369 to i8
  store i8 %34370, i8* %33, align 1
  %34371 = lshr i32 %34353, 31
  %34372 = xor i32 %34369, %34371
  %34373 = add nuw nsw i32 %34372, %34369
  %34374 = icmp eq i32 %34373, 2
  %34375 = zext i1 %34374 to i8
  store i8 %34375, i8* %39, align 1
  %34376 = sext i32 %34354 to i64
  store i64 %34376, i64* %RCX.i11580, align 8
  %34377 = shl nsw i64 %34376, 1
  %34378 = add i64 %34349, %34377
  %34379 = add i64 %34346, 18
  store i64 %34379, i64* %3, align 8
  %34380 = inttoptr i64 %34378 to i16*
  %34381 = load i16, i16* %34380, align 2
  store i16 %34381, i16* %R9W.i2497, align 2
  %34382 = add i64 %34344, -198
  %34383 = add i64 %34346, 26
  store i64 %34383, i64* %3, align 8
  %34384 = inttoptr i64 %34382 to i16*
  store i16 %34381, i16* %34384, align 2
  %34385 = load i64, i64* %RBP.i, align 8
  %34386 = add i64 %34385, -8
  %34387 = load i64, i64* %3, align 8
  %34388 = add i64 %34387, 4
  store i64 %34388, i64* %3, align 8
  %34389 = inttoptr i64 %34386 to i64*
  %34390 = load i64, i64* %34389, align 8
  store i64 %34390, i64* %RDI.i2910, align 8
  %34391 = add i64 %34385, -208
  %34392 = add i64 %34387, 11
  store i64 %34392, i64* %3, align 8
  %34393 = inttoptr i64 %34391 to i64*
  %34394 = load i64, i64* %34393, align 8
  store i64 %34394, i64* %RAX.i11582.pre-phi, align 8
  %34395 = add i64 %34385, -198
  %34396 = add i64 %34387, 18
  store i64 %34396, i64* %3, align 8
  %34397 = inttoptr i64 %34395 to i16*
  %34398 = load i16, i16* %34397, align 2
  %34399 = zext i16 %34398 to i64
  store i64 %34399, i64* %576, align 8
  %34400 = zext i16 %34398 to i64
  store i64 %34400, i64* %RCX.i11580, align 8
  %34401 = add i64 %34394, %34400
  %34402 = add i64 %34387, 24
  store i64 %34402, i64* %3, align 8
  %34403 = inttoptr i64 %34401 to i8*
  %34404 = load i8, i8* %34403, align 1
  %34405 = zext i8 %34404 to i64
  store i64 %34405, i64* %RSI.i11312, align 8
  %34406 = add i64 %34385, -216
  %34407 = add i64 %34387, 31
  store i64 %34407, i64* %3, align 8
  %34408 = inttoptr i64 %34406 to i64*
  %34409 = load i64, i64* %34408, align 8
  store i64 %34409, i64* %RAX.i11582.pre-phi, align 8
  %34410 = add i64 %34387, 38
  store i64 %34410, i64* %3, align 8
  %34411 = load i16, i16* %34397, align 2
  %34412 = zext i16 %34411 to i64
  store i64 %34412, i64* %576, align 8
  %34413 = zext i16 %34411 to i64
  store i64 %34413, i64* %RCX.i11580, align 8
  %34414 = shl nuw nsw i64 %34413, 2
  %34415 = add i64 %34409, %34414
  %34416 = add i64 %34387, 43
  store i64 %34416, i64* %3, align 8
  %34417 = inttoptr i64 %34415 to i32*
  %34418 = load i32, i32* %34417, align 4
  %34419 = zext i32 %34418 to i64
  store i64 %34419, i64* %576, align 8
  %34420 = add i64 %34387, -19246
  %34421 = add i64 %34387, 48
  %34422 = load i64, i64* %6, align 8
  %34423 = add i64 %34422, -8
  %34424 = inttoptr i64 %34423 to i64*
  store i64 %34421, i64* %34424, align 8
  store i64 %34423, i64* %6, align 8
  store i64 %34420, i64* %3, align 8
  %call2_40e7f9 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34420, %struct.Memory* %call2_40e7af)
  %34425 = load i64, i64* %RBP.i, align 8
  %34426 = add i64 %34425, -120
  %34427 = load i64, i64* %3, align 8
  %34428 = add i64 %34427, 4
  store i64 %34428, i64* %3, align 8
  %34429 = inttoptr i64 %34426 to i64*
  %34430 = load i64, i64* %34429, align 8
  store i64 %34430, i64* %RAX.i11582.pre-phi, align 8
  %34431 = add i64 %34425, -28
  %34432 = add i64 %34427, 7
  store i64 %34432, i64* %3, align 8
  %34433 = inttoptr i64 %34431 to i32*
  %34434 = load i32, i32* %34433, align 4
  %34435 = add i32 %34434, 48
  %34436 = zext i32 %34435 to i64
  store i64 %34436, i64* %576, align 8
  %34437 = icmp ugt i32 %34434, -49
  %34438 = zext i1 %34437 to i8
  store i8 %34438, i8* %14, align 1
  %34439 = and i32 %34435, 255
  %34440 = tail call i32 @llvm.ctpop.i32(i32 %34439)
  %34441 = trunc i32 %34440 to i8
  %34442 = and i8 %34441, 1
  %34443 = xor i8 %34442, 1
  store i8 %34443, i8* %21, align 1
  %34444 = xor i32 %34434, 16
  %34445 = xor i32 %34444, %34435
  %34446 = lshr i32 %34445, 4
  %34447 = trunc i32 %34446 to i8
  %34448 = and i8 %34447, 1
  store i8 %34448, i8* %27, align 1
  %34449 = icmp eq i32 %34435, 0
  %34450 = zext i1 %34449 to i8
  store i8 %34450, i8* %30, align 1
  %34451 = lshr i32 %34435, 31
  %34452 = trunc i32 %34451 to i8
  store i8 %34452, i8* %33, align 1
  %34453 = lshr i32 %34434, 31
  %34454 = xor i32 %34451, %34453
  %34455 = add nuw nsw i32 %34454, %34451
  %34456 = icmp eq i32 %34455, 2
  %34457 = zext i1 %34456 to i8
  store i8 %34457, i8* %39, align 1
  %34458 = sext i32 %34435 to i64
  store i64 %34458, i64* %RCX.i11580, align 8
  %34459 = shl nsw i64 %34458, 1
  %34460 = add i64 %34430, %34459
  %34461 = add i64 %34427, 18
  store i64 %34461, i64* %3, align 8
  %34462 = inttoptr i64 %34460 to i16*
  %34463 = load i16, i16* %34462, align 2
  store i16 %34463, i16* %R9W.i2497, align 2
  %34464 = add i64 %34425, -198
  %34465 = add i64 %34427, 26
  store i64 %34465, i64* %3, align 8
  %34466 = inttoptr i64 %34464 to i16*
  store i16 %34463, i16* %34466, align 2
  %34467 = load i64, i64* %RBP.i, align 8
  %34468 = add i64 %34467, -8
  %34469 = load i64, i64* %3, align 8
  %34470 = add i64 %34469, 4
  store i64 %34470, i64* %3, align 8
  %34471 = inttoptr i64 %34468 to i64*
  %34472 = load i64, i64* %34471, align 8
  store i64 %34472, i64* %RDI.i2910, align 8
  %34473 = add i64 %34467, -208
  %34474 = add i64 %34469, 11
  store i64 %34474, i64* %3, align 8
  %34475 = inttoptr i64 %34473 to i64*
  %34476 = load i64, i64* %34475, align 8
  store i64 %34476, i64* %RAX.i11582.pre-phi, align 8
  %34477 = add i64 %34467, -198
  %34478 = add i64 %34469, 18
  store i64 %34478, i64* %3, align 8
  %34479 = inttoptr i64 %34477 to i16*
  %34480 = load i16, i16* %34479, align 2
  %34481 = zext i16 %34480 to i64
  store i64 %34481, i64* %576, align 8
  %34482 = zext i16 %34480 to i64
  store i64 %34482, i64* %RCX.i11580, align 8
  %34483 = add i64 %34476, %34482
  %34484 = add i64 %34469, 24
  store i64 %34484, i64* %3, align 8
  %34485 = inttoptr i64 %34483 to i8*
  %34486 = load i8, i8* %34485, align 1
  %34487 = zext i8 %34486 to i64
  store i64 %34487, i64* %RSI.i11312, align 8
  %34488 = add i64 %34467, -216
  %34489 = add i64 %34469, 31
  store i64 %34489, i64* %3, align 8
  %34490 = inttoptr i64 %34488 to i64*
  %34491 = load i64, i64* %34490, align 8
  store i64 %34491, i64* %RAX.i11582.pre-phi, align 8
  %34492 = add i64 %34469, 38
  store i64 %34492, i64* %3, align 8
  %34493 = load i16, i16* %34479, align 2
  %34494 = zext i16 %34493 to i64
  store i64 %34494, i64* %576, align 8
  %34495 = zext i16 %34493 to i64
  store i64 %34495, i64* %RCX.i11580, align 8
  %34496 = shl nuw nsw i64 %34495, 2
  %34497 = add i64 %34491, %34496
  %34498 = add i64 %34469, 43
  store i64 %34498, i64* %3, align 8
  %34499 = inttoptr i64 %34497 to i32*
  %34500 = load i32, i32* %34499, align 4
  %34501 = zext i32 %34500 to i64
  store i64 %34501, i64* %576, align 8
  %34502 = add i64 %34469, -19320
  %34503 = add i64 %34469, 48
  %34504 = load i64, i64* %6, align 8
  %34505 = add i64 %34504, -8
  %34506 = inttoptr i64 %34505 to i64*
  store i64 %34503, i64* %34506, align 8
  store i64 %34505, i64* %6, align 8
  store i64 %34502, i64* %3, align 8
  %call2_40e843 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34502, %struct.Memory* %call2_40e7f9)
  %34507 = load i64, i64* %RBP.i, align 8
  %34508 = add i64 %34507, -120
  %34509 = load i64, i64* %3, align 8
  %34510 = add i64 %34509, 4
  store i64 %34510, i64* %3, align 8
  %34511 = inttoptr i64 %34508 to i64*
  %34512 = load i64, i64* %34511, align 8
  store i64 %34512, i64* %RAX.i11582.pre-phi, align 8
  %34513 = add i64 %34507, -28
  %34514 = add i64 %34509, 7
  store i64 %34514, i64* %3, align 8
  %34515 = inttoptr i64 %34513 to i32*
  %34516 = load i32, i32* %34515, align 4
  %34517 = add i32 %34516, 49
  %34518 = zext i32 %34517 to i64
  store i64 %34518, i64* %576, align 8
  %34519 = icmp ugt i32 %34516, -50
  %34520 = zext i1 %34519 to i8
  store i8 %34520, i8* %14, align 1
  %34521 = and i32 %34517, 255
  %34522 = tail call i32 @llvm.ctpop.i32(i32 %34521)
  %34523 = trunc i32 %34522 to i8
  %34524 = and i8 %34523, 1
  %34525 = xor i8 %34524, 1
  store i8 %34525, i8* %21, align 1
  %34526 = xor i32 %34516, 16
  %34527 = xor i32 %34526, %34517
  %34528 = lshr i32 %34527, 4
  %34529 = trunc i32 %34528 to i8
  %34530 = and i8 %34529, 1
  store i8 %34530, i8* %27, align 1
  %34531 = icmp eq i32 %34517, 0
  %34532 = zext i1 %34531 to i8
  store i8 %34532, i8* %30, align 1
  %34533 = lshr i32 %34517, 31
  %34534 = trunc i32 %34533 to i8
  store i8 %34534, i8* %33, align 1
  %34535 = lshr i32 %34516, 31
  %34536 = xor i32 %34533, %34535
  %34537 = add nuw nsw i32 %34536, %34533
  %34538 = icmp eq i32 %34537, 2
  %34539 = zext i1 %34538 to i8
  store i8 %34539, i8* %39, align 1
  %34540 = sext i32 %34517 to i64
  store i64 %34540, i64* %RCX.i11580, align 8
  %34541 = shl nsw i64 %34540, 1
  %34542 = add i64 %34512, %34541
  %34543 = add i64 %34509, 18
  store i64 %34543, i64* %3, align 8
  %34544 = inttoptr i64 %34542 to i16*
  %34545 = load i16, i16* %34544, align 2
  store i16 %34545, i16* %R9W.i2497, align 2
  %34546 = add i64 %34507, -198
  %34547 = add i64 %34509, 26
  store i64 %34547, i64* %3, align 8
  %34548 = inttoptr i64 %34546 to i16*
  store i16 %34545, i16* %34548, align 2
  %34549 = load i64, i64* %RBP.i, align 8
  %34550 = add i64 %34549, -8
  %34551 = load i64, i64* %3, align 8
  %34552 = add i64 %34551, 4
  store i64 %34552, i64* %3, align 8
  %34553 = inttoptr i64 %34550 to i64*
  %34554 = load i64, i64* %34553, align 8
  store i64 %34554, i64* %RDI.i2910, align 8
  %34555 = add i64 %34549, -208
  %34556 = add i64 %34551, 11
  store i64 %34556, i64* %3, align 8
  %34557 = inttoptr i64 %34555 to i64*
  %34558 = load i64, i64* %34557, align 8
  store i64 %34558, i64* %RAX.i11582.pre-phi, align 8
  %34559 = add i64 %34549, -198
  %34560 = add i64 %34551, 18
  store i64 %34560, i64* %3, align 8
  %34561 = inttoptr i64 %34559 to i16*
  %34562 = load i16, i16* %34561, align 2
  %34563 = zext i16 %34562 to i64
  store i64 %34563, i64* %576, align 8
  %34564 = zext i16 %34562 to i64
  store i64 %34564, i64* %RCX.i11580, align 8
  %34565 = add i64 %34558, %34564
  %34566 = add i64 %34551, 24
  store i64 %34566, i64* %3, align 8
  %34567 = inttoptr i64 %34565 to i8*
  %34568 = load i8, i8* %34567, align 1
  %34569 = zext i8 %34568 to i64
  store i64 %34569, i64* %RSI.i11312, align 8
  %34570 = add i64 %34549, -216
  %34571 = add i64 %34551, 31
  store i64 %34571, i64* %3, align 8
  %34572 = inttoptr i64 %34570 to i64*
  %34573 = load i64, i64* %34572, align 8
  store i64 %34573, i64* %RAX.i11582.pre-phi, align 8
  %34574 = add i64 %34551, 38
  store i64 %34574, i64* %3, align 8
  %34575 = load i16, i16* %34561, align 2
  %34576 = zext i16 %34575 to i64
  store i64 %34576, i64* %576, align 8
  %34577 = zext i16 %34575 to i64
  store i64 %34577, i64* %RCX.i11580, align 8
  %34578 = shl nuw nsw i64 %34577, 2
  %34579 = add i64 %34573, %34578
  %34580 = add i64 %34551, 43
  store i64 %34580, i64* %3, align 8
  %34581 = inttoptr i64 %34579 to i32*
  %34582 = load i32, i32* %34581, align 4
  %34583 = zext i32 %34582 to i64
  store i64 %34583, i64* %576, align 8
  %34584 = add i64 %34551, -19394
  %34585 = add i64 %34551, 48
  %34586 = load i64, i64* %6, align 8
  %34587 = add i64 %34586, -8
  %34588 = inttoptr i64 %34587 to i64*
  store i64 %34585, i64* %34588, align 8
  store i64 %34587, i64* %6, align 8
  store i64 %34584, i64* %3, align 8
  %call2_40e88d = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34584, %struct.Memory* %call2_40e843)
  %34589 = load i64, i64* %3, align 8
  %34590 = add i64 %34589, 166
  %.pre235 = load i64, i64* %RBP.i, align 8
  br label %block_.L_40e938

block_.L_40e897:                                  ; preds = %block_40d9ae, %block_.L_40d9a4
  %34591 = phi i64 [ %30355, %block_.L_40d9a4 ], [ %30389, %block_40d9ae ]
  %34592 = phi i64 [ %30329, %block_.L_40d9a4 ], [ %30390, %block_40d9ae ]
  %34593 = add i64 %34592, -28
  %34594 = add i64 %34591, 3
  store i64 %34594, i64* %3, align 8
  %34595 = inttoptr i64 %34593 to i32*
  %34596 = load i32, i32* %34595, align 4
  %34597 = zext i32 %34596 to i64
  store i64 %34597, i64* %RAX.i11582.pre-phi, align 8
  %34598 = add i64 %34592, -20
  %34599 = add i64 %34591, 6
  store i64 %34599, i64* %3, align 8
  %34600 = inttoptr i64 %34598 to i32*
  store i32 %34596, i32* %34600, align 4
  %.pre234 = load i64, i64* %3, align 8
  br label %block_.L_40e89d

block_.L_40e89d:                                  ; preds = %block_40e8a9, %block_.L_40e897
  %34601 = phi i64 [ %34853, %block_40e8a9 ], [ %.pre234, %block_.L_40e897 ]
  %34602 = load i64, i64* %RBP.i, align 8
  %34603 = add i64 %34602, -20
  %34604 = add i64 %34601, 3
  store i64 %34604, i64* %3, align 8
  %34605 = inttoptr i64 %34603 to i32*
  %34606 = load i32, i32* %34605, align 4
  %34607 = zext i32 %34606 to i64
  store i64 %34607, i64* %RAX.i11582.pre-phi, align 8
  %34608 = add i64 %34602, -32
  %34609 = add i64 %34601, 6
  store i64 %34609, i64* %3, align 8
  %34610 = inttoptr i64 %34608 to i32*
  %34611 = load i32, i32* %34610, align 4
  %34612 = sub i32 %34606, %34611
  %34613 = icmp ult i32 %34606, %34611
  %34614 = zext i1 %34613 to i8
  store i8 %34614, i8* %14, align 1
  %34615 = and i32 %34612, 255
  %34616 = tail call i32 @llvm.ctpop.i32(i32 %34615)
  %34617 = trunc i32 %34616 to i8
  %34618 = and i8 %34617, 1
  %34619 = xor i8 %34618, 1
  store i8 %34619, i8* %21, align 1
  %34620 = xor i32 %34611, %34606
  %34621 = xor i32 %34620, %34612
  %34622 = lshr i32 %34621, 4
  %34623 = trunc i32 %34622 to i8
  %34624 = and i8 %34623, 1
  store i8 %34624, i8* %27, align 1
  %34625 = icmp eq i32 %34612, 0
  %34626 = zext i1 %34625 to i8
  store i8 %34626, i8* %30, align 1
  %34627 = lshr i32 %34612, 31
  %34628 = trunc i32 %34627 to i8
  store i8 %34628, i8* %33, align 1
  %34629 = lshr i32 %34606, 31
  %34630 = lshr i32 %34611, 31
  %34631 = xor i32 %34630, %34629
  %34632 = xor i32 %34627, %34629
  %34633 = add nuw nsw i32 %34632, %34631
  %34634 = icmp eq i32 %34633, 2
  %34635 = zext i1 %34634 to i8
  store i8 %34635, i8* %39, align 1
  %34636 = icmp ne i8 %34628, 0
  %34637 = xor i1 %34636, %34634
  %.demorgan288 = or i1 %34625, %34637
  %.v327 = select i1 %.demorgan288, i64 12, i64 150
  %34638 = add i64 %34601, %.v327
  store i64 %34638, i64* %3, align 8
  br i1 %.demorgan288, label %block_40e8a9, label %block_.L_40e933

block_40e8a9:                                     ; preds = %block_.L_40e89d
  %34639 = add i64 %34602, -8
  %34640 = add i64 %34638, 4
  store i64 %34640, i64* %3, align 8
  %34641 = inttoptr i64 %34639 to i64*
  %34642 = load i64, i64* %34641, align 8
  store i64 %34642, i64* %RDI.i2910, align 8
  %34643 = add i64 %34638, 8
  store i64 %34643, i64* %3, align 8
  %34644 = load i64, i64* %34641, align 8
  %34645 = add i64 %34644, 37708
  store i64 %34645, i64* %RAX.i11582.pre-phi, align 8
  %34646 = icmp ugt i64 %34644, -37709
  %34647 = zext i1 %34646 to i8
  store i8 %34647, i8* %14, align 1
  %34648 = trunc i64 %34645 to i32
  %34649 = and i32 %34648, 255
  %34650 = tail call i32 @llvm.ctpop.i32(i32 %34649)
  %34651 = trunc i32 %34650 to i8
  %34652 = and i8 %34651, 1
  %34653 = xor i8 %34652, 1
  store i8 %34653, i8* %21, align 1
  %34654 = xor i64 %34645, %34644
  %34655 = lshr i64 %34654, 4
  %34656 = trunc i64 %34655 to i8
  %34657 = and i8 %34656, 1
  store i8 %34657, i8* %27, align 1
  %34658 = icmp eq i64 %34645, 0
  %34659 = zext i1 %34658 to i8
  store i8 %34659, i8* %30, align 1
  %34660 = lshr i64 %34645, 63
  %34661 = trunc i64 %34660 to i8
  store i8 %34661, i8* %33, align 1
  %34662 = lshr i64 %34644, 63
  %34663 = xor i64 %34660, %34662
  %34664 = add nuw nsw i64 %34663, %34660
  %34665 = icmp eq i64 %34664, 2
  %34666 = zext i1 %34665 to i8
  store i8 %34666, i8* %39, align 1
  %34667 = add i64 %34638, 18
  store i64 %34667, i64* %3, align 8
  %34668 = load i64, i64* %34641, align 8
  store i64 %34668, i64* %RCX.i11580, align 8
  %34669 = add i64 %34602, -68
  %34670 = add i64 %34638, 22
  store i64 %34670, i64* %3, align 8
  %34671 = inttoptr i64 %34669 to i32*
  %34672 = load i32, i32* %34671, align 4
  %34673 = sext i32 %34672 to i64
  store i64 %34673, i64* %576, align 8
  %34674 = add nsw i64 %34673, 1704
  %34675 = add i64 %34674, %34668
  %34676 = add i64 %34638, 30
  store i64 %34676, i64* %3, align 8
  %34677 = inttoptr i64 %34675 to i8*
  %34678 = load i8, i8* %34677, align 1
  %34679 = zext i8 %34678 to i64
  store i64 %34679, i64* %RSI.i11312, align 8
  %34680 = zext i8 %34678 to i64
  %34681 = mul nuw nsw i64 %34680, 258
  store i64 %34681, i64* %RCX.i11580, align 8
  %34682 = add i64 %34681, %34645
  store i64 %34682, i64* %RAX.i11582.pre-phi, align 8
  %34683 = icmp ult i64 %34682, %34645
  %34684 = icmp ult i64 %34682, %34681
  %34685 = or i1 %34683, %34684
  %34686 = zext i1 %34685 to i8
  store i8 %34686, i8* %14, align 1
  %34687 = trunc i64 %34682 to i32
  %34688 = and i32 %34687, 255
  %34689 = tail call i32 @llvm.ctpop.i32(i32 %34688)
  %34690 = trunc i32 %34689 to i8
  %34691 = and i8 %34690, 1
  %34692 = xor i8 %34691, 1
  store i8 %34692, i8* %21, align 1
  %34693 = xor i64 %34681, %34645
  %34694 = xor i64 %34693, %34682
  %34695 = lshr i64 %34694, 4
  %34696 = trunc i64 %34695 to i8
  %34697 = and i8 %34696, 1
  store i8 %34697, i8* %27, align 1
  %34698 = icmp eq i64 %34682, 0
  %34699 = zext i1 %34698 to i8
  store i8 %34699, i8* %30, align 1
  %34700 = lshr i64 %34682, 63
  %34701 = trunc i64 %34700 to i8
  store i8 %34701, i8* %33, align 1
  %34702 = xor i64 %34700, %34660
  %34703 = add nuw nsw i64 %34702, %34700
  %34704 = icmp eq i64 %34703, 2
  %34705 = zext i1 %34704 to i8
  store i8 %34705, i8* %39, align 1
  %34706 = load i64, i64* %RBP.i, align 8
  %34707 = add i64 %34706, -120
  %34708 = add i64 %34638, 46
  store i64 %34708, i64* %3, align 8
  %34709 = inttoptr i64 %34707 to i64*
  %34710 = load i64, i64* %34709, align 8
  store i64 %34710, i64* %RCX.i11580, align 8
  %34711 = add i64 %34706, -20
  %34712 = add i64 %34638, 50
  store i64 %34712, i64* %3, align 8
  %34713 = inttoptr i64 %34711 to i32*
  %34714 = load i32, i32* %34713, align 4
  %34715 = sext i32 %34714 to i64
  store i64 %34715, i64* %576, align 8
  %34716 = shl nsw i64 %34715, 1
  %34717 = add i64 %34716, %34710
  %34718 = add i64 %34638, 54
  store i64 %34718, i64* %3, align 8
  %34719 = inttoptr i64 %34717 to i16*
  %34720 = load i16, i16* %34719, align 2
  %34721 = zext i16 %34720 to i64
  store i64 %34721, i64* %RSI.i11312, align 8
  %34722 = zext i16 %34720 to i64
  store i64 %34722, i64* %RCX.i11580, align 8
  %34723 = add i64 %34682, %34722
  %34724 = add i64 %34638, 60
  store i64 %34724, i64* %3, align 8
  %34725 = inttoptr i64 %34723 to i8*
  %34726 = load i8, i8* %34725, align 1
  %34727 = zext i8 %34726 to i64
  store i64 %34727, i64* %RSI.i11312, align 8
  %34728 = add i64 %34706, -8
  %34729 = add i64 %34638, 64
  store i64 %34729, i64* %3, align 8
  %34730 = inttoptr i64 %34728 to i64*
  %34731 = load i64, i64* %34730, align 8
  %34732 = add i64 %34731, 39256
  store i64 %34732, i64* %RAX.i11582.pre-phi, align 8
  %34733 = icmp ugt i64 %34731, -39257
  %34734 = zext i1 %34733 to i8
  store i8 %34734, i8* %14, align 1
  %34735 = trunc i64 %34732 to i32
  %34736 = and i32 %34735, 255
  %34737 = tail call i32 @llvm.ctpop.i32(i32 %34736)
  %34738 = trunc i32 %34737 to i8
  %34739 = and i8 %34738, 1
  %34740 = xor i8 %34739, 1
  store i8 %34740, i8* %21, align 1
  %34741 = xor i64 %34731, 16
  %34742 = xor i64 %34741, %34732
  %34743 = lshr i64 %34742, 4
  %34744 = trunc i64 %34743 to i8
  %34745 = and i8 %34744, 1
  store i8 %34745, i8* %27, align 1
  %34746 = icmp eq i64 %34732, 0
  %34747 = zext i1 %34746 to i8
  store i8 %34747, i8* %30, align 1
  %34748 = lshr i64 %34732, 63
  %34749 = trunc i64 %34748 to i8
  store i8 %34749, i8* %33, align 1
  %34750 = lshr i64 %34731, 63
  %34751 = xor i64 %34748, %34750
  %34752 = add nuw nsw i64 %34751, %34748
  %34753 = icmp eq i64 %34752, 2
  %34754 = zext i1 %34753 to i8
  store i8 %34754, i8* %39, align 1
  %34755 = add i64 %34638, 74
  store i64 %34755, i64* %3, align 8
  %34756 = load i64, i64* %34730, align 8
  store i64 %34756, i64* %RCX.i11580, align 8
  %34757 = add i64 %34706, -68
  %34758 = add i64 %34638, 78
  store i64 %34758, i64* %3, align 8
  %34759 = inttoptr i64 %34757 to i32*
  %34760 = load i32, i32* %34759, align 4
  %34761 = sext i32 %34760 to i64
  store i64 %34761, i64* %576, align 8
  %34762 = add nsw i64 %34761, 1704
  %34763 = add i64 %34762, %34756
  %34764 = add i64 %34638, 87
  store i64 %34764, i64* %3, align 8
  %34765 = inttoptr i64 %34763 to i8*
  %34766 = load i8, i8* %34765, align 1
  %34767 = zext i8 %34766 to i64
  store i64 %34767, i64* %1608, align 8
  %34768 = zext i8 %34766 to i64
  %34769 = mul nuw nsw i64 %34768, 1032
  store i64 %34769, i64* %RCX.i11580, align 8
  %34770 = add i64 %34769, %34732
  store i64 %34770, i64* %RAX.i11582.pre-phi, align 8
  %34771 = icmp ult i64 %34770, %34732
  %34772 = icmp ult i64 %34770, %34769
  %34773 = or i1 %34771, %34772
  %34774 = zext i1 %34773 to i8
  store i8 %34774, i8* %14, align 1
  %34775 = trunc i64 %34770 to i32
  %34776 = and i32 %34775, 255
  %34777 = tail call i32 @llvm.ctpop.i32(i32 %34776)
  %34778 = trunc i32 %34777 to i8
  %34779 = and i8 %34778, 1
  %34780 = xor i8 %34779, 1
  store i8 %34780, i8* %21, align 1
  %34781 = xor i64 %34769, %34732
  %34782 = xor i64 %34781, %34770
  %34783 = lshr i64 %34782, 4
  %34784 = trunc i64 %34783 to i8
  %34785 = and i8 %34784, 1
  store i8 %34785, i8* %27, align 1
  %34786 = icmp eq i64 %34770, 0
  %34787 = zext i1 %34786 to i8
  store i8 %34787, i8* %30, align 1
  %34788 = lshr i64 %34770, 63
  %34789 = trunc i64 %34788 to i8
  store i8 %34789, i8* %33, align 1
  %34790 = xor i64 %34788, %34748
  %34791 = add nuw nsw i64 %34790, %34788
  %34792 = icmp eq i64 %34791, 2
  %34793 = zext i1 %34792 to i8
  store i8 %34793, i8* %39, align 1
  %34794 = load i64, i64* %RBP.i, align 8
  %34795 = add i64 %34794, -120
  %34796 = add i64 %34638, 104
  store i64 %34796, i64* %3, align 8
  %34797 = inttoptr i64 %34795 to i64*
  %34798 = load i64, i64* %34797, align 8
  store i64 %34798, i64* %RCX.i11580, align 8
  %34799 = add i64 %34794, -20
  %34800 = add i64 %34638, 108
  store i64 %34800, i64* %3, align 8
  %34801 = inttoptr i64 %34799 to i32*
  %34802 = load i32, i32* %34801, align 4
  %34803 = sext i32 %34802 to i64
  store i64 %34803, i64* %576, align 8
  %34804 = shl nsw i64 %34803, 1
  %34805 = add i64 %34804, %34798
  %34806 = add i64 %34638, 113
  store i64 %34806, i64* %3, align 8
  %34807 = inttoptr i64 %34805 to i16*
  %34808 = load i16, i16* %34807, align 2
  %34809 = zext i16 %34808 to i64
  store i64 %34809, i64* %1608, align 8
  %34810 = zext i16 %34808 to i64
  store i64 %34810, i64* %RCX.i11580, align 8
  %34811 = shl nuw nsw i64 %34810, 2
  %34812 = add i64 %34770, %34811
  %34813 = add i64 %34638, 119
  store i64 %34813, i64* %3, align 8
  %34814 = inttoptr i64 %34812 to i32*
  %34815 = load i32, i32* %34814, align 4
  %34816 = zext i32 %34815 to i64
  store i64 %34816, i64* %576, align 8
  %34817 = add i64 %34638, -19465
  %34818 = add i64 %34638, 124
  %34819 = load i64, i64* %6, align 8
  %34820 = add i64 %34819, -8
  %34821 = inttoptr i64 %34820 to i64*
  store i64 %34818, i64* %34821, align 8
  store i64 %34820, i64* %6, align 8
  store i64 %34817, i64* %3, align 8
  %call2_40e920 = tail call %struct.Memory* @sub_409ca0.bsW(%struct.State* nonnull %0, i64 %34817, %struct.Memory* %MEMORY.67)
  %34822 = load i64, i64* %RBP.i, align 8
  %34823 = add i64 %34822, -20
  %34824 = load i64, i64* %3, align 8
  %34825 = add i64 %34824, 3
  store i64 %34825, i64* %3, align 8
  %34826 = inttoptr i64 %34823 to i32*
  %34827 = load i32, i32* %34826, align 4
  %34828 = add i32 %34827, 1
  %34829 = zext i32 %34828 to i64
  store i64 %34829, i64* %RAX.i11582.pre-phi, align 8
  %34830 = icmp eq i32 %34827, -1
  %34831 = icmp eq i32 %34828, 0
  %34832 = or i1 %34830, %34831
  %34833 = zext i1 %34832 to i8
  store i8 %34833, i8* %14, align 1
  %34834 = and i32 %34828, 255
  %34835 = tail call i32 @llvm.ctpop.i32(i32 %34834)
  %34836 = trunc i32 %34835 to i8
  %34837 = and i8 %34836, 1
  %34838 = xor i8 %34837, 1
  store i8 %34838, i8* %21, align 1
  %34839 = xor i32 %34828, %34827
  %34840 = lshr i32 %34839, 4
  %34841 = trunc i32 %34840 to i8
  %34842 = and i8 %34841, 1
  store i8 %34842, i8* %27, align 1
  %34843 = zext i1 %34831 to i8
  store i8 %34843, i8* %30, align 1
  %34844 = lshr i32 %34828, 31
  %34845 = trunc i32 %34844 to i8
  store i8 %34845, i8* %33, align 1
  %34846 = lshr i32 %34827, 31
  %34847 = xor i32 %34844, %34846
  %34848 = add nuw nsw i32 %34847, %34844
  %34849 = icmp eq i32 %34848, 2
  %34850 = zext i1 %34849 to i8
  store i8 %34850, i8* %39, align 1
  %34851 = add i64 %34824, 9
  store i64 %34851, i64* %3, align 8
  store i32 %34828, i32* %34826, align 4
  %34852 = load i64, i64* %3, align 8
  %34853 = add i64 %34852, -145
  store i64 %34853, i64* %3, align 8
  br label %block_.L_40e89d

block_.L_40e933:                                  ; preds = %block_.L_40e89d
  %34854 = add i64 %34638, 5
  store i64 %34854, i64* %3, align 8
  br label %block_.L_40e938

block_.L_40e938:                                  ; preds = %block_.L_40e933, %block_40d9c4
  %34855 = phi i64 [ %.pre235, %block_40d9c4 ], [ %34602, %block_.L_40e933 ]
  %storemerge87 = phi i64 [ %34590, %block_40d9c4 ], [ %34854, %block_.L_40e933 ]
  %MEMORY.72 = phi %struct.Memory* [ %call2_40e88d, %block_40d9c4 ], [ %MEMORY.67, %block_.L_40e933 ]
  %34856 = add i64 %34855, -32
  %34857 = add i64 %storemerge87, 3
  store i64 %34857, i64* %3, align 8
  %34858 = inttoptr i64 %34856 to i32*
  %34859 = load i32, i32* %34858, align 4
  %34860 = add i32 %34859, 1
  %34861 = zext i32 %34860 to i64
  store i64 %34861, i64* %RAX.i11582.pre-phi, align 8
  %34862 = icmp eq i32 %34859, -1
  %34863 = icmp eq i32 %34860, 0
  %34864 = or i1 %34862, %34863
  %34865 = zext i1 %34864 to i8
  store i8 %34865, i8* %14, align 1
  %34866 = and i32 %34860, 255
  %34867 = tail call i32 @llvm.ctpop.i32(i32 %34866)
  %34868 = trunc i32 %34867 to i8
  %34869 = and i8 %34868, 1
  %34870 = xor i8 %34869, 1
  store i8 %34870, i8* %21, align 1
  %34871 = xor i32 %34860, %34859
  %34872 = lshr i32 %34871, 4
  %34873 = trunc i32 %34872 to i8
  %34874 = and i8 %34873, 1
  store i8 %34874, i8* %27, align 1
  %34875 = zext i1 %34863 to i8
  store i8 %34875, i8* %30, align 1
  %34876 = lshr i32 %34860, 31
  %34877 = trunc i32 %34876 to i8
  store i8 %34877, i8* %33, align 1
  %34878 = lshr i32 %34859, 31
  %34879 = xor i32 %34876, %34878
  %34880 = add nuw nsw i32 %34879, %34876
  %34881 = icmp eq i32 %34880, 2
  %34882 = zext i1 %34881 to i8
  store i8 %34882, i8* %39, align 1
  %34883 = add i64 %34855, -28
  %34884 = add i64 %storemerge87, 9
  store i64 %34884, i64* %3, align 8
  %34885 = inttoptr i64 %34883 to i32*
  store i32 %34860, i32* %34885, align 4
  %34886 = load i64, i64* %RBP.i, align 8
  %34887 = add i64 %34886, -68
  %34888 = load i64, i64* %3, align 8
  %34889 = add i64 %34888, 3
  store i64 %34889, i64* %3, align 8
  %34890 = inttoptr i64 %34887 to i32*
  %34891 = load i32, i32* %34890, align 4
  %34892 = add i32 %34891, 1
  %34893 = zext i32 %34892 to i64
  store i64 %34893, i64* %RAX.i11582.pre-phi, align 8
  %34894 = icmp eq i32 %34891, -1
  %34895 = icmp eq i32 %34892, 0
  %34896 = or i1 %34894, %34895
  %34897 = zext i1 %34896 to i8
  store i8 %34897, i8* %14, align 1
  %34898 = and i32 %34892, 255
  %34899 = tail call i32 @llvm.ctpop.i32(i32 %34898)
  %34900 = trunc i32 %34899 to i8
  %34901 = and i8 %34900, 1
  %34902 = xor i8 %34901, 1
  store i8 %34902, i8* %21, align 1
  %34903 = xor i32 %34892, %34891
  %34904 = lshr i32 %34903, 4
  %34905 = trunc i32 %34904 to i8
  %34906 = and i8 %34905, 1
  store i8 %34906, i8* %27, align 1
  %34907 = zext i1 %34895 to i8
  store i8 %34907, i8* %30, align 1
  %34908 = lshr i32 %34892, 31
  %34909 = trunc i32 %34908 to i8
  store i8 %34909, i8* %33, align 1
  %34910 = lshr i32 %34891, 31
  %34911 = xor i32 %34908, %34910
  %34912 = add nuw nsw i32 %34911, %34908
  %34913 = icmp eq i32 %34912, 2
  %34914 = zext i1 %34913 to i8
  store i8 %34914, i8* %39, align 1
  %34915 = add i64 %34888, 9
  store i64 %34915, i64* %3, align 8
  store i32 %34892, i32* %34890, align 4
  %34916 = load i64, i64* %3, align 8
  %34917 = add i64 %34916, -4112
  store i64 %34917, i64* %3, align 8
  br label %block_.L_40d93a

block_40e95b:                                     ; preds = %block_40d94d
  store i64 3007, i64* %RDI.i2910, align 8
  %34918 = add i64 %30177, -39483
  %34919 = add i64 %30177, 10
  %34920 = load i64, i64* %6, align 8
  %34921 = add i64 %34920, -8
  %34922 = inttoptr i64 %34921 to i64*
  store i64 %34919, i64* %34922, align 8
  store i64 %34921, i64* %6, align 8
  store i64 %34918, i64* %3, align 8
  %call2_40e960 = tail call %struct.Memory* @sub_404f20.BZ2_bz__AssertH__fail(%struct.State* nonnull %0, i64 %34918, %struct.Memory* %MEMORY.67)
  %.pre226 = load i64, i64* %RBP.i, align 8
  %.pre227 = load i64, i64* %3, align 8
  br label %block_.L_40e965

block_.L_40e965:                                  ; preds = %block_40e95b, %block_40d94d
  %34923 = phi i64 [ %30177, %block_40d94d ], [ %.pre227, %block_40e95b ]
  %34924 = phi i64 [ %30103, %block_40d94d ], [ %.pre226, %block_40e95b ]
  %MEMORY.73 = phi %struct.Memory* [ %MEMORY.67, %block_40d94d ], [ %call2_40e960, %block_40e95b ]
  %34925 = add i64 %34924, -8
  %34926 = add i64 %34923, 4
  store i64 %34926, i64* %3, align 8
  %34927 = inttoptr i64 %34925 to i64*
  %34928 = load i64, i64* %34927, align 8
  store i64 %34928, i64* %RAX.i11582.pre-phi, align 8
  %34929 = add i64 %34928, 656
  %34930 = add i64 %34923, 11
  store i64 %34930, i64* %3, align 8
  %34931 = inttoptr i64 %34929 to i32*
  %34932 = load i32, i32* %34931, align 4
  %34933 = add i32 %34932, -3
  %34934 = icmp ult i32 %34932, 3
  %34935 = zext i1 %34934 to i8
  store i8 %34935, i8* %14, align 1
  %34936 = and i32 %34933, 255
  %34937 = tail call i32 @llvm.ctpop.i32(i32 %34936)
  %34938 = trunc i32 %34937 to i8
  %34939 = and i8 %34938, 1
  %34940 = xor i8 %34939, 1
  store i8 %34940, i8* %21, align 1
  %34941 = xor i32 %34933, %34932
  %34942 = lshr i32 %34941, 4
  %34943 = trunc i32 %34942 to i8
  %34944 = and i8 %34943, 1
  store i8 %34944, i8* %27, align 1
  %34945 = icmp eq i32 %34933, 0
  %34946 = zext i1 %34945 to i8
  store i8 %34946, i8* %30, align 1
  %34947 = lshr i32 %34933, 31
  %34948 = trunc i32 %34947 to i8
  store i8 %34948, i8* %33, align 1
  %34949 = lshr i32 %34932, 31
  %34950 = xor i32 %34947, %34949
  %34951 = add nuw nsw i32 %34950, %34949
  %34952 = icmp eq i32 %34951, 2
  %34953 = zext i1 %34952 to i8
  store i8 %34953, i8* %39, align 1
  %34954 = icmp ne i8 %34948, 0
  %34955 = xor i1 %34954, %34952
  %.v323 = select i1 %34955, i64 60, i64 17
  %34956 = add i64 %34923, %.v323
  store i64 %34956, i64* %3, align 8
  br i1 %34955, label %block_.L_40e9a1, label %block_40e976

block_40e976:                                     ; preds = %block_.L_40e965
  store i64 ptrtoint (%G__0x4165fc_type* @G__0x4165fc to i64), i64* %RSI.i11312, align 8
  %34957 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %34957, i64* %RDI.i2910, align 8
  %34958 = add i64 %34956, 22
  store i64 %34958, i64* %3, align 8
  %34959 = load i64, i64* %34927, align 8
  store i64 %34959, i64* %RAX.i11582.pre-phi, align 8
  %34960 = add i64 %34959, 116
  %34961 = add i64 %34956, 25
  store i64 %34961, i64* %3, align 8
  %34962 = inttoptr i64 %34960 to i32*
  %34963 = load i32, i32* %34962, align 4
  %34964 = zext i32 %34963 to i64
  store i64 %34964, i64* %RCX.i11580, align 8
  %34965 = add i64 %34924, -76
  %34966 = add i64 %34956, 28
  store i64 %34966, i64* %3, align 8
  %34967 = inttoptr i64 %34965 to i32*
  %34968 = load i32, i32* %34967, align 4
  %34969 = sub i32 %34963, %34968
  %34970 = zext i32 %34969 to i64
  store i64 %34970, i64* %RCX.i11580, align 8
  %34971 = icmp ult i32 %34963, %34968
  %34972 = zext i1 %34971 to i8
  store i8 %34972, i8* %14, align 1
  %34973 = and i32 %34969, 255
  %34974 = tail call i32 @llvm.ctpop.i32(i32 %34973)
  %34975 = trunc i32 %34974 to i8
  %34976 = and i8 %34975, 1
  %34977 = xor i8 %34976, 1
  store i8 %34977, i8* %21, align 1
  %34978 = xor i32 %34968, %34963
  %34979 = xor i32 %34978, %34969
  %34980 = lshr i32 %34979, 4
  %34981 = trunc i32 %34980 to i8
  %34982 = and i8 %34981, 1
  store i8 %34982, i8* %27, align 1
  %34983 = icmp eq i32 %34969, 0
  %34984 = zext i1 %34983 to i8
  store i8 %34984, i8* %30, align 1
  %34985 = lshr i32 %34969, 31
  %34986 = trunc i32 %34985 to i8
  store i8 %34986, i8* %33, align 1
  %34987 = lshr i32 %34963, 31
  %34988 = lshr i32 %34968, 31
  %34989 = xor i32 %34988, %34987
  %34990 = xor i32 %34985, %34987
  %34991 = add nuw nsw i32 %34990, %34989
  %34992 = icmp eq i32 %34991, 2
  %34993 = zext i1 %34992 to i8
  store i8 %34993, i8* %39, align 1
  store i64 %34970, i64* %576, align 8
  store i8 0, i8* %AL.i11425, align 1
  %34994 = add i64 %34956, -57414
  %34995 = add i64 %34956, 37
  %34996 = load i64, i64* %6, align 8
  %34997 = add i64 %34996, -8
  %34998 = inttoptr i64 %34997 to i64*
  store i64 %34995, i64* %34998, align 8
  store i64 %34997, i64* %6, align 8
  store i64 %34994, i64* %3, align 8
  %34999 = tail call %struct.Memory* @__remill_function_call(%struct.State* nonnull %0, i64 ptrtoint (i64 (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64)* @fprintf to i64), %struct.Memory* %MEMORY.73)
  %35000 = load i64, i64* %RBP.i, align 8
  %35001 = add i64 %35000, -272
  %35002 = load i32, i32* %EAX.i11561.pre-phi, align 4
  %35003 = load i64, i64* %3, align 8
  %35004 = add i64 %35003, 6
  store i64 %35004, i64* %3, align 8
  %35005 = inttoptr i64 %35001 to i32*
  store i32 %35002, i32* %35005, align 4
  %.pre228 = load i64, i64* %3, align 8
  br label %block_.L_40e9a1

block_.L_40e9a1:                                  ; preds = %block_40e976, %block_.L_40e965
  %35006 = phi i64 [ %34956, %block_.L_40e965 ], [ %.pre228, %block_40e976 ]
  %MEMORY.74 = phi %struct.Memory* [ %MEMORY.73, %block_.L_40e965 ], [ %34999, %block_40e976 ]
  %35007 = load i64, i64* %6, align 8
  %35008 = add i64 %35007, 272
  store i64 %35008, i64* %6, align 8
  %35009 = icmp ugt i64 %35007, -273
  %35010 = zext i1 %35009 to i8
  store i8 %35010, i8* %14, align 1
  %35011 = trunc i64 %35008 to i32
  %35012 = and i32 %35011, 255
  %35013 = tail call i32 @llvm.ctpop.i32(i32 %35012)
  %35014 = trunc i32 %35013 to i8
  %35015 = and i8 %35014, 1
  %35016 = xor i8 %35015, 1
  store i8 %35016, i8* %21, align 1
  %35017 = xor i64 %35007, 16
  %35018 = xor i64 %35017, %35008
  %35019 = lshr i64 %35018, 4
  %35020 = trunc i64 %35019 to i8
  %35021 = and i8 %35020, 1
  store i8 %35021, i8* %27, align 1
  %35022 = icmp eq i64 %35008, 0
  %35023 = zext i1 %35022 to i8
  store i8 %35023, i8* %30, align 1
  %35024 = lshr i64 %35008, 63
  %35025 = trunc i64 %35024 to i8
  store i8 %35025, i8* %33, align 1
  %35026 = lshr i64 %35007, 63
  %35027 = xor i64 %35024, %35026
  %35028 = add nuw nsw i64 %35027, %35024
  %35029 = icmp eq i64 %35028, 2
  %35030 = zext i1 %35029 to i8
  store i8 %35030, i8* %39, align 1
  %35031 = add i64 %35006, 8
  store i64 %35031, i64* %3, align 8
  %35032 = add i64 %35007, 280
  %35033 = inttoptr i64 %35008 to i64*
  %35034 = load i64, i64* %35033, align 8
  store i64 %35034, i64* %RBP.i, align 8
  store i64 %35032, i64* %6, align 8
  %35035 = add i64 %35006, 9
  store i64 %35035, i64* %3, align 8
  %35036 = inttoptr i64 %35032 to i64*
  %35037 = load i64, i64* %35036, align 8
  store i64 %35037, i64* %3, align 8
  %35038 = add i64 %35007, 288
  store i64 %35038, i64* %6, align 8
  ret %struct.Memory* %MEMORY.74
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_pushq__rbp(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 1
  store i64 %5, i64* %PC, align 8
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = add i64 %7, -8
  %9 = inttoptr i64 %8 to i64*
  store i64 %3, i64* %9, align 8
  store i64 %8, i64* %6, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rsp___rbp(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RSP, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  store i64 %3, i64* %RBP, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subq__0x110___rsp(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %3 = load i64, i64* %RSP, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, -272
  store i64 %6, i64* %RSP, align 8
  %7 = icmp ult i64 %3, 272
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %3, 16
  %18 = xor i64 %17, %6
  %19 = lshr i64 %18, 4
  %20 = trunc i64 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i64 %6, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i64 %6, 63
  %27 = trunc i64 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %3, 63
  %30 = xor i64 %26, %29
  %31 = add nuw nsw i64 %30, %29
  %32 = icmp eq i64 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rdi__MINUS0x8__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -8
  %5 = load i64, i64* %RDI, align 8
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %4 to i64*
  store i64 %5, i64* %8, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0x8__rbp____rdi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_0x48__rdi____rdi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %RDI, align 8
  %4 = add i64 %3, 72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rdi__MINUS0x78__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -120
  %5 = load i64, i64* %RDI, align 8
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %4 to i64*
  store i64 %5, i64* %8, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x3__0x290__rdi_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %RDI, align 8
  %4 = add i64 %3, 656
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -3
  %10 = icmp ult i32 %8, 3
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40a2b4(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x416519___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x416519_type* @G__0x416519 to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_0x618d80___rdi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 8
  store i64 %4, i64* %PC, align 8
  %5 = load i64, i64* bitcast (%G_0x618d80_type* @G_0x618d80 to i64*), align 8
  store i64 %5, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0x8__rbp____rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x6c__rax____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 108
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x29c__rax____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 668
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x7c__rax____r8d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = add i64 %4, 124
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i32*
  %9 = load i32, i32* %8, align 4
  %10 = zext i32 %9 to i64
  store i64 %10, i64* %3, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__0x0___al(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 2
  store i64 %5, i64* %PC, align 8
  store i8 0, i8* %AL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_callq_.fprintf_plt(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  %5 = add i64 %3, %rel_off2
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = add i64 %7, -8
  %9 = inttoptr i64 %8 to i64*
  store i64 %5, i64* %9, align 8
  store i64 %8, i64* %6, align 8
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xdc__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -220
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x7c__rax____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 124
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 2
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -3
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x38__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -56
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x10__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x6__MINUS0x10__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -6
  %10 = icmp ult i32 %8, 6
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a326(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0xc__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -12
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0xc__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -12
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x38__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -56
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a313(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__0x934c___rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 6
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, 37708
  store i64 %6, i64* %RAX, align 8
  %7 = icmp ugt i64 %3, -37709
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %6, %3
  %18 = lshr i64 %17, 4
  %19 = trunc i64 %18 to i8
  %20 = and i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %20, i8* %21, align 1
  %22 = icmp eq i64 %6, 0
  %23 = zext i1 %22 to i8
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %23, i8* %24, align 1
  %25 = lshr i64 %6, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = lshr i64 %3, 63
  %29 = xor i64 %25, %28
  %30 = add nuw nsw i64 %29, %25
  %31 = icmp eq i64 %30, 2
  %32 = zext i1 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %32, i8* %33, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x10__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_imulq__0x102___rcx___rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = sext i64 %3 to i128
  %7 = and i128 %6, -18446744073709551616
  %8 = zext i64 %3 to i128
  %9 = or i128 %7, %8
  %10 = mul nsw i128 %9, 258
  %11 = trunc i128 %10 to i64
  store i64 %11, i64* %RCX, align 8
  %12 = sext i64 %11 to i128
  %13 = icmp ne i128 %12, %10
  %14 = zext i1 %13 to i8
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %14, i8* %15, align 1
  %16 = trunc i128 %10 to i32
  %17 = and i32 %16, 254
  %18 = tail call i32 @llvm.ctpop.i32(i32 %17)
  %19 = trunc i32 %18 to i8
  %20 = and i8 %19, 1
  %21 = xor i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %21, i8* %22, align 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %23, align 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 0, i8* %24, align 1
  %25 = lshr i64 %11, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %14, i8* %28, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__rcx___rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = add i64 %4, %3
  store i64 %7, i64* %RAX, align 8
  %8 = icmp ult i64 %7, %3
  %9 = icmp ult i64 %7, %4
  %10 = or i1 %8, %9
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = trunc i64 %7 to i32
  %14 = and i32 %13, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i64 %4, %3
  %21 = xor i64 %20, %7
  %22 = lshr i64 %21, 4
  %23 = trunc i64 %22 to i8
  %24 = and i8 %23, 1
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %24, i8* %25, align 1
  %26 = icmp eq i64 %7, 0
  %27 = zext i1 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %7, 63
  %30 = trunc i64 %29 to i8
  %31 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %30, i8* %31, align 1
  %32 = lshr i64 %3, 63
  %33 = lshr i64 %4, 63
  %34 = xor i64 %29, %32
  %35 = xor i64 %29, %33
  %36 = add nuw nsw i64 %34, %35
  %37 = icmp eq i64 %36, 2
  %38 = zext i1 %37 to i8
  %39 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %38, i8* %39, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0xc__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -12
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__0xf____rax__rcx_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %4, %3
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  store i8 15, i8* %8, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RAX, align 8
  %9 = icmp eq i32 %6, -1
  %10 = icmp eq i32 %7, 0
  %11 = or i1 %9, %10
  %12 = zext i1 %11 to i8
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %12, i8* %13, align 1
  %14 = and i32 %7, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i32 %7, %6
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = zext i1 %10 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xc__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -12
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a2d9(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a318(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x10__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x10__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -16
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a2c8(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x0__0x29c__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 668
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %9, align 1
  %10 = and i32 %8, 255
  %11 = tail call i32 @llvm.ctpop.i32(i32 %10)
  %12 = trunc i32 %11 to i8
  %13 = and i8 %12, 1
  %14 = xor i8 %13, 1
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %14, i8* %15, align 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %16, align 1
  %17 = icmp eq i32 %8, 0
  %18 = zext i1 %17 to i8
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %18, i8* %19, align 1
  %20 = lshr i32 %8, 31
  %21 = trunc i32 %20 to i8
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %21, i8* %22, align 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %23, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jg_.L_40a341(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = xor i1 %13, true
  %15 = and i1 %6, %14
  %16 = zext i1 %15 to i8
  store i8 %16, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %15, i64 %rel_off1, i64 %rel_off2
  %17 = add i64 %.v, %3
  store i64 %17, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xbb9___edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3001, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_callq_.BZ2_bz__AssertH__fail(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  %5 = add i64 %3, %rel_off2
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = add i64 %7, -8
  %9 = inttoptr i64 %8 to i64*
  store i64 %5, i64* %9, align 8
  store i64 %8, i64* %6, align 8
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0xc8__0x29c__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 668
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -200
  %10 = icmp ult i32 %8, 200
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a361(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x2__MINUS0x48__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 2, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a3d7(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x258__0x29c__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 668
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -600
  %10 = icmp ult i32 %8, 600
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %8, 16
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %8, 31
  %32 = xor i32 %28, %31
  %33 = add nuw nsw i32 %32, %31
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a381(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x3__MINUS0x48__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 3, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a3d2(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x4b0__0x29c__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 668
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -1200
  %10 = icmp ult i32 %8, 1200
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %8, 16
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %8, 31
  %32 = xor i32 %28, %31
  %33 = add nuw nsw i32 %32, %31
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a3a1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x4__MINUS0x48__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 4, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a3cd(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x960__0x29c__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 668
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -2400
  %10 = icmp ult i32 %8, 2400
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a3c1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x5__MINUS0x48__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 5, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a3c8(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x6__MINUS0x48__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 6, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x48__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x7c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -124
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0x8__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x29c__rcx____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = add i64 %3, 668
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x80__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -128
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x1c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -28
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x0__MINUS0x7c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -124
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %9, align 1
  %10 = and i32 %8, 255
  %11 = tail call i32 @llvm.ctpop.i32(i32 %10)
  %12 = trunc i32 %11 to i8
  %13 = and i8 %12, 1
  %14 = xor i8 %13, 1
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %14, i8* %15, align 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %16, align 1
  %17 = icmp eq i32 %8, 0
  %18 = zext i1 %17 to i8
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %18, i8* %19, align 1
  %20 = lshr i32 %8, 31
  %21 = trunc i32 %20 to i8
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %21, i8* %22, align 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %23, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jle_.L_40a613(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = or i1 %6, %13
  %15 = zext i1 %14 to i8
  store i8 %15, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %14, i64 %rel_off1, i64 %rel_off2
  %16 = add i64 %.v, %3
  store i64 %16, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x80__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -128
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_cltd(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 1
  store i64 %4, i64* %PC, align 8
  %5 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %7 = bitcast %union.anon* %6 to i32*
  %8 = load i32, i32* %7, align 8
  %9 = sext i32 %8 to i64
  %10 = lshr i64 %9, 32
  store i64 %10, i64* %5, align 8
  ret %struct.Memory* %2
}

define %struct.Memory* @routine_idivl_MINUS0x7c__rbp_(%struct.State* dereferenceable(3376), i64, %struct.Memory*) local_unnamed_addr {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -124
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %8 = bitcast %union.anon* %7 to i32*
  %9 = load i32, i32* %8, align 8
  %10 = zext i32 %9 to i64
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %12 = bitcast %union.anon* %11 to i32*
  %13 = load i32, i32* %12, align 8
  %14 = zext i32 %13 to i64
  %15 = inttoptr i64 %4 to i32*
  %16 = load i32, i32* %15, align 4
  %17 = sext i32 %16 to i64
  %18 = shl nuw i64 %14, 32
  %19 = or i64 %18, %10
  %20 = sdiv i64 %19, %17
  %21 = shl i64 %20, 32
  %22 = ashr exact i64 %21, 32
  %23 = icmp eq i64 %20, %22
  br i1 %23, label %26, label %24

; <label>:24:                                     ; preds = %block_400488
  %25 = tail call %struct.Memory* @__remill_error(%struct.State* nonnull dereferenceable(3376) %0, i64 %6, %struct.Memory* %2)
  br label %_ZN12_GLOBAL__N_1L10IDIVedxeaxI2MnIjEEEP6MemoryS4_R5StateT_.exit

; <label>:26:                                     ; preds = %block_400488
  %27 = srem i64 %19, %17
  %28 = getelementptr inbounds %union.anon, %union.anon* %7, i64 0, i32 0
  %29 = and i64 %20, 4294967295
  store i64 %29, i64* %28, align 8
  %30 = getelementptr inbounds %union.anon, %union.anon* %11, i64 0, i32 0
  %31 = and i64 %27, 4294967295
  store i64 %31, i64* %30, align 8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %32, align 1
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 0, i8* %33, align 1
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %34, align 1
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 0, i8* %35, align 1
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %36, align 1
  %37 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %37, align 1
  br label %_ZN12_GLOBAL__N_1L10IDIVedxeaxI2MnIjEEEP6MemoryS4_R5StateT_.exit

_ZN12_GLOBAL__N_1L10IDIVedxeaxI2MnIjEEEP6MemoryS4_R5StateT_.exit: ; preds = %26, %24
  %38 = phi %struct.Memory* [ %25, %24 ], [ %2, %26 ]
  ret %struct.Memory* %38
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x84__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -132
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x1c__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -28
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subl__0x1___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, -1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RAX, align 8
  %9 = icmp eq i32 %6, 0
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %29
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x20__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -32
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x88__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -136
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_xorl__eax___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 2
  store i64 %4, i64* %PC, align 8
  store i64 0, i64* %RAX, align 8
  %5 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %5, align 1
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 1, i8* %6, align 1
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 1, i8* %7, align 1
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %8, align 1
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %9, align 1
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %10, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__al___cl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %4 to i8*
  %5 = load i8, i8* %AL, align 1
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i8 %5, i8* %CL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x88__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -136
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x84__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -132
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__cl__MINUS0xdd__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -221
  %6 = load i8, i8* %CL, align 1
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i8*
  store i8 %6, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a44b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x20__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -32
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x38__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -56
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subl__0x1___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, -1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp eq i32 %6, 0
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %29
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__ecx___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %4 to i32*
  %5 = load i32, i32* %EAX, align 4
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 2
  store i64 %8, i64* %PC, align 8
  %9 = sub i32 %5, %6
  %10 = icmp ult i32 %5, %6
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %6, %5
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %5, 31
  %32 = lshr i32 %6, 31
  %33 = xor i32 %32, %31
  %34 = xor i32 %28, %31
  %35 = add nuw nsw i32 %34, %33
  %36 = icmp eq i32 %35, 2
  %37 = zext i1 %36 to i8
  %38 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %37, i8* %38, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_setl__dl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %3 to i8*
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %7 = load i8, i8* %6, align 1
  %8 = icmp ne i8 %7, 0
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %10 = load i8, i8* %9, align 1
  %11 = icmp ne i8 %10, 0
  %12 = xor i1 %8, %11
  %13 = zext i1 %12 to i8
  store i8 %13, i8* %DL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__dl__MINUS0xdd__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -221
  %6 = load i8, i8* %DL, align 1
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i8*
  store i8 %6, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb_MINUS0xdd__rbp____al(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -221
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  %9 = load i8, i8* %8, align 1
  store i8 %9, i8* %AL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_testb__0x1___al(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %4 = load i8, i8* %AL, align 1
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 2
  store i64 %6, i64* %PC, align 8
  %7 = and i8 %4, 1
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %8, align 1
  %9 = zext i8 %7 to i32
  %10 = tail call i32 @llvm.ctpop.i32(i32 %9)
  %11 = trunc i32 %10 to i8
  %12 = xor i8 %11, 1
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %12, i8* %13, align 1
  %14 = xor i8 %7, 1
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %14, i8* %15, align 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %16, align 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %17, align 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %18, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jne_.L_40a45e(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = zext i1 %6 to i8
  store i8 %7, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %8 = add i64 %.v, %3
  store i64 %8, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a487(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x20__rbp____rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -32
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x2a0__rcx__rdx_4____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %3, 672
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 7
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i32*
  %11 = load i32, i32* %10, align 4
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl_MINUS0x88__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -136
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = add i32 %10, %8
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RAX, align 8
  %13 = icmp ult i32 %11, %8
  %14 = icmp ult i32 %11, %10
  %15 = or i1 %13, %14
  %16 = zext i1 %15 to i8
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %16, i8* %17, align 1
  %18 = and i32 %11, 255
  %19 = tail call i32 @llvm.ctpop.i32(i32 %18)
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = xor i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %22, i8* %23, align 1
  %24 = xor i32 %10, %8
  %25 = xor i32 %24, %11
  %26 = lshr i32 %25, 4
  %27 = trunc i32 %26 to i8
  %28 = and i8 %27, 1
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %28, i8* %29, align 1
  %30 = icmp eq i32 %11, 0
  %31 = zext i1 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %11, 31
  %34 = trunc i32 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %34, i8* %35, align 1
  %36 = lshr i32 %8, 31
  %37 = lshr i32 %10, 31
  %38 = xor i32 %33, %36
  %39 = xor i32 %33, %37
  %40 = add nuw nsw i32 %38, %39
  %41 = icmp eq i32 %40, 2
  %42 = zext i1 %41 to i8
  %43 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %42, i8* %43, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x88__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -136
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a41b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x1c__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -28
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jle_.L_40a4f4(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = or i1 %6, %13
  %15 = zext i1 %14 to i8
  store i8 %15, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %14, i64 %rel_off1, i64 %rel_off2
  %16 = add i64 %.v, %3
  store i64 %16, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x7c__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -124
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x48__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -72
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_je_.L_40a4f4(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  store i8 %5, i8* %BRANCH_TAKEN, align 1
  %6 = icmp ne i8 %5, 0
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %7 = add i64 %.v, %3
  store i64 %7, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x1__MINUS0x7c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -124
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -1
  %10 = icmp eq i32 %8, 0
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x2___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 2, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x48__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subl_MINUS0x7c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -124
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %8, %10
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = icmp ult i32 %8, %10
  %14 = zext i1 %13 to i8
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %14, i8* %15, align 1
  %16 = and i32 %11, 255
  %17 = tail call i32 @llvm.ctpop.i32(i32 %16)
  %18 = trunc i32 %17 to i8
  %19 = and i8 %18, 1
  %20 = xor i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %20, i8* %21, align 1
  %22 = xor i32 %10, %8
  %23 = xor i32 %22, %11
  %24 = lshr i32 %23, 4
  %25 = trunc i32 %24 to i8
  %26 = and i8 %25, 1
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %26, i8* %27, align 1
  %28 = icmp eq i32 %11, 0
  %29 = zext i1 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %11, 31
  %32 = trunc i32 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %32, i8* %33, align 1
  %34 = lshr i32 %8, 31
  %35 = lshr i32 %10, 31
  %36 = xor i32 %35, %34
  %37 = xor i32 %31, %34
  %38 = add nuw nsw i32 %37, %36
  %39 = icmp eq i32 %38, 2
  %40 = zext i1 %39 to i8
  %41 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %40, i8* %41, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xe4__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -228
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %4 = load i32, i32* %ECX, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0xe4__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -228
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

define %struct.Memory* @routine_idivl__ecx(%struct.State* dereferenceable(3376), i64, %struct.Memory*) local_unnamed_addr {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %4 = load i32, i32* %ECX, align 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 2
  store i64 %6, i64* %PC, align 8
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %8 = bitcast %union.anon* %7 to i32*
  %9 = load i32, i32* %8, align 8
  %10 = zext i32 %9 to i64
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %12 = bitcast %union.anon* %11 to i32*
  %13 = load i32, i32* %12, align 8
  %14 = zext i32 %13 to i64
  %15 = sext i32 %4 to i64
  %16 = shl nuw i64 %14, 32
  %17 = or i64 %16, %10
  %18 = sdiv i64 %17, %15
  %19 = shl i64 %18, 32
  %20 = ashr exact i64 %19, 32
  %21 = icmp eq i64 %18, %20
  br i1 %21, label %24, label %22

; <label>:22:                                     ; preds = %block_400488
  %23 = tail call %struct.Memory* @__remill_error(%struct.State* nonnull dereferenceable(3376) %0, i64 %6, %struct.Memory* %2)
  br label %_ZN12_GLOBAL__N_1L10IDIVedxeaxI2RnIjEEEP6MemoryS4_R5StateT_.exit

; <label>:24:                                     ; preds = %block_400488
  %25 = srem i64 %17, %15
  %26 = getelementptr inbounds %union.anon, %union.anon* %7, i64 0, i32 0
  %27 = and i64 %18, 4294967295
  store i64 %27, i64* %26, align 8
  %28 = getelementptr inbounds %union.anon, %union.anon* %11, i64 0, i32 0
  %29 = and i64 %25, 4294967295
  store i64 %29, i64* %28, align 8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %30, align 1
  %31 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 0, i8* %31, align 1
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %32, align 1
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 0, i8* %33, align 1
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %34, align 1
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %35, align 1
  br label %_ZN12_GLOBAL__N_1L10IDIVedxeaxI2RnIjEEEP6MemoryS4_R5StateT_.exit

_ZN12_GLOBAL__N_1L10IDIVedxeaxI2RnIjEEEP6MemoryS4_R5StateT_.exit: ; preds = %24, %22
  %36 = phi %struct.Memory* [ %23, %22 ], [ %2, %24 ]
  ret %struct.Memory* %36
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x1___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %4 = load i32, i32* %EDX, align 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = add i32 %4, -1
  %8 = icmp eq i32 %4, 0
  %9 = zext i1 %8 to i8
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %9, i8* %10, align 1
  %11 = and i32 %7, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i32 %7, %4
  %18 = lshr i32 %17, 4
  %19 = trunc i32 %18 to i8
  %20 = and i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %20, i8* %21, align 1
  %22 = icmp eq i32 %7, 0
  %23 = zext i1 %22 to i8
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %23, i8* %24, align 1
  %25 = lshr i32 %7, 31
  %26 = trunc i32 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %4, 31
  %29 = xor i32 %25, %28
  %30 = add nuw nsw i32 %29, %28
  %31 = icmp eq i32 %30, 2
  %32 = zext i1 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %32, i8* %33, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jne_.L_40a4f4(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = zext i1 %6 to i8
  store i8 %7, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %8 = add i64 %.v, %3
  store i64 %8, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x20__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -32
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x2a0__rax__rcx_4____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %3, 672
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 7
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i32*
  %11 = load i32, i32* %10, align 4
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x88__rbp____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -136
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subl__edx___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %4 = load i64, i64* %RSI, align 8
  %5 = load i32, i32* %EDX, align 4
  %6 = zext i32 %5 to i64
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 2
  store i64 %8, i64* %PC, align 8
  %9 = trunc i64 %4 to i32
  %10 = sub i32 %9, %5
  %11 = zext i32 %10 to i64
  store i64 %11, i64* %RSI, align 8
  %12 = icmp ult i32 %9, %5
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %10, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i64 %6, %4
  %22 = trunc i64 %21 to i32
  %23 = xor i32 %22, %10
  %24 = lshr i32 %23, 4
  %25 = trunc i32 %24 to i8
  %26 = and i8 %25, 1
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %26, i8* %27, align 1
  %28 = icmp eq i32 %10, 0
  %29 = zext i1 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %10, 31
  %32 = trunc i32 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %32, i8* %33, align 1
  %34 = lshr i32 %9, 31
  %35 = lshr i32 %5, 31
  %36 = xor i32 %35, %34
  %37 = xor i32 %31, %34
  %38 = add nuw nsw i32 %37, %36
  %39 = icmp eq i32 %38, 2
  %40 = zext i1 %39 to i8
  %41 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %40, i8* %41, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__esi__MINUS0x88__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %ESI = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -136
  %6 = load i32, i32* %ESI, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x20__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -32
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xffffffff___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, -1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ne i32 %6, 0
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = xor i32 %27, 1
  %33 = add nuw nsw i32 %31, %32
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx__MINUS0x20__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -32
  %6 = load i32, i32* %EDX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x3__0x290__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 656
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -3
  %10 = icmp ult i32 %8, 3
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40a561(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x416559___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x416559_type* @G__0x416559 to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movsd_0xb231__rip____xmm0(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, ptrtoint (%G_0xb231__rip__type* @G_0xb231__rip_ to i64)
  %5 = add i64 %3, 8
  store i64 %5, i64* %PC, align 8
  %6 = inttoptr i64 %4 to i64*
  %7 = load i64, i64* %6, align 8
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 0, i32 0, i32 0, i32 0, i64 0
  store i64 %7, i64* %8, align 1
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 0, i32 0, i32 0, i32 0, i64 1
  %10 = bitcast i64* %9 to double*
  store double 0.000000e+00, double* %10, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x7c__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -124
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x1c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -28
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x20__rbp____r8d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -32
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i32*
  %9 = load i32, i32* %8, align 4
  %10 = zext i32 %9 to i64
  store i64 %10, i64* %3, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x88__rbp____r9d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 19, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -136
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 7
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i32*
  %9 = load i32, i32* %8, align 4
  %10 = zext i32 %9 to i64
  store i64 %10, i64* %3, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_cvtsi2ssl_MINUS0x88__rbp____xmm1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1
  %4 = bitcast %union.VectorReg* %3 to i8*
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -136
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = bitcast %union.VectorReg* %3 to <2 x i32>*
  %10 = load <2 x i32>, <2 x i32>* %9, align 1
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1, i32 0, i32 0, i32 0, i64 1
  %12 = bitcast i64* %11 to <2 x i32>*
  %13 = load <2 x i32>, <2 x i32>* %12, align 1
  %14 = inttoptr i64 %6 to i32*
  %15 = load i32, i32* %14, align 4
  %16 = sitofp i32 %15 to float
  %17 = bitcast %union.VectorReg* %3 to float*
  store float %16, float* %17, align 1
  %18 = extractelement <2 x i32> %10, i32 1
  %19 = getelementptr inbounds i8, i8* %4, i64 4
  %20 = bitcast i8* %19 to i32*
  store i32 %18, i32* %20, align 1
  %21 = extractelement <2 x i32> %13, i32 0
  %22 = bitcast i64* %11 to i32*
  store i32 %21, i32* %22, align 1
  %23 = extractelement <2 x i32> %13, i32 1
  %24 = getelementptr inbounds i8, i8* %4, i64 12
  %25 = bitcast i8* %24 to i32*
  store i32 %23, i32* %25, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_cvtss2sd__xmm1___xmm1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 4
  store i64 %5, i64* %PC, align 8
  %6 = bitcast %union.VectorReg* %3 to <2 x float>*
  %7 = load <2 x float>, <2 x float>* %6, align 1
  %8 = extractelement <2 x float> %7, i32 0
  %9 = fpext float %8 to double
  %10 = bitcast %union.VectorReg* %3 to double*
  store double %9, double* %10, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_mulsd__xmm1___xmm0(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = bitcast [32 x %union.VectorReg]* %3 to double*
  %8 = load double, double* %7, align 1
  %9 = bitcast %union.VectorReg* %4 to double*
  %10 = load double, double* %9, align 1
  %11 = fmul double %8, %10
  store double %11, double* %7, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_cvtsi2ssl_0x29c__rax____xmm1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1
  %4 = bitcast %union.VectorReg* %3 to i8*
  %5 = load i64, i64* %RAX, align 8
  %6 = add i64 %5, 668
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = bitcast %union.VectorReg* %3 to <2 x i32>*
  %10 = load <2 x i32>, <2 x i32>* %9, align 1
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1, i32 0, i32 0, i32 0, i64 1
  %12 = bitcast i64* %11 to <2 x i32>*
  %13 = load <2 x i32>, <2 x i32>* %12, align 1
  %14 = inttoptr i64 %6 to i32*
  %15 = load i32, i32* %14, align 4
  %16 = sitofp i32 %15 to float
  %17 = bitcast %union.VectorReg* %3 to float*
  store float %16, float* %17, align 1
  %18 = extractelement <2 x i32> %10, i32 1
  %19 = getelementptr inbounds i8, i8* %4, i64 4
  %20 = bitcast i8* %19 to i32*
  store i32 %18, i32* %20, align 1
  %21 = extractelement <2 x i32> %13, i32 0
  %22 = bitcast i64* %11 to i32*
  store i32 %21, i32* %22, align 1
  %23 = extractelement <2 x i32> %13, i32 1
  %24 = getelementptr inbounds i8, i8* %4, i64 12
  %25 = bitcast i8* %24 to i32*
  store i32 %23, i32* %25, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_divsd__xmm1___xmm0(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 1, i64 1
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = bitcast [32 x %union.VectorReg]* %3 to double*
  %8 = load double, double* %7, align 1
  %9 = bitcast %union.VectorReg* %4 to double*
  %10 = load double, double* %9, align 1
  %11 = fdiv double %8, %10
  store double %11, double* %7, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__0x1___al(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 2
  store i64 %5, i64* %PC, align 8
  store i8 1, i8* %AL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xe8__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -232
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a5ee(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40a5b6(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x20__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -32
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jg_.L_40a5b6(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = xor i1 %13, true
  %15 = and i1 %6, %14
  %16 = zext i1 %15 to i8
  store i8 %16, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %15, i64 %rel_off1, i64 %rel_off2
  %17 = add i64 %.v, %3
  store i64 %17, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x7c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -124
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq__ecx___rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %4 = load i32, i32* %ECX, align 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = sext i32 %4 to i64
  store i64 %7, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_imulq__0x102___rdx___rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = sext i64 %3 to i128
  %7 = and i128 %6, -18446744073709551616
  %8 = zext i64 %3 to i128
  %9 = or i128 %7, %8
  %10 = mul nsw i128 %9, 258
  %11 = trunc i128 %10 to i64
  store i64 %11, i64* %RDX, align 8
  %12 = sext i64 %11 to i128
  %13 = icmp ne i128 %12, %10
  %14 = zext i1 %13 to i8
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %14, i8* %15, align 1
  %16 = trunc i128 %10 to i32
  %17 = and i32 %16, 254
  %18 = tail call i32 @llvm.ctpop.i32(i32 %17)
  %19 = trunc i32 %18 to i8
  %20 = and i8 %19, 1
  %21 = xor i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %21, i8* %22, align 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %23, align 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 0, i8* %24, align 1
  %25 = lshr i64 %11, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %14, i8* %28, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__rdx___rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = add i64 %4, %3
  store i64 %7, i64* %RAX, align 8
  %8 = icmp ult i64 %7, %3
  %9 = icmp ult i64 %7, %4
  %10 = or i1 %8, %9
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = trunc i64 %7 to i32
  %14 = and i32 %13, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i64 %4, %3
  %21 = xor i64 %20, %7
  %22 = lshr i64 %21, 4
  %23 = trunc i64 %22 to i8
  %24 = and i8 %23, 1
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %24, i8* %25, align 1
  %26 = icmp eq i64 %7, 0
  %27 = zext i1 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %7, 63
  %30 = trunc i64 %29 to i8
  %31 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %30, i8* %31, align 1
  %32 = lshr i64 %3, 63
  %33 = lshr i64 %4, 63
  %34 = xor i64 %29, %32
  %35 = xor i64 %29, %33
  %36 = add nuw nsw i64 %34, %35
  %37 = icmp eq i64 %36, 2
  %38 = zext i1 %37 to i8
  %39 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %38, i8* %39, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0xc__rbp____rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -12
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__0x0____rax__rdx_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = add i64 %4, %3
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  store i8 0, i8* %8, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a5db(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__0xf____rax__rdx_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = add i64 %4, %3
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  store i8 15, i8* %8, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a5e0(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a568(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xffffffff___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, -1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RAX, align 8
  %9 = icmp ne i32 %6, 0
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = xor i32 %27, 1
  %33 = add nuw nsw i32 %31, %32
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x1c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -28
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x80__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -128
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subl__eax___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i64, i64* %RCX, align 8
  %5 = load i32, i32* %EAX, align 4
  %6 = zext i32 %5 to i64
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 2
  store i64 %8, i64* %PC, align 8
  %9 = trunc i64 %4 to i32
  %10 = sub i32 %9, %5
  %11 = zext i32 %10 to i64
  store i64 %11, i64* %RCX, align 8
  %12 = icmp ult i32 %9, %5
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %10, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i64 %6, %4
  %22 = trunc i64 %21 to i32
  %23 = xor i32 %22, %10
  %24 = lshr i32 %23, 4
  %25 = trunc i32 %24 to i8
  %26 = and i8 %25, 1
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %26, i8* %27, align 1
  %28 = icmp eq i32 %10, 0
  %29 = zext i1 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %10, 31
  %32 = trunc i32 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %32, i8* %33, align 1
  %34 = lshr i32 %9, 31
  %35 = lshr i32 %5, 31
  %36 = xor i32 %35, %34
  %37 = xor i32 %31, %34
  %38 = add nuw nsw i32 %37, %36
  %39 = icmp eq i32 %38, 2
  %40 = zext i1 %39 to i8
  %41 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %40, i8* %41, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x80__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -128
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a3f1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x30__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -48
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x4__MINUS0x30__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -48
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -4
  %10 = icmp ult i32 %8, 4
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d26b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a651(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x10__rbp____rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x70__rbp__rax_4_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %3, -112
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 8
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i32*
  store i32 0, i32* %10, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a62b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a6bb(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a6a8(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__0xb188___rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 6
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, 45448
  store i64 %6, i64* %RAX, align 8
  %7 = icmp ugt i64 %3, -45449
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %6, %3
  %18 = lshr i64 %17, 4
  %19 = trunc i64 %18 to i8
  %20 = and i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %20, i8* %21, align 1
  %22 = icmp eq i64 %6, 0
  %23 = zext i1 %22 to i8
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %23, i8* %24, align 1
  %25 = lshr i64 %6, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = lshr i64 %3, 63
  %29 = xor i64 %25, %28
  %30 = add nuw nsw i64 %29, %25
  %31 = icmp eq i64 %30, 2
  %32 = zext i1 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %32, i8* %33, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_imulq__0x408___rcx___rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = sext i64 %3 to i128
  %7 = and i128 %6, -18446744073709551616
  %8 = zext i64 %3 to i128
  %9 = or i128 %7, %8
  %10 = mul nsw i128 %9, 1032
  %11 = trunc i128 %10 to i64
  store i64 %11, i64* %RCX, align 8
  %12 = sext i64 %11 to i128
  %13 = icmp ne i128 %12, %10
  %14 = zext i1 %13 to i8
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %14, i8* %15, align 1
  %16 = trunc i128 %10 to i32
  %17 = and i32 %16, 248
  %18 = tail call i32 @llvm.ctpop.i32(i32 %17)
  %19 = trunc i32 %18 to i8
  %20 = and i8 %19, 1
  %21 = xor i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %21, i8* %22, align 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %23, align 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 0, i8* %24, align 1
  %25 = lshr i64 %11, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %14, i8* %28, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0____rax__rcx_4_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %5, %3
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 7
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  store i32 0, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a66b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a6ad(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a658(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x6__MINUS0x48__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -6
  %10 = icmp ult i32 %8, 6
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jne_.L_40a7a1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = zext i1 %6 to i8
  store i8 %7, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %8 = add i64 %.v, %3
  store i64 %8, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a79c(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x944e__rax__rcx_1____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %3, 37966
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_shll__0x10___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %.tr = trunc i64 %3 to i32
  %6 = shl i32 %.tr, 16
  %7 = zext i32 %6 to i64
  store i64 %7, i64* %RDX, align 8
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  %9 = lshr i64 %3, 16
  %10 = trunc i64 %9 to i8
  %11 = and i8 %10, 1
  store i8 %11, i8* %8, align 1
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 1, i8* %12, align 1
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %13, align 1
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %15 = icmp eq i32 %6, 0
  %16 = zext i1 %15 to i8
  store i8 %16, i8* %14, align 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %18 = lshr i32 %.tr, 15
  %19 = trunc i32 %18 to i8
  %20 = and i8 %19, 1
  store i8 %20, i8* %17, align 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %21, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x934c__rax__rcx_1____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %3, 37708
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_orl__esi___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %ESI = bitcast %union.anon* %3 to i32*
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %4 = load i64, i64* %RDX, align 8
  %5 = load i32, i32* %ESI, align 4
  %6 = zext i32 %5 to i64
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 2
  store i64 %8, i64* %PC, align 8
  %9 = or i64 %6, %4
  %10 = trunc i64 %9 to i32
  %11 = and i64 %9, 4294967295
  store i64 %11, i64* %RDX, align 8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %12, align 1
  %13 = and i32 %10, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = icmp eq i32 %10, 0
  %20 = zext i1 %19 to i8
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %20, i8* %21, align 1
  %22 = lshr i32 %10, 31
  %23 = trunc i32 %22 to i8
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %23, i8* %24, align 1
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %25, align 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %26, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__0xc9b8___rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 6
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, 51640
  store i64 %6, i64* %RAX, align 8
  %7 = icmp ugt i64 %3, -51641
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %3, 16
  %18 = xor i64 %17, %6
  %19 = lshr i64 %18, 4
  %20 = trunc i64 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i64 %6, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i64 %6, 63
  %27 = trunc i64 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %3, 63
  %30 = xor i64 %26, %29
  %31 = add nuw nsw i64 %30, %26
  %32 = icmp eq i64 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_shlq__0x4___rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 4
  store i64 %5, i64* %PC, align 8
  %6 = shl i64 %3, 4
  store i64 %6, i64* %RCX, align 8
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  %8 = lshr i64 %3, 60
  %9 = trunc i64 %8 to i8
  %10 = and i8 %9, 1
  store i8 %10, i8* %7, align 1
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  %12 = trunc i64 %6 to i32
  %13 = and i32 %12, 240
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  store i8 %17, i8* %11, align 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %18, align 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %20 = icmp eq i64 %6, 0
  %21 = zext i1 %20 to i8
  store i8 %21, i8* %19, align 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %23 = lshr i64 %3, 59
  %24 = trunc i64 %23 to i8
  %25 = and i8 %24, 1
  store i8 %25, i8* %22, align 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %26, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx____rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %4 = bitcast i64* %RAX to i32**
  %5 = load i32*, i32** %4, align 8
  %6 = load i32, i32* %EDX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 2
  store i64 %8, i64* %PC, align 8
  store i32 %6, i32* %5, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x9652__rax__rcx_1____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %3, 38482
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x9550__rax__rcx_1____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %3, 38224
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx__0x4__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = add i64 %4, 4
  %6 = load i32, i32* %EDX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x9856__rax__rcx_1____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %3, 38998
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x9754__rax__rcx_1____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %3, 38740
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx__0x8__rax_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = add i64 %4, 8
  %6 = load i32, i32* %EDX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a6cc(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a7a1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x34__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -52
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x24__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -36
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_0x29c__rcx____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = load i64, i64* %RCX, align 8
  %6 = add i64 %5, 668
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40a7ce(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d12b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x32___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 50
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RAX, align 8
  %9 = icmp ugt i32 %6, -51
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40a7fd(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x20__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -32
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40a829(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__0x0__MINUS0x58__rbp__rax_2_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = shl i64 %4, 1
  %6 = add i64 %3, -88
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 7
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  store i16 0, i16* %10, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a804(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jne_.L_40c559(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = zext i1 %6 to i8
  store i8 %7, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %8 = add i64 %.v, %3
  store i64 %8, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x32___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 50, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x20__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -32
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subl_MINUS0x1c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -28
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %8, %10
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = icmp ult i32 %8, %10
  %14 = zext i1 %13 to i8
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %14, i8* %15, align 1
  %16 = and i32 %11, 255
  %17 = tail call i32 @llvm.ctpop.i32(i32 %16)
  %18 = trunc i32 %17 to i8
  %19 = and i8 %18, 1
  %20 = xor i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %20, i8* %21, align 1
  %22 = xor i32 %10, %8
  %23 = xor i32 %22, %11
  %24 = lshr i32 %23, 4
  %25 = trunc i32 %24 to i8
  %26 = and i8 %25, 1
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %26, i8* %27, align 1
  %28 = icmp eq i32 %11, 0
  %29 = zext i1 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %11, 31
  %32 = trunc i32 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %32, i8* %33, align 1
  %34 = lshr i32 %8, 31
  %35 = lshr i32 %10, 31
  %36 = xor i32 %35, %34
  %37 = xor i32 %31, %34
  %38 = add nuw nsw i32 %37, %36
  %39 = icmp eq i32 %38, 2
  %40 = zext i1 %39 to i8
  %41 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %40, i8* %41, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp eq i32 %6, -1
  %10 = icmp eq i32 %7, 0
  %11 = or i1 %9, %10
  %12 = zext i1 %11 to i8
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %12, i8* %13, align 1
  %14 = and i32 %7, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i32 %7, %6
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = zext i1 %10 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x94__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -148
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x90__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -144
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x8c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -140
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 10
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0x78__rbp____rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -120
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x0___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = and i64 %3, 4294967295
  store i64 %7, i64* %RCX, align 8
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %8, align 1
  %9 = and i32 %6, 255
  %10 = tail call i32 @llvm.ctpop.i32(i32 %9)
  %11 = trunc i32 %10 to i8
  %12 = and i8 %11, 1
  %13 = xor i8 %12, 1
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %13, i8* %14, align 1
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %15, align 1
  %16 = icmp eq i32 %6, 0
  %17 = zext i1 %16 to i8
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %17, i8* %18, align 1
  %19 = lshr i32 %6, 31
  %20 = trunc i32 %19 to i8
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %20, i8* %21, align 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %22, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw___rax__rdx_2____si(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = load i64, i64* %RDX, align 8
  %6 = shl i64 %5, 1
  %7 = add i64 %6, %4
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 4
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  %11 = load i16, i16* %10, align 2
  store i16 %11, i16* %SI, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si__MINUS0x96__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -150
  %6 = load i16, i16* %SI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 7
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl_MINUS0x96__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -150
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i16*
  %8 = load i16, i16* %7, align 2
  %9 = zext i16 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %4 = load i32, i32* %ECX, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_shlq__0x4___rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 4
  store i64 %5, i64* %PC, align 8
  %6 = shl i64 %3, 4
  store i64 %6, i64* %RDX, align 8
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  %8 = lshr i64 %3, 60
  %9 = trunc i64 %8 to i8
  %10 = and i8 %9, 1
  store i8 %10, i8* %7, align 1
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  %12 = trunc i64 %6 to i32
  %13 = and i32 %12, 240
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  store i8 %17, i8* %11, align 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %18, align 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %20 = icmp eq i64 %6, 0
  %21 = zext i1 %20 to i8
  store i8 %21, i8* %19, align 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %23 = lshr i64 %3, 59
  %24 = trunc i64 %23 to i8
  %25 = and i8 %24, 1
  store i8 %25, i8* %22, align 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %26, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl___rax____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = bitcast i64* %RAX to i32**
  %4 = load i32*, i32** %3, align 8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 2
  store i64 %6, i64* %PC, align 8
  %7 = load i32, i32* %4, align 4
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl_MINUS0x8c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -140
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = add i32 %10, %8
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = icmp ult i32 %11, %8
  %14 = icmp ult i32 %11, %10
  %15 = or i1 %13, %14
  %16 = zext i1 %15 to i8
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %16, i8* %17, align 1
  %18 = and i32 %11, 255
  %19 = tail call i32 @llvm.ctpop.i32(i32 %18)
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = xor i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %22, i8* %23, align 1
  %24 = xor i32 %10, %8
  %25 = xor i32 %24, %11
  %26 = lshr i32 %25, 4
  %27 = trunc i32 %26 to i8
  %28 = and i8 %27, 1
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %28, i8* %29, align 1
  %30 = icmp eq i32 %11, 0
  %31 = zext i1 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %11, 31
  %34 = trunc i32 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %34, i8* %35, align 1
  %36 = lshr i32 %8, 31
  %37 = lshr i32 %10, 31
  %38 = xor i32 %33, %36
  %39 = xor i32 %33, %37
  %40 = add nuw nsw i32 %38, %39
  %41 = icmp eq i32 %40, 2
  %42 = zext i1 %41 to i8
  %43 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %42, i8* %43, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x8c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -140
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x4__rax____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl_MINUS0x90__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -144
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = add i32 %10, %8
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = icmp ult i32 %11, %8
  %14 = icmp ult i32 %11, %10
  %15 = or i1 %13, %14
  %16 = zext i1 %15 to i8
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %16, i8* %17, align 1
  %18 = and i32 %11, 255
  %19 = tail call i32 @llvm.ctpop.i32(i32 %18)
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = xor i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %22, i8* %23, align 1
  %24 = xor i32 %10, %8
  %25 = xor i32 %24, %11
  %26 = lshr i32 %25, 4
  %27 = trunc i32 %26 to i8
  %28 = and i8 %27, 1
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %28, i8* %29, align 1
  %30 = icmp eq i32 %11, 0
  %31 = zext i1 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %11, 31
  %34 = trunc i32 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %34, i8* %35, align 1
  %36 = lshr i32 %8, 31
  %37 = lshr i32 %10, 31
  %38 = xor i32 %33, %36
  %39 = xor i32 %33, %37
  %40 = add nuw nsw i32 %38, %39
  %41 = icmp eq i32 %40, 2
  %42 = zext i1 %41 to i8
  %43 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %42, i8* %43, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x90__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -144
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x8__rax____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl_MINUS0x94__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -148
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = add i32 %10, %8
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = icmp ult i32 %11, %8
  %14 = icmp ult i32 %11, %10
  %15 = or i1 %13, %14
  %16 = zext i1 %15 to i8
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %16, i8* %17, align 1
  %18 = and i32 %11, 255
  %19 = tail call i32 @llvm.ctpop.i32(i32 %18)
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = xor i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %22, i8* %23, align 1
  %24 = xor i32 %10, %8
  %25 = xor i32 %24, %11
  %26 = lshr i32 %25, 4
  %27 = trunc i32 %26 to i8
  %28 = and i8 %27, 1
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %28, i8* %29, align 1
  %30 = icmp eq i32 %11, 0
  %31 = zext i1 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %11, 31
  %34 = trunc i32 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %34, i8* %35, align 1
  %36 = lshr i32 %8, 31
  %37 = lshr i32 %10, 31
  %38 = xor i32 %33, %36
  %39 = xor i32 %33, %37
  %40 = add nuw nsw i32 %38, %39
  %41 = icmp eq i32 %40, 2
  %42 = zext i1 %41 to i8
  %43 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %42, i8* %43, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x94__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -148
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x3___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 3
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -4
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x4___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 4
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -5
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x5___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 5
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -6
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x6___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 6
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -7
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x7___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 7
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -8
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x8___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 8
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -9
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x9___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 9
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -10
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xa___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 10
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -11
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xb___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 11
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -12
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xc___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 12
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -13
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xd___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 13
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -14
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xe___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 14
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -15
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xf___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 15
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -16
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x10___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 16
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -17
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x11___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 17
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -18
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x12___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 18
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -19
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x13___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 19
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -20
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x14___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 20
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -21
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x15___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 21
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -22
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x16___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 22
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -23
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x17___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 23
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -24
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x18___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 24
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -25
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x19___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 25
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -26
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1a___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 26
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -27
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1b___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 27
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -28
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1c___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 28
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -29
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1d___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 29
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -30
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1e___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 30
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -31
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1f___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 31
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -32
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x20___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 32
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -33
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x21___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 33
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -34
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x22___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 34
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -35
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x23___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 35
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -36
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x24___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 36
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -37
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x25___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 37
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -38
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x26___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 38
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -39
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x27___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 39
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -40
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x28___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 40
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -41
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x29___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 41
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -42
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2a___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 42
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -43
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2b___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 43
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -44
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2c___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 44
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -45
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2d___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 45
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -46
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2e___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 46
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -47
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2f___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 47
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -48
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x30___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 48
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -49
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x31___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 49
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RCX, align 8
  %9 = icmp ugt i32 %6, -50
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x8c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -140
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_andl__0xffff___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 6
  store i64 %5, i64* %PC, align 8
  %6 = and i64 %3, 65535
  store i64 %6, i64* %RCX, align 8
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %7, align 1
  %and.shrunk = trunc i64 %3 to i32
  %8 = and i32 %and.shrunk, 255
  %9 = tail call i32 @llvm.ctpop.i32(i32 %8)
  %10 = trunc i32 %9 to i8
  %11 = and i8 %10, 1
  %12 = xor i8 %11, 1
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %12, i8* %13, align 1
  %14 = icmp eq i64 %6, 0
  %15 = zext i1 %14 to i8
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %15, i8* %16, align 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %17, align 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %18, align 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %19, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__cx___si(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CX = bitcast %union.anon* %3 to i16*
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %4 to i16*
  %5 = load i16, i16* %CX, align 2
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  store i16 %5, i16* %SI, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si__MINUS0x58__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -88
  %6 = load i16, i16* %SI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_shrl__0x10___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = lshr i64 %3, 15
  %7 = trunc i64 %6 to i8
  %8 = and i8 %7, 1
  %9 = lshr i64 %3, 16
  %10 = trunc i64 %9 to i32
  %11 = and i32 %10, 65535
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %13, align 1
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  %15 = and i32 %10, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  store i8 %19, i8* %14, align 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %20, align 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %22 = icmp eq i32 %11, 0
  %23 = zext i1 %22 to i8
  store i8 %23, i8* %21, align 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %24, align 1
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %25, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si__MINUS0x56__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -86
  %6 = load i16, i16* %SI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x90__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -144
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si__MINUS0x54__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -84
  %6 = load i16, i16* %SI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si__MINUS0x52__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -82
  %6 = load i16, i16* %SI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x94__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -148
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si__MINUS0x50__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -80
  %6 = load i16, i16* %SI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si__MINUS0x4e__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -78
  %6 = load i16, i16* %SI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40c5ef(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x14__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -20
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x14__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jg_.L_40c5ea(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = xor i1 %13, true
  %15 = and i1 %6, %14
  %16 = zext i1 %15 to i8
  store i8 %16, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %15, i64 %rel_off1, i64 %rel_off2
  %17 = add i64 %.v, %3
  store i64 %17, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x14__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw___rax__rcx_2____dx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DX = bitcast %union.anon* %3 to i16*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = load i64, i64* %RCX, align 8
  %6 = shl i64 %5, 1
  %7 = add i64 %6, %4
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 4
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  %11 = load i16, i16* %10, align 2
  store i16 %11, i16* %DX, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__dx__MINUS0x98__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DX = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -152
  %6 = load i16, i16* %DX, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 7
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40c5d7(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl_MINUS0x98__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -152
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i16*
  %8 = load i16, i16* %7, align 2
  %9 = zext i16 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i32, i32* %EDX, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl___rax__rcx_1____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %4, %3
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  %9 = load i8, i8* %8, align 1
  %10 = zext i8 %9 to i64
  store i64 %10, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl_MINUS0x58__rbp__rax_2____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = shl i64 %4, 1
  %6 = add i64 %3, -88
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 5
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  %11 = load i16, i16* %10, align 2
  %12 = zext i16 %11 to i64
  store i64 %12, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__edx___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %4 = load i64, i64* %RSI, align 8
  %5 = load i32, i32* %EDX, align 4
  %6 = zext i32 %5 to i64
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 2
  store i64 %8, i64* %PC, align 8
  %9 = trunc i64 %4 to i32
  %10 = add i32 %5, %9
  %11 = zext i32 %10 to i64
  store i64 %11, i64* %RSI, align 8
  %12 = icmp ult i32 %10, %9
  %13 = icmp ult i32 %10, %5
  %14 = or i1 %12, %13
  %15 = zext i1 %14 to i8
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %15, i8* %16, align 1
  %17 = and i32 %10, 255
  %18 = tail call i32 @llvm.ctpop.i32(i32 %17)
  %19 = trunc i32 %18 to i8
  %20 = and i8 %19, 1
  %21 = xor i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %21, i8* %22, align 1
  %23 = xor i64 %6, %4
  %24 = trunc i64 %23 to i32
  %25 = xor i32 %24, %10
  %26 = lshr i32 %25, 4
  %27 = trunc i32 %26 to i8
  %28 = and i8 %27, 1
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %28, i8* %29, align 1
  %30 = icmp eq i32 %10, 0
  %31 = zext i1 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %10, 31
  %34 = trunc i32 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %34, i8* %35, align 1
  %36 = lshr i32 %9, 31
  %37 = lshr i32 %5, 31
  %38 = xor i32 %33, %36
  %39 = xor i32 %33, %37
  %40 = add nuw nsw i32 %38, %39
  %41 = icmp eq i32 %40, 2
  %42 = zext i1 %41 to i8
  %43 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %42, i8* %43, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__si___di(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %SI = bitcast %union.anon* %3 to i16*
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0
  %DI = bitcast %union.anon* %4 to i16*
  %5 = load i16, i16* %SI, align 2
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  store i16 %5, i16* %DI, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__di__MINUS0x58__rbp__rax_2_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0
  %DI = bitcast %union.anon* %3 to i16*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = load i64, i64* %RAX, align 8
  %6 = shl i64 %5, 1
  %7 = add i64 %4, -88
  %8 = add i64 %7, %6
  %9 = load i16, i16* %DI, align 2
  %10 = load i64, i64* %PC, align 8
  %11 = add i64 %10, 5
  store i64 %11, i64* %PC, align 8
  %12 = inttoptr i64 %8 to i16*
  store i16 %9, i16* %12, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40c585(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40c5dc(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40c55f(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x3b9ac9ff__MINUS0x2c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -44
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 999999999, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xffffffff__MINUS0x28__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -40
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 -1, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40c647(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl_MINUS0x58__rbp__rax_2____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = shl i64 %4, 1
  %6 = add i64 %3, -88
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 5
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  %11 = load i16, i16* %10, align 2
  %12 = zext i16 %11 to i64
  store i64 %12, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x2c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %ECX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -44
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40c634(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x2c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -44
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x10__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x28__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -40
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40c639(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40c604(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x2c__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -44
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl_MINUS0x24__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -36
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = add i32 %10, %8
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RAX, align 8
  %13 = icmp ult i32 %11, %8
  %14 = icmp ult i32 %11, %10
  %15 = or i1 %13, %14
  %16 = zext i1 %15 to i8
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %16, i8* %17, align 1
  %18 = and i32 %11, 255
  %19 = tail call i32 @llvm.ctpop.i32(i32 %18)
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = xor i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %22, i8* %23, align 1
  %24 = xor i32 %10, %8
  %25 = xor i32 %24, %11
  %26 = lshr i32 %25, 4
  %27 = trunc i32 %26 to i8
  %28 = and i8 %27, 1
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %28, i8* %29, align 1
  %30 = icmp eq i32 %11, 0
  %31 = zext i1 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %11, 31
  %34 = trunc i32 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %34, i8* %35, align 1
  %36 = lshr i32 %8, 31
  %37 = lshr i32 %10, 31
  %38 = xor i32 %33, %36
  %39 = xor i32 %33, %37
  %40 = add nuw nsw i32 %38, %39
  %41 = icmp eq i32 %40, 2
  %42 = zext i1 %41 to i8
  %43 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %42, i8* %43, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x24__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -36
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x28__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -40
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x70__rbp__rcx_4____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %3, -112
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 4
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i32*
  %11 = load i32, i32* %10, align 4
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x70__rbp__rcx_4_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = load i64, i64* %RCX, align 8
  %6 = shl i64 %5, 2
  %7 = add i64 %4, -112
  %8 = add i64 %7, %6
  %9 = load i32, i32* %EAX, align 4
  %10 = load i64, i64* %PC, align 8
  %11 = add i64 %10, 4
  store i64 %11, i64* %PC, align 8
  %12 = inttoptr i64 %8 to i32*
  store i32 %9, i32* %12, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x28__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -40
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__al___dl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %4 to i8*
  %5 = load i8, i8* %AL, align 1
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i8 %5, i8* %DL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x34__rbp____rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -52
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__dl__0x6a8__rcx__rsi_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %3 to i8*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %4 = load i64, i64* %RCX, align 8
  %5 = load i64, i64* %RSI, align 8
  %6 = add i64 %4, 1704
  %7 = add i64 %6, %5
  %8 = load i8, i8* %DL, align 1
  %9 = load i64, i64* %PC, align 8
  %10 = add i64 %9, 7
  store i64 %10, i64* %PC, align 8
  %11 = inttoptr i64 %7 to i8*
  store i8 %8, i8* %11, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x34__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -52
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x34__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -52
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jne_.L_40d0c9(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = zext i1 %6 to i8
  store i8 %7, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %8 = add i64 %.v, %3
  store i64 %8, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0x78__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -120
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x1c__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -28
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x0___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = and i64 %3, 4294967295
  store i64 %7, i64* %RDX, align 8
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %8, align 1
  %9 = and i32 %6, 255
  %10 = tail call i32 @llvm.ctpop.i32(i32 %9)
  %11 = trunc i32 %10 to i8
  %12 = and i8 %11, 1
  %13 = xor i8 %12, 1
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %13, i8* %14, align 1
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %15, align 1
  %16 = icmp eq i32 %6, 0
  %17 = zext i1 %16 to i8
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %17, i8* %18, align 1
  %19 = lshr i32 %6, 31
  %20 = trunc i32 %19 to i8
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %20, i8* %21, align 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %22, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq__edx___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %4 = load i32, i32* %EDX, align 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = sext i32 %4 to i64
  store i64 %7, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl___rcx__rsi_2____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RSI, align 8
  %5 = shl i64 %4, 1
  %6 = add i64 %5, %3
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i16*
  %10 = load i16, i16* %9, align 2
  %11 = zext i16 %10 to i64
  store i64 %11, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl___rax__rcx_4____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %5, %3
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = zext i32 %10 to i64
  store i64 %11, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp eq i32 %6, -1
  %10 = icmp eq i32 %7, 0
  %11 = or i1 %9, %10
  %12 = zext i1 %11 to i8
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %12, i8* %13, align 1
  %14 = and i32 %7, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i32 %7, %6
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = zext i1 %10 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx____rax__rcx_4_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = load i64, i64* %RCX, align 8
  %6 = shl i64 %5, 2
  %7 = add i64 %6, %4
  %8 = load i32, i32* %EDX, align 4
  %9 = load i64, i64* %PC, align 8
  %10 = add i64 %9, 3
  store i64 %10, i64* %PC, align 8
  %11 = inttoptr i64 %7 to i32*
  store i32 %8, i32* %11, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 2
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -3
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x3___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 3
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -4
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x4___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 4
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -5
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x5___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 5
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -6
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x6___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 6
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -7
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x7___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 7
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -8
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x8___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 8
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -9
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x9___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 9
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -10
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xa___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 10
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -11
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xb___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 11
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -12
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xc___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 12
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -13
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xd___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 13
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -14
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xe___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 14
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -15
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0xf___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 15
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -16
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x10___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 16
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -17
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x11___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 17
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -18
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x12___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 18
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -19
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x13___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 19
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -20
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x14___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 20
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -21
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x15___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 21
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -22
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x16___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 22
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -23
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x17___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 23
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -24
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x18___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 24
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -25
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x19___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 25
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -26
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1a___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 26
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -27
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1b___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 27
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -28
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1c___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 28
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -29
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1d___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 29
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -30
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1e___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 30
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -31
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1f___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 31
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -32
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x20___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 32
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -33
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x21___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 33
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -34
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x22___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 34
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -35
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x23___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 35
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -36
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x24___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 36
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -37
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x25___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 37
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -38
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x26___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 38
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -39
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x27___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 39
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -40
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x28___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 40
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -41
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x29___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 41
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -42
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2a___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 42
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -43
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2b___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 43
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -44
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2c___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 44
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -45
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2d___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 45
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -46
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2e___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 46
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -47
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x2f___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 47
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -48
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %7, %6
  %19 = lshr i32 %18, 4
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i32 %7, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i32 %7, 31
  %27 = trunc i32 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i32 %6, 31
  %30 = xor i32 %26, %29
  %31 = add nuw nsw i32 %30, %26
  %32 = icmp eq i32 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x30___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 48
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -49
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x31___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 49
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RDX, align 8
  %9 = icmp ugt i32 %6, -50
  %10 = zext i1 %9 to i8
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %10, i8* %11, align 1
  %12 = and i32 %7, 255
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = xor i32 %6, 16
  %19 = xor i32 %18, %7
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %7, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d11d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jg_.L_40d118(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = xor i1 %13, true
  %15 = and i1 %6, %14
  %16 = zext i1 %15 to i8
  store i8 %16, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %15, i64 %rel_off1, i64 %rel_off2
  %17 = add i64 %.v, %3
  store i64 %17, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x14__rbp____rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl___rcx__rdx_2____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = shl i64 %4, 1
  %6 = add i64 %5, %3
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 4
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i16*
  %10 = load i16, i16* %9, align 2
  %11 = zext i16 %10 to i64
  store i64 %11, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__esi___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %ESI = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i32, i32* %ESI, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl___rax__rcx_4____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %5, %3
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = zext i32 %10 to i64
  store i64 %11, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x1___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RSI, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = add i32 %6, 1
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %RSI, align 8
  %9 = icmp eq i32 %6, -1
  %10 = icmp eq i32 %7, 0
  %11 = or i1 %9, %10
  %12 = zext i1 %11 to i8
  %13 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %12, i8* %13, align 1
  %14 = and i32 %7, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i32 %7, %6
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = zext i1 %10 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %7, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %6, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %27
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__esi____rax__rcx_4_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %ESI = bitcast %union.anon* %3 to i32*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = load i64, i64* %RCX, align 8
  %6 = shl i64 %5, 2
  %7 = add i64 %6, %4
  %8 = load i32, i32* %ESI, align 4
  %9 = load i64, i64* %PC, align 8
  %10 = add i64 %9, 3
  store i64 %10, i64* %PC, align 8
  %11 = inttoptr i64 %7 to i32*
  store i32 %8, i32* %11, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d0cf(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a7b6(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d1e7(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x416594___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x416594_type* @G__0x416594 to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x8___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 8, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x30__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -48
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x24__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -36
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xec__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -236
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %4 = load i32, i32* %EDX, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0xec__rbp____r8d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -236
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 7
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i32*
  %9 = load i32, i32* %8, align 4
  %10 = zext i32 %9 to i64
  store i64 %10, i64* %3, align 8
  ret %struct.Memory* %2
}

define %struct.Memory* @routine_idivl__r8d(%struct.State* dereferenceable(3376), i64, %struct.Memory*) local_unnamed_addr {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0
  %R8D = bitcast %union.anon* %3 to i32*
  %4 = load i32, i32* %R8D, align 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %8 = bitcast %union.anon* %7 to i32*
  %9 = load i32, i32* %8, align 8
  %10 = zext i32 %9 to i64
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %12 = bitcast %union.anon* %11 to i32*
  %13 = load i32, i32* %12, align 8
  %14 = zext i32 %13 to i64
  %15 = sext i32 %4 to i64
  %16 = shl nuw i64 %14, 32
  %17 = or i64 %16, %10
  %18 = sdiv i64 %17, %15
  %19 = shl i64 %18, 32
  %20 = ashr exact i64 %19, 32
  %21 = icmp eq i64 %18, %20
  br i1 %21, label %24, label %22

; <label>:22:                                     ; preds = %block_400488
  %23 = tail call %struct.Memory* @__remill_error(%struct.State* nonnull dereferenceable(3376) %0, i64 %6, %struct.Memory* %2)
  br label %_ZN12_GLOBAL__N_1L10IDIVedxeaxI2RnIjEEEP6MemoryS4_R5StateT_.exit

; <label>:24:                                     ; preds = %block_400488
  %25 = srem i64 %17, %15
  %26 = getelementptr inbounds %union.anon, %union.anon* %7, i64 0, i32 0
  %27 = and i64 %18, 4294967295
  store i64 %27, i64* %26, align 8
  %28 = getelementptr inbounds %union.anon, %union.anon* %11, i64 0, i32 0
  %29 = and i64 %25, 4294967295
  store i64 %29, i64* %28, align 8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %30, align 1
  %31 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 0, i8* %31, align 1
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %32, align 1
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 0, i8* %33, align 1
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %34, align 1
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %35, align 1
  br label %_ZN12_GLOBAL__N_1L10IDIVedxeaxI2RnIjEEEP6MemoryS4_R5StateT_.exit

_ZN12_GLOBAL__N_1L10IDIVedxeaxI2RnIjEEEP6MemoryS4_R5StateT_.exit: ; preds = %24, %22
  %36 = phi %struct.Memory* [ %23, %22 ], [ %2, %24 ]
  ret %struct.Memory* %36
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xf0__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -240
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d1c8(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x4165bd___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x4165bd_type* @G__0x4165bd to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x70__rbp__rax_4____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = shl i64 %4, 2
  %6 = add i64 %3, -112
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 4
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i32*
  %11 = load i32, i32* %10, align 4
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xf4__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -244
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d187(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x415fb8___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x415fb8_type* @G__0x415fb8 to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xf8__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -248
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d258(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x11___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 17, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x10__rbp____rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0x8__rbp____rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__0xb188___rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, 45448
  store i64 %6, i64* %RDX, align 8
  %7 = icmp ugt i64 %3, -45449
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %6, %3
  %18 = lshr i64 %17, 4
  %19 = trunc i64 %18 to i8
  %20 = and i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %20, i8* %21, align 1
  %22 = icmp eq i64 %6, 0
  %23 = zext i1 %22 to i8
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %23, i8* %24, align 1
  %25 = lshr i64 %6, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = lshr i64 %3, 63
  %29 = xor i64 %25, %28
  %30 = add nuw nsw i64 %29, %25
  %31 = icmp eq i64 %30, 2
  %32 = zext i1 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %32, i8* %33, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x10__rbp____rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -16
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_imulq__0x408___rsi___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RSI, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = sext i64 %3 to i128
  %7 = and i128 %6, -18446744073709551616
  %8 = zext i64 %3 to i128
  %9 = or i128 %7, %8
  %10 = mul nsw i128 %9, 1032
  %11 = trunc i128 %10 to i64
  store i64 %11, i64* %RSI, align 8
  %12 = sext i64 %11 to i128
  %13 = icmp ne i128 %12, %10
  %14 = zext i1 %13 to i8
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %14, i8* %15, align 1
  %16 = trunc i128 %10 to i32
  %17 = and i32 %16, 248
  %18 = tail call i32 @llvm.ctpop.i32(i32 %17)
  %19 = trunc i32 %18 to i8
  %20 = and i8 %19, 1
  %21 = xor i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %21, i8* %22, align 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %23, align 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 0, i8* %24, align 1
  %25 = lshr i64 %11, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %14, i8* %28, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__rsi___rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %RSI, align 8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = add i64 %4, %3
  store i64 %7, i64* %RDX, align 8
  %8 = icmp ult i64 %7, %3
  %9 = icmp ult i64 %7, %4
  %10 = or i1 %8, %9
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = trunc i64 %7 to i32
  %14 = and i32 %13, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i64 %4, %3
  %21 = xor i64 %20, %7
  %22 = lshr i64 %21, 4
  %23 = trunc i64 %22 to i8
  %24 = and i8 %23, 1
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %24, i8* %25, align 1
  %26 = icmp eq i64 %7, 0
  %27 = zext i1 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %7, 63
  %30 = trunc i64 %29 to i8
  %31 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %30, i8* %31, align 1
  %32 = lshr i64 %3, 63
  %33 = lshr i64 %4, 63
  %34 = xor i64 %29, %32
  %35 = xor i64 %29, %33
  %36 = add nuw nsw i64 %34, %35
  %37 = icmp eq i64 %36, 2
  %38 = zext i1 %37 to i8
  %39 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %38, i8* %39, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x38__rbp____edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -56
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edi__MINUS0xfc__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0
  %EDI = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -252
  %6 = load i32, i32* %EDI, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rax___rdi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  store i64 %3, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rdx___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RDX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  store i64 %3, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0xfc__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -252
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_callq_.BZ2_hbMakeCodeLengths(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  %5 = add i64 %3, %rel_off2
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = add i64 %7, -8
  %9 = inttoptr i64 %8 to i64*
  store i64 %5, i64* %9, align 8
  store i64 %8, i64* %6, align 8
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d1ee(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d25d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x30__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -48
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x30__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -48
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40a61a(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x8__MINUS0x48__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -8
  %10 = icmp ult i32 %8, 8
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d27f(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xbba___edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3002, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x8000__MINUS0x34__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -52
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -32768
  %10 = icmp ult i32 %8, 32768
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d299(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x4652__MINUS0x34__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -52
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -18002
  %10 = icmp ult i32 %8, 18002
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %8, 16
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %8, 31
  %32 = xor i32 %28, %31
  %33 = add nuw nsw i32 %32, %31
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jle_.L_40d2a3(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = or i1 %6, %13
  %15 = zext i1 %14 to i8
  store i8 %15, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %14, i64 %rel_off1, i64 %rel_off2
  %16 = add i64 %.v, %3
  store i64 %16, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xbbb___edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3003, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x14__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d2d4(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__cl__MINUS0x9e__rbp__rdx_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %3 to i8*
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = load i64, i64* %RDX, align 8
  %6 = add i64 %4, -158
  %7 = add i64 %6, %5
  %8 = load i8, i8* %CL, align 1
  %9 = load i64, i64* %PC, align 8
  %10 = add i64 %9, 7
  store i64 %10, i64* %PC, align 8
  %11 = inttoptr i64 %7 to i8*
  store i8 %8, i8* %11, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d2aa(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x34__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -52
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d394(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb_0x6a8__rax__rcx_1____dl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %3 to i8*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = load i64, i64* %RCX, align 8
  %6 = add i64 %4, 1704
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 7
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i8*
  %11 = load i8, i8* %10, align 1
  store i8 %11, i8* %DL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__dl__MINUS0x9f__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -159
  %6 = load i8, i8* %DL, align 1
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i8*
  store i8 %6, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x18__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -24
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x18__rbp____rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -24
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb_MINUS0x9e__rbp__rax_1____dl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %3 to i8*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = load i64, i64* %RAX, align 8
  %6 = add i64 %4, -158
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 7
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i8*
  %11 = load i8, i8* %10, align 1
  store i8 %11, i8* %DL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__dl__MINUS0xa1__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %DL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -161
  %6 = load i8, i8* %DL, align 1
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i8*
  store i8 %6, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_MINUS0x9f__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -159
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i8*
  %8 = load i8, i8* %7, align 1
  %9 = zext i8 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_MINUS0xa1__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -161
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i8*
  %8 = load i8, i8* %7, align 1
  %9 = zext i8 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_je_.L_40d366(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  store i8 %5, i8* %BRANCH_TAKEN, align 1
  %6 = icmp ne i8 %5, 0
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %7 = add i64 %.v, %3
  store i64 %7, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x18__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -24
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x18__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -24
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb_MINUS0xa1__rbp____cl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -161
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  %9 = load i8, i8* %8, align 1
  store i8 %9, i8* %CL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__cl__MINUS0xa0__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -160
  %6 = load i8, i8* %CL, align 1
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i8*
  store i8 %6, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x18__rbp____rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -24
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb_MINUS0x9e__rbp__rdx_1____cl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %3 to i8*
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = load i64, i64* %RDX, align 8
  %6 = add i64 %4, -158
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 7
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i8*
  %11 = load i8, i8* %10, align 1
  store i8 %11, i8* %CL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__cl__MINUS0xa1__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -161
  %6 = load i8, i8* %CL, align 1
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i8*
  store i8 %6, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb_MINUS0xa0__rbp____cl(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -160
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  %9 = load i8, i8* %8, align 1
  store i8 %9, i8* %CL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d314(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb_MINUS0xa1__rbp____al(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -161
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 6
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  %9 = load i8, i8* %8, align 1
  store i8 %9, i8* %AL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__al__MINUS0x9e__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -158
  %6 = load i8, i8* %AL, align 1
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i8*
  store i8 %6, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x18__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -24
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__cl___al(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %CL = bitcast %union.anon* %4 to i8*
  %5 = load i8, i8* %CL, align 1
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i8 %5, i8* %AL, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x14__rbp____rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__al__0x4cfa__rdx__rsi_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %AL = bitcast %union.anon* %3 to i8*
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %4 = load i64, i64* %RDX, align 8
  %5 = load i64, i64* %RSI, align 8
  %6 = add i64 %4, 19706
  %7 = add i64 %6, %5
  %8 = load i8, i8* %AL, align 1
  %9 = load i64, i64* %PC, align 8
  %10 = add i64 %9, 7
  store i64 %10, i64* %PC, align 8
  %11 = inttoptr i64 %7 to i8*
  store i8 %8, i8* %11, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d2db(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d4fb(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x20__MINUS0x3c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -60
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 32, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x40__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -64
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d473(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x40__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EDX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -64
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jle_.L_40d414(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = or i1 %6, %13
  %15 = zext i1 %14 to i8
  store i8 %15, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %14, i64 %rel_off1, i64 %rel_off2
  %16 = add i64 %.v, %3
  store i64 %16, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx__MINUS0x40__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -64
  %6 = load i32, i32* %EDX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x3c__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EDX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -60
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d460(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx__MINUS0x3c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -60
  %6 = load i32, i32* %EDX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d465(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d3bc(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x11__MINUS0x40__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -64
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -17
  %10 = icmp ult i32 %8, 17
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %8, 16
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %8, 31
  %32 = xor i32 %28, %31
  %33 = add nuw nsw i32 %32, %31
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jle_.L_40d487(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = or i1 %6, %13
  %15 = zext i1 %14 to i8
  store i8 %15, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %14, i64 %rel_off1, i64 %rel_off2
  %16 = add i64 %.v, %3
  store i64 %16, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xbbc___edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3004, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x1__MINUS0x3c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -60
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -1
  %10 = icmp eq i32 %8, 0
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %9, %8
  %20 = lshr i32 %19, 4
  %21 = trunc i32 %20 to i8
  %22 = and i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %22, i8* %23, align 1
  %24 = icmp eq i32 %9, 0
  %25 = zext i1 %24 to i8
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %25, i8* %26, align 1
  %27 = lshr i32 %9, 31
  %28 = trunc i32 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %8, 31
  %31 = xor i32 %27, %30
  %32 = add nuw nsw i32 %31, %30
  %33 = icmp eq i32 %32, 2
  %34 = zext i1 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %34, i8* %35, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d49b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xbbd___edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3005, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__0x9958___rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 6
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, 39256
  store i64 %6, i64* %RAX, align 8
  %7 = icmp ugt i64 %3, -39257
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %3, 16
  %18 = xor i64 %17, %6
  %19 = lshr i64 %18, 4
  %20 = trunc i64 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i64 %6, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i64 %6, 63
  %27 = trunc i64 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %3, 63
  %30 = xor i64 %26, %29
  %31 = add nuw nsw i64 %30, %26
  %32 = icmp eq i64 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__0x934c___rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, 37708
  store i64 %6, i64* %RCX, align 8
  %7 = icmp ugt i64 %3, -37709
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %6, %3
  %18 = lshr i64 %17, 4
  %19 = trunc i64 %18 to i8
  %20 = and i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %20, i8* %21, align 1
  %22 = icmp eq i64 %6, 0
  %23 = zext i1 %22 to i8
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %23, i8* %24, align 1
  %25 = lshr i64 %6, 63
  %26 = trunc i64 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %26, i8* %27, align 1
  %28 = lshr i64 %3, 63
  %29 = xor i64 %25, %28
  %30 = add nuw nsw i64 %29, %25
  %31 = icmp eq i64 %30, 2
  %32 = zext i1 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %32, i8* %33, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__rdx___rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = add i64 %4, %3
  store i64 %7, i64* %RCX, align 8
  %8 = icmp ult i64 %7, %3
  %9 = icmp ult i64 %7, %4
  %10 = or i1 %8, %9
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = trunc i64 %7 to i32
  %14 = and i32 %13, 255
  %15 = tail call i32 @llvm.ctpop.i32(i32 %14)
  %16 = trunc i32 %15 to i8
  %17 = and i8 %16, 1
  %18 = xor i8 %17, 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %18, i8* %19, align 1
  %20 = xor i64 %4, %3
  %21 = xor i64 %20, %7
  %22 = lshr i64 %21, 4
  %23 = trunc i64 %22 to i8
  %24 = and i8 %23, 1
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %24, i8* %25, align 1
  %26 = icmp eq i64 %7, 0
  %27 = zext i1 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %7, 63
  %30 = trunc i64 %29 to i8
  %31 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %30, i8* %31, align 1
  %32 = lshr i64 %3, 63
  %33 = lshr i64 %4, 63
  %34 = xor i64 %29, %32
  %35 = xor i64 %29, %33
  %36 = add nuw nsw i64 %34, %35
  %37 = icmp eq i64 %36, 2
  %38 = zext i1 %37 to i8
  %39 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %38, i8* %39, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x3c__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -60
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x40__rbp____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -64
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x38__rbp____r8d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -56
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i32*
  %9 = load i32, i32* %8, align 4
  %10 = zext i32 %9 to i64
  store i64 %10, i64* %3, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__esi__MINUS0x100__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %ESI = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -256
  %6 = load i32, i32* %ESI, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rcx___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  store i64 %3, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x100__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -256
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_callq_.BZ2_hbAssignCodes(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  %5 = add i64 %3, %rel_off2
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = add i64 %7, -8
  %9 = inttoptr i64 %8 to i64*
  store i64 %5, i64* %9, align 8
  store i64 %8, i64* %6, align 8
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d39b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x10__MINUS0x14__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -16
  %10 = icmp ult i32 %8, 16
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %8, 16
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %8, 31
  %32 = xor i32 %28, %31
  %33 = add nuw nsw i32 %32, %31
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d579(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x14__rbp____rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__0x0__MINUS0xc0__rbp__rax_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = add i64 %3, -192
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  store i8 0, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__0x10__MINUS0x18__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -24
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = add i32 %8, -16
  %10 = icmp ult i32 %8, 16
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %8, 16
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %8, 31
  %32 = xor i32 %28, %31
  %33 = add nuw nsw i32 %32, %31
  %34 = icmp eq i32 %33, 2
  %35 = zext i1 %34 to i8
  %36 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %35, i8* %36, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d566(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x14__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -20
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_shll__0x4___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %.tr = trunc i64 %3 to i32
  %6 = shl i32 %.tr, 4
  %7 = zext i32 %6 to i64
  store i64 %7, i64* %RCX, align 8
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  %9 = lshr i64 %3, 28
  %10 = trunc i64 %9 to i8
  %11 = and i8 %10, 1
  store i8 %11, i8* %8, align 1
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  %13 = and i32 %6, 240
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  store i8 %17, i8* %12, align 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %18, align 1
  %19 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %20 = icmp eq i32 %6, 0
  %21 = zext i1 %20 to i8
  store i8 %21, i8* %19, align 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %23 = lshr i32 %.tr, 27
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  store i8 %25, i8* %22, align 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %26, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl_MINUS0x18__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -24
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = add i32 %10, %8
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = icmp ult i32 %11, %8
  %14 = icmp ult i32 %11, %10
  %15 = or i1 %13, %14
  %16 = zext i1 %15 to i8
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %16, i8* %17, align 1
  %18 = and i32 %11, 255
  %19 = tail call i32 @llvm.ctpop.i32(i32 %18)
  %20 = trunc i32 %19 to i8
  %21 = and i8 %20, 1
  %22 = xor i8 %21, 1
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %22, i8* %23, align 1
  %24 = xor i32 %10, %8
  %25 = xor i32 %24, %11
  %26 = lshr i32 %25, 4
  %27 = trunc i32 %26 to i8
  %28 = and i8 %27, 1
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %28, i8* %29, align 1
  %30 = icmp eq i32 %11, 0
  %31 = zext i1 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %11, 31
  %34 = trunc i32 %33 to i8
  %35 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %34, i8* %35, align 1
  %36 = lshr i32 %8, 31
  %37 = lshr i32 %10, 31
  %38 = xor i32 %33, %36
  %39 = xor i32 %33, %37
  %40 = add nuw nsw i32 %38, %39
  %41 = icmp eq i32 %40, 2
  %42 = zext i1 %41 to i8
  %43 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %42, i8* %43, align 1
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpb__0x0__0x80__rax__rdx_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = add i64 %3, 128
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %11, align 1
  %12 = zext i8 %10 to i32
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %18, align 1
  %19 = icmp eq i8 %10, 0
  %20 = zext i1 %19 to i8
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %20, i8* %21, align 1
  %22 = lshr i8 %10, 7
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %22, i8* %23, align 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %24, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_je_.L_40d553(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  store i8 %5, i8* %BRANCH_TAKEN, align 1
  %6 = icmp ne i8 %5, 0
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %7 = add i64 %.v, %3
  store i64 %7, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movb__0x1__MINUS0xc0__rbp__rax_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = add i64 %3, -192
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  store i8 1, i8* %9, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d558(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d51f(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d56b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d502(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_0x74__rax____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = add i64 %3, 116
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__ecx__MINUS0x4c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0
  %ECX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -76
  %6 = load i32, i32* %ECX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d5e0(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpb__0x0__MINUS0xc0__rbp__rax_1_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = load i64, i64* %RAX, align 8
  %5 = add i64 %3, -192
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %11, align 1
  %12 = zext i8 %10 to i32
  %13 = tail call i32 @llvm.ctpop.i32(i32 %12)
  %14 = trunc i32 %13 to i8
  %15 = and i8 %14, 1
  %16 = xor i8 %15, 1
  %17 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %16, i8* %17, align 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %18, align 1
  %19 = icmp eq i8 %10, 0
  %20 = zext i1 %19 to i8
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %20, i8* %21, align 1
  %22 = lshr i8 %10, 7
  %23 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %22, i8* %23, align 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %24, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_je_.L_40d5bd(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  store i8 %5, i8* %BRANCH_TAKEN, align 1
  %6 = icmp ne i8 %5, 0
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %7 = add i64 %.v, %3
  store i64 %7, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x1___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 1, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %4 = load i32, i32* %EAX, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 2
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_callq_.bsW(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  %5 = add i64 %3, %rel_off2
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %7 = load i64, i64* %6, align 8
  %8 = add i64 %7, -8
  %9 = inttoptr i64 %8 to i64*
  store i64 %5, i64* %9, align 8
  store i64 %8, i64* %6, align 8
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d5cd(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x1___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 1, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_xorl__edx___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 2
  store i64 %4, i64* %PC, align 8
  store i64 0, i64* %RDX, align 8
  %5 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %5, align 1
  %6 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 1, i8* %6, align 1
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 1, i8* %7, align 1
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 0, i8* %8, align 1
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %9, align 1
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %10, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d5d2(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d58a(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d684(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_je_.L_40d671(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  store i8 %5, i8* %BRANCH_TAKEN, align 1
  %6 = icmp ne i8 %5, 0
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %7 = add i64 %.v, %3
  store i64 %7, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d66c(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_je_.L_40d649(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  store i8 %5, i8* %BRANCH_TAKEN, align 1
  %6 = icmp ne i8 %5, 0
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %7 = add i64 %.v, %3
  store i64 %7, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d659(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d65e(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d60a(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d671(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d676(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d5e7(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d6c0(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x4165c1___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x4165c1_type* @G__0x4165c1 to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_subl_MINUS0x4c__rbp____ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -76
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  %8 = trunc i64 %3 to i32
  %9 = inttoptr i64 %5 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %8, %10
  %12 = zext i32 %11 to i64
  store i64 %12, i64* %RCX, align 8
  %13 = icmp ult i32 %8, %10
  %14 = zext i1 %13 to i8
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %14, i8* %15, align 1
  %16 = and i32 %11, 255
  %17 = tail call i32 @llvm.ctpop.i32(i32 %16)
  %18 = trunc i32 %17 to i8
  %19 = and i8 %18, 1
  %20 = xor i8 %19, 1
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %20, i8* %21, align 1
  %22 = xor i32 %10, %8
  %23 = xor i32 %22, %11
  %24 = lshr i32 %23, 4
  %25 = trunc i32 %24 to i8
  %26 = and i8 %25, 1
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %26, i8* %27, align 1
  %28 = icmp eq i32 %11, 0
  %29 = zext i1 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %11, 31
  %32 = trunc i32 %31 to i8
  %33 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %32, i8* %33, align 1
  %34 = lshr i32 %8, 31
  %35 = lshr i32 %10, 31
  %36 = xor i32 %35, %34
  %37 = xor i32 %31, %34
  %38 = add nuw nsw i32 %37, %36
  %39 = icmp eq i32 %38, 2
  %40 = zext i1 %39 to i8
  %41 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %40, i8* %41, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x104__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -260
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x3___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x48__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -72
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xf___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 15, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x34__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -52
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d75f(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x4cfa__rcx__rdx_1____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = add i64 %3, 19706
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl__esi___eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %ESI = bitcast %union.anon* %4 to i32*
  %5 = load i32, i32* %EAX, align 4
  %6 = load i32, i32* %ESI, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 2
  store i64 %8, i64* %PC, align 8
  %9 = sub i32 %5, %6
  %10 = icmp ult i32 %5, %6
  %11 = zext i1 %10 to i8
  %12 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %11, i8* %12, align 1
  %13 = and i32 %9, 255
  %14 = tail call i32 @llvm.ctpop.i32(i32 %13)
  %15 = trunc i32 %14 to i8
  %16 = and i8 %15, 1
  %17 = xor i8 %16, 1
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %17, i8* %18, align 1
  %19 = xor i32 %6, %5
  %20 = xor i32 %19, %9
  %21 = lshr i32 %20, 4
  %22 = trunc i32 %21 to i8
  %23 = and i8 %22, 1
  %24 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %23, i8* %24, align 1
  %25 = icmp eq i32 %9, 0
  %26 = zext i1 %25 to i8
  %27 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %26, i8* %27, align 1
  %28 = lshr i32 %9, 31
  %29 = trunc i32 %28 to i8
  %30 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %29, i8* %30, align 1
  %31 = lshr i32 %5, 31
  %32 = lshr i32 %6, 31
  %33 = xor i32 %32, %31
  %34 = xor i32 %28, %31
  %35 = add nuw nsw i32 %34, %33
  %36 = icmp eq i32 %35, 2
  %37 = zext i1 %36 to i8
  %38 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %37, i8* %38, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d741(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d706(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d6f3(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d79b(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x4165db___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x4165db_type* @G__0x4165db to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x108__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -264
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d8e6(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x5___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 5, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl___rax____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = bitcast i64* %RAX to i8**
  %4 = load i8*, i8** %3, align 8
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = load i8, i8* %4, align 1
  %8 = zext i8 %7 to i64
  store i64 %8, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__edx__MINUS0xc4__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -196
  %6 = load i32, i32* %EDX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0xc4__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -196
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d8d3(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d805(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0xc4__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -196
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 6
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl___rcx__rdx_1____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = add i64 %4, %3
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  %9 = load i8, i8* %8, align 1
  %10 = zext i8 %9 to i64
  store i64 %10, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jge_.L_40d85a(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = xor i1 %10, true
  %12 = zext i1 %11 to i8
  store i8 %12, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off2, i64 %rel_off1
  %13 = add i64 %.v, %3
  store i64 %13, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0xc4__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -196
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d85f(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jle_.L_40d8b5(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = or i1 %6, %13
  %15 = zext i1 %14 to i8
  store i8 %15, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %14, i64 %rel_off1, i64 %rel_off2
  %16 = add i64 %.v, %3
  store i64 %16, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x2___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 2, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x3___edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d7f4(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d8d8(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d7ac(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d922(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x4165ea___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x4165ea_type* @G__0x4165ea to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x10c__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -268
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0x0__MINUS0x44__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -68
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  store i32 0, i32* %7, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d952(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40e94f(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d981(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x44__rbp____rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -68
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x6a8__rax__rcx_1____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %3, 1704
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_cmpl_MINUS0x48__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i32, i32* %EDX, align 4
  %5 = load i64, i64* %RBP, align 8
  %6 = add i64 %5, -72
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i32*
  %10 = load i32, i32* %9, align 4
  %11 = sub i32 %4, %10
  %12 = icmp ult i32 %4, %10
  %13 = zext i1 %12 to i8
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %13, i8* %14, align 1
  %15 = and i32 %11, 255
  %16 = tail call i32 @llvm.ctpop.i32(i32 %15)
  %17 = trunc i32 %16 to i8
  %18 = and i8 %17, 1
  %19 = xor i8 %18, 1
  %20 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %19, i8* %20, align 1
  %21 = xor i32 %10, %4
  %22 = xor i32 %21, %11
  %23 = lshr i32 %22, 4
  %24 = trunc i32 %23 to i8
  %25 = and i8 %24, 1
  %26 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %25, i8* %26, align 1
  %27 = icmp eq i32 %11, 0
  %28 = zext i1 %27 to i8
  %29 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %28, i8* %29, align 1
  %30 = lshr i32 %11, 31
  %31 = trunc i32 %30 to i8
  %32 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %31, i8* %32, align 1
  %33 = lshr i32 %4, 31
  %34 = lshr i32 %10, 31
  %35 = xor i32 %34, %33
  %36 = xor i32 %30, %33
  %37 = add nuw nsw i32 %36, %35
  %38 = icmp eq i32 %37, 2
  %39 = zext i1 %38 to i8
  %40 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %39, i8* %40, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40d9a4(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xbbe___edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3006, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jne_.L_40e897(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = zext i1 %6 to i8
  store i8 %7, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %8 = add i64 %.v, %3
  store i64 %8, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq_MINUS0x44__rbp____rdx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -68
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 4
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x6a8__rcx__rdx_1____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RCX, align 8
  %4 = load i64, i64* %RDX, align 8
  %5 = add i64 %3, 1704
  %6 = add i64 %5, %4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %6 to i8*
  %10 = load i8, i8* %9, align 1
  %11 = zext i8 %10 to i64
  store i64 %11, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rax__MINUS0xd0__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -208
  %5 = load i64, i64* %RAX, align 8
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 7
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %4 to i64*
  store i64 %5, i64* %8, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__rax__MINUS0xd8__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -216
  %5 = load i64, i64* %RAX, align 8
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 7
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %4 to i64*
  store i64 %5, i64* %8, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x1c__rbp____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -28
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addl__0x0___esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RSI, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 3
  store i64 %5, i64* %PC, align 8
  %6 = trunc i64 %3 to i32
  %7 = and i64 %3, 4294967295
  store i64 %7, i64* %RSI, align 8
  %8 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 0, i8* %8, align 1
  %9 = and i32 %6, 255
  %10 = tail call i32 @llvm.ctpop.i32(i32 %9)
  %11 = trunc i32 %10 to i8
  %12 = and i8 %11, 1
  %13 = xor i8 %12, 1
  %14 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %13, i8* %14, align 1
  %15 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 0, i8* %15, align 1
  %16 = icmp eq i32 %6, 0
  %17 = zext i1 %16 to i8
  %18 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %17, i8* %18, align 1
  %19 = lshr i32 %6, 31
  %20 = trunc i32 %19 to i8
  %21 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %20, i8* %21, align 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 0, i8* %22, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq__esi___rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0
  %ESI = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i32, i32* %ESI, align 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = sext i32 %4 to i64
  store i64 %7, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw___rax__rcx_2____di(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0
  %DI = bitcast %union.anon* %3 to i16*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = load i64, i64* %RCX, align 8
  %6 = shl i64 %5, 1
  %7 = add i64 %6, %4
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 4
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  %11 = load i16, i16* %10, align 2
  store i16 %11, i16* %DI, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__di__MINUS0xc6__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0
  %DI = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -198
  %6 = load i16, i16* %DI, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 7
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0xd0__rbp____rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -208
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl_MINUS0xc6__rbp____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -198
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i16*
  %8 = load i16, i16* %7, align 2
  %9 = zext i16 %8 to i64
  store i64 %9, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl___rax__rcx_1____esi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %RAX, align 8
  %4 = load i64, i64* %RCX, align 8
  %5 = add i64 %4, %3
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 4
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i8*
  %9 = load i8, i8* %8, align 1
  %10 = zext i8 %9 to i64
  store i64 %10, i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq_MINUS0xd8__rbp____rax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -216
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl_MINUS0xc6__rbp____r8d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -198
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 8
  store i64 %7, i64* %PC, align 8
  %8 = inttoptr i64 %5 to i16*
  %9 = load i16, i16* %8, align 2
  %10 = zext i16 %9 to i64
  store i64 %10, i64* %3, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__r8d___ecx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0
  %R8D = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i32, i32* %R8D, align 4
  %5 = zext i32 %4 to i64
  %6 = load i64, i64* %PC, align 8
  %7 = add i64 %6, 3
  store i64 %7, i64* %PC, align 8
  store i64 %5, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movslq__edx___rcx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0
  %EDX = bitcast %union.anon* %3 to i32*
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i32, i32* %EDX, align 4
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = sext i32 %4 to i64
  store i64 %7, i64* %RCX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw___rax__rcx_2____r9w(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 19, i32 0
  %R9W = bitcast %union.anon* %3 to i16*
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %4 = load i64, i64* %RAX, align 8
  %5 = load i64, i64* %RCX, align 8
  %6 = shl i64 %5, 1
  %7 = add i64 %6, %4
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 5
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  %11 = load i16, i16* %10, align 2
  store i16 %11, i16* %R9W, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movw__r9w__MINUS0xc6__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 19, i32 0
  %R9W = bitcast %union.anon* %3 to i16*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -198
  %6 = load i16, i16* %R9W, align 2
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 8
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %9, align 2
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl_MINUS0xc6__rbp____edx(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -198
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 7
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i16*
  %8 = load i16, i16* %7, align 2
  %9 = zext i16 %8 to i64
  store i64 %9, i64* %RDX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40e938(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jg_.L_40e933(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  %6 = icmp eq i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %11 = load i8, i8* %10, align 1
  %12 = icmp ne i8 %11, 0
  %13 = xor i1 %9, %12
  %14 = xor i1 %13, true
  %15 = and i1 %6, %14
  %16 = zext i1 %15 to i8
  store i8 %16, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %15, i64 %rel_off1, i64 %rel_off2
  %17 = add i64 %.v, %3
  store i64 %17, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzbl_0x6a8__rcx__rdx_1____r8d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %4 = load i64, i64* %RCX, align 8
  %5 = load i64, i64* %RDX, align 8
  %6 = add i64 %4, 1704
  %7 = add i64 %6, %5
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 9
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i8*
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i64
  store i64 %12, i64* %3, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movzwl___rcx__rdx_2____r8d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RCX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 5, i32 0, i32 0
  %RDX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 7, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 17, i32 0, i32 0
  %4 = load i64, i64* %RCX, align 8
  %5 = load i64, i64* %RDX, align 8
  %6 = shl i64 %5, 1
  %7 = add i64 %6, %4
  %8 = load i64, i64* %PC, align 8
  %9 = add i64 %8, 5
  store i64 %9, i64* %PC, align 8
  %10 = inttoptr i64 %7 to i16*
  %11 = load i16, i16* %10, align 2
  %12 = zext i16 %11 to i64
  store i64 %12, i64* %3, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40e89d(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl_MINUS0x44__rbp____eax(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RAX = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %RBP, align 8
  %4 = add i64 %3, -68
  %5 = load i64, i64* %PC, align 8
  %6 = add i64 %5, 3
  store i64 %6, i64* %PC, align 8
  %7 = inttoptr i64 %4 to i32*
  %8 = load i32, i32* %7, align 4
  %9 = zext i32 %8 to i64
  store i64 %9, i64* %RAX, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x44__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -68
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 3
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jmpq_.L_40d93a(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i64 %rel_off1, i64 %rel_off2) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, %rel_off1
  store i64 %4, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_je_.L_40e965(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  %5 = load i8, i8* %4, align 1
  store i8 %5, i8* %BRANCH_TAKEN, align 1
  %6 = icmp ne i8 %5, 0
  %.v = select i1 %6, i64 %rel_off1, i64 %rel_off2
  %7 = add i64 %.v, %3
  store i64 %7, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__0xbbf___edi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RDI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 11, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 5
  store i64 %4, i64* %PC, align 8
  store i64 3007, i64* %RDI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_jl_.L_40e9a1(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned, i8* nocapture %BRANCH_TAKEN, i64 %rel_off1, i64 %rel_off2, i64 %rel_off3) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  %5 = load i8, i8* %4, align 1
  %6 = icmp ne i8 %5, 0
  %7 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  %8 = load i8, i8* %7, align 1
  %9 = icmp ne i8 %8, 0
  %10 = xor i1 %6, %9
  %11 = zext i1 %10 to i8
  store i8 %11, i8* %BRANCH_TAKEN, align 1
  %.v = select i1 %10, i64 %rel_off1, i64 %rel_off2
  %12 = add i64 %.v, %3
  store i64 %12, i64* %PC, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movq__0x4165fc___rsi(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSI = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 9, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 10
  store i64 %4, i64* %PC, align 8
  store i64 ptrtoint (%G__0x4165fc_type* @G__0x4165fc to i64), i64* %RSI, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_movl__eax__MINUS0x110__rbp_(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 1, i32 0
  %EAX = bitcast %union.anon* %3 to i32*
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %4 = load i64, i64* %RBP, align 8
  %5 = add i64 %4, -272
  %6 = load i32, i32* %EAX, align 4
  %7 = load i64, i64* %PC, align 8
  %8 = add i64 %7, 6
  store i64 %8, i64* %PC, align 8
  %9 = inttoptr i64 %5 to i32*
  store i32 %6, i32* %9, align 4
  ret %struct.Memory* %2
}

; Function Attrs: nounwind
define %struct.Memory* @routine_addq__0x110___rsp(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #3 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RSP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %3 = load i64, i64* %RSP, align 8
  %4 = load i64, i64* %PC, align 8
  %5 = add i64 %4, 7
  store i64 %5, i64* %PC, align 8
  %6 = add i64 %3, 272
  store i64 %6, i64* %RSP, align 8
  %7 = icmp ugt i64 %3, -273
  %8 = zext i1 %7 to i8
  %9 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 1
  store i8 %8, i8* %9, align 1
  %10 = trunc i64 %6 to i32
  %11 = and i32 %10, 255
  %12 = tail call i32 @llvm.ctpop.i32(i32 %11)
  %13 = trunc i32 %12 to i8
  %14 = and i8 %13, 1
  %15 = xor i8 %14, 1
  %16 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 3
  store i8 %15, i8* %16, align 1
  %17 = xor i64 %3, 16
  %18 = xor i64 %17, %6
  %19 = lshr i64 %18, 4
  %20 = trunc i64 %19 to i8
  %21 = and i8 %20, 1
  %22 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 5
  store i8 %21, i8* %22, align 1
  %23 = icmp eq i64 %6, 0
  %24 = zext i1 %23 to i8
  %25 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 7
  store i8 %24, i8* %25, align 1
  %26 = lshr i64 %6, 63
  %27 = trunc i64 %26 to i8
  %28 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 9
  store i8 %27, i8* %28, align 1
  %29 = lshr i64 %3, 63
  %30 = xor i64 %26, %29
  %31 = add nuw nsw i64 %30, %26
  %32 = icmp eq i64 %31, 2
  %33 = zext i1 %32 to i8
  %34 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 2, i32 13
  store i8 %33, i8* %34, align 1
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_popq__rbp(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %RBP = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 15, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 1
  store i64 %4, i64* %PC, align 8
  %5 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %6 = load i64, i64* %5, align 8
  %7 = add i64 %6, 8
  %8 = inttoptr i64 %6 to i64*
  %9 = load i64, i64* %8, align 8
  store i64 %9, i64* %RBP, align 8
  store i64 %7, i64* %5, align 8
  ret %struct.Memory* %2
}

; Function Attrs: norecurse nounwind
define %struct.Memory* @routine_retq(%struct.State* nocapture dereferenceable(3376), i64, %struct.Memory* readnone returned) local_unnamed_addr #2 {
block_400488:
  %PC = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 33, i32 0, i32 0
  %3 = load i64, i64* %PC, align 8
  %4 = add i64 %3, 1
  store i64 %4, i64* %PC, align 8
  %5 = getelementptr inbounds %struct.State, %struct.State* %0, i64 0, i32 6, i32 13, i32 0, i32 0
  %6 = load i64, i64* %5, align 8
  %7 = inttoptr i64 %6 to i64*
  %8 = load i64, i64* %7, align 8
  store i64 %8, i64* %PC, align 8
  %9 = add i64 %6, 8
  store i64 %9, i64* %5, align 8
  ret %struct.Memory* %2
}

attributes #0 = { nounwind readnone }
attributes #1 = { alwaysinline }
attributes #2 = { norecurse nounwind }
attributes #3 = { nounwind }
